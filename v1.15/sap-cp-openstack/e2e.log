Conformance test: not doing test setup.
I0814 06:25:52.509735    3092 e2e.go:241] Starting e2e run "a80b271f-2ac5-4304-af3e-63f31fe4a4be" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1565763951 - Will randomize all specs
Will run 212 of 4413 specs

Aug 14 06:26:24.436: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 14 06:26:24.441: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 14 06:26:24.463: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 14 06:26:24.500: INFO: 15 / 15 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 14 06:26:24.500: INFO: expected 9 pod replicas in namespace 'kube-system', 9 are Running and Ready.
Aug 14 06:26:24.500: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 14 06:26:24.510: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Aug 14 06:26:24.510: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Aug 14 06:26:24.510: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Aug 14 06:26:24.510: INFO: e2e test version: v1.15.2
Aug 14 06:26:24.512: INFO: kube-apiserver version: v1.15.2
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:26:24.512: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
Aug 14 06:26:24.567: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Aug 14 06:26:24.581: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4101
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 06:26:24.700: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2251da5f-bd68-411b-9f5b-fcdc2a1774d3" in namespace "projected-4101" to be "success or failure"
Aug 14 06:26:24.704: INFO: Pod "downwardapi-volume-2251da5f-bd68-411b-9f5b-fcdc2a1774d3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.783137ms
Aug 14 06:26:26.708: INFO: Pod "downwardapi-volume-2251da5f-bd68-411b-9f5b-fcdc2a1774d3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008206382s
Aug 14 06:26:28.712: INFO: Pod "downwardapi-volume-2251da5f-bd68-411b-9f5b-fcdc2a1774d3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012218774s
STEP: Saw pod success
Aug 14 06:26:28.712: INFO: Pod "downwardapi-volume-2251da5f-bd68-411b-9f5b-fcdc2a1774d3" satisfied condition "success or failure"
Aug 14 06:26:28.715: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod downwardapi-volume-2251da5f-bd68-411b-9f5b-fcdc2a1774d3 container client-container: <nil>
STEP: delete the pod
Aug 14 06:26:28.856: INFO: Waiting for pod downwardapi-volume-2251da5f-bd68-411b-9f5b-fcdc2a1774d3 to disappear
Aug 14 06:26:28.859: INFO: Pod downwardapi-volume-2251da5f-bd68-411b-9f5b-fcdc2a1774d3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:26:28.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4101" for this suite.
Aug 14 06:26:34.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:26:34.978: INFO: namespace projected-4101 deletion completed in 6.114790234s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:26:34.979: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2002
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Aug 14 06:26:35.138: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2002,SelfLink:/api/v1/namespaces/watch-2002/configmaps/e2e-watch-test-label-changed,UID:414e61cf-91c2-4c10-9513-c773feb0a917,ResourceVersion:3502,Generation:0,CreationTimestamp:2019-08-14 06:26:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 14 06:26:35.138: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2002,SelfLink:/api/v1/namespaces/watch-2002/configmaps/e2e-watch-test-label-changed,UID:414e61cf-91c2-4c10-9513-c773feb0a917,ResourceVersion:3503,Generation:0,CreationTimestamp:2019-08-14 06:26:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 14 06:26:35.139: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2002,SelfLink:/api/v1/namespaces/watch-2002/configmaps/e2e-watch-test-label-changed,UID:414e61cf-91c2-4c10-9513-c773feb0a917,ResourceVersion:3504,Generation:0,CreationTimestamp:2019-08-14 06:26:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Aug 14 06:26:45.165: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2002,SelfLink:/api/v1/namespaces/watch-2002/configmaps/e2e-watch-test-label-changed,UID:414e61cf-91c2-4c10-9513-c773feb0a917,ResourceVersion:3529,Generation:0,CreationTimestamp:2019-08-14 06:26:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 14 06:26:45.166: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2002,SelfLink:/api/v1/namespaces/watch-2002/configmaps/e2e-watch-test-label-changed,UID:414e61cf-91c2-4c10-9513-c773feb0a917,ResourceVersion:3530,Generation:0,CreationTimestamp:2019-08-14 06:26:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Aug 14 06:26:45.166: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2002,SelfLink:/api/v1/namespaces/watch-2002/configmaps/e2e-watch-test-label-changed,UID:414e61cf-91c2-4c10-9513-c773feb0a917,ResourceVersion:3531,Generation:0,CreationTimestamp:2019-08-14 06:26:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:26:45.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2002" for this suite.
Aug 14 06:26:51.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:26:51.318: INFO: namespace watch-2002 deletion completed in 6.146612332s
â€¢SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:26:51.318: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9726
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 14 06:26:51.466: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-9726'
Aug 14 06:26:51.576: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 14 06:26:51.576: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Aug 14 06:26:51.591: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-qk7bj]
Aug 14 06:26:51.591: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-qk7bj" in namespace "kubectl-9726" to be "running and ready"
Aug 14 06:26:51.594: INFO: Pod "e2e-test-nginx-rc-qk7bj": Phase="Pending", Reason="", readiness=false. Elapsed: 3.257926ms
Aug 14 06:26:53.598: INFO: Pod "e2e-test-nginx-rc-qk7bj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006698779s
Aug 14 06:26:55.601: INFO: Pod "e2e-test-nginx-rc-qk7bj": Phase="Running", Reason="", readiness=true. Elapsed: 4.009981526s
Aug 14 06:26:55.601: INFO: Pod "e2e-test-nginx-rc-qk7bj" satisfied condition "running and ready"
Aug 14 06:26:55.601: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-qk7bj]
Aug 14 06:26:55.601: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs rc/e2e-test-nginx-rc --namespace=kubectl-9726'
Aug 14 06:26:55.860: INFO: stderr: ""
Aug 14 06:26:55.860: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Aug 14 06:26:55.860: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete rc e2e-test-nginx-rc --namespace=kubectl-9726'
Aug 14 06:26:55.940: INFO: stderr: ""
Aug 14 06:26:55.940: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:26:55.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9726" for this suite.
Aug 14 06:27:01.957: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:27:02.066: INFO: namespace kubectl-9726 deletion completed in 6.120145672s
â€¢SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:27:02.067: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6936
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 14 06:27:02.215: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-6936'
Aug 14 06:27:02.306: INFO: stderr: ""
Aug 14 06:27:02.306: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Aug 14 06:27:07.356: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod e2e-test-nginx-pod --namespace=kubectl-6936 -o json'
Aug 14 06:27:07.426: INFO: stderr: ""
Aug 14 06:27:07.426: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.96.1.4/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-08-14T06:27:02Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-6936\",\n        \"resourceVersion\": \"3612\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6936/pods/e2e-test-nginx-pod\",\n        \"uid\": \"0d5016af-0005-42f8-aec4-ff1d44e89ec8\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-6sjtn\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-6sjtn\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-6sjtn\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-14T06:27:02Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-14T06:27:03Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-14T06:27:03Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-08-14T06:27:02Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://c584c74fa3850dccdc77eb9027cd606d3ccee0ab385964b562b3123d4b9cf0a2\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-08-14T06:27:03Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.0.8\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.96.1.4\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-08-14T06:27:02Z\"\n    }\n}\n"
STEP: replace the image in the pod
Aug 14 06:27:07.426: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config replace -f - --namespace=kubectl-6936'
Aug 14 06:27:07.672: INFO: stderr: ""
Aug 14 06:27:07.672: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Aug 14 06:27:07.676: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pods e2e-test-nginx-pod --namespace=kubectl-6936'
Aug 14 06:27:11.485: INFO: stderr: ""
Aug 14 06:27:11.485: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:27:11.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6936" for this suite.
Aug 14 06:27:17.498: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:27:17.603: INFO: namespace kubectl-6936 deletion completed in 6.11535272s
â€¢SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:27:17.604: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-8814
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-8814
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8814 to expose endpoints map[]
Aug 14 06:27:17.785: INFO: Get endpoints failed (3.444691ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Aug 14 06:27:18.789: INFO: successfully validated that service endpoint-test2 in namespace services-8814 exposes endpoints map[] (1.007267507s elapsed)
STEP: Creating pod pod1 in namespace services-8814
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8814 to expose endpoints map[pod1:[80]]
Aug 14 06:27:20.816: INFO: successfully validated that service endpoint-test2 in namespace services-8814 exposes endpoints map[pod1:[80]] (2.021108086s elapsed)
STEP: Creating pod pod2 in namespace services-8814
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8814 to expose endpoints map[pod1:[80] pod2:[80]]
Aug 14 06:27:22.851: INFO: successfully validated that service endpoint-test2 in namespace services-8814 exposes endpoints map[pod1:[80] pod2:[80]] (2.03071594s elapsed)
STEP: Deleting pod pod1 in namespace services-8814
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8814 to expose endpoints map[pod2:[80]]
Aug 14 06:27:22.865: INFO: successfully validated that service endpoint-test2 in namespace services-8814 exposes endpoints map[pod2:[80]] (6.576764ms elapsed)
STEP: Deleting pod pod2 in namespace services-8814
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8814 to expose endpoints map[]
Aug 14 06:27:22.874: INFO: successfully validated that service endpoint-test2 in namespace services-8814 exposes endpoints map[] (3.27697ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:27:22.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8814" for this suite.
Aug 14 06:27:44.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:27:45.055: INFO: namespace services-8814 deletion completed in 22.162690158s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92
â€¢S
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:27:45.055: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-9511
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 06:27:49.234: INFO: Waiting up to 5m0s for pod "client-envvars-e57ac886-8866-42eb-87f8-1aee27878486" in namespace "pods-9511" to be "success or failure"
Aug 14 06:27:49.243: INFO: Pod "client-envvars-e57ac886-8866-42eb-87f8-1aee27878486": Phase="Pending", Reason="", readiness=false. Elapsed: 8.919726ms
Aug 14 06:27:51.247: INFO: Pod "client-envvars-e57ac886-8866-42eb-87f8-1aee27878486": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013214177s
STEP: Saw pod success
Aug 14 06:27:51.247: INFO: Pod "client-envvars-e57ac886-8866-42eb-87f8-1aee27878486" satisfied condition "success or failure"
Aug 14 06:27:51.250: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod client-envvars-e57ac886-8866-42eb-87f8-1aee27878486 container env3cont: <nil>
STEP: delete the pod
Aug 14 06:27:51.275: INFO: Waiting for pod client-envvars-e57ac886-8866-42eb-87f8-1aee27878486 to disappear
Aug 14 06:27:51.281: INFO: Pod client-envvars-e57ac886-8866-42eb-87f8-1aee27878486 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:27:51.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9511" for this suite.
Aug 14 06:28:37.295: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:28:37.396: INFO: namespace pods-9511 deletion completed in 46.111475812s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:28:37.397: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6510
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 06:28:37.693: INFO: Waiting up to 5m0s for pod "downwardapi-volume-38c7a286-2758-4a7a-b0a1-d90ec106a027" in namespace "projected-6510" to be "success or failure"
Aug 14 06:28:37.697: INFO: Pod "downwardapi-volume-38c7a286-2758-4a7a-b0a1-d90ec106a027": Phase="Pending", Reason="", readiness=false. Elapsed: 3.318304ms
Aug 14 06:28:39.701: INFO: Pod "downwardapi-volume-38c7a286-2758-4a7a-b0a1-d90ec106a027": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007292278s
STEP: Saw pod success
Aug 14 06:28:39.701: INFO: Pod "downwardapi-volume-38c7a286-2758-4a7a-b0a1-d90ec106a027" satisfied condition "success or failure"
Aug 14 06:28:39.705: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod downwardapi-volume-38c7a286-2758-4a7a-b0a1-d90ec106a027 container client-container: <nil>
STEP: delete the pod
Aug 14 06:28:39.728: INFO: Waiting for pod downwardapi-volume-38c7a286-2758-4a7a-b0a1-d90ec106a027 to disappear
Aug 14 06:28:39.731: INFO: Pod downwardapi-volume-38c7a286-2758-4a7a-b0a1-d90ec106a027 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:28:39.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6510" for this suite.
Aug 14 06:28:45.745: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:28:45.844: INFO: namespace projected-6510 deletion completed in 6.10950055s
â€¢SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:28:45.845: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8317
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-22be821b-79c6-4db1-b8fd-25b704e11393
STEP: Creating a pod to test consume configMaps
Aug 14 06:28:46.004: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-aabbf34a-ec4f-4f91-b951-4080330c8e9e" in namespace "projected-8317" to be "success or failure"
Aug 14 06:28:46.007: INFO: Pod "pod-projected-configmaps-aabbf34a-ec4f-4f91-b951-4080330c8e9e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.783529ms
Aug 14 06:28:48.011: INFO: Pod "pod-projected-configmaps-aabbf34a-ec4f-4f91-b951-4080330c8e9e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006854419s
STEP: Saw pod success
Aug 14 06:28:48.011: INFO: Pod "pod-projected-configmaps-aabbf34a-ec4f-4f91-b951-4080330c8e9e" satisfied condition "success or failure"
Aug 14 06:28:48.014: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-projected-configmaps-aabbf34a-ec4f-4f91-b951-4080330c8e9e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 06:28:48.031: INFO: Waiting for pod pod-projected-configmaps-aabbf34a-ec4f-4f91-b951-4080330c8e9e to disappear
Aug 14 06:28:48.034: INFO: Pod pod-projected-configmaps-aabbf34a-ec4f-4f91-b951-4080330c8e9e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:28:48.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8317" for this suite.
Aug 14 06:28:54.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:28:54.151: INFO: namespace projected-8317 deletion completed in 6.113060267s
â€¢SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:28:54.151: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6913
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-27d82b92-6347-4d40-9548-96118659b822
STEP: Creating a pod to test consume secrets
Aug 14 06:28:54.307: INFO: Waiting up to 5m0s for pod "pod-secrets-dec01379-c069-47cb-a7d8-c870a8dfbe00" in namespace "secrets-6913" to be "success or failure"
Aug 14 06:28:54.310: INFO: Pod "pod-secrets-dec01379-c069-47cb-a7d8-c870a8dfbe00": Phase="Pending", Reason="", readiness=false. Elapsed: 2.607278ms
Aug 14 06:28:56.315: INFO: Pod "pod-secrets-dec01379-c069-47cb-a7d8-c870a8dfbe00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008063078s
STEP: Saw pod success
Aug 14 06:28:56.315: INFO: Pod "pod-secrets-dec01379-c069-47cb-a7d8-c870a8dfbe00" satisfied condition "success or failure"
Aug 14 06:28:56.318: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-secrets-dec01379-c069-47cb-a7d8-c870a8dfbe00 container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 06:28:56.336: INFO: Waiting for pod pod-secrets-dec01379-c069-47cb-a7d8-c870a8dfbe00 to disappear
Aug 14 06:28:56.338: INFO: Pod pod-secrets-dec01379-c069-47cb-a7d8-c870a8dfbe00 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:28:56.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6913" for this suite.
Aug 14 06:29:02.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:29:02.503: INFO: namespace secrets-6913 deletion completed in 6.161710677s
â€¢SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:29:02.504: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9571
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 06:29:02.653: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cef5b9e4-dde6-43f8-a164-6ee7b06eb80b" in namespace "projected-9571" to be "success or failure"
Aug 14 06:29:02.656: INFO: Pod "downwardapi-volume-cef5b9e4-dde6-43f8-a164-6ee7b06eb80b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.617568ms
Aug 14 06:29:04.661: INFO: Pod "downwardapi-volume-cef5b9e4-dde6-43f8-a164-6ee7b06eb80b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007854426s
STEP: Saw pod success
Aug 14 06:29:04.661: INFO: Pod "downwardapi-volume-cef5b9e4-dde6-43f8-a164-6ee7b06eb80b" satisfied condition "success or failure"
Aug 14 06:29:04.663: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod downwardapi-volume-cef5b9e4-dde6-43f8-a164-6ee7b06eb80b container client-container: <nil>
STEP: delete the pod
Aug 14 06:29:04.680: INFO: Waiting for pod downwardapi-volume-cef5b9e4-dde6-43f8-a164-6ee7b06eb80b to disappear
Aug 14 06:29:04.683: INFO: Pod downwardapi-volume-cef5b9e4-dde6-43f8-a164-6ee7b06eb80b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:29:04.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9571" for this suite.
Aug 14 06:29:10.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:29:10.806: INFO: namespace projected-9571 deletion completed in 6.11898875s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:29:10.807: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2454
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 06:29:10.957: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config version'
Aug 14 06:29:11.036: INFO: stderr: ""
Aug 14 06:29:11.036: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.2\", GitCommit:\"f6278300bebbb750328ac16ee6dd3aa7d3549568\", GitTreeState:\"clean\", BuildDate:\"2019-08-05T09:23:26Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.2\", GitCommit:\"f6278300bebbb750328ac16ee6dd3aa7d3549568\", GitTreeState:\"clean\", BuildDate:\"2019-08-05T09:15:22Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:29:11.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2454" for this suite.
Aug 14 06:29:17.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:29:17.155: INFO: namespace kubectl-2454 deletion completed in 6.114592357s
â€¢SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:29:17.155: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4030
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 06:29:17.382: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39308321-d89f-4974-9344-3de0c1f6655c" in namespace "projected-4030" to be "success or failure"
Aug 14 06:29:17.385: INFO: Pod "downwardapi-volume-39308321-d89f-4974-9344-3de0c1f6655c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.540004ms
Aug 14 06:29:19.389: INFO: Pod "downwardapi-volume-39308321-d89f-4974-9344-3de0c1f6655c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007351006s
STEP: Saw pod success
Aug 14 06:29:19.390: INFO: Pod "downwardapi-volume-39308321-d89f-4974-9344-3de0c1f6655c" satisfied condition "success or failure"
Aug 14 06:29:19.394: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod downwardapi-volume-39308321-d89f-4974-9344-3de0c1f6655c container client-container: <nil>
STEP: delete the pod
Aug 14 06:29:19.417: INFO: Waiting for pod downwardapi-volume-39308321-d89f-4974-9344-3de0c1f6655c to disappear
Aug 14 06:29:19.419: INFO: Pod downwardapi-volume-39308321-d89f-4974-9344-3de0c1f6655c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:29:19.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4030" for this suite.
Aug 14 06:29:25.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:29:25.538: INFO: namespace projected-4030 deletion completed in 6.115138203s
â€¢SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:29:25.538: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7307
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-78d2bb4c-ec8d-4729-abfd-bfb60427c4d7
STEP: Creating secret with name secret-projected-all-test-volume-f76ac34e-4195-482d-9418-42a32e26e436
STEP: Creating a pod to test Check all projections for projected volume plugin
Aug 14 06:29:25.694: INFO: Waiting up to 5m0s for pod "projected-volume-d0bc55d3-21b4-4fe4-92e4-2d97485063ce" in namespace "projected-7307" to be "success or failure"
Aug 14 06:29:25.697: INFO: Pod "projected-volume-d0bc55d3-21b4-4fe4-92e4-2d97485063ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.773128ms
Aug 14 06:29:27.702: INFO: Pod "projected-volume-d0bc55d3-21b4-4fe4-92e4-2d97485063ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007378386s
STEP: Saw pod success
Aug 14 06:29:27.702: INFO: Pod "projected-volume-d0bc55d3-21b4-4fe4-92e4-2d97485063ce" satisfied condition "success or failure"
Aug 14 06:29:27.705: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod projected-volume-d0bc55d3-21b4-4fe4-92e4-2d97485063ce container projected-all-volume-test: <nil>
STEP: delete the pod
Aug 14 06:29:27.724: INFO: Waiting for pod projected-volume-d0bc55d3-21b4-4fe4-92e4-2d97485063ce to disappear
Aug 14 06:29:27.727: INFO: Pod projected-volume-d0bc55d3-21b4-4fe4-92e4-2d97485063ce no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:29:27.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7307" for this suite.
Aug 14 06:29:33.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:29:33.851: INFO: namespace projected-7307 deletion completed in 6.119709951s
â€¢SSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:29:33.851: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-6590
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:29:39.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6590" for this suite.
Aug 14 06:29:45.710: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:29:45.807: INFO: namespace watch-6590 deletion completed in 6.200116132s
â€¢SS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:29:45.808: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7459
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Aug 14 06:29:45.979: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7459,SelfLink:/api/v1/namespaces/watch-7459/configmaps/e2e-watch-test-resource-version,UID:d997fbea-6817-47f7-a0a1-cbf52254a316,ResourceVersion:4388,Generation:0,CreationTimestamp:2019-08-14 06:29:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 14 06:29:45.980: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7459,SelfLink:/api/v1/namespaces/watch-7459/configmaps/e2e-watch-test-resource-version,UID:d997fbea-6817-47f7-a0a1-cbf52254a316,ResourceVersion:4389,Generation:0,CreationTimestamp:2019-08-14 06:29:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:29:45.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7459" for this suite.
Aug 14 06:29:52.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:29:52.101: INFO: namespace watch-7459 deletion completed in 6.117192097s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:29:52.103: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-255
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 06:29:52.258: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c54fcf44-7adb-48b5-bbfc-43f2b9dc26ad" in namespace "downward-api-255" to be "success or failure"
Aug 14 06:29:52.261: INFO: Pod "downwardapi-volume-c54fcf44-7adb-48b5-bbfc-43f2b9dc26ad": Phase="Pending", Reason="", readiness=false. Elapsed: 2.691098ms
Aug 14 06:29:54.266: INFO: Pod "downwardapi-volume-c54fcf44-7adb-48b5-bbfc-43f2b9dc26ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007745543s
STEP: Saw pod success
Aug 14 06:29:54.266: INFO: Pod "downwardapi-volume-c54fcf44-7adb-48b5-bbfc-43f2b9dc26ad" satisfied condition "success or failure"
Aug 14 06:29:54.269: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod downwardapi-volume-c54fcf44-7adb-48b5-bbfc-43f2b9dc26ad container client-container: <nil>
STEP: delete the pod
Aug 14 06:29:54.288: INFO: Waiting for pod downwardapi-volume-c54fcf44-7adb-48b5-bbfc-43f2b9dc26ad to disappear
Aug 14 06:29:54.291: INFO: Pod downwardapi-volume-c54fcf44-7adb-48b5-bbfc-43f2b9dc26ad no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:29:54.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-255" for this suite.
Aug 14 06:30:00.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:30:00.416: INFO: namespace downward-api-255 deletion completed in 6.121978151s
â€¢SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:30:00.417: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6921
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-6921
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Aug 14 06:30:00.578: INFO: Found 0 stateful pods, waiting for 3
Aug 14 06:30:10.582: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 06:30:10.582: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 06:30:10.582: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 06:30:10.591: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6921 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 14 06:30:11.083: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 14 06:30:11.083: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 14 06:30:11.083: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Aug 14 06:30:21.119: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Aug 14 06:30:31.137: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6921 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:30:31.699: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 14 06:30:31.699: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 14 06:30:31.699: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 14 06:30:41.720: INFO: Waiting for StatefulSet statefulset-6921/ss2 to complete update
Aug 14 06:30:41.720: INFO: Waiting for Pod statefulset-6921/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 14 06:30:41.720: INFO: Waiting for Pod statefulset-6921/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 14 06:30:51.727: INFO: Waiting for StatefulSet statefulset-6921/ss2 to complete update
Aug 14 06:30:51.727: INFO: Waiting for Pod statefulset-6921/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 14 06:30:51.727: INFO: Waiting for Pod statefulset-6921/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 14 06:31:01.728: INFO: Waiting for StatefulSet statefulset-6921/ss2 to complete update
Aug 14 06:31:01.728: INFO: Waiting for Pod statefulset-6921/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Aug 14 06:31:11.731: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6921 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 14 06:31:12.264: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 14 06:31:12.264: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 14 06:31:12.264: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 14 06:31:22.302: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Aug 14 06:31:32.331: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6921 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:31:32.818: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 14 06:31:32.818: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 14 06:31:32.818: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 14 06:31:42.841: INFO: Waiting for StatefulSet statefulset-6921/ss2 to complete update
Aug 14 06:31:42.841: INFO: Waiting for Pod statefulset-6921/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 14 06:31:42.841: INFO: Waiting for Pod statefulset-6921/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Aug 14 06:31:52.849: INFO: Waiting for StatefulSet statefulset-6921/ss2 to complete update
Aug 14 06:31:52.849: INFO: Waiting for Pod statefulset-6921/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 14 06:32:02.849: INFO: Deleting all statefulset in ns statefulset-6921
Aug 14 06:32:02.852: INFO: Scaling statefulset ss2 to 0
Aug 14 06:32:12.868: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 06:32:12.871: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:32:12.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6921" for this suite.
Aug 14 06:32:18.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:32:19.001: INFO: namespace statefulset-6921 deletion completed in 6.113385977s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:32:19.002: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8666
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-1e8d0462-93d3-483d-8f80-8a3defbf24ee
STEP: Creating a pod to test consume secrets
Aug 14 06:32:19.164: INFO: Waiting up to 5m0s for pod "pod-secrets-33c5a9d6-ca13-4666-9049-c47ff73e3139" in namespace "secrets-8666" to be "success or failure"
Aug 14 06:32:19.167: INFO: Pod "pod-secrets-33c5a9d6-ca13-4666-9049-c47ff73e3139": Phase="Pending", Reason="", readiness=false. Elapsed: 2.591356ms
Aug 14 06:32:21.173: INFO: Pod "pod-secrets-33c5a9d6-ca13-4666-9049-c47ff73e3139": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008934286s
STEP: Saw pod success
Aug 14 06:32:21.173: INFO: Pod "pod-secrets-33c5a9d6-ca13-4666-9049-c47ff73e3139" satisfied condition "success or failure"
Aug 14 06:32:21.176: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-secrets-33c5a9d6-ca13-4666-9049-c47ff73e3139 container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 06:32:21.213: INFO: Waiting for pod pod-secrets-33c5a9d6-ca13-4666-9049-c47ff73e3139 to disappear
Aug 14 06:32:21.215: INFO: Pod pod-secrets-33c5a9d6-ca13-4666-9049-c47ff73e3139 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:32:21.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8666" for this suite.
Aug 14 06:32:27.229: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:32:27.348: INFO: namespace secrets-8666 deletion completed in 6.129108319s
â€¢SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:32:27.348: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9656
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-9656
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9656 to expose endpoints map[]
Aug 14 06:32:27.515: INFO: successfully validated that service multi-endpoint-test in namespace services-9656 exposes endpoints map[] (3.666224ms elapsed)
STEP: Creating pod pod1 in namespace services-9656
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9656 to expose endpoints map[pod1:[100]]
Aug 14 06:32:29.545: INFO: successfully validated that service multi-endpoint-test in namespace services-9656 exposes endpoints map[pod1:[100]] (2.022591896s elapsed)
STEP: Creating pod pod2 in namespace services-9656
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9656 to expose endpoints map[pod1:[100] pod2:[101]]
Aug 14 06:32:31.578: INFO: successfully validated that service multi-endpoint-test in namespace services-9656 exposes endpoints map[pod1:[100] pod2:[101]] (2.02923482s elapsed)
STEP: Deleting pod pod1 in namespace services-9656
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9656 to expose endpoints map[pod2:[101]]
Aug 14 06:32:32.597: INFO: successfully validated that service multi-endpoint-test in namespace services-9656 exposes endpoints map[pod2:[101]] (1.013005773s elapsed)
STEP: Deleting pod pod2 in namespace services-9656
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9656 to expose endpoints map[]
Aug 14 06:32:33.608: INFO: successfully validated that service multi-endpoint-test in namespace services-9656 exposes endpoints map[] (1.006376792s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:32:33.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9656" for this suite.
Aug 14 06:32:39.639: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:32:39.788: INFO: namespace services-9656 deletion completed in 6.160789123s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92
â€¢SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:32:39.789: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-63
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-63
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Aug 14 06:32:39.950: INFO: Found 0 stateful pods, waiting for 3
Aug 14 06:32:49.955: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 06:32:49.955: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 06:32:49.955: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Aug 14 06:32:49.984: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Aug 14 06:33:00.019: INFO: Updating stateful set ss2
Aug 14 06:33:00.026: INFO: Waiting for Pod statefulset-63/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 14 06:33:10.036: INFO: Waiting for Pod statefulset-63/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Aug 14 06:33:20.053: INFO: Found 2 stateful pods, waiting for 3
Aug 14 06:33:30.058: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 06:33:30.058: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 06:33:30.058: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Aug 14 06:33:30.082: INFO: Updating stateful set ss2
Aug 14 06:33:30.088: INFO: Waiting for Pod statefulset-63/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 14 06:33:40.116: INFO: Updating stateful set ss2
Aug 14 06:33:40.126: INFO: Waiting for StatefulSet statefulset-63/ss2 to complete update
Aug 14 06:33:40.126: INFO: Waiting for Pod statefulset-63/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Aug 14 06:33:50.133: INFO: Waiting for StatefulSet statefulset-63/ss2 to complete update
Aug 14 06:33:50.133: INFO: Waiting for Pod statefulset-63/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 14 06:34:00.133: INFO: Deleting all statefulset in ns statefulset-63
Aug 14 06:34:00.136: INFO: Scaling statefulset ss2 to 0
Aug 14 06:34:30.152: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 06:34:30.155: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:34:30.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-63" for this suite.
Aug 14 06:34:36.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:34:36.340: INFO: namespace statefulset-63 deletion completed in 6.161127709s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:34:36.341: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4200
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 06:34:36.509: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Aug 14 06:34:36.520: INFO: Number of nodes with available pods: 0
Aug 14 06:34:36.520: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 is running more than one daemon pod
Aug 14 06:34:37.529: INFO: Number of nodes with available pods: 0
Aug 14 06:34:37.529: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 is running more than one daemon pod
Aug 14 06:34:38.528: INFO: Number of nodes with available pods: 2
Aug 14 06:34:38.528: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Aug 14 06:34:38.560: INFO: Wrong image for pod: daemon-set-4jbxn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:38.561: INFO: Wrong image for pod: daemon-set-5bwjq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:39.570: INFO: Wrong image for pod: daemon-set-4jbxn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:39.570: INFO: Wrong image for pod: daemon-set-5bwjq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:40.570: INFO: Wrong image for pod: daemon-set-4jbxn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:40.570: INFO: Wrong image for pod: daemon-set-5bwjq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:41.570: INFO: Wrong image for pod: daemon-set-4jbxn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:41.570: INFO: Pod daemon-set-4jbxn is not available
Aug 14 06:34:41.570: INFO: Wrong image for pod: daemon-set-5bwjq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:42.570: INFO: Wrong image for pod: daemon-set-4jbxn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:42.570: INFO: Pod daemon-set-4jbxn is not available
Aug 14 06:34:42.570: INFO: Wrong image for pod: daemon-set-5bwjq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:43.571: INFO: Wrong image for pod: daemon-set-4jbxn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:43.571: INFO: Pod daemon-set-4jbxn is not available
Aug 14 06:34:43.571: INFO: Wrong image for pod: daemon-set-5bwjq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:44.573: INFO: Wrong image for pod: daemon-set-4jbxn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:44.573: INFO: Pod daemon-set-4jbxn is not available
Aug 14 06:34:44.573: INFO: Wrong image for pod: daemon-set-5bwjq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:45.571: INFO: Wrong image for pod: daemon-set-4jbxn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:45.571: INFO: Pod daemon-set-4jbxn is not available
Aug 14 06:34:45.571: INFO: Wrong image for pod: daemon-set-5bwjq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:46.571: INFO: Wrong image for pod: daemon-set-4jbxn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:46.572: INFO: Pod daemon-set-4jbxn is not available
Aug 14 06:34:46.572: INFO: Wrong image for pod: daemon-set-5bwjq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:47.572: INFO: Wrong image for pod: daemon-set-4jbxn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:47.572: INFO: Pod daemon-set-4jbxn is not available
Aug 14 06:34:47.572: INFO: Wrong image for pod: daemon-set-5bwjq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:48.570: INFO: Wrong image for pod: daemon-set-4jbxn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:48.570: INFO: Pod daemon-set-4jbxn is not available
Aug 14 06:34:48.570: INFO: Wrong image for pod: daemon-set-5bwjq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:49.570: INFO: Wrong image for pod: daemon-set-5bwjq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:49.570: INFO: Pod daemon-set-w2trf is not available
Aug 14 06:34:50.571: INFO: Wrong image for pod: daemon-set-5bwjq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:50.571: INFO: Pod daemon-set-w2trf is not available
Aug 14 06:34:51.570: INFO: Wrong image for pod: daemon-set-5bwjq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:51.570: INFO: Pod daemon-set-w2trf is not available
Aug 14 06:34:52.570: INFO: Wrong image for pod: daemon-set-5bwjq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:52.570: INFO: Pod daemon-set-5bwjq is not available
Aug 14 06:34:53.570: INFO: Wrong image for pod: daemon-set-5bwjq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:53.570: INFO: Pod daemon-set-5bwjq is not available
Aug 14 06:34:54.570: INFO: Wrong image for pod: daemon-set-5bwjq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:54.570: INFO: Pod daemon-set-5bwjq is not available
Aug 14 06:34:55.570: INFO: Wrong image for pod: daemon-set-5bwjq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:55.570: INFO: Pod daemon-set-5bwjq is not available
Aug 14 06:34:56.570: INFO: Wrong image for pod: daemon-set-5bwjq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:56.570: INFO: Pod daemon-set-5bwjq is not available
Aug 14 06:34:57.570: INFO: Wrong image for pod: daemon-set-5bwjq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:57.571: INFO: Pod daemon-set-5bwjq is not available
Aug 14 06:34:58.571: INFO: Wrong image for pod: daemon-set-5bwjq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:58.571: INFO: Pod daemon-set-5bwjq is not available
Aug 14 06:34:59.570: INFO: Wrong image for pod: daemon-set-5bwjq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:34:59.570: INFO: Pod daemon-set-5bwjq is not available
Aug 14 06:35:00.571: INFO: Wrong image for pod: daemon-set-5bwjq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Aug 14 06:35:00.571: INFO: Pod daemon-set-5bwjq is not available
Aug 14 06:35:01.573: INFO: Pod daemon-set-wftkk is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Aug 14 06:35:01.604: INFO: Number of nodes with available pods: 1
Aug 14 06:35:01.604: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 is running more than one daemon pod
Aug 14 06:35:02.612: INFO: Number of nodes with available pods: 1
Aug 14 06:35:02.612: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 is running more than one daemon pod
Aug 14 06:35:03.612: INFO: Number of nodes with available pods: 2
Aug 14 06:35:03.612: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4200, will wait for the garbage collector to delete the pods
Aug 14 06:35:03.693: INFO: Deleting DaemonSet.extensions daemon-set took: 6.482355ms
Aug 14 06:35:03.794: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.24999ms
Aug 14 06:35:08.897: INFO: Number of nodes with available pods: 0
Aug 14 06:35:08.897: INFO: Number of running nodes: 0, number of available pods: 0
Aug 14 06:35:08.900: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4200/daemonsets","resourceVersion":"5730"},"items":null}

Aug 14 06:35:08.903: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4200/pods","resourceVersion":"5730"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:35:08.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4200" for this suite.
Aug 14 06:35:14.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:35:15.037: INFO: namespace daemonsets-4200 deletion completed in 6.120586263s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:35:15.039: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5949
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Aug 14 06:35:15.201: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 14 06:35:20.206: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:35:21.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5949" for this suite.
Aug 14 06:35:27.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:35:27.374: INFO: namespace replication-controller-5949 deletion completed in 6.148252837s
â€¢SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:35:27.375: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-7395
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-10f38d9c-d696-4637-bdfe-a052d703ebbd
Aug 14 06:35:27.602: INFO: Pod name my-hostname-basic-10f38d9c-d696-4637-bdfe-a052d703ebbd: Found 0 pods out of 1
Aug 14 06:35:32.607: INFO: Pod name my-hostname-basic-10f38d9c-d696-4637-bdfe-a052d703ebbd: Found 1 pods out of 1
Aug 14 06:35:32.607: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-10f38d9c-d696-4637-bdfe-a052d703ebbd" are running
Aug 14 06:35:32.611: INFO: Pod "my-hostname-basic-10f38d9c-d696-4637-bdfe-a052d703ebbd-8ghmj" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-14 06:35:27 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-14 06:35:28 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-14 06:35:28 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-14 06:35:27 +0000 UTC Reason: Message:}])
Aug 14 06:35:32.611: INFO: Trying to dial the pod
Aug 14 06:35:37.703: INFO: Controller my-hostname-basic-10f38d9c-d696-4637-bdfe-a052d703ebbd: Got expected result from replica 1 [my-hostname-basic-10f38d9c-d696-4637-bdfe-a052d703ebbd-8ghmj]: "my-hostname-basic-10f38d9c-d696-4637-bdfe-a052d703ebbd-8ghmj", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:35:37.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7395" for this suite.
Aug 14 06:35:43.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:35:43.828: INFO: namespace replication-controller-7395 deletion completed in 6.118741483s
â€¢SSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:35:43.828: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4584
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-4584/configmap-test-f8ab09ba-33e2-468d-9ebf-1a0d7847601c
STEP: Creating a pod to test consume configMaps
Aug 14 06:35:43.989: INFO: Waiting up to 5m0s for pod "pod-configmaps-b2b344d7-7e42-436d-9d50-e04925b4fde6" in namespace "configmap-4584" to be "success or failure"
Aug 14 06:35:43.999: INFO: Pod "pod-configmaps-b2b344d7-7e42-436d-9d50-e04925b4fde6": Phase="Pending", Reason="", readiness=false. Elapsed: 10.548433ms
Aug 14 06:35:46.005: INFO: Pod "pod-configmaps-b2b344d7-7e42-436d-9d50-e04925b4fde6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016322844s
STEP: Saw pod success
Aug 14 06:35:46.005: INFO: Pod "pod-configmaps-b2b344d7-7e42-436d-9d50-e04925b4fde6" satisfied condition "success or failure"
Aug 14 06:35:46.009: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-configmaps-b2b344d7-7e42-436d-9d50-e04925b4fde6 container env-test: <nil>
STEP: delete the pod
Aug 14 06:35:46.030: INFO: Waiting for pod pod-configmaps-b2b344d7-7e42-436d-9d50-e04925b4fde6 to disappear
Aug 14 06:35:46.034: INFO: Pod pod-configmaps-b2b344d7-7e42-436d-9d50-e04925b4fde6 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:35:46.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4584" for this suite.
Aug 14 06:35:52.057: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:35:52.290: INFO: namespace configmap-4584 deletion completed in 6.251466613s
â€¢SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:35:52.291: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-896
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 06:35:52.445: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ee6c836b-aeb6-4185-a1a5-b8f12de3b986" in namespace "downward-api-896" to be "success or failure"
Aug 14 06:35:52.448: INFO: Pod "downwardapi-volume-ee6c836b-aeb6-4185-a1a5-b8f12de3b986": Phase="Pending", Reason="", readiness=false. Elapsed: 3.417716ms
Aug 14 06:35:54.454: INFO: Pod "downwardapi-volume-ee6c836b-aeb6-4185-a1a5-b8f12de3b986": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009523513s
STEP: Saw pod success
Aug 14 06:35:54.454: INFO: Pod "downwardapi-volume-ee6c836b-aeb6-4185-a1a5-b8f12de3b986" satisfied condition "success or failure"
Aug 14 06:35:54.459: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod downwardapi-volume-ee6c836b-aeb6-4185-a1a5-b8f12de3b986 container client-container: <nil>
STEP: delete the pod
Aug 14 06:35:54.481: INFO: Waiting for pod downwardapi-volume-ee6c836b-aeb6-4185-a1a5-b8f12de3b986 to disappear
Aug 14 06:35:54.485: INFO: Pod downwardapi-volume-ee6c836b-aeb6-4185-a1a5-b8f12de3b986 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:35:54.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-896" for this suite.
Aug 14 06:36:00.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:36:00.617: INFO: namespace downward-api-896 deletion completed in 6.126161252s
â€¢SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:36:00.617: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7061
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 06:36:00.771: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 14 06:36:05.778: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 14 06:36:05.778: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 14 06:36:07.783: INFO: Creating deployment "test-rollover-deployment"
Aug 14 06:36:07.792: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 14 06:36:09.799: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 14 06:36:09.806: INFO: Ensure that both replica sets have 1 created replica
Aug 14 06:36:09.811: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 14 06:36:09.817: INFO: Updating deployment test-rollover-deployment
Aug 14 06:36:09.818: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 14 06:36:11.824: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 14 06:36:11.830: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 14 06:36:11.844: INFO: all replica sets need to contain the pod-template-hash label
Aug 14 06:36:11.844: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361367, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361367, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361371, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361367, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 06:36:13.852: INFO: all replica sets need to contain the pod-template-hash label
Aug 14 06:36:13.852: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361367, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361367, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361371, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361367, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 06:36:15.852: INFO: all replica sets need to contain the pod-template-hash label
Aug 14 06:36:15.852: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361367, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361367, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361371, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361367, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 06:36:17.852: INFO: all replica sets need to contain the pod-template-hash label
Aug 14 06:36:17.852: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361367, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361367, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361371, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361367, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 06:36:19.852: INFO: all replica sets need to contain the pod-template-hash label
Aug 14 06:36:19.852: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361367, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361367, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361371, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361367, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 06:36:21.851: INFO: 
Aug 14 06:36:21.851: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 14 06:36:21.860: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-7061,SelfLink:/apis/apps/v1/namespaces/deployment-7061/deployments/test-rollover-deployment,UID:3c51a523-0894-4d7d-8904-4a0f0852196f,ResourceVersion:6080,Generation:2,CreationTimestamp:2019-08-14 06:36:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-14 06:36:07 +0000 UTC 2019-08-14 06:36:07 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-14 06:36:21 +0000 UTC 2019-08-14 06:36:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 14 06:36:21.864: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-7061,SelfLink:/apis/apps/v1/namespaces/deployment-7061/replicasets/test-rollover-deployment-854595fc44,UID:192ebc20-28a3-4a8b-8b58-4c2d96a74fca,ResourceVersion:6073,Generation:2,CreationTimestamp:2019-08-14 06:36:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 3c51a523-0894-4d7d-8904-4a0f0852196f 0xc000ac8c57 0xc000ac8c58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 14 06:36:21.864: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 14 06:36:21.865: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-7061,SelfLink:/apis/apps/v1/namespaces/deployment-7061/replicasets/test-rollover-controller,UID:e5939940-e3db-4d2c-bf31-404468218092,ResourceVersion:6079,Generation:2,CreationTimestamp:2019-08-14 06:36:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 3c51a523-0894-4d7d-8904-4a0f0852196f 0xc000ac8b6f 0xc000ac8b80}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 14 06:36:21.865: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-7061,SelfLink:/apis/apps/v1/namespaces/deployment-7061/replicasets/test-rollover-deployment-9b8b997cf,UID:755df4b9-ece5-4eeb-9354-b718550172a6,ResourceVersion:6039,Generation:2,CreationTimestamp:2019-08-14 06:36:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 3c51a523-0894-4d7d-8904-4a0f0852196f 0xc000ac8d10 0xc000ac8d11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 14 06:36:21.870: INFO: Pod "test-rollover-deployment-854595fc44-pgtcq" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-pgtcq,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-7061,SelfLink:/api/v1/namespaces/deployment-7061/pods/test-rollover-deployment-854595fc44-pgtcq,UID:c03f0ced-f020-4843-8257-21cb1313531b,ResourceVersion:6048,Generation:0,CreationTimestamp:2019-08-14 06:36:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.38/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 192ebc20-28a3-4a8b-8b58-4c2d96a74fca 0xc000ac9957 0xc000ac9958}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-km98l {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-km98l,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-km98l true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000ac99c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000ac99e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:36:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:36:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:36:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:36:09 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:100.96.1.38,StartTime:2019-08-14 06:36:09 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-14 06:36:10 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://6f4d275fcfbbbe9999e0eb203c015b4ed3adf0c377f2c12b5474ff590aeb1cca}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:36:21.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7061" for this suite.
Aug 14 06:36:27.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:36:27.985: INFO: namespace deployment-7061 deletion completed in 6.110812847s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:36:27.987: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-1986
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:36:30.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1986" for this suite.
Aug 14 06:37:12.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:37:12.288: INFO: namespace kubelet-test-1986 deletion completed in 42.115983248s
â€¢SSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:37:12.288: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-8998
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Aug 14 06:37:12.435: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Aug 14 06:37:12.773: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Aug 14 06:37:14.807: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361432, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361432, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361432, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361432, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 06:37:16.811: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361432, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361432, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361432, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361432, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 06:37:18.811: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361432, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361432, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361432, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361432, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 06:37:20.811: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361432, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361432, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361432, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361432, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 06:37:22.811: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361432, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361432, loc:(*time.Location)(0x80bfa40)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361432, loc:(*time.Location)(0x80bfa40)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361432, loc:(*time.Location)(0x80bfa40)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 14 06:37:25.996: INFO: Waited 1.179286561s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:37:26.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-8998" for this suite.
Aug 14 06:37:32.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:37:32.941: INFO: namespace aggregator-8998 deletion completed in 6.115969388s
â€¢SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:37:32.942: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2937
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-84eca654-9687-4831-830e-03d0217695f7
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:37:35.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2937" for this suite.
Aug 14 06:37:57.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:37:57.325: INFO: namespace configmap-2937 deletion completed in 22.113317651s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:37:57.326: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8330
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Aug 14 06:38:00.024: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-8330 pod-service-account-0ee584f3-9e9b-4385-9117-cc7ef0351386 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Aug 14 06:38:00.664: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-8330 pod-service-account-0ee584f3-9e9b-4385-9117-cc7ef0351386 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Aug 14 06:38:01.181: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-8330 pod-service-account-0ee584f3-9e9b-4385-9117-cc7ef0351386 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:38:01.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8330" for this suite.
Aug 14 06:38:07.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:38:07.792: INFO: namespace svcaccounts-8330 deletion completed in 6.119089304s
â€¢SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:38:07.792: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4832
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 14 06:38:07.967: INFO: Number of nodes with available pods: 0
Aug 14 06:38:07.967: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 is running more than one daemon pod
Aug 14 06:38:08.975: INFO: Number of nodes with available pods: 0
Aug 14 06:38:08.975: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 is running more than one daemon pod
Aug 14 06:38:09.975: INFO: Number of nodes with available pods: 2
Aug 14 06:38:09.975: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Aug 14 06:38:09.995: INFO: Number of nodes with available pods: 1
Aug 14 06:38:09.995: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx is running more than one daemon pod
Aug 14 06:38:11.002: INFO: Number of nodes with available pods: 1
Aug 14 06:38:11.003: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx is running more than one daemon pod
Aug 14 06:38:12.003: INFO: Number of nodes with available pods: 1
Aug 14 06:38:12.003: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx is running more than one daemon pod
Aug 14 06:38:13.003: INFO: Number of nodes with available pods: 1
Aug 14 06:38:13.003: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx is running more than one daemon pod
Aug 14 06:38:14.002: INFO: Number of nodes with available pods: 1
Aug 14 06:38:14.002: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx is running more than one daemon pod
Aug 14 06:38:15.005: INFO: Number of nodes with available pods: 2
Aug 14 06:38:15.005: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4832, will wait for the garbage collector to delete the pods
Aug 14 06:38:15.070: INFO: Deleting DaemonSet.extensions daemon-set took: 6.31521ms
Aug 14 06:38:15.470: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.283985ms
Aug 14 06:38:28.874: INFO: Number of nodes with available pods: 0
Aug 14 06:38:28.875: INFO: Number of running nodes: 0, number of available pods: 0
Aug 14 06:38:28.877: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4832/daemonsets","resourceVersion":"6642"},"items":null}

Aug 14 06:38:28.880: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4832/pods","resourceVersion":"6642"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:38:28.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4832" for this suite.
Aug 14 06:38:34.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:38:35.058: INFO: namespace daemonsets-4832 deletion completed in 6.165545874s
â€¢SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:38:35.058: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5986
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-5986
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-5986
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5986
Aug 14 06:38:35.222: INFO: Found 0 stateful pods, waiting for 1
Aug 14 06:38:45.227: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Aug 14 06:38:45.231: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 14 06:38:45.696: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 14 06:38:45.696: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 14 06:38:45.696: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 14 06:38:45.700: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 14 06:38:55.705: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 06:38:55.705: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 06:38:55.718: INFO: POD   NODE                                                PHASE    GRACE  CONDITIONS
Aug 14 06:38:55.718: INFO: ss-0  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:35 +0000 UTC  }]
Aug 14 06:38:55.718: INFO: 
Aug 14 06:38:55.718: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 14 06:38:56.723: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996890706s
Aug 14 06:38:57.728: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.991665719s
Aug 14 06:38:58.732: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987266332s
Aug 14 06:38:59.736: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982774506s
Aug 14 06:39:00.741: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978414289s
Aug 14 06:39:01.746: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973754023s
Aug 14 06:39:02.751: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.968722398s
Aug 14 06:39:03.756: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.963665719s
Aug 14 06:39:04.761: INFO: Verifying statefulset ss doesn't scale past 3 for another 959.291846ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5986
Aug 14 06:39:05.766: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:39:06.263: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 14 06:39:06.263: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 14 06:39:06.263: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 14 06:39:06.263: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:39:06.733: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 14 06:39:06.733: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 14 06:39:06.733: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 14 06:39:06.733: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:39:07.204: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 14 06:39:07.204: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 14 06:39:07.204: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 14 06:39:07.209: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 06:39:07.209: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 06:39:07.209: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Aug 14 06:39:07.213: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 14 06:39:07.679: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 14 06:39:07.679: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 14 06:39:07.679: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 14 06:39:07.680: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 14 06:39:08.144: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 14 06:39:08.144: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 14 06:39:08.144: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 14 06:39:08.144: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 14 06:39:08.641: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 14 06:39:08.641: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 14 06:39:08.641: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 14 06:39:08.641: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 06:39:08.645: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Aug 14 06:39:18.656: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 06:39:18.656: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 06:39:18.656: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 06:39:18.668: INFO: POD   NODE                                                PHASE    GRACE  CONDITIONS
Aug 14 06:39:18.668: INFO: ss-0  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:35 +0000 UTC  }]
Aug 14 06:39:18.668: INFO: ss-1  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  }]
Aug 14 06:39:18.668: INFO: ss-2  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  }]
Aug 14 06:39:18.668: INFO: 
Aug 14 06:39:18.668: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 14 06:39:19.673: INFO: POD   NODE                                                PHASE    GRACE  CONDITIONS
Aug 14 06:39:19.673: INFO: ss-0  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:35 +0000 UTC  }]
Aug 14 06:39:19.673: INFO: ss-1  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  }]
Aug 14 06:39:19.673: INFO: ss-2  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  }]
Aug 14 06:39:19.673: INFO: 
Aug 14 06:39:19.673: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 14 06:39:20.679: INFO: POD   NODE                                                PHASE    GRACE  CONDITIONS
Aug 14 06:39:20.680: INFO: ss-0  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:35 +0000 UTC  }]
Aug 14 06:39:20.680: INFO: ss-1  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  }]
Aug 14 06:39:20.680: INFO: ss-2  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  }]
Aug 14 06:39:20.680: INFO: 
Aug 14 06:39:20.680: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 14 06:39:21.684: INFO: POD   NODE                                                PHASE    GRACE  CONDITIONS
Aug 14 06:39:21.684: INFO: ss-0  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:35 +0000 UTC  }]
Aug 14 06:39:21.684: INFO: ss-2  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  }]
Aug 14 06:39:21.684: INFO: 
Aug 14 06:39:21.684: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 14 06:39:22.690: INFO: POD   NODE                                                PHASE    GRACE  CONDITIONS
Aug 14 06:39:22.690: INFO: ss-0  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:35 +0000 UTC  }]
Aug 14 06:39:22.690: INFO: ss-2  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  }]
Aug 14 06:39:22.690: INFO: 
Aug 14 06:39:22.690: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 14 06:39:23.695: INFO: POD   NODE                                                PHASE    GRACE  CONDITIONS
Aug 14 06:39:23.695: INFO: ss-0  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:35 +0000 UTC  }]
Aug 14 06:39:23.695: INFO: ss-2  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  }]
Aug 14 06:39:23.695: INFO: 
Aug 14 06:39:23.695: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 14 06:39:24.699: INFO: POD   NODE                                                PHASE    GRACE  CONDITIONS
Aug 14 06:39:24.699: INFO: ss-0  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:35 +0000 UTC  }]
Aug 14 06:39:24.700: INFO: ss-2  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  }]
Aug 14 06:39:24.700: INFO: 
Aug 14 06:39:24.700: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 14 06:39:25.707: INFO: POD   NODE                                                PHASE    GRACE  CONDITIONS
Aug 14 06:39:25.707: INFO: ss-0  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:35 +0000 UTC  }]
Aug 14 06:39:25.707: INFO: ss-2  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  }]
Aug 14 06:39:25.707: INFO: 
Aug 14 06:39:25.707: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 14 06:39:26.712: INFO: POD   NODE                                                PHASE    GRACE  CONDITIONS
Aug 14 06:39:26.712: INFO: ss-0  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:35 +0000 UTC  }]
Aug 14 06:39:26.712: INFO: ss-2  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  }]
Aug 14 06:39:26.712: INFO: 
Aug 14 06:39:26.712: INFO: StatefulSet ss has not reached scale 0, at 2
Aug 14 06:39:27.717: INFO: POD   NODE                                                PHASE    GRACE  CONDITIONS
Aug 14 06:39:27.717: INFO: ss-0  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:35 +0000 UTC  }]
Aug 14 06:39:27.717: INFO: ss-2  shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:39:08 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 06:38:55 +0000 UTC  }]
Aug 14 06:39:27.717: INFO: 
Aug 14 06:39:27.717: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5986
Aug 14 06:39:28.724: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:39:28.956: INFO: rc: 1
Aug 14 06:39:28.956: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc002f22cf0 exit status 1 <nil> <nil> true [0xc001a69188 0xc001a691a0 0xc001a691b8] [0xc001a69188 0xc001a691a0 0xc001a691b8] [0xc001a69198 0xc001a691b0] [0x9d17b0 0x9d17b0] 0xc0030cea20 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Aug 14 06:39:38.956: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:39:39.040: INFO: rc: 1
Aug 14 06:39:39.041: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002afa1e0 exit status 1 <nil> <nil> true [0xc00312a828 0xc00312a840 0xc00312a858] [0xc00312a828 0xc00312a840 0xc00312a858] [0xc00312a838 0xc00312a850] [0x9d17b0 0x9d17b0] 0xc003160a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:39:49.041: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:39:49.115: INFO: rc: 1
Aug 14 06:39:49.115: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002df0570 exit status 1 <nil> <nil> true [0xc002772020 0xc0027720a0 0xc002772120] [0xc002772020 0xc0027720a0 0xc002772120] [0xc002772030 0xc0027720e0] [0x9d17b0 0x9d17b0] 0xc002b96240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:39:59.115: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:39:59.251: INFO: rc: 1
Aug 14 06:39:59.251: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002df0bd0 exit status 1 <nil> <nil> true [0xc002772138 0xc0027721a8 0xc002772220] [0xc002772138 0xc0027721a8 0xc002772220] [0xc002772198 0xc0027721f0] [0x9d17b0 0x9d17b0] 0xc002b966c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:40:09.251: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:40:09.323: INFO: rc: 1
Aug 14 06:40:09.323: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002df11a0 exit status 1 <nil> <nil> true [0xc002772230 0xc0027722b8 0xc002772328] [0xc002772230 0xc0027722b8 0xc002772328] [0xc002772270 0xc002772318] [0x9d17b0 0x9d17b0] 0xc002b96a20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:40:19.323: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:40:19.427: INFO: rc: 1
Aug 14 06:40:19.427: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002d80570 exit status 1 <nil> <nil> true [0xc0001cd600 0xc0001cd7f0 0xc0001cd840] [0xc0001cd600 0xc0001cd7f0 0xc0001cd840] [0xc0001cd7b8 0xc0001cd828] [0x9d17b0 0x9d17b0] 0xc001a95200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:40:29.427: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:40:29.505: INFO: rc: 1
Aug 14 06:40:29.505: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002df1770 exit status 1 <nil> <nil> true [0xc002772348 0xc0027723c0 0xc0027724a0] [0xc002772348 0xc0027723c0 0xc0027724a0] [0xc0027723a0 0xc002772440] [0x9d17b0 0x9d17b0] 0xc002b96d20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:40:39.505: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:40:39.619: INFO: rc: 1
Aug 14 06:40:39.619: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002bb8570 exit status 1 <nil> <nil> true [0xc000010468 0xc000010508 0xc0000105b0] [0xc000010468 0xc000010508 0xc0000105b0] [0xc0000104b8 0xc000010560] [0x9d17b0 0x9d17b0] 0xc002da0ae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:40:49.621: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:40:49.705: INFO: rc: 1
Aug 14 06:40:49.705: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002bb8b40 exit status 1 <nil> <nil> true [0xc0000105c8 0xc0000106a8 0xc000010738] [0xc0000105c8 0xc0000106a8 0xc000010738] [0xc000010660 0xc000010708] [0x9d17b0 0x9d17b0] 0xc002da0e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:40:59.706: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:40:59.805: INFO: rc: 1
Aug 14 06:40:59.805: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002df1d10 exit status 1 <nil> <nil> true [0xc0027724f8 0xc002772550 0xc0027725d8] [0xc0027724f8 0xc002772550 0xc0027725d8] [0xc002772540 0xc0027725a0] [0x9d17b0 0x9d17b0] 0xc002b97080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:41:09.805: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:41:09.921: INFO: rc: 1
Aug 14 06:41:09.921: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002d80b70 exit status 1 <nil> <nil> true [0xc0001cd8e0 0xc0001cd9a0 0xc0001cda00] [0xc0001cd8e0 0xc0001cd9a0 0xc0001cda00] [0xc0001cd958 0xc0001cd9f0] [0x9d17b0 0x9d17b0] 0xc001a95500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:41:19.921: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:41:20.011: INFO: rc: 1
Aug 14 06:41:20.011: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002bb9170 exit status 1 <nil> <nil> true [0xc000010788 0xc000010878 0xc000010a00] [0xc000010788 0xc000010878 0xc000010a00] [0xc000010860 0xc000010948] [0x9d17b0 0x9d17b0] 0xc002da11a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:41:30.011: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:41:30.089: INFO: rc: 1
Aug 14 06:41:30.089: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002bb9710 exit status 1 <nil> <nil> true [0xc000010a08 0xc000010e28 0xc000010ec0] [0xc000010a08 0xc000010e28 0xc000010ec0] [0xc000010e08 0xc000010e68] [0x9d17b0 0x9d17b0] 0xc002da1500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:41:40.089: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:41:40.207: INFO: rc: 1
Aug 14 06:41:40.207: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002ac8330 exit status 1 <nil> <nil> true [0xc002772630 0xc0027726d0 0xc002772720] [0xc002772630 0xc0027726d0 0xc002772720] [0xc002772680 0xc002772718] [0x9d17b0 0x9d17b0] 0xc002b97380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:41:50.207: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:41:50.289: INFO: rc: 1
Aug 14 06:41:50.289: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002d805a0 exit status 1 <nil> <nil> true [0xc0001cd770 0xc0001cd818 0xc0001cd8e0] [0xc0001cd770 0xc0001cd818 0xc0001cd8e0] [0xc0001cd7f0 0xc0001cd840] [0x9d17b0 0x9d17b0] 0xc001a95200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:42:00.289: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:42:00.395: INFO: rc: 1
Aug 14 06:42:00.395: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002d80b40 exit status 1 <nil> <nil> true [0xc0001cd940 0xc0001cd9b8 0xc0001cda08] [0xc0001cd940 0xc0001cd9b8 0xc0001cda08] [0xc0001cd9a0 0xc0001cda00] [0x9d17b0 0x9d17b0] 0xc001a95500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:42:10.396: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:42:10.546: INFO: rc: 1
Aug 14 06:42:10.547: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002bb85d0 exit status 1 <nil> <nil> true [0xc000010468 0xc000010508 0xc0000105b0] [0xc000010468 0xc000010508 0xc0000105b0] [0xc0000104b8 0xc000010560] [0x9d17b0 0x9d17b0] 0xc002da0ae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:42:20.547: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:42:20.667: INFO: rc: 1
Aug 14 06:42:20.667: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002df0600 exit status 1 <nil> <nil> true [0xc002772000 0xc002772030 0xc002772070] [0xc002772000 0xc002772030 0xc002772070] [0xc002772028 0xc002772060] [0x9d17b0 0x9d17b0] 0xc002b96240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:42:30.667: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:42:30.754: INFO: rc: 1
Aug 14 06:42:30.754: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002bb8bd0 exit status 1 <nil> <nil> true [0xc0000105c8 0xc0000106a8 0xc000010738] [0xc0000105c8 0xc0000106a8 0xc000010738] [0xc000010660 0xc000010708] [0x9d17b0 0x9d17b0] 0xc002da0e40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:42:40.755: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:42:40.833: INFO: rc: 1
Aug 14 06:42:40.833: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002bb91d0 exit status 1 <nil> <nil> true [0xc000010788 0xc000010878 0xc000010a00] [0xc000010788 0xc000010878 0xc000010a00] [0xc000010860 0xc000010948] [0x9d17b0 0x9d17b0] 0xc002da11a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:42:50.834: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:42:50.916: INFO: rc: 1
Aug 14 06:42:50.916: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002bb97d0 exit status 1 <nil> <nil> true [0xc000010a08 0xc000010e28 0xc000010ec0] [0xc000010a08 0xc000010e28 0xc000010ec0] [0xc000010e08 0xc000010e68] [0x9d17b0 0x9d17b0] 0xc002da1500 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:43:00.916: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:43:01.029: INFO: rc: 1
Aug 14 06:43:01.029: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002df0c30 exit status 1 <nil> <nil> true [0xc0027720a0 0xc002772120 0xc002772198] [0xc0027720a0 0xc002772120 0xc002772198] [0xc0027720e0 0xc002772168] [0x9d17b0 0x9d17b0] 0xc002b966c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:43:11.029: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:43:11.111: INFO: rc: 1
Aug 14 06:43:11.112: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc003194600 exit status 1 <nil> <nil> true [0xc0004fe080 0xc0009b2010 0xc0009b20e8] [0xc0004fe080 0xc0009b2010 0xc0009b20e8] [0xc0004fe0f8 0xc0009b20c0] [0x9d17b0 0x9d17b0] 0xc000bbc240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:43:21.112: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:43:21.192: INFO: rc: 1
Aug 14 06:43:21.192: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002bb9da0 exit status 1 <nil> <nil> true [0xc000010f28 0xc000010ff8 0xc000011050] [0xc000010f28 0xc000010ff8 0xc000011050] [0xc000010fa8 0xc000011048] [0x9d17b0 0x9d17b0] 0xc002da18c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:43:31.194: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:43:31.350: INFO: rc: 1
Aug 14 06:43:31.351: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc003194ba0 exit status 1 <nil> <nil> true [0xc0009b2208 0xc0009b2480 0xc0009b27e0] [0xc0009b2208 0xc0009b2480 0xc0009b27e0] [0xc0009b2390 0xc0009b26f0] [0x9d17b0 0x9d17b0] 0xc000bbc5a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:43:41.351: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:43:41.428: INFO: rc: 1
Aug 14 06:43:41.428: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc003195140 exit status 1 <nil> <nil> true [0xc0009b2810 0xc0009b29d0 0xc0009b2c38] [0xc0009b2810 0xc0009b29d0 0xc0009b2c38] [0xc0009b2908 0xc0009b2bd0] [0x9d17b0 0x9d17b0] 0xc000bbc8a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:43:51.429: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:43:51.508: INFO: rc: 1
Aug 14 06:43:51.508: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002df1200 exit status 1 <nil> <nil> true [0xc0027721c8 0xc002772230 0xc0027722b8] [0xc0027721c8 0xc002772230 0xc0027722b8] [0xc002772220 0xc002772270] [0x9d17b0 0x9d17b0] 0xc002b969c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:44:01.509: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:44:01.602: INFO: rc: 1
Aug 14 06:44:01.602: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002d805d0 exit status 1 <nil> <nil> true [0xc0004fe080 0xc0001cd600 0xc0001cd7f0] [0xc0004fe080 0xc0001cd600 0xc0001cd7f0] [0xc0004fe0f8 0xc0001cd7b8] [0x9d17b0 0x9d17b0] 0xc001a95200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:44:11.602: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:44:11.679: INFO: rc: 1
Aug 14 06:44:11.679: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0031945a0 exit status 1 <nil> <nil> true [0xc0009b2010 0xc0009b20e8 0xc0009b2390] [0xc0009b2010 0xc0009b20e8 0xc0009b2390] [0xc0009b20c0 0xc0009b22b0] [0x9d17b0 0x9d17b0] 0xc000bbc240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:44:21.680: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:44:21.762: INFO: rc: 1
Aug 14 06:44:21.762: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002df0570 exit status 1 <nil> <nil> true [0xc002772000 0xc002772030 0xc002772070] [0xc002772000 0xc002772030 0xc002772070] [0xc002772028 0xc002772060] [0x9d17b0 0x9d17b0] 0xc002b96240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Aug 14 06:44:31.762: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-5986 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 06:44:31.839: INFO: rc: 1
Aug 14 06:44:31.839: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Aug 14 06:44:31.839: INFO: Scaling statefulset ss to 0
Aug 14 06:44:31.865: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 14 06:44:31.868: INFO: Deleting all statefulset in ns statefulset-5986
Aug 14 06:44:31.871: INFO: Scaling statefulset ss to 0
Aug 14 06:44:31.880: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 06:44:31.882: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:44:31.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5986" for this suite.
Aug 14 06:44:37.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:44:38.017: INFO: namespace statefulset-5986 deletion completed in 6.119600059s

â€¢ [SLOW TEST:362.958 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:44:38.018: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3535
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Aug 14 06:44:38.165: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-3535'
Aug 14 06:44:38.415: INFO: stderr: ""
Aug 14 06:44:38.415: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 14 06:44:38.415: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3535'
Aug 14 06:44:38.508: INFO: stderr: ""
Aug 14 06:44:38.508: INFO: stdout: "update-demo-nautilus-7whz2 update-demo-nautilus-cxx8z "
Aug 14 06:44:38.508: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-7whz2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3535'
Aug 14 06:44:38.583: INFO: stderr: ""
Aug 14 06:44:38.583: INFO: stdout: ""
Aug 14 06:44:38.583: INFO: update-demo-nautilus-7whz2 is created but not running
Aug 14 06:44:43.584: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3535'
Aug 14 06:44:43.664: INFO: stderr: ""
Aug 14 06:44:43.664: INFO: stdout: "update-demo-nautilus-7whz2 update-demo-nautilus-cxx8z "
Aug 14 06:44:43.664: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-7whz2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3535'
Aug 14 06:44:43.747: INFO: stderr: ""
Aug 14 06:44:43.747: INFO: stdout: "true"
Aug 14 06:44:43.747: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-7whz2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3535'
Aug 14 06:44:43.819: INFO: stderr: ""
Aug 14 06:44:43.819: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 06:44:43.819: INFO: validating pod update-demo-nautilus-7whz2
Aug 14 06:44:43.904: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 06:44:43.904: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 06:44:43.904: INFO: update-demo-nautilus-7whz2 is verified up and running
Aug 14 06:44:43.904: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-cxx8z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3535'
Aug 14 06:44:43.985: INFO: stderr: ""
Aug 14 06:44:43.985: INFO: stdout: "true"
Aug 14 06:44:43.985: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-cxx8z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3535'
Aug 14 06:44:44.061: INFO: stderr: ""
Aug 14 06:44:44.061: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 06:44:44.061: INFO: validating pod update-demo-nautilus-cxx8z
Aug 14 06:44:44.144: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 06:44:44.144: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 06:44:44.144: INFO: update-demo-nautilus-cxx8z is verified up and running
STEP: using delete to clean up resources
Aug 14 06:44:44.144: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-3535'
Aug 14 06:44:44.220: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 06:44:44.220: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 14 06:44:44.220: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3535'
Aug 14 06:44:44.303: INFO: stderr: "No resources found.\n"
Aug 14 06:44:44.303: INFO: stdout: ""
Aug 14 06:44:44.303: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-3535 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 14 06:44:44.378: INFO: stderr: ""
Aug 14 06:44:44.378: INFO: stdout: "update-demo-nautilus-7whz2\nupdate-demo-nautilus-cxx8z\n"
Aug 14 06:44:44.878: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3535'
Aug 14 06:44:44.979: INFO: stderr: "No resources found.\n"
Aug 14 06:44:44.979: INFO: stdout: ""
Aug 14 06:44:44.979: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-3535 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 14 06:44:45.064: INFO: stderr: ""
Aug 14 06:44:45.064: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:44:45.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3535" for this suite.
Aug 14 06:45:07.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:45:07.188: INFO: namespace kubectl-3535 deletion completed in 22.120660129s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:45:07.190: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2669
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-3a170953-5dd6-4a97-89ff-51121b1fe806
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:45:07.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2669" for this suite.
Aug 14 06:45:13.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:45:13.511: INFO: namespace configmap-2669 deletion completed in 6.163666126s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:45:13.512: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9133
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-59da63dc-1f6d-4bc2-9972-72fe2b052815
STEP: Creating a pod to test consume configMaps
Aug 14 06:45:13.670: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1aa3642f-c275-4908-ac45-9ebeed0a5ef3" in namespace "projected-9133" to be "success or failure"
Aug 14 06:45:13.675: INFO: Pod "pod-projected-configmaps-1aa3642f-c275-4908-ac45-9ebeed0a5ef3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.572095ms
Aug 14 06:45:15.680: INFO: Pod "pod-projected-configmaps-1aa3642f-c275-4908-ac45-9ebeed0a5ef3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00998419s
STEP: Saw pod success
Aug 14 06:45:15.680: INFO: Pod "pod-projected-configmaps-1aa3642f-c275-4908-ac45-9ebeed0a5ef3" satisfied condition "success or failure"
Aug 14 06:45:15.684: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-projected-configmaps-1aa3642f-c275-4908-ac45-9ebeed0a5ef3 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 06:45:15.707: INFO: Waiting for pod pod-projected-configmaps-1aa3642f-c275-4908-ac45-9ebeed0a5ef3 to disappear
Aug 14 06:45:15.710: INFO: Pod pod-projected-configmaps-1aa3642f-c275-4908-ac45-9ebeed0a5ef3 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:45:15.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9133" for this suite.
Aug 14 06:45:21.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:45:21.835: INFO: namespace projected-9133 deletion completed in 6.118067896s
â€¢SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:45:21.836: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-646
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 14 06:45:28.034: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 14 06:45:28.037: INFO: Pod pod-with-poststart-http-hook still exists
Aug 14 06:45:30.037: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 14 06:45:30.041: INFO: Pod pod-with-poststart-http-hook still exists
Aug 14 06:45:32.037: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 14 06:45:32.040: INFO: Pod pod-with-poststart-http-hook still exists
Aug 14 06:45:34.037: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 14 06:45:34.042: INFO: Pod pod-with-poststart-http-hook still exists
Aug 14 06:45:36.037: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 14 06:45:36.042: INFO: Pod pod-with-poststart-http-hook still exists
Aug 14 06:45:38.037: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 14 06:45:38.041: INFO: Pod pod-with-poststart-http-hook still exists
Aug 14 06:45:40.037: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 14 06:45:40.042: INFO: Pod pod-with-poststart-http-hook still exists
Aug 14 06:45:42.037: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 14 06:45:42.041: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:45:42.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-646" for this suite.
Aug 14 06:46:04.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:46:04.168: INFO: namespace container-lifecycle-hook-646 deletion completed in 22.122988014s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:46:04.168: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6140
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 14 06:46:06.342: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:46:06.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6140" for this suite.
Aug 14 06:46:12.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:46:12.499: INFO: namespace container-runtime-6140 deletion completed in 6.137226621s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:46:12.501: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4233
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-b57ae5fa-c31d-4d1e-9362-ba864d807b6f
STEP: Creating a pod to test consume secrets
Aug 14 06:46:12.660: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9e467add-2b7f-4ac8-bdd9-87989f5184bd" in namespace "projected-4233" to be "success or failure"
Aug 14 06:46:12.663: INFO: Pod "pod-projected-secrets-9e467add-2b7f-4ac8-bdd9-87989f5184bd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.395739ms
Aug 14 06:46:14.668: INFO: Pod "pod-projected-secrets-9e467add-2b7f-4ac8-bdd9-87989f5184bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008614793s
STEP: Saw pod success
Aug 14 06:46:14.668: INFO: Pod "pod-projected-secrets-9e467add-2b7f-4ac8-bdd9-87989f5184bd" satisfied condition "success or failure"
Aug 14 06:46:14.672: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-projected-secrets-9e467add-2b7f-4ac8-bdd9-87989f5184bd container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 14 06:46:14.690: INFO: Waiting for pod pod-projected-secrets-9e467add-2b7f-4ac8-bdd9-87989f5184bd to disappear
Aug 14 06:46:14.692: INFO: Pod pod-projected-secrets-9e467add-2b7f-4ac8-bdd9-87989f5184bd no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:46:14.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4233" for this suite.
Aug 14 06:46:20.706: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:46:20.815: INFO: namespace projected-4233 deletion completed in 6.119214034s
â€¢SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:46:20.815: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1414
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 14 06:46:20.963: INFO: PodSpec: initContainers in spec.initContainers
Aug 14 06:47:02.127: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-c411d96e-e454-4b5d-85ba-b86d25ba5cda", GenerateName:"", Namespace:"init-container-1414", SelfLink:"/api/v1/namespaces/init-container-1414/pods/pod-init-c411d96e-e454-4b5d-85ba-b86d25ba5cda", UID:"bbfc9c27-2ab9-49b2-b4e9-c3032cfee028", ResourceVersion:"8288", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63701361980, loc:(*time.Location)(0x80bfa40)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"963644241"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.96.1.52/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-6nfgv", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002ba65c0), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-6nfgv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-6nfgv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-6nfgv", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc000c6b6b8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0024f9a40), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000c6b730)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc000c6b750)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc000c6b758), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc000c6b75c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361980, loc:(*time.Location)(0x80bfa40)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361980, loc:(*time.Location)(0x80bfa40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361980, loc:(*time.Location)(0x80bfa40)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63701361980, loc:(*time.Location)(0x80bfa40)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.8", PodIP:"100.96.1.52", StartTime:(*v1.Time)(0xc0031768e0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000740a80)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000740af0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://5c8159c10c4d848e7e7dce33cd1b774b4159c20a8df3eef76b65619563bf0896"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003176aa0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc003176960), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:47:02.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1414" for this suite.
Aug 14 06:47:24.147: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:47:24.253: INFO: namespace init-container-1414 deletion completed in 22.11897011s
â€¢SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:47:24.254: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-7618
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-qksk
STEP: Creating a pod to test atomic-volume-subpath
Aug 14 06:47:24.412: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-qksk" in namespace "subpath-7618" to be "success or failure"
Aug 14 06:47:24.414: INFO: Pod "pod-subpath-test-projected-qksk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.528013ms
Aug 14 06:47:26.421: INFO: Pod "pod-subpath-test-projected-qksk": Phase="Running", Reason="", readiness=true. Elapsed: 2.009554475s
Aug 14 06:47:28.426: INFO: Pod "pod-subpath-test-projected-qksk": Phase="Running", Reason="", readiness=true. Elapsed: 4.01410299s
Aug 14 06:47:30.430: INFO: Pod "pod-subpath-test-projected-qksk": Phase="Running", Reason="", readiness=true. Elapsed: 6.018435193s
Aug 14 06:47:32.436: INFO: Pod "pod-subpath-test-projected-qksk": Phase="Running", Reason="", readiness=true. Elapsed: 8.024102152s
Aug 14 06:47:34.440: INFO: Pod "pod-subpath-test-projected-qksk": Phase="Running", Reason="", readiness=true. Elapsed: 10.028510305s
Aug 14 06:47:36.444: INFO: Pod "pod-subpath-test-projected-qksk": Phase="Running", Reason="", readiness=true. Elapsed: 12.032860543s
Aug 14 06:47:38.449: INFO: Pod "pod-subpath-test-projected-qksk": Phase="Running", Reason="", readiness=true. Elapsed: 14.03749166s
Aug 14 06:47:40.455: INFO: Pod "pod-subpath-test-projected-qksk": Phase="Running", Reason="", readiness=true. Elapsed: 16.04300645s
Aug 14 06:47:42.459: INFO: Pod "pod-subpath-test-projected-qksk": Phase="Running", Reason="", readiness=true. Elapsed: 18.04771204s
Aug 14 06:47:44.464: INFO: Pod "pod-subpath-test-projected-qksk": Phase="Running", Reason="", readiness=true. Elapsed: 20.052489077s
Aug 14 06:47:46.469: INFO: Pod "pod-subpath-test-projected-qksk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.057605218s
STEP: Saw pod success
Aug 14 06:47:46.469: INFO: Pod "pod-subpath-test-projected-qksk" satisfied condition "success or failure"
Aug 14 06:47:46.472: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-subpath-test-projected-qksk container test-container-subpath-projected-qksk: <nil>
STEP: delete the pod
Aug 14 06:47:46.489: INFO: Waiting for pod pod-subpath-test-projected-qksk to disappear
Aug 14 06:47:46.495: INFO: Pod pod-subpath-test-projected-qksk no longer exists
STEP: Deleting pod pod-subpath-test-projected-qksk
Aug 14 06:47:46.495: INFO: Deleting pod "pod-subpath-test-projected-qksk" in namespace "subpath-7618"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:47:46.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7618" for this suite.
Aug 14 06:47:52.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:47:52.624: INFO: namespace subpath-7618 deletion completed in 6.122307804s
â€¢SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:47:52.625: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-385
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-vtpx
STEP: Creating a pod to test atomic-volume-subpath
Aug 14 06:47:52.786: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-vtpx" in namespace "subpath-385" to be "success or failure"
Aug 14 06:47:52.789: INFO: Pod "pod-subpath-test-configmap-vtpx": Phase="Pending", Reason="", readiness=false. Elapsed: 2.699465ms
Aug 14 06:47:54.794: INFO: Pod "pod-subpath-test-configmap-vtpx": Phase="Running", Reason="", readiness=true. Elapsed: 2.007709203s
Aug 14 06:47:56.799: INFO: Pod "pod-subpath-test-configmap-vtpx": Phase="Running", Reason="", readiness=true. Elapsed: 4.012690603s
Aug 14 06:47:58.804: INFO: Pod "pod-subpath-test-configmap-vtpx": Phase="Running", Reason="", readiness=true. Elapsed: 6.017170035s
Aug 14 06:48:00.808: INFO: Pod "pod-subpath-test-configmap-vtpx": Phase="Running", Reason="", readiness=true. Elapsed: 8.021707981s
Aug 14 06:48:02.813: INFO: Pod "pod-subpath-test-configmap-vtpx": Phase="Running", Reason="", readiness=true. Elapsed: 10.0262597s
Aug 14 06:48:04.819: INFO: Pod "pod-subpath-test-configmap-vtpx": Phase="Running", Reason="", readiness=true. Elapsed: 12.032929825s
Aug 14 06:48:06.824: INFO: Pod "pod-subpath-test-configmap-vtpx": Phase="Running", Reason="", readiness=true. Elapsed: 14.03715202s
Aug 14 06:48:08.829: INFO: Pod "pod-subpath-test-configmap-vtpx": Phase="Running", Reason="", readiness=true. Elapsed: 16.042363953s
Aug 14 06:48:10.833: INFO: Pod "pod-subpath-test-configmap-vtpx": Phase="Running", Reason="", readiness=true. Elapsed: 18.046970442s
Aug 14 06:48:12.838: INFO: Pod "pod-subpath-test-configmap-vtpx": Phase="Running", Reason="", readiness=true. Elapsed: 20.051136278s
Aug 14 06:48:14.842: INFO: Pod "pod-subpath-test-configmap-vtpx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.055780415s
STEP: Saw pod success
Aug 14 06:48:14.842: INFO: Pod "pod-subpath-test-configmap-vtpx" satisfied condition "success or failure"
Aug 14 06:48:14.845: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-subpath-test-configmap-vtpx container test-container-subpath-configmap-vtpx: <nil>
STEP: delete the pod
Aug 14 06:48:14.865: INFO: Waiting for pod pod-subpath-test-configmap-vtpx to disappear
Aug 14 06:48:14.867: INFO: Pod pod-subpath-test-configmap-vtpx no longer exists
STEP: Deleting pod pod-subpath-test-configmap-vtpx
Aug 14 06:48:14.867: INFO: Deleting pod "pod-subpath-test-configmap-vtpx" in namespace "subpath-385"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:48:14.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-385" for this suite.
Aug 14 06:48:20.889: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:48:20.995: INFO: namespace subpath-385 deletion completed in 6.11769473s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:48:20.997: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8118
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-3680397a-bf17-450e-8b10-544504ac41d7
STEP: Creating a pod to test consume configMaps
Aug 14 06:48:21.160: INFO: Waiting up to 5m0s for pod "pod-configmaps-94f7120e-d71c-4a8b-a3ef-25d789910d69" in namespace "configmap-8118" to be "success or failure"
Aug 14 06:48:21.167: INFO: Pod "pod-configmaps-94f7120e-d71c-4a8b-a3ef-25d789910d69": Phase="Pending", Reason="", readiness=false. Elapsed: 6.977406ms
Aug 14 06:48:23.172: INFO: Pod "pod-configmaps-94f7120e-d71c-4a8b-a3ef-25d789910d69": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011491094s
STEP: Saw pod success
Aug 14 06:48:23.172: INFO: Pod "pod-configmaps-94f7120e-d71c-4a8b-a3ef-25d789910d69" satisfied condition "success or failure"
Aug 14 06:48:23.175: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-configmaps-94f7120e-d71c-4a8b-a3ef-25d789910d69 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 06:48:23.197: INFO: Waiting for pod pod-configmaps-94f7120e-d71c-4a8b-a3ef-25d789910d69 to disappear
Aug 14 06:48:23.200: INFO: Pod pod-configmaps-94f7120e-d71c-4a8b-a3ef-25d789910d69 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:48:23.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8118" for this suite.
Aug 14 06:48:29.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:48:29.370: INFO: namespace configmap-8118 deletion completed in 6.166012031s
â€¢SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:48:29.370: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2356
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 14 06:48:29.518: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:48:31.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2356" for this suite.
Aug 14 06:48:37.844: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:48:37.949: INFO: namespace init-container-2356 deletion completed in 6.115600318s
â€¢SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:48:37.949: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1139
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Aug 14 06:48:38.096: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-1139'
Aug 14 06:48:38.478: INFO: stderr: ""
Aug 14 06:48:38.478: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 14 06:48:38.478: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1139'
Aug 14 06:48:38.555: INFO: stderr: ""
Aug 14 06:48:38.555: INFO: stdout: "update-demo-nautilus-5hsg9 update-demo-nautilus-6stgc "
Aug 14 06:48:38.555: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-5hsg9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1139'
Aug 14 06:48:38.626: INFO: stderr: ""
Aug 14 06:48:38.626: INFO: stdout: ""
Aug 14 06:48:38.626: INFO: update-demo-nautilus-5hsg9 is created but not running
Aug 14 06:48:43.627: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1139'
Aug 14 06:48:43.705: INFO: stderr: ""
Aug 14 06:48:43.705: INFO: stdout: "update-demo-nautilus-5hsg9 update-demo-nautilus-6stgc "
Aug 14 06:48:43.705: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-5hsg9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1139'
Aug 14 06:48:43.777: INFO: stderr: ""
Aug 14 06:48:43.777: INFO: stdout: "true"
Aug 14 06:48:43.778: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-5hsg9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1139'
Aug 14 06:48:43.849: INFO: stderr: ""
Aug 14 06:48:43.849: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 06:48:43.849: INFO: validating pod update-demo-nautilus-5hsg9
Aug 14 06:48:43.936: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 06:48:43.936: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 06:48:43.936: INFO: update-demo-nautilus-5hsg9 is verified up and running
Aug 14 06:48:43.936: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-6stgc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1139'
Aug 14 06:48:44.014: INFO: stderr: ""
Aug 14 06:48:44.014: INFO: stdout: "true"
Aug 14 06:48:44.014: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-6stgc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1139'
Aug 14 06:48:44.095: INFO: stderr: ""
Aug 14 06:48:44.095: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 06:48:44.095: INFO: validating pod update-demo-nautilus-6stgc
Aug 14 06:48:44.184: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 06:48:44.184: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 06:48:44.184: INFO: update-demo-nautilus-6stgc is verified up and running
STEP: scaling down the replication controller
Aug 14 06:48:44.186: INFO: scanned /root for discovery docs: <nil>
Aug 14 06:48:44.186: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-1139'
Aug 14 06:48:45.286: INFO: stderr: ""
Aug 14 06:48:45.286: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 14 06:48:45.286: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1139'
Aug 14 06:48:45.380: INFO: stderr: ""
Aug 14 06:48:45.380: INFO: stdout: "update-demo-nautilus-5hsg9 update-demo-nautilus-6stgc "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 14 06:48:50.381: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1139'
Aug 14 06:48:50.456: INFO: stderr: ""
Aug 14 06:48:50.456: INFO: stdout: "update-demo-nautilus-5hsg9 update-demo-nautilus-6stgc "
STEP: Replicas for name=update-demo: expected=1 actual=2
Aug 14 06:48:55.457: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1139'
Aug 14 06:48:55.537: INFO: stderr: ""
Aug 14 06:48:55.537: INFO: stdout: "update-demo-nautilus-6stgc "
Aug 14 06:48:55.537: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-6stgc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1139'
Aug 14 06:48:55.626: INFO: stderr: ""
Aug 14 06:48:55.626: INFO: stdout: "true"
Aug 14 06:48:55.626: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-6stgc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1139'
Aug 14 06:48:55.701: INFO: stderr: ""
Aug 14 06:48:55.701: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 06:48:55.701: INFO: validating pod update-demo-nautilus-6stgc
Aug 14 06:48:55.708: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 06:48:55.708: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 06:48:55.708: INFO: update-demo-nautilus-6stgc is verified up and running
STEP: scaling up the replication controller
Aug 14 06:48:55.710: INFO: scanned /root for discovery docs: <nil>
Aug 14 06:48:55.710: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-1139'
Aug 14 06:48:56.813: INFO: stderr: ""
Aug 14 06:48:56.813: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 14 06:48:56.813: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1139'
Aug 14 06:48:56.887: INFO: stderr: ""
Aug 14 06:48:56.887: INFO: stdout: "update-demo-nautilus-6stgc update-demo-nautilus-tbmmk "
Aug 14 06:48:56.887: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-6stgc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1139'
Aug 14 06:48:56.955: INFO: stderr: ""
Aug 14 06:48:56.955: INFO: stdout: "true"
Aug 14 06:48:56.955: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-6stgc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1139'
Aug 14 06:48:57.028: INFO: stderr: ""
Aug 14 06:48:57.028: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 06:48:57.028: INFO: validating pod update-demo-nautilus-6stgc
Aug 14 06:48:57.035: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 06:48:57.035: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 06:48:57.035: INFO: update-demo-nautilus-6stgc is verified up and running
Aug 14 06:48:57.035: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-tbmmk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1139'
Aug 14 06:48:57.109: INFO: stderr: ""
Aug 14 06:48:57.109: INFO: stdout: ""
Aug 14 06:48:57.109: INFO: update-demo-nautilus-tbmmk is created but not running
Aug 14 06:49:02.109: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1139'
Aug 14 06:49:02.192: INFO: stderr: ""
Aug 14 06:49:02.192: INFO: stdout: "update-demo-nautilus-6stgc update-demo-nautilus-tbmmk "
Aug 14 06:49:02.192: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-6stgc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1139'
Aug 14 06:49:02.269: INFO: stderr: ""
Aug 14 06:49:02.269: INFO: stdout: "true"
Aug 14 06:49:02.269: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-6stgc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1139'
Aug 14 06:49:02.346: INFO: stderr: ""
Aug 14 06:49:02.346: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 06:49:02.346: INFO: validating pod update-demo-nautilus-6stgc
Aug 14 06:49:02.353: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 06:49:02.353: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 06:49:02.353: INFO: update-demo-nautilus-6stgc is verified up and running
Aug 14 06:49:02.353: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-tbmmk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1139'
Aug 14 06:49:02.432: INFO: stderr: ""
Aug 14 06:49:02.432: INFO: stdout: "true"
Aug 14 06:49:02.432: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-tbmmk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1139'
Aug 14 06:49:02.516: INFO: stderr: ""
Aug 14 06:49:02.516: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 06:49:02.516: INFO: validating pod update-demo-nautilus-tbmmk
Aug 14 06:49:02.604: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 06:49:02.604: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 06:49:02.604: INFO: update-demo-nautilus-tbmmk is verified up and running
STEP: using delete to clean up resources
Aug 14 06:49:02.604: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-1139'
Aug 14 06:49:02.686: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 06:49:02.686: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 14 06:49:02.686: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1139'
Aug 14 06:49:02.779: INFO: stderr: "No resources found.\n"
Aug 14 06:49:02.779: INFO: stdout: ""
Aug 14 06:49:02.779: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-1139 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 14 06:49:02.852: INFO: stderr: ""
Aug 14 06:49:02.852: INFO: stdout: "update-demo-nautilus-6stgc\nupdate-demo-nautilus-tbmmk\n"
Aug 14 06:49:03.352: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1139'
Aug 14 06:49:03.475: INFO: stderr: "No resources found.\n"
Aug 14 06:49:03.475: INFO: stdout: ""
Aug 14 06:49:03.475: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-1139 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 14 06:49:03.557: INFO: stderr: ""
Aug 14 06:49:03.558: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:49:03.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1139" for this suite.
Aug 14 06:49:25.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:49:25.681: INFO: namespace kubectl-1139 deletion completed in 22.119122819s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:49:25.685: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2374
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 14 06:49:25.831: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:49:29.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2374" for this suite.
Aug 14 06:49:35.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:49:35.657: INFO: namespace init-container-2374 deletion completed in 6.160964377s
â€¢SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:49:35.657: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5499
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 06:49:35.807: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84e2b9bb-978d-4110-b8eb-8b87204bf61f" in namespace "projected-5499" to be "success or failure"
Aug 14 06:49:35.810: INFO: Pod "downwardapi-volume-84e2b9bb-978d-4110-b8eb-8b87204bf61f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.804963ms
Aug 14 06:49:37.814: INFO: Pod "downwardapi-volume-84e2b9bb-978d-4110-b8eb-8b87204bf61f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007142775s
STEP: Saw pod success
Aug 14 06:49:37.814: INFO: Pod "downwardapi-volume-84e2b9bb-978d-4110-b8eb-8b87204bf61f" satisfied condition "success or failure"
Aug 14 06:49:37.817: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod downwardapi-volume-84e2b9bb-978d-4110-b8eb-8b87204bf61f container client-container: <nil>
STEP: delete the pod
Aug 14 06:49:37.840: INFO: Waiting for pod downwardapi-volume-84e2b9bb-978d-4110-b8eb-8b87204bf61f to disappear
Aug 14 06:49:37.843: INFO: Pod downwardapi-volume-84e2b9bb-978d-4110-b8eb-8b87204bf61f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:49:37.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5499" for this suite.
Aug 14 06:49:43.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:49:43.963: INFO: namespace projected-5499 deletion completed in 6.11526993s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:49:43.964: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1064
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-22238bf5-ffa8-4d88-8966-da3637a3d184
STEP: Creating a pod to test consume secrets
Aug 14 06:49:44.122: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-413eb927-ae31-42c3-8469-df62527f8b04" in namespace "projected-1064" to be "success or failure"
Aug 14 06:49:44.125: INFO: Pod "pod-projected-secrets-413eb927-ae31-42c3-8469-df62527f8b04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.986555ms
Aug 14 06:49:46.129: INFO: Pod "pod-projected-secrets-413eb927-ae31-42c3-8469-df62527f8b04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007324164s
STEP: Saw pod success
Aug 14 06:49:46.129: INFO: Pod "pod-projected-secrets-413eb927-ae31-42c3-8469-df62527f8b04" satisfied condition "success or failure"
Aug 14 06:49:46.132: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-projected-secrets-413eb927-ae31-42c3-8469-df62527f8b04 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 14 06:49:46.179: INFO: Waiting for pod pod-projected-secrets-413eb927-ae31-42c3-8469-df62527f8b04 to disappear
Aug 14 06:49:46.186: INFO: Pod pod-projected-secrets-413eb927-ae31-42c3-8469-df62527f8b04 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:49:46.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1064" for this suite.
Aug 14 06:49:52.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:49:52.307: INFO: namespace projected-1064 deletion completed in 6.117002112s
â€¢SSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:49:52.307: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5706
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:49:52.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5706" for this suite.
Aug 14 06:49:58.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:49:58.582: INFO: namespace services-5706 deletion completed in 6.121615199s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:49:58.582: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-9376
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Aug 14 06:49:58.733: INFO: Waiting up to 5m0s for pod "client-containers-2500c94b-57da-4607-8cf2-afc4c34fbefb" in namespace "containers-9376" to be "success or failure"
Aug 14 06:49:58.736: INFO: Pod "client-containers-2500c94b-57da-4607-8cf2-afc4c34fbefb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.622634ms
Aug 14 06:50:00.739: INFO: Pod "client-containers-2500c94b-57da-4607-8cf2-afc4c34fbefb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006184159s
Aug 14 06:50:02.744: INFO: Pod "client-containers-2500c94b-57da-4607-8cf2-afc4c34fbefb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010496427s
STEP: Saw pod success
Aug 14 06:50:02.744: INFO: Pod "client-containers-2500c94b-57da-4607-8cf2-afc4c34fbefb" satisfied condition "success or failure"
Aug 14 06:50:02.746: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod client-containers-2500c94b-57da-4607-8cf2-afc4c34fbefb container test-container: <nil>
STEP: delete the pod
Aug 14 06:50:02.767: INFO: Waiting for pod client-containers-2500c94b-57da-4607-8cf2-afc4c34fbefb to disappear
Aug 14 06:50:02.769: INFO: Pod client-containers-2500c94b-57da-4607-8cf2-afc4c34fbefb no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:50:02.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9376" for this suite.
Aug 14 06:50:08.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:50:08.882: INFO: namespace containers-9376 deletion completed in 6.108673365s
â€¢SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:50:08.883: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8018
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Aug 14 06:50:09.026: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config api-versions'
Aug 14 06:50:09.141: INFO: stderr: ""
Aug 14 06:50:09.141: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:50:09.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8018" for this suite.
Aug 14 06:50:15.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:50:15.278: INFO: namespace kubectl-8018 deletion completed in 6.133196508s
â€¢SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:50:15.278: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9507
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-7539c508-e5f6-4699-a007-4506e8644a7e
STEP: Creating a pod to test consume configMaps
Aug 14 06:50:15.437: INFO: Waiting up to 5m0s for pod "pod-configmaps-ac7dedce-7e55-43ac-9801-807c36628c7c" in namespace "configmap-9507" to be "success or failure"
Aug 14 06:50:15.440: INFO: Pod "pod-configmaps-ac7dedce-7e55-43ac-9801-807c36628c7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.915596ms
Aug 14 06:50:17.444: INFO: Pod "pod-configmaps-ac7dedce-7e55-43ac-9801-807c36628c7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007325235s
STEP: Saw pod success
Aug 14 06:50:17.444: INFO: Pod "pod-configmaps-ac7dedce-7e55-43ac-9801-807c36628c7c" satisfied condition "success or failure"
Aug 14 06:50:17.447: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-configmaps-ac7dedce-7e55-43ac-9801-807c36628c7c container configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 06:50:17.472: INFO: Waiting for pod pod-configmaps-ac7dedce-7e55-43ac-9801-807c36628c7c to disappear
Aug 14 06:50:17.476: INFO: Pod pod-configmaps-ac7dedce-7e55-43ac-9801-807c36628c7c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:50:17.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9507" for this suite.
Aug 14 06:50:23.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:50:23.599: INFO: namespace configmap-9507 deletion completed in 6.119418609s
â€¢SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:50:23.599: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5295
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 14 06:50:23.742: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-5295'
Aug 14 06:50:28.819: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 14 06:50:28.819: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Aug 14 06:50:30.827: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete deployment e2e-test-nginx-deployment --namespace=kubectl-5295'
Aug 14 06:50:30.911: INFO: stderr: ""
Aug 14 06:50:30.911: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:50:30.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5295" for this suite.
Aug 14 06:50:52.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:50:53.039: INFO: namespace kubectl-5295 deletion completed in 22.12343406s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:50:53.043: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2432
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Aug 14 06:50:53.713: INFO: created pod pod-service-account-defaultsa
Aug 14 06:50:53.713: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 14 06:50:53.717: INFO: created pod pod-service-account-mountsa
Aug 14 06:50:53.718: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 14 06:50:53.721: INFO: created pod pod-service-account-nomountsa
Aug 14 06:50:53.721: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 14 06:50:53.725: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 14 06:50:53.725: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 14 06:50:53.729: INFO: created pod pod-service-account-mountsa-mountspec
Aug 14 06:50:53.729: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 14 06:50:53.735: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 14 06:50:53.735: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 14 06:50:53.739: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 14 06:50:53.739: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 14 06:50:53.745: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 14 06:50:53.745: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 14 06:50:53.749: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 14 06:50:53.749: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:50:53.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2432" for this suite.
Aug 14 06:50:59.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:50:59.875: INFO: namespace svcaccounts-2432 deletion completed in 6.121673011s
â€¢SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:50:59.876: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5871
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Aug 14 06:51:00.023: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-5871'
Aug 14 06:51:00.228: INFO: stderr: ""
Aug 14 06:51:00.228: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 14 06:51:00.228: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5871'
Aug 14 06:51:00.362: INFO: stderr: ""
Aug 14 06:51:00.362: INFO: stdout: "update-demo-nautilus-p99f4 update-demo-nautilus-wjhhh "
Aug 14 06:51:00.362: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-p99f4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5871'
Aug 14 06:51:00.438: INFO: stderr: ""
Aug 14 06:51:00.438: INFO: stdout: ""
Aug 14 06:51:00.438: INFO: update-demo-nautilus-p99f4 is created but not running
Aug 14 06:51:05.438: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5871'
Aug 14 06:51:05.522: INFO: stderr: ""
Aug 14 06:51:05.522: INFO: stdout: "update-demo-nautilus-p99f4 update-demo-nautilus-wjhhh "
Aug 14 06:51:05.522: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-p99f4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5871'
Aug 14 06:51:05.600: INFO: stderr: ""
Aug 14 06:51:05.600: INFO: stdout: "true"
Aug 14 06:51:05.600: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-p99f4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5871'
Aug 14 06:51:05.675: INFO: stderr: ""
Aug 14 06:51:05.675: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 06:51:05.675: INFO: validating pod update-demo-nautilus-p99f4
Aug 14 06:51:05.760: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 06:51:05.760: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 06:51:05.761: INFO: update-demo-nautilus-p99f4 is verified up and running
Aug 14 06:51:05.761: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-wjhhh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5871'
Aug 14 06:51:05.841: INFO: stderr: ""
Aug 14 06:51:05.841: INFO: stdout: "true"
Aug 14 06:51:05.841: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-wjhhh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5871'
Aug 14 06:51:05.915: INFO: stderr: ""
Aug 14 06:51:05.915: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Aug 14 06:51:05.915: INFO: validating pod update-demo-nautilus-wjhhh
Aug 14 06:51:06.000: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 14 06:51:06.000: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 14 06:51:06.000: INFO: update-demo-nautilus-wjhhh is verified up and running
STEP: rolling-update to new replication controller
Aug 14 06:51:06.002: INFO: scanned /root for discovery docs: <nil>
Aug 14 06:51:06.002: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-5871'
Aug 14 06:51:28.449: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 14 06:51:28.449: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Aug 14 06:51:28.449: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5871'
Aug 14 06:51:28.520: INFO: stderr: ""
Aug 14 06:51:28.520: INFO: stdout: "update-demo-kitten-qkrjb update-demo-kitten-xr99c "
Aug 14 06:51:28.520: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-qkrjb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5871'
Aug 14 06:51:28.590: INFO: stderr: ""
Aug 14 06:51:28.590: INFO: stdout: "true"
Aug 14 06:51:28.591: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-qkrjb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5871'
Aug 14 06:51:28.667: INFO: stderr: ""
Aug 14 06:51:28.667: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 14 06:51:28.667: INFO: validating pod update-demo-kitten-qkrjb
Aug 14 06:51:28.752: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 14 06:51:28.752: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 14 06:51:28.752: INFO: update-demo-kitten-qkrjb is verified up and running
Aug 14 06:51:28.752: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-xr99c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5871'
Aug 14 06:51:28.827: INFO: stderr: ""
Aug 14 06:51:28.827: INFO: stdout: "true"
Aug 14 06:51:28.827: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-xr99c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5871'
Aug 14 06:51:28.910: INFO: stderr: ""
Aug 14 06:51:28.910: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Aug 14 06:51:28.910: INFO: validating pod update-demo-kitten-xr99c
Aug 14 06:51:29.000: INFO: got data: {
  "image": "kitten.jpg"
}

Aug 14 06:51:29.000: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Aug 14 06:51:29.000: INFO: update-demo-kitten-xr99c is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:51:29.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5871" for this suite.
Aug 14 06:51:51.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:51:51.160: INFO: namespace kubectl-5871 deletion completed in 22.155355043s
â€¢SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:51:51.160: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2940
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Aug 14 06:51:51.310: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:51:51.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2940" for this suite.
Aug 14 06:51:57.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:51:57.601: INFO: namespace kubectl-2940 deletion completed in 6.165114866s
â€¢SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:51:57.601: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3495
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 06:51:57.746: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:51:59.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3495" for this suite.
Aug 14 06:52:37.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:52:37.924: INFO: namespace pods-3495 deletion completed in 38.127366059s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:52:37.928: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3231
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Aug 14 06:52:38.076: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-3231'
Aug 14 06:52:38.346: INFO: stderr: ""
Aug 14 06:52:38.347: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Aug 14 06:52:39.354: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 06:52:39.354: INFO: Found 1 / 1
Aug 14 06:52:39.354: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 14 06:52:39.357: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 06:52:39.357: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Aug 14 06:52:39.357: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-v9d5l redis-master --namespace=kubectl-3231'
Aug 14 06:52:39.485: INFO: stderr: ""
Aug 14 06:52:39.485: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 14 Aug 06:52:39.147 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 14 Aug 06:52:39.147 # Server started, Redis version 3.2.12\n1:M 14 Aug 06:52:39.148 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 Aug 06:52:39.148 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Aug 14 06:52:39.485: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-v9d5l redis-master --namespace=kubectl-3231 --tail=1'
Aug 14 06:52:39.585: INFO: stderr: ""
Aug 14 06:52:39.585: INFO: stdout: "1:M 14 Aug 06:52:39.148 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Aug 14 06:52:39.585: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-v9d5l redis-master --namespace=kubectl-3231 --limit-bytes=1'
Aug 14 06:52:39.726: INFO: stderr: ""
Aug 14 06:52:39.726: INFO: stdout: " "
STEP: exposing timestamps
Aug 14 06:52:39.726: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-v9d5l redis-master --namespace=kubectl-3231 --tail=1 --timestamps'
Aug 14 06:52:39.813: INFO: stderr: ""
Aug 14 06:52:39.813: INFO: stdout: "2019-08-14T06:52:39.148351504Z 1:M 14 Aug 06:52:39.148 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Aug 14 06:52:42.313: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-v9d5l redis-master --namespace=kubectl-3231 --since=1s'
Aug 14 06:52:42.397: INFO: stderr: ""
Aug 14 06:52:42.397: INFO: stdout: ""
Aug 14 06:52:42.397: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config log redis-master-v9d5l redis-master --namespace=kubectl-3231 --since=24h'
Aug 14 06:52:42.487: INFO: stderr: ""
Aug 14 06:52:42.487: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 14 Aug 06:52:39.147 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 14 Aug 06:52:39.147 # Server started, Redis version 3.2.12\n1:M 14 Aug 06:52:39.148 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 Aug 06:52:39.148 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Aug 14 06:52:42.487: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-3231'
Aug 14 06:52:42.571: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 06:52:42.571: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Aug 14 06:52:42.571: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=nginx --no-headers --namespace=kubectl-3231'
Aug 14 06:52:42.642: INFO: stderr: "No resources found.\n"
Aug 14 06:52:42.642: INFO: stdout: ""
Aug 14 06:52:42.642: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=nginx --namespace=kubectl-3231 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 14 06:52:42.713: INFO: stderr: ""
Aug 14 06:52:42.713: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:52:42.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3231" for this suite.
Aug 14 06:53:04.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:53:04.838: INFO: namespace kubectl-3231 deletion completed in 22.119825626s
â€¢SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:53:04.839: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1562
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-m4cm
STEP: Creating a pod to test atomic-volume-subpath
Aug 14 06:53:04.998: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-m4cm" in namespace "subpath-1562" to be "success or failure"
Aug 14 06:53:05.000: INFO: Pod "pod-subpath-test-secret-m4cm": Phase="Pending", Reason="", readiness=false. Elapsed: 2.75636ms
Aug 14 06:53:07.006: INFO: Pod "pod-subpath-test-secret-m4cm": Phase="Running", Reason="", readiness=true. Elapsed: 2.008484341s
Aug 14 06:53:09.010: INFO: Pod "pod-subpath-test-secret-m4cm": Phase="Running", Reason="", readiness=true. Elapsed: 4.012848258s
Aug 14 06:53:11.017: INFO: Pod "pod-subpath-test-secret-m4cm": Phase="Running", Reason="", readiness=true. Elapsed: 6.019443631s
Aug 14 06:53:13.022: INFO: Pod "pod-subpath-test-secret-m4cm": Phase="Running", Reason="", readiness=true. Elapsed: 8.024207895s
Aug 14 06:53:15.027: INFO: Pod "pod-subpath-test-secret-m4cm": Phase="Running", Reason="", readiness=true. Elapsed: 10.028944862s
Aug 14 06:53:17.033: INFO: Pod "pod-subpath-test-secret-m4cm": Phase="Running", Reason="", readiness=true. Elapsed: 12.035423351s
Aug 14 06:53:19.040: INFO: Pod "pod-subpath-test-secret-m4cm": Phase="Running", Reason="", readiness=true. Elapsed: 14.041953865s
Aug 14 06:53:21.050: INFO: Pod "pod-subpath-test-secret-m4cm": Phase="Running", Reason="", readiness=true. Elapsed: 16.05248199s
Aug 14 06:53:23.055: INFO: Pod "pod-subpath-test-secret-m4cm": Phase="Running", Reason="", readiness=true. Elapsed: 18.057535365s
Aug 14 06:53:25.061: INFO: Pod "pod-subpath-test-secret-m4cm": Phase="Running", Reason="", readiness=true. Elapsed: 20.063137991s
Aug 14 06:53:27.065: INFO: Pod "pod-subpath-test-secret-m4cm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.067708908s
STEP: Saw pod success
Aug 14 06:53:27.065: INFO: Pod "pod-subpath-test-secret-m4cm" satisfied condition "success or failure"
Aug 14 06:53:27.069: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-subpath-test-secret-m4cm container test-container-subpath-secret-m4cm: <nil>
STEP: delete the pod
Aug 14 06:53:27.088: INFO: Waiting for pod pod-subpath-test-secret-m4cm to disappear
Aug 14 06:53:27.091: INFO: Pod pod-subpath-test-secret-m4cm no longer exists
STEP: Deleting pod pod-subpath-test-secret-m4cm
Aug 14 06:53:27.091: INFO: Deleting pod "pod-subpath-test-secret-m4cm" in namespace "subpath-1562"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:53:27.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1562" for this suite.
Aug 14 06:53:33.107: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:53:33.213: INFO: namespace subpath-1562 deletion completed in 6.11566075s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:53:33.214: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-434
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:53:33.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-434" for this suite.
Aug 14 06:53:55.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:53:55.504: INFO: namespace pods-434 deletion completed in 22.129374105s
â€¢SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:53:55.505: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8010
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-ce08b8ba-17a9-4b7c-b04c-924ed53f9f16 in namespace container-probe-8010
Aug 14 06:53:57.666: INFO: Started pod busybox-ce08b8ba-17a9-4b7c-b04c-924ed53f9f16 in namespace container-probe-8010
STEP: checking the pod's current state and verifying that restartCount is present
Aug 14 06:53:57.669: INFO: Initial restart count of pod busybox-ce08b8ba-17a9-4b7c-b04c-924ed53f9f16 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:57:58.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8010" for this suite.
Aug 14 06:58:04.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:58:04.424: INFO: namespace container-probe-8010 deletion completed in 6.1363012s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:58:04.425: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-897
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-343f01fb-c548-4da1-9d1a-fefd6dd4a55d
STEP: Creating a pod to test consume configMaps
Aug 14 06:58:04.586: INFO: Waiting up to 5m0s for pod "pod-configmaps-1eb87ec3-2bc9-4eac-b894-3a779f29c4d8" in namespace "configmap-897" to be "success or failure"
Aug 14 06:58:04.589: INFO: Pod "pod-configmaps-1eb87ec3-2bc9-4eac-b894-3a779f29c4d8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.51533ms
Aug 14 06:58:06.593: INFO: Pod "pod-configmaps-1eb87ec3-2bc9-4eac-b894-3a779f29c4d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00692848s
STEP: Saw pod success
Aug 14 06:58:06.593: INFO: Pod "pod-configmaps-1eb87ec3-2bc9-4eac-b894-3a779f29c4d8" satisfied condition "success or failure"
Aug 14 06:58:06.596: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-configmaps-1eb87ec3-2bc9-4eac-b894-3a779f29c4d8 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 06:58:06.616: INFO: Waiting for pod pod-configmaps-1eb87ec3-2bc9-4eac-b894-3a779f29c4d8 to disappear
Aug 14 06:58:06.618: INFO: Pod pod-configmaps-1eb87ec3-2bc9-4eac-b894-3a779f29c4d8 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:58:06.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-897" for this suite.
Aug 14 06:58:12.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:58:12.784: INFO: namespace configmap-897 deletion completed in 6.162377682s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:58:12.785: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3532
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-3532
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-3532
STEP: Creating statefulset with conflicting port in namespace statefulset-3532
STEP: Waiting until pod test-pod will start running in namespace statefulset-3532
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-3532
Aug 14 06:58:15.005: INFO: Observed stateful pod in namespace: statefulset-3532, name: ss-0, uid: fd4d70af-f23f-454c-b901-b98fe75daf7f, status phase: Pending. Waiting for statefulset controller to delete.
Aug 14 06:58:15.015: INFO: Observed stateful pod in namespace: statefulset-3532, name: ss-0, uid: fd4d70af-f23f-454c-b901-b98fe75daf7f, status phase: Failed. Waiting for statefulset controller to delete.
Aug 14 06:58:15.054: INFO: Observed stateful pod in namespace: statefulset-3532, name: ss-0, uid: fd4d70af-f23f-454c-b901-b98fe75daf7f, status phase: Failed. Waiting for statefulset controller to delete.
Aug 14 06:58:15.057: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-3532
STEP: Removing pod with conflicting port in namespace statefulset-3532
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-3532 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 14 06:58:17.070: INFO: Deleting all statefulset in ns statefulset-3532
Aug 14 06:58:17.074: INFO: Scaling statefulset ss to 0
Aug 14 06:58:27.088: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 06:58:27.092: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:58:27.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3532" for this suite.
Aug 14 06:58:33.117: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:58:33.267: INFO: namespace statefulset-3532 deletion completed in 6.161474996s
â€¢SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:58:33.268: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-4445
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-4445
I0814 06:58:33.419465    3092 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4445, replica count: 1
I0814 06:58:34.470066    3092 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 06:58:35.470353    3092 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 14 06:58:35.596: INFO: Created: latency-svc-dn77v
Aug 14 06:58:35.598: INFO: Got endpoints: latency-svc-dn77v [28.163989ms]
Aug 14 06:58:35.609: INFO: Created: latency-svc-tcqwj
Aug 14 06:58:35.617: INFO: Got endpoints: latency-svc-tcqwj [18.289588ms]
Aug 14 06:58:35.618: INFO: Created: latency-svc-w7d97
Aug 14 06:58:35.621: INFO: Got endpoints: latency-svc-w7d97 [22.713971ms]
Aug 14 06:58:35.624: INFO: Created: latency-svc-bmnhb
Aug 14 06:58:35.627: INFO: Got endpoints: latency-svc-bmnhb [28.137054ms]
Aug 14 06:58:35.631: INFO: Created: latency-svc-9c5vv
Aug 14 06:58:35.637: INFO: Got endpoints: latency-svc-9c5vv [38.139497ms]
Aug 14 06:58:35.639: INFO: Created: latency-svc-fr5wf
Aug 14 06:58:35.641: INFO: Got endpoints: latency-svc-fr5wf [41.979511ms]
Aug 14 06:58:35.646: INFO: Created: latency-svc-jfs56
Aug 14 06:58:35.648: INFO: Got endpoints: latency-svc-jfs56 [48.311973ms]
Aug 14 06:58:35.652: INFO: Created: latency-svc-nnd6r
Aug 14 06:58:35.654: INFO: Got endpoints: latency-svc-nnd6r [54.900795ms]
Aug 14 06:58:35.659: INFO: Created: latency-svc-hnwg5
Aug 14 06:58:35.661: INFO: Got endpoints: latency-svc-hnwg5 [62.037807ms]
Aug 14 06:58:35.665: INFO: Created: latency-svc-7npsj
Aug 14 06:58:35.667: INFO: Got endpoints: latency-svc-7npsj [67.855837ms]
Aug 14 06:58:35.678: INFO: Created: latency-svc-dd54d
Aug 14 06:58:35.682: INFO: Got endpoints: latency-svc-dd54d [82.709488ms]
Aug 14 06:58:35.687: INFO: Created: latency-svc-mrntx
Aug 14 06:58:35.688: INFO: Got endpoints: latency-svc-mrntx [88.867126ms]
Aug 14 06:58:35.693: INFO: Created: latency-svc-qmrfw
Aug 14 06:58:35.698: INFO: Got endpoints: latency-svc-qmrfw [98.417978ms]
Aug 14 06:58:35.699: INFO: Created: latency-svc-csx9b
Aug 14 06:58:35.701: INFO: Got endpoints: latency-svc-csx9b [18.578502ms]
Aug 14 06:58:35.705: INFO: Created: latency-svc-zn488
Aug 14 06:58:35.707: INFO: Got endpoints: latency-svc-zn488 [107.502004ms]
Aug 14 06:58:35.712: INFO: Created: latency-svc-7p4pb
Aug 14 06:58:35.715: INFO: Got endpoints: latency-svc-7p4pb [115.884191ms]
Aug 14 06:58:35.719: INFO: Created: latency-svc-27hjb
Aug 14 06:58:35.725: INFO: Got endpoints: latency-svc-27hjb [125.590371ms]
Aug 14 06:58:35.725: INFO: Created: latency-svc-vswjx
Aug 14 06:58:35.730: INFO: Got endpoints: latency-svc-vswjx [112.988966ms]
Aug 14 06:58:35.731: INFO: Created: latency-svc-vkhlq
Aug 14 06:58:35.737: INFO: Got endpoints: latency-svc-vkhlq [115.481431ms]
Aug 14 06:58:35.737: INFO: Created: latency-svc-pvjf4
Aug 14 06:58:35.743: INFO: Got endpoints: latency-svc-pvjf4 [115.827733ms]
Aug 14 06:58:35.744: INFO: Created: latency-svc-vpttj
Aug 14 06:58:35.750: INFO: Got endpoints: latency-svc-vpttj [112.453214ms]
Aug 14 06:58:35.758: INFO: Created: latency-svc-q2kvd
Aug 14 06:58:35.759: INFO: Created: latency-svc-wl92r
Aug 14 06:58:35.762: INFO: Got endpoints: latency-svc-wl92r [114.306726ms]
Aug 14 06:58:35.763: INFO: Got endpoints: latency-svc-q2kvd [121.85302ms]
Aug 14 06:58:35.767: INFO: Created: latency-svc-tz7pz
Aug 14 06:58:35.769: INFO: Got endpoints: latency-svc-tz7pz [114.982817ms]
Aug 14 06:58:35.773: INFO: Created: latency-svc-fkfht
Aug 14 06:58:35.782: INFO: Got endpoints: latency-svc-fkfht [120.183557ms]
Aug 14 06:58:35.786: INFO: Created: latency-svc-9pc2g
Aug 14 06:58:35.788: INFO: Got endpoints: latency-svc-9pc2g [120.500994ms]
Aug 14 06:58:35.792: INFO: Created: latency-svc-l5hdq
Aug 14 06:58:35.795: INFO: Got endpoints: latency-svc-l5hdq [106.400225ms]
Aug 14 06:58:35.801: INFO: Created: latency-svc-5s295
Aug 14 06:58:35.803: INFO: Got endpoints: latency-svc-5s295 [104.847995ms]
Aug 14 06:58:35.808: INFO: Created: latency-svc-nwnlv
Aug 14 06:58:35.810: INFO: Got endpoints: latency-svc-nwnlv [109.428473ms]
Aug 14 06:58:35.815: INFO: Created: latency-svc-8xdtn
Aug 14 06:58:35.818: INFO: Got endpoints: latency-svc-8xdtn [110.864024ms]
Aug 14 06:58:35.824: INFO: Created: latency-svc-dxfs2
Aug 14 06:58:35.827: INFO: Got endpoints: latency-svc-dxfs2 [111.355637ms]
Aug 14 06:58:35.831: INFO: Created: latency-svc-fqsbq
Aug 14 06:58:35.834: INFO: Got endpoints: latency-svc-fqsbq [108.726391ms]
Aug 14 06:58:35.838: INFO: Created: latency-svc-cb8wj
Aug 14 06:58:35.840: INFO: Got endpoints: latency-svc-cb8wj [110.001546ms]
Aug 14 06:58:35.845: INFO: Created: latency-svc-xrghq
Aug 14 06:58:35.848: INFO: Got endpoints: latency-svc-xrghq [111.379642ms]
Aug 14 06:58:35.852: INFO: Created: latency-svc-5w4x5
Aug 14 06:58:35.859: INFO: Got endpoints: latency-svc-5w4x5 [116.137791ms]
Aug 14 06:58:35.860: INFO: Created: latency-svc-nwzg7
Aug 14 06:58:35.869: INFO: Created: latency-svc-scdvn
Aug 14 06:58:35.881: INFO: Created: latency-svc-jjv8v
Aug 14 06:58:35.902: INFO: Created: latency-svc-njl5v
Aug 14 06:58:35.904: INFO: Got endpoints: latency-svc-nwzg7 [153.458348ms]
Aug 14 06:58:35.909: INFO: Created: latency-svc-sxwhh
Aug 14 06:58:35.915: INFO: Created: latency-svc-bnx47
Aug 14 06:58:35.922: INFO: Created: latency-svc-nw75s
Aug 14 06:58:35.928: INFO: Created: latency-svc-n8zcr
Aug 14 06:58:35.935: INFO: Created: latency-svc-mpq7n
Aug 14 06:58:35.941: INFO: Created: latency-svc-qkh6k
Aug 14 06:58:35.949: INFO: Created: latency-svc-kqkk4
Aug 14 06:58:35.949: INFO: Got endpoints: latency-svc-scdvn [187.081226ms]
Aug 14 06:58:35.956: INFO: Created: latency-svc-5h82v
Aug 14 06:58:35.962: INFO: Created: latency-svc-fjtv4
Aug 14 06:58:35.969: INFO: Created: latency-svc-qpdgj
Aug 14 06:58:35.974: INFO: Created: latency-svc-2jj8h
Aug 14 06:58:35.980: INFO: Created: latency-svc-kqwct
Aug 14 06:58:35.986: INFO: Created: latency-svc-k2gll
Aug 14 06:58:35.999: INFO: Got endpoints: latency-svc-jjv8v [235.642053ms]
Aug 14 06:58:36.014: INFO: Created: latency-svc-88gcz
Aug 14 06:58:36.049: INFO: Got endpoints: latency-svc-njl5v [279.281023ms]
Aug 14 06:58:36.058: INFO: Created: latency-svc-kq2xz
Aug 14 06:58:36.101: INFO: Got endpoints: latency-svc-sxwhh [319.003526ms]
Aug 14 06:58:36.114: INFO: Created: latency-svc-bc6nd
Aug 14 06:58:36.149: INFO: Got endpoints: latency-svc-bnx47 [361.041672ms]
Aug 14 06:58:36.159: INFO: Created: latency-svc-qmhfx
Aug 14 06:58:36.199: INFO: Got endpoints: latency-svc-nw75s [404.338424ms]
Aug 14 06:58:36.210: INFO: Created: latency-svc-nxkbm
Aug 14 06:58:36.249: INFO: Got endpoints: latency-svc-n8zcr [446.631521ms]
Aug 14 06:58:36.260: INFO: Created: latency-svc-c7chc
Aug 14 06:58:36.299: INFO: Got endpoints: latency-svc-mpq7n [488.808986ms]
Aug 14 06:58:36.310: INFO: Created: latency-svc-pr7cb
Aug 14 06:58:36.351: INFO: Got endpoints: latency-svc-qkh6k [533.196741ms]
Aug 14 06:58:36.376: INFO: Created: latency-svc-5cdbj
Aug 14 06:58:36.399: INFO: Got endpoints: latency-svc-kqkk4 [572.761877ms]
Aug 14 06:58:36.412: INFO: Created: latency-svc-t2n2l
Aug 14 06:58:36.450: INFO: Got endpoints: latency-svc-5h82v [615.72512ms]
Aug 14 06:58:36.463: INFO: Created: latency-svc-4rq49
Aug 14 06:58:36.498: INFO: Got endpoints: latency-svc-fjtv4 [657.989931ms]
Aug 14 06:58:36.510: INFO: Created: latency-svc-bxrfg
Aug 14 06:58:36.549: INFO: Got endpoints: latency-svc-qpdgj [700.665884ms]
Aug 14 06:58:36.559: INFO: Created: latency-svc-gs2kw
Aug 14 06:58:36.599: INFO: Got endpoints: latency-svc-2jj8h [739.783968ms]
Aug 14 06:58:36.612: INFO: Created: latency-svc-gmmbz
Aug 14 06:58:36.650: INFO: Got endpoints: latency-svc-kqwct [746.712117ms]
Aug 14 06:58:36.663: INFO: Created: latency-svc-bmn2v
Aug 14 06:58:36.699: INFO: Got endpoints: latency-svc-k2gll [750.103589ms]
Aug 14 06:58:36.711: INFO: Created: latency-svc-tk8ct
Aug 14 06:58:36.750: INFO: Got endpoints: latency-svc-88gcz [751.273869ms]
Aug 14 06:58:36.762: INFO: Created: latency-svc-p57qm
Aug 14 06:58:36.799: INFO: Got endpoints: latency-svc-kq2xz [750.289916ms]
Aug 14 06:58:36.811: INFO: Created: latency-svc-69whq
Aug 14 06:58:36.853: INFO: Got endpoints: latency-svc-bc6nd [752.000839ms]
Aug 14 06:58:36.864: INFO: Created: latency-svc-7glt2
Aug 14 06:58:36.900: INFO: Got endpoints: latency-svc-qmhfx [751.095277ms]
Aug 14 06:58:36.911: INFO: Created: latency-svc-b4z7l
Aug 14 06:58:36.950: INFO: Got endpoints: latency-svc-nxkbm [750.735398ms]
Aug 14 06:58:36.965: INFO: Created: latency-svc-nzbjr
Aug 14 06:58:37.000: INFO: Got endpoints: latency-svc-c7chc [750.660535ms]
Aug 14 06:58:37.012: INFO: Created: latency-svc-hl42t
Aug 14 06:58:37.049: INFO: Got endpoints: latency-svc-pr7cb [750.157443ms]
Aug 14 06:58:37.064: INFO: Created: latency-svc-48hcj
Aug 14 06:58:37.099: INFO: Got endpoints: latency-svc-5cdbj [748.322922ms]
Aug 14 06:58:37.110: INFO: Created: latency-svc-7pnt6
Aug 14 06:58:37.149: INFO: Got endpoints: latency-svc-t2n2l [749.82366ms]
Aug 14 06:58:37.160: INFO: Created: latency-svc-rz427
Aug 14 06:58:37.200: INFO: Got endpoints: latency-svc-4rq49 [749.77125ms]
Aug 14 06:58:37.220: INFO: Created: latency-svc-kvq26
Aug 14 06:58:37.249: INFO: Got endpoints: latency-svc-bxrfg [750.410066ms]
Aug 14 06:58:37.259: INFO: Created: latency-svc-h2nsd
Aug 14 06:58:37.299: INFO: Got endpoints: latency-svc-gs2kw [749.927399ms]
Aug 14 06:58:37.309: INFO: Created: latency-svc-mm7qt
Aug 14 06:58:37.349: INFO: Got endpoints: latency-svc-gmmbz [749.637601ms]
Aug 14 06:58:37.360: INFO: Created: latency-svc-qhxvk
Aug 14 06:58:37.399: INFO: Got endpoints: latency-svc-bmn2v [748.509725ms]
Aug 14 06:58:37.410: INFO: Created: latency-svc-7cqp6
Aug 14 06:58:37.449: INFO: Got endpoints: latency-svc-tk8ct [749.696913ms]
Aug 14 06:58:37.461: INFO: Created: latency-svc-src2n
Aug 14 06:58:37.500: INFO: Got endpoints: latency-svc-p57qm [750.056713ms]
Aug 14 06:58:37.513: INFO: Created: latency-svc-dqk59
Aug 14 06:58:37.549: INFO: Got endpoints: latency-svc-69whq [749.920108ms]
Aug 14 06:58:37.559: INFO: Created: latency-svc-lfks2
Aug 14 06:58:37.599: INFO: Got endpoints: latency-svc-7glt2 [746.466606ms]
Aug 14 06:58:37.610: INFO: Created: latency-svc-xlzp8
Aug 14 06:58:37.650: INFO: Got endpoints: latency-svc-b4z7l [749.93567ms]
Aug 14 06:58:37.661: INFO: Created: latency-svc-9m6d7
Aug 14 06:58:37.700: INFO: Got endpoints: latency-svc-nzbjr [749.452434ms]
Aug 14 06:58:37.710: INFO: Created: latency-svc-mbldq
Aug 14 06:58:37.749: INFO: Got endpoints: latency-svc-hl42t [749.02455ms]
Aug 14 06:58:37.759: INFO: Created: latency-svc-pwq8r
Aug 14 06:58:37.798: INFO: Got endpoints: latency-svc-48hcj [749.000375ms]
Aug 14 06:58:37.809: INFO: Created: latency-svc-jld42
Aug 14 06:58:37.849: INFO: Got endpoints: latency-svc-7pnt6 [749.248051ms]
Aug 14 06:58:37.860: INFO: Created: latency-svc-nkrrl
Aug 14 06:58:37.900: INFO: Got endpoints: latency-svc-rz427 [751.082204ms]
Aug 14 06:58:37.910: INFO: Created: latency-svc-7jgc2
Aug 14 06:58:37.949: INFO: Got endpoints: latency-svc-kvq26 [748.859002ms]
Aug 14 06:58:37.959: INFO: Created: latency-svc-7lxvb
Aug 14 06:58:37.999: INFO: Got endpoints: latency-svc-h2nsd [749.764952ms]
Aug 14 06:58:38.012: INFO: Created: latency-svc-42mmz
Aug 14 06:58:38.049: INFO: Got endpoints: latency-svc-mm7qt [749.867ms]
Aug 14 06:58:38.073: INFO: Created: latency-svc-5tmr7
Aug 14 06:58:38.155: INFO: Got endpoints: latency-svc-qhxvk [806.342905ms]
Aug 14 06:58:38.156: INFO: Got endpoints: latency-svc-7cqp6 [756.49645ms]
Aug 14 06:58:38.166: INFO: Created: latency-svc-dsvdm
Aug 14 06:58:38.172: INFO: Created: latency-svc-npzhr
Aug 14 06:58:38.199: INFO: Got endpoints: latency-svc-src2n [749.258617ms]
Aug 14 06:58:38.208: INFO: Created: latency-svc-tqnzx
Aug 14 06:58:38.249: INFO: Got endpoints: latency-svc-dqk59 [748.585501ms]
Aug 14 06:58:38.259: INFO: Created: latency-svc-s2xrl
Aug 14 06:58:38.299: INFO: Got endpoints: latency-svc-lfks2 [749.711472ms]
Aug 14 06:58:38.310: INFO: Created: latency-svc-cqnq7
Aug 14 06:58:38.349: INFO: Got endpoints: latency-svc-xlzp8 [750.044407ms]
Aug 14 06:58:38.360: INFO: Created: latency-svc-vjmzh
Aug 14 06:58:38.399: INFO: Got endpoints: latency-svc-9m6d7 [748.547918ms]
Aug 14 06:58:38.408: INFO: Created: latency-svc-lmqbl
Aug 14 06:58:38.449: INFO: Got endpoints: latency-svc-mbldq [749.420641ms]
Aug 14 06:58:38.459: INFO: Created: latency-svc-bcj8w
Aug 14 06:58:38.499: INFO: Got endpoints: latency-svc-pwq8r [749.266912ms]
Aug 14 06:58:38.509: INFO: Created: latency-svc-2mz97
Aug 14 06:58:38.548: INFO: Got endpoints: latency-svc-jld42 [749.874734ms]
Aug 14 06:58:38.558: INFO: Created: latency-svc-jwjpp
Aug 14 06:58:38.598: INFO: Got endpoints: latency-svc-nkrrl [749.603099ms]
Aug 14 06:58:38.608: INFO: Created: latency-svc-97mj6
Aug 14 06:58:38.649: INFO: Got endpoints: latency-svc-7jgc2 [748.319337ms]
Aug 14 06:58:38.659: INFO: Created: latency-svc-vmqrx
Aug 14 06:58:38.700: INFO: Got endpoints: latency-svc-7lxvb [751.234291ms]
Aug 14 06:58:38.712: INFO: Created: latency-svc-49xfw
Aug 14 06:58:38.749: INFO: Got endpoints: latency-svc-42mmz [750.530512ms]
Aug 14 06:58:38.760: INFO: Created: latency-svc-g8hr2
Aug 14 06:58:38.799: INFO: Got endpoints: latency-svc-5tmr7 [750.54666ms]
Aug 14 06:58:38.810: INFO: Created: latency-svc-98xqn
Aug 14 06:58:38.849: INFO: Got endpoints: latency-svc-dsvdm [693.638941ms]
Aug 14 06:58:38.860: INFO: Created: latency-svc-2wnxq
Aug 14 06:58:38.899: INFO: Got endpoints: latency-svc-npzhr [743.52176ms]
Aug 14 06:58:38.911: INFO: Created: latency-svc-qzwvf
Aug 14 06:58:38.949: INFO: Got endpoints: latency-svc-tqnzx [750.5535ms]
Aug 14 06:58:38.960: INFO: Created: latency-svc-vv6vx
Aug 14 06:58:38.999: INFO: Got endpoints: latency-svc-s2xrl [750.347816ms]
Aug 14 06:58:39.023: INFO: Created: latency-svc-p47q7
Aug 14 06:58:39.053: INFO: Got endpoints: latency-svc-cqnq7 [754.055729ms]
Aug 14 06:58:39.063: INFO: Created: latency-svc-rhkww
Aug 14 06:58:39.099: INFO: Got endpoints: latency-svc-vjmzh [749.517939ms]
Aug 14 06:58:39.108: INFO: Created: latency-svc-g5qhw
Aug 14 06:58:39.149: INFO: Got endpoints: latency-svc-lmqbl [750.542299ms]
Aug 14 06:58:39.165: INFO: Created: latency-svc-cbs8m
Aug 14 06:58:39.199: INFO: Got endpoints: latency-svc-bcj8w [749.96617ms]
Aug 14 06:58:39.210: INFO: Created: latency-svc-nmkb8
Aug 14 06:58:39.249: INFO: Got endpoints: latency-svc-2mz97 [750.281243ms]
Aug 14 06:58:39.262: INFO: Created: latency-svc-kffr7
Aug 14 06:58:39.299: INFO: Got endpoints: latency-svc-jwjpp [750.071413ms]
Aug 14 06:58:39.308: INFO: Created: latency-svc-dc4bz
Aug 14 06:58:39.349: INFO: Got endpoints: latency-svc-97mj6 [750.55423ms]
Aug 14 06:58:39.360: INFO: Created: latency-svc-m4ljc
Aug 14 06:58:39.400: INFO: Got endpoints: latency-svc-vmqrx [750.906824ms]
Aug 14 06:58:39.411: INFO: Created: latency-svc-cxqtx
Aug 14 06:58:39.449: INFO: Got endpoints: latency-svc-49xfw [749.115713ms]
Aug 14 06:58:39.459: INFO: Created: latency-svc-wln25
Aug 14 06:58:39.499: INFO: Got endpoints: latency-svc-g8hr2 [749.88625ms]
Aug 14 06:58:39.510: INFO: Created: latency-svc-pnkw9
Aug 14 06:58:39.549: INFO: Got endpoints: latency-svc-98xqn [749.554961ms]
Aug 14 06:58:39.560: INFO: Created: latency-svc-44gmr
Aug 14 06:58:39.599: INFO: Got endpoints: latency-svc-2wnxq [750.194616ms]
Aug 14 06:58:39.610: INFO: Created: latency-svc-kkg6c
Aug 14 06:58:39.649: INFO: Got endpoints: latency-svc-qzwvf [749.604026ms]
Aug 14 06:58:39.658: INFO: Created: latency-svc-qkplq
Aug 14 06:58:39.700: INFO: Got endpoints: latency-svc-vv6vx [750.89808ms]
Aug 14 06:58:39.710: INFO: Created: latency-svc-xxzp8
Aug 14 06:58:39.749: INFO: Got endpoints: latency-svc-p47q7 [749.670574ms]
Aug 14 06:58:39.760: INFO: Created: latency-svc-fxwmc
Aug 14 06:58:39.799: INFO: Got endpoints: latency-svc-rhkww [745.966865ms]
Aug 14 06:58:39.810: INFO: Created: latency-svc-lp4lm
Aug 14 06:58:39.849: INFO: Got endpoints: latency-svc-g5qhw [750.20476ms]
Aug 14 06:58:39.860: INFO: Created: latency-svc-7w8r5
Aug 14 06:58:39.899: INFO: Got endpoints: latency-svc-cbs8m [750.144412ms]
Aug 14 06:58:39.910: INFO: Created: latency-svc-9xb5j
Aug 14 06:58:39.949: INFO: Got endpoints: latency-svc-nmkb8 [750.083459ms]
Aug 14 06:58:39.961: INFO: Created: latency-svc-v5cvk
Aug 14 06:58:39.999: INFO: Got endpoints: latency-svc-kffr7 [750.411167ms]
Aug 14 06:58:40.012: INFO: Created: latency-svc-qcttt
Aug 14 06:58:40.049: INFO: Got endpoints: latency-svc-dc4bz [750.84716ms]
Aug 14 06:58:40.060: INFO: Created: latency-svc-hmq29
Aug 14 06:58:40.102: INFO: Got endpoints: latency-svc-m4ljc [753.328016ms]
Aug 14 06:58:40.113: INFO: Created: latency-svc-55bjk
Aug 14 06:58:40.149: INFO: Got endpoints: latency-svc-cxqtx [749.1193ms]
Aug 14 06:58:40.160: INFO: Created: latency-svc-m7cqt
Aug 14 06:58:40.199: INFO: Got endpoints: latency-svc-wln25 [749.886488ms]
Aug 14 06:58:40.214: INFO: Created: latency-svc-mqs2f
Aug 14 06:58:40.249: INFO: Got endpoints: latency-svc-pnkw9 [749.633212ms]
Aug 14 06:58:40.260: INFO: Created: latency-svc-wgb9c
Aug 14 06:58:40.299: INFO: Got endpoints: latency-svc-44gmr [749.862053ms]
Aug 14 06:58:40.312: INFO: Created: latency-svc-jchqc
Aug 14 06:58:40.350: INFO: Got endpoints: latency-svc-kkg6c [750.605132ms]
Aug 14 06:58:40.362: INFO: Created: latency-svc-n95hd
Aug 14 06:58:40.400: INFO: Got endpoints: latency-svc-qkplq [750.950081ms]
Aug 14 06:58:40.410: INFO: Created: latency-svc-sjphc
Aug 14 06:58:40.449: INFO: Got endpoints: latency-svc-xxzp8 [748.873944ms]
Aug 14 06:58:40.459: INFO: Created: latency-svc-n4tgw
Aug 14 06:58:40.499: INFO: Got endpoints: latency-svc-fxwmc [749.929175ms]
Aug 14 06:58:40.514: INFO: Created: latency-svc-bdwm5
Aug 14 06:58:40.549: INFO: Got endpoints: latency-svc-lp4lm [750.019003ms]
Aug 14 06:58:40.559: INFO: Created: latency-svc-bc6z8
Aug 14 06:58:40.602: INFO: Got endpoints: latency-svc-7w8r5 [752.28512ms]
Aug 14 06:58:40.613: INFO: Created: latency-svc-v5n7d
Aug 14 06:58:40.649: INFO: Got endpoints: latency-svc-9xb5j [749.573881ms]
Aug 14 06:58:40.660: INFO: Created: latency-svc-5nng8
Aug 14 06:58:40.700: INFO: Got endpoints: latency-svc-v5cvk [750.487989ms]
Aug 14 06:58:40.711: INFO: Created: latency-svc-bl42w
Aug 14 06:58:40.749: INFO: Got endpoints: latency-svc-qcttt [749.509763ms]
Aug 14 06:58:40.760: INFO: Created: latency-svc-2vqjv
Aug 14 06:58:40.798: INFO: Got endpoints: latency-svc-hmq29 [748.784519ms]
Aug 14 06:58:40.811: INFO: Created: latency-svc-q7sv8
Aug 14 06:58:40.849: INFO: Got endpoints: latency-svc-55bjk [746.598672ms]
Aug 14 06:58:40.862: INFO: Created: latency-svc-9hnzs
Aug 14 06:58:40.899: INFO: Got endpoints: latency-svc-m7cqt [749.62159ms]
Aug 14 06:58:40.913: INFO: Created: latency-svc-b6vsk
Aug 14 06:58:40.949: INFO: Got endpoints: latency-svc-mqs2f [750.225119ms]
Aug 14 06:58:40.960: INFO: Created: latency-svc-vdnsd
Aug 14 06:58:40.999: INFO: Got endpoints: latency-svc-wgb9c [750.428548ms]
Aug 14 06:58:41.011: INFO: Created: latency-svc-t8qsj
Aug 14 06:58:41.049: INFO: Got endpoints: latency-svc-jchqc [750.316694ms]
Aug 14 06:58:41.059: INFO: Created: latency-svc-qjmkf
Aug 14 06:58:41.098: INFO: Got endpoints: latency-svc-n95hd [748.546564ms]
Aug 14 06:58:41.108: INFO: Created: latency-svc-kdxsc
Aug 14 06:58:41.153: INFO: Got endpoints: latency-svc-sjphc [753.147917ms]
Aug 14 06:58:41.168: INFO: Created: latency-svc-7frcl
Aug 14 06:58:41.199: INFO: Got endpoints: latency-svc-n4tgw [750.29062ms]
Aug 14 06:58:41.210: INFO: Created: latency-svc-j2psk
Aug 14 06:58:41.249: INFO: Got endpoints: latency-svc-bdwm5 [749.942546ms]
Aug 14 06:58:41.262: INFO: Created: latency-svc-6lq98
Aug 14 06:58:41.299: INFO: Got endpoints: latency-svc-bc6z8 [749.699797ms]
Aug 14 06:58:41.310: INFO: Created: latency-svc-8z9qt
Aug 14 06:58:41.349: INFO: Got endpoints: latency-svc-v5n7d [747.195291ms]
Aug 14 06:58:41.360: INFO: Created: latency-svc-th6nl
Aug 14 06:58:41.400: INFO: Got endpoints: latency-svc-5nng8 [750.492138ms]
Aug 14 06:58:41.411: INFO: Created: latency-svc-xqbhg
Aug 14 06:58:41.452: INFO: Got endpoints: latency-svc-bl42w [752.299953ms]
Aug 14 06:58:41.462: INFO: Created: latency-svc-fnsh4
Aug 14 06:58:41.500: INFO: Got endpoints: latency-svc-2vqjv [751.228927ms]
Aug 14 06:58:41.511: INFO: Created: latency-svc-kt9dk
Aug 14 06:58:41.549: INFO: Got endpoints: latency-svc-q7sv8 [750.775632ms]
Aug 14 06:58:41.560: INFO: Created: latency-svc-k9qpz
Aug 14 06:58:41.600: INFO: Got endpoints: latency-svc-9hnzs [751.200874ms]
Aug 14 06:58:41.610: INFO: Created: latency-svc-cjs6z
Aug 14 06:58:41.649: INFO: Got endpoints: latency-svc-b6vsk [750.396386ms]
Aug 14 06:58:41.660: INFO: Created: latency-svc-ld6l7
Aug 14 06:58:41.699: INFO: Got endpoints: latency-svc-vdnsd [749.621677ms]
Aug 14 06:58:41.708: INFO: Created: latency-svc-mtcrx
Aug 14 06:58:41.750: INFO: Got endpoints: latency-svc-t8qsj [750.342056ms]
Aug 14 06:58:41.760: INFO: Created: latency-svc-sxtpw
Aug 14 06:58:41.799: INFO: Got endpoints: latency-svc-qjmkf [749.598225ms]
Aug 14 06:58:41.811: INFO: Created: latency-svc-6c6d2
Aug 14 06:58:41.849: INFO: Got endpoints: latency-svc-kdxsc [750.593814ms]
Aug 14 06:58:41.859: INFO: Created: latency-svc-hf9ss
Aug 14 06:58:41.899: INFO: Got endpoints: latency-svc-7frcl [746.20487ms]
Aug 14 06:58:41.910: INFO: Created: latency-svc-72c2s
Aug 14 06:58:41.948: INFO: Got endpoints: latency-svc-j2psk [748.97232ms]
Aug 14 06:58:41.959: INFO: Created: latency-svc-f8ggr
Aug 14 06:58:41.999: INFO: Got endpoints: latency-svc-6lq98 [749.924916ms]
Aug 14 06:58:42.009: INFO: Created: latency-svc-7stln
Aug 14 06:58:42.050: INFO: Got endpoints: latency-svc-8z9qt [750.698602ms]
Aug 14 06:58:42.060: INFO: Created: latency-svc-x6nns
Aug 14 06:58:42.107: INFO: Got endpoints: latency-svc-th6nl [758.331648ms]
Aug 14 06:58:42.118: INFO: Created: latency-svc-xwtqh
Aug 14 06:58:42.149: INFO: Got endpoints: latency-svc-xqbhg [749.296147ms]
Aug 14 06:58:42.160: INFO: Created: latency-svc-gn8r9
Aug 14 06:58:42.199: INFO: Got endpoints: latency-svc-fnsh4 [746.649135ms]
Aug 14 06:58:42.213: INFO: Created: latency-svc-m667f
Aug 14 06:58:42.249: INFO: Got endpoints: latency-svc-kt9dk [748.417844ms]
Aug 14 06:58:42.259: INFO: Created: latency-svc-ldxjb
Aug 14 06:58:42.299: INFO: Got endpoints: latency-svc-k9qpz [750.093465ms]
Aug 14 06:58:42.310: INFO: Created: latency-svc-m6nnt
Aug 14 06:58:42.349: INFO: Got endpoints: latency-svc-cjs6z [748.87952ms]
Aug 14 06:58:42.360: INFO: Created: latency-svc-pqnrv
Aug 14 06:58:42.400: INFO: Got endpoints: latency-svc-ld6l7 [750.56258ms]
Aug 14 06:58:42.410: INFO: Created: latency-svc-2jl7j
Aug 14 06:58:42.449: INFO: Got endpoints: latency-svc-mtcrx [749.952537ms]
Aug 14 06:58:42.459: INFO: Created: latency-svc-lc8th
Aug 14 06:58:42.499: INFO: Got endpoints: latency-svc-sxtpw [748.902552ms]
Aug 14 06:58:42.509: INFO: Created: latency-svc-nhzpc
Aug 14 06:58:42.549: INFO: Got endpoints: latency-svc-6c6d2 [749.635261ms]
Aug 14 06:58:42.560: INFO: Created: latency-svc-l2l4w
Aug 14 06:58:42.599: INFO: Got endpoints: latency-svc-hf9ss [750.13001ms]
Aug 14 06:58:42.610: INFO: Created: latency-svc-nvhz5
Aug 14 06:58:42.650: INFO: Got endpoints: latency-svc-72c2s [750.763891ms]
Aug 14 06:58:42.660: INFO: Created: latency-svc-l7chc
Aug 14 06:58:42.700: INFO: Got endpoints: latency-svc-f8ggr [751.049346ms]
Aug 14 06:58:42.710: INFO: Created: latency-svc-h497w
Aug 14 06:58:42.750: INFO: Got endpoints: latency-svc-7stln [750.42067ms]
Aug 14 06:58:42.762: INFO: Created: latency-svc-mw9m6
Aug 14 06:58:42.800: INFO: Got endpoints: latency-svc-x6nns [750.509595ms]
Aug 14 06:58:42.811: INFO: Created: latency-svc-t9wd8
Aug 14 06:58:42.849: INFO: Got endpoints: latency-svc-xwtqh [741.977611ms]
Aug 14 06:58:42.860: INFO: Created: latency-svc-df42k
Aug 14 06:58:42.899: INFO: Got endpoints: latency-svc-gn8r9 [750.167206ms]
Aug 14 06:58:42.910: INFO: Created: latency-svc-wlm7m
Aug 14 06:58:42.949: INFO: Got endpoints: latency-svc-m667f [750.241653ms]
Aug 14 06:58:42.959: INFO: Created: latency-svc-bhd76
Aug 14 06:58:42.999: INFO: Got endpoints: latency-svc-ldxjb [749.998352ms]
Aug 14 06:58:43.024: INFO: Created: latency-svc-h69xt
Aug 14 06:58:43.048: INFO: Got endpoints: latency-svc-m6nnt [748.898865ms]
Aug 14 06:58:43.059: INFO: Created: latency-svc-krdxc
Aug 14 06:58:43.100: INFO: Got endpoints: latency-svc-pqnrv [750.257168ms]
Aug 14 06:58:43.109: INFO: Created: latency-svc-lp9tv
Aug 14 06:58:43.153: INFO: Got endpoints: latency-svc-2jl7j [752.787272ms]
Aug 14 06:58:43.163: INFO: Created: latency-svc-ttfb8
Aug 14 06:58:43.199: INFO: Got endpoints: latency-svc-lc8th [749.91658ms]
Aug 14 06:58:43.210: INFO: Created: latency-svc-bk5qr
Aug 14 06:58:43.249: INFO: Got endpoints: latency-svc-nhzpc [749.914998ms]
Aug 14 06:58:43.262: INFO: Created: latency-svc-zlmsx
Aug 14 06:58:43.300: INFO: Got endpoints: latency-svc-l2l4w [751.296ms]
Aug 14 06:58:43.317: INFO: Created: latency-svc-zgnzm
Aug 14 06:58:43.349: INFO: Got endpoints: latency-svc-nvhz5 [749.945479ms]
Aug 14 06:58:43.368: INFO: Created: latency-svc-dlwk4
Aug 14 06:58:43.451: INFO: Got endpoints: latency-svc-h497w [751.519066ms]
Aug 14 06:58:43.451: INFO: Got endpoints: latency-svc-l7chc [801.13057ms]
Aug 14 06:58:43.462: INFO: Created: latency-svc-lqhrx
Aug 14 06:58:43.499: INFO: Got endpoints: latency-svc-mw9m6 [749.738992ms]
Aug 14 06:58:43.550: INFO: Got endpoints: latency-svc-t9wd8 [749.403424ms]
Aug 14 06:58:43.600: INFO: Got endpoints: latency-svc-df42k [750.322108ms]
Aug 14 06:58:43.649: INFO: Got endpoints: latency-svc-wlm7m [749.786766ms]
Aug 14 06:58:43.700: INFO: Got endpoints: latency-svc-bhd76 [750.662452ms]
Aug 14 06:58:43.749: INFO: Got endpoints: latency-svc-h69xt [750.329926ms]
Aug 14 06:58:43.799: INFO: Got endpoints: latency-svc-krdxc [750.761364ms]
Aug 14 06:58:43.850: INFO: Got endpoints: latency-svc-lp9tv [750.33032ms]
Aug 14 06:58:43.900: INFO: Got endpoints: latency-svc-ttfb8 [747.022365ms]
Aug 14 06:58:43.949: INFO: Got endpoints: latency-svc-bk5qr [750.475357ms]
Aug 14 06:58:43.999: INFO: Got endpoints: latency-svc-zlmsx [750.21223ms]
Aug 14 06:58:44.049: INFO: Got endpoints: latency-svc-zgnzm [748.830046ms]
Aug 14 06:58:44.101: INFO: Got endpoints: latency-svc-dlwk4 [751.732332ms]
Aug 14 06:58:44.150: INFO: Got endpoints: latency-svc-lqhrx [698.989036ms]
Aug 14 06:58:44.150: INFO: Latencies: [18.289588ms 18.578502ms 22.713971ms 28.137054ms 38.139497ms 41.979511ms 48.311973ms 54.900795ms 62.037807ms 67.855837ms 82.709488ms 88.867126ms 98.417978ms 104.847995ms 106.400225ms 107.502004ms 108.726391ms 109.428473ms 110.001546ms 110.864024ms 111.355637ms 111.379642ms 112.453214ms 112.988966ms 114.306726ms 114.982817ms 115.481431ms 115.827733ms 115.884191ms 116.137791ms 120.183557ms 120.500994ms 121.85302ms 125.590371ms 153.458348ms 187.081226ms 235.642053ms 279.281023ms 319.003526ms 361.041672ms 404.338424ms 446.631521ms 488.808986ms 533.196741ms 572.761877ms 615.72512ms 657.989931ms 693.638941ms 698.989036ms 700.665884ms 739.783968ms 741.977611ms 743.52176ms 745.966865ms 746.20487ms 746.466606ms 746.598672ms 746.649135ms 746.712117ms 747.022365ms 747.195291ms 748.319337ms 748.322922ms 748.417844ms 748.509725ms 748.546564ms 748.547918ms 748.585501ms 748.784519ms 748.830046ms 748.859002ms 748.873944ms 748.87952ms 748.898865ms 748.902552ms 748.97232ms 749.000375ms 749.02455ms 749.115713ms 749.1193ms 749.248051ms 749.258617ms 749.266912ms 749.296147ms 749.403424ms 749.420641ms 749.452434ms 749.509763ms 749.517939ms 749.554961ms 749.573881ms 749.598225ms 749.603099ms 749.604026ms 749.62159ms 749.621677ms 749.633212ms 749.635261ms 749.637601ms 749.670574ms 749.696913ms 749.699797ms 749.711472ms 749.738992ms 749.764952ms 749.77125ms 749.786766ms 749.82366ms 749.862053ms 749.867ms 749.874734ms 749.88625ms 749.886488ms 749.914998ms 749.91658ms 749.920108ms 749.924916ms 749.927399ms 749.929175ms 749.93567ms 749.942546ms 749.945479ms 749.952537ms 749.96617ms 749.998352ms 750.019003ms 750.044407ms 750.056713ms 750.071413ms 750.083459ms 750.093465ms 750.103589ms 750.13001ms 750.144412ms 750.157443ms 750.167206ms 750.194616ms 750.20476ms 750.21223ms 750.225119ms 750.241653ms 750.257168ms 750.281243ms 750.289916ms 750.29062ms 750.316694ms 750.322108ms 750.329926ms 750.33032ms 750.342056ms 750.347816ms 750.396386ms 750.410066ms 750.411167ms 750.42067ms 750.428548ms 750.475357ms 750.487989ms 750.492138ms 750.509595ms 750.530512ms 750.542299ms 750.54666ms 750.5535ms 750.55423ms 750.56258ms 750.593814ms 750.605132ms 750.660535ms 750.662452ms 750.698602ms 750.735398ms 750.761364ms 750.763891ms 750.775632ms 750.84716ms 750.89808ms 750.906824ms 750.950081ms 751.049346ms 751.082204ms 751.095277ms 751.200874ms 751.228927ms 751.234291ms 751.273869ms 751.296ms 751.519066ms 751.732332ms 752.000839ms 752.28512ms 752.299953ms 752.787272ms 753.147917ms 753.328016ms 754.055729ms 756.49645ms 758.331648ms 801.13057ms 806.342905ms]
Aug 14 06:58:44.150: INFO: 50 %ile: 749.696913ms
Aug 14 06:58:44.150: INFO: 90 %ile: 751.082204ms
Aug 14 06:58:44.150: INFO: 99 %ile: 801.13057ms
Aug 14 06:58:44.151: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:58:44.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-4445" for this suite.
Aug 14 06:59:06.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:59:06.281: INFO: namespace svc-latency-4445 deletion completed in 22.126684106s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:59:06.284: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7755
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 14 06:59:06.438: INFO: Waiting up to 5m0s for pod "downward-api-7ebe9e5b-667e-44ff-a414-27f11823192b" in namespace "downward-api-7755" to be "success or failure"
Aug 14 06:59:06.441: INFO: Pod "downward-api-7ebe9e5b-667e-44ff-a414-27f11823192b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.822996ms
Aug 14 06:59:08.447: INFO: Pod "downward-api-7ebe9e5b-667e-44ff-a414-27f11823192b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008447433s
STEP: Saw pod success
Aug 14 06:59:08.447: INFO: Pod "downward-api-7ebe9e5b-667e-44ff-a414-27f11823192b" satisfied condition "success or failure"
Aug 14 06:59:08.450: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod downward-api-7ebe9e5b-667e-44ff-a414-27f11823192b container dapi-container: <nil>
STEP: delete the pod
Aug 14 06:59:08.468: INFO: Waiting for pod downward-api-7ebe9e5b-667e-44ff-a414-27f11823192b to disappear
Aug 14 06:59:08.471: INFO: Pod downward-api-7ebe9e5b-667e-44ff-a414-27f11823192b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:59:08.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7755" for this suite.
Aug 14 06:59:14.488: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:59:14.603: INFO: namespace downward-api-7755 deletion completed in 6.127978813s
â€¢SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:59:14.603: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8483
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 14 06:59:14.759: INFO: Waiting up to 5m0s for pod "pod-95a2e40e-9735-447f-a26a-616654fe3fe2" in namespace "emptydir-8483" to be "success or failure"
Aug 14 06:59:14.765: INFO: Pod "pod-95a2e40e-9735-447f-a26a-616654fe3fe2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.154635ms
Aug 14 06:59:16.770: INFO: Pod "pod-95a2e40e-9735-447f-a26a-616654fe3fe2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01035319s
STEP: Saw pod success
Aug 14 06:59:16.770: INFO: Pod "pod-95a2e40e-9735-447f-a26a-616654fe3fe2" satisfied condition "success or failure"
Aug 14 06:59:16.773: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-95a2e40e-9735-447f-a26a-616654fe3fe2 container test-container: <nil>
STEP: delete the pod
Aug 14 06:59:16.791: INFO: Waiting for pod pod-95a2e40e-9735-447f-a26a-616654fe3fe2 to disappear
Aug 14 06:59:16.794: INFO: Pod pod-95a2e40e-9735-447f-a26a-616654fe3fe2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:59:16.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8483" for this suite.
Aug 14 06:59:22.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:59:22.915: INFO: namespace emptydir-8483 deletion completed in 6.117002023s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:59:22.917: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7694
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 14 06:59:23.069: INFO: Waiting up to 5m0s for pod "pod-845da902-e134-4cd6-b8d0-f9895ca82b7c" in namespace "emptydir-7694" to be "success or failure"
Aug 14 06:59:23.083: INFO: Pod "pod-845da902-e134-4cd6-b8d0-f9895ca82b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.447371ms
Aug 14 06:59:25.088: INFO: Pod "pod-845da902-e134-4cd6-b8d0-f9895ca82b7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018941698s
Aug 14 06:59:27.093: INFO: Pod "pod-845da902-e134-4cd6-b8d0-f9895ca82b7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024066382s
STEP: Saw pod success
Aug 14 06:59:27.094: INFO: Pod "pod-845da902-e134-4cd6-b8d0-f9895ca82b7c" satisfied condition "success or failure"
Aug 14 06:59:27.097: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-845da902-e134-4cd6-b8d0-f9895ca82b7c container test-container: <nil>
STEP: delete the pod
Aug 14 06:59:27.116: INFO: Waiting for pod pod-845da902-e134-4cd6-b8d0-f9895ca82b7c to disappear
Aug 14 06:59:27.118: INFO: Pod pod-845da902-e134-4cd6-b8d0-f9895ca82b7c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:59:27.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7694" for this suite.
Aug 14 06:59:33.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:59:33.242: INFO: namespace emptydir-7694 deletion completed in 6.120582311s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:59:33.243: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2811
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Aug 14 06:59:33.940: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0814 06:59:33.940276    3092 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:59:33.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2811" for this suite.
Aug 14 06:59:39.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:59:40.075: INFO: namespace gc-2811 deletion completed in 6.130434865s
â€¢
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:59:40.075: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-4776
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-6349
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8731
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 06:59:46.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4776" for this suite.
Aug 14 06:59:52.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:59:52.709: INFO: namespace namespaces-4776 deletion completed in 6.162902904s
STEP: Destroying namespace "nsdeletetest-6349" for this suite.
Aug 14 06:59:52.712: INFO: Namespace nsdeletetest-6349 was already deleted
STEP: Destroying namespace "nsdeletetest-8731" for this suite.
Aug 14 06:59:58.723: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 06:59:58.828: INFO: namespace nsdeletetest-8731 deletion completed in 6.115849694s
â€¢SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 06:59:58.828: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5165
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-320109c7-8c2f-4cc9-a9af-264df2cb412d in namespace container-probe-5165
Aug 14 07:00:02.994: INFO: Started pod liveness-320109c7-8c2f-4cc9-a9af-264df2cb412d in namespace container-probe-5165
STEP: checking the pod's current state and verifying that restartCount is present
Aug 14 07:00:02.997: INFO: Initial restart count of pod liveness-320109c7-8c2f-4cc9-a9af-264df2cb412d is 0
Aug 14 07:00:21.043: INFO: Restart count of pod container-probe-5165/liveness-320109c7-8c2f-4cc9-a9af-264df2cb412d is now 1 (18.045922319s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:00:21.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5165" for this suite.
Aug 14 07:00:27.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:00:27.225: INFO: namespace container-probe-5165 deletion completed in 6.168815282s
â€¢SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:00:27.225: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6107
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 07:00:27.382: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1331cacc-a43a-492a-9686-194434cf2543" in namespace "downward-api-6107" to be "success or failure"
Aug 14 07:00:27.385: INFO: Pod "downwardapi-volume-1331cacc-a43a-492a-9686-194434cf2543": Phase="Pending", Reason="", readiness=false. Elapsed: 3.621681ms
Aug 14 07:00:29.391: INFO: Pod "downwardapi-volume-1331cacc-a43a-492a-9686-194434cf2543": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009043081s
STEP: Saw pod success
Aug 14 07:00:29.391: INFO: Pod "downwardapi-volume-1331cacc-a43a-492a-9686-194434cf2543" satisfied condition "success or failure"
Aug 14 07:00:29.395: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod downwardapi-volume-1331cacc-a43a-492a-9686-194434cf2543 container client-container: <nil>
STEP: delete the pod
Aug 14 07:00:29.421: INFO: Waiting for pod downwardapi-volume-1331cacc-a43a-492a-9686-194434cf2543 to disappear
Aug 14 07:00:29.424: INFO: Pod downwardapi-volume-1331cacc-a43a-492a-9686-194434cf2543 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:00:29.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6107" for this suite.
Aug 14 07:00:35.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:00:35.561: INFO: namespace downward-api-6107 deletion completed in 6.132018843s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:00:35.564: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-483
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:01:35.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-483" for this suite.
Aug 14 07:01:57.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:01:57.850: INFO: namespace container-probe-483 deletion completed in 22.121435464s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:01:57.852: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7602
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-e70c9827-e546-4198-a65e-6beba0e1ef94
STEP: Creating a pod to test consume secrets
Aug 14 07:01:58.018: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6bd9aaa2-8f2d-4bd3-b916-5ca268610578" in namespace "projected-7602" to be "success or failure"
Aug 14 07:01:58.021: INFO: Pod "pod-projected-secrets-6bd9aaa2-8f2d-4bd3-b916-5ca268610578": Phase="Pending", Reason="", readiness=false. Elapsed: 2.376716ms
Aug 14 07:02:00.027: INFO: Pod "pod-projected-secrets-6bd9aaa2-8f2d-4bd3-b916-5ca268610578": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008579353s
STEP: Saw pod success
Aug 14 07:02:00.027: INFO: Pod "pod-projected-secrets-6bd9aaa2-8f2d-4bd3-b916-5ca268610578" satisfied condition "success or failure"
Aug 14 07:02:00.030: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-projected-secrets-6bd9aaa2-8f2d-4bd3-b916-5ca268610578 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 14 07:02:00.051: INFO: Waiting for pod pod-projected-secrets-6bd9aaa2-8f2d-4bd3-b916-5ca268610578 to disappear
Aug 14 07:02:00.053: INFO: Pod pod-projected-secrets-6bd9aaa2-8f2d-4bd3-b916-5ca268610578 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:02:00.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7602" for this suite.
Aug 14 07:02:06.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:02:06.185: INFO: namespace projected-7602 deletion completed in 6.125384602s
â€¢SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:02:06.187: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5046
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Aug 14 07:02:06.343: INFO: Waiting up to 5m0s for pod "client-containers-880efaea-a04f-452d-b3b1-d0eccd756a85" in namespace "containers-5046" to be "success or failure"
Aug 14 07:02:06.348: INFO: Pod "client-containers-880efaea-a04f-452d-b3b1-d0eccd756a85": Phase="Pending", Reason="", readiness=false. Elapsed: 5.121922ms
Aug 14 07:02:08.353: INFO: Pod "client-containers-880efaea-a04f-452d-b3b1-d0eccd756a85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010162911s
STEP: Saw pod success
Aug 14 07:02:08.353: INFO: Pod "client-containers-880efaea-a04f-452d-b3b1-d0eccd756a85" satisfied condition "success or failure"
Aug 14 07:02:08.356: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod client-containers-880efaea-a04f-452d-b3b1-d0eccd756a85 container test-container: <nil>
STEP: delete the pod
Aug 14 07:02:08.374: INFO: Waiting for pod client-containers-880efaea-a04f-452d-b3b1-d0eccd756a85 to disappear
Aug 14 07:02:08.377: INFO: Pod client-containers-880efaea-a04f-452d-b3b1-d0eccd756a85 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:02:08.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5046" for this suite.
Aug 14 07:02:14.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:02:14.542: INFO: namespace containers-5046 deletion completed in 6.159202508s
â€¢SSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:02:14.543: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-1625
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Aug 14 07:02:16.709: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-f1461d8a-d44b-47f7-bade-161e76ef41ae,GenerateName:,Namespace:events-1625,SelfLink:/api/v1/namespaces/events-1625/pods/send-events-f1461d8a-d44b-47f7-bade-161e76ef41ae,UID:fe8e5ffb-8407-472c-9d2c-8d5517a2e967,ResourceVersion:12956,Generation:0,CreationTimestamp:2019-08-14 07:02:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 688330928,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.90/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-t7nn6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-t7nn6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-t7nn6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d388f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d38910}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:02:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:02:15 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:02:15 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:02:14 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:100.96.1.90,StartTime:2019-08-14 07:02:14 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-08-14 07:02:15 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://025435c26c6e5a422e8fc74195f672a77257f76cca8cab13ec46e9b67dae523e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Aug 14 07:02:18.715: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Aug 14 07:02:20.719: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:02:20.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1625" for this suite.
Aug 14 07:03:02.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:03:02.891: INFO: namespace events-1625 deletion completed in 42.162574899s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:03:02.893: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-6766
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-bd9f04bf-ddb4-4be3-b5de-af91a388e719 in namespace container-probe-6766
Aug 14 07:03:05.060: INFO: Started pod liveness-bd9f04bf-ddb4-4be3-b5de-af91a388e719 in namespace container-probe-6766
STEP: checking the pod's current state and verifying that restartCount is present
Aug 14 07:03:05.062: INFO: Initial restart count of pod liveness-bd9f04bf-ddb4-4be3-b5de-af91a388e719 is 0
Aug 14 07:03:21.102: INFO: Restart count of pod container-probe-6766/liveness-bd9f04bf-ddb4-4be3-b5de-af91a388e719 is now 1 (16.039396621s elapsed)
Aug 14 07:03:41.149: INFO: Restart count of pod container-probe-6766/liveness-bd9f04bf-ddb4-4be3-b5de-af91a388e719 is now 2 (36.086491967s elapsed)
Aug 14 07:04:01.197: INFO: Restart count of pod container-probe-6766/liveness-bd9f04bf-ddb4-4be3-b5de-af91a388e719 is now 3 (56.134677359s elapsed)
Aug 14 07:04:21.247: INFO: Restart count of pod container-probe-6766/liveness-bd9f04bf-ddb4-4be3-b5de-af91a388e719 is now 4 (1m16.184905167s elapsed)
Aug 14 07:05:35.415: INFO: Restart count of pod container-probe-6766/liveness-bd9f04bf-ddb4-4be3-b5de-af91a388e719 is now 5 (2m30.353109493s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:05:35.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6766" for this suite.
Aug 14 07:05:41.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:05:41.596: INFO: namespace container-probe-6766 deletion completed in 6.165356649s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:05:41.598: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2474
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Aug 14 07:05:43.766: INFO: Pod pod-hostip-587e2cb0-28d5-44b6-bd7c-41df496e20d2 has hostIP: 10.250.0.8
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:05:43.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2474" for this suite.
Aug 14 07:06:05.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:06:05.892: INFO: namespace pods-2474 deletion completed in 22.121516381s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:06:05.893: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8899
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 07:06:06.049: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 14 07:06:11.055: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 14 07:06:11.055: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 14 07:06:13.081: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-8899,SelfLink:/apis/apps/v1/namespaces/deployment-8899/deployments/test-cleanup-deployment,UID:a141584c-051a-474c-8842-0e55d05fe5a4,ResourceVersion:13700,Generation:1,CreationTimestamp:2019-08-14 07:06:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-14 07:06:11 +0000 UTC 2019-08-14 07:06:11 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-14 07:06:12 +0000 UTC 2019-08-14 07:06:11 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 14 07:06:13.084: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-8899,SelfLink:/apis/apps/v1/namespaces/deployment-8899/replicasets/test-cleanup-deployment-55bbcbc84c,UID:ca79d0cb-43fc-4b2a-84bc-65cea51a3ffb,ResourceVersion:13693,Generation:1,CreationTimestamp:2019-08-14 07:06:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment a141584c-051a-474c-8842-0e55d05fe5a4 0xc001d39017 0xc001d39018}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 14 07:06:13.088: INFO: Pod "test-cleanup-deployment-55bbcbc84c-s929h" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-s929h,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-8899,SelfLink:/api/v1/namespaces/deployment-8899/pods/test-cleanup-deployment-55bbcbc84c-s929h,UID:e8f48677-29f1-4cf7-8047-9a70cef37ef2,ResourceVersion:13692,Generation:0,CreationTimestamp:2019-08-14 07:06:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.94/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c ca79d0cb-43fc-4b2a-84bc-65cea51a3ffb 0xc001d39897 0xc001d39898}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-cr9fl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-cr9fl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-cr9fl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001d39910} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001d39930}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:06:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:06:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:06:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:06:11 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:100.96.1.94,StartTime:2019-08-14 07:06:11 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-14 07:06:11 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://2407fd4e62c43b86528c1793b4e55bf7dc0c6db57d41bae32b1f081ad23c7631}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:06:13.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8899" for this suite.
Aug 14 07:06:19.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:06:19.246: INFO: namespace deployment-8899 deletion completed in 6.154266806s
â€¢SSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:06:19.246: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2895
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-778550f6-bde0-45a8-a94f-7f038cad777b
STEP: Creating a pod to test consume secrets
Aug 14 07:06:19.403: INFO: Waiting up to 5m0s for pod "pod-secrets-7d987be4-048b-49cc-b690-49758eb8d0c2" in namespace "secrets-2895" to be "success or failure"
Aug 14 07:06:19.405: INFO: Pod "pod-secrets-7d987be4-048b-49cc-b690-49758eb8d0c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.430365ms
Aug 14 07:06:21.409: INFO: Pod "pod-secrets-7d987be4-048b-49cc-b690-49758eb8d0c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006094712s
STEP: Saw pod success
Aug 14 07:06:21.409: INFO: Pod "pod-secrets-7d987be4-048b-49cc-b690-49758eb8d0c2" satisfied condition "success or failure"
Aug 14 07:06:21.412: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-secrets-7d987be4-048b-49cc-b690-49758eb8d0c2 container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 07:06:21.430: INFO: Waiting for pod pod-secrets-7d987be4-048b-49cc-b690-49758eb8d0c2 to disappear
Aug 14 07:06:21.433: INFO: Pod pod-secrets-7d987be4-048b-49cc-b690-49758eb8d0c2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:06:21.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2895" for this suite.
Aug 14 07:06:27.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:06:27.559: INFO: namespace secrets-2895 deletion completed in 6.119347246s
â€¢SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:06:27.559: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3865
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 07:06:27.717: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1470b754-cd1e-451a-915b-85f61ae33fb5" in namespace "downward-api-3865" to be "success or failure"
Aug 14 07:06:27.720: INFO: Pod "downwardapi-volume-1470b754-cd1e-451a-915b-85f61ae33fb5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.281083ms
Aug 14 07:06:29.724: INFO: Pod "downwardapi-volume-1470b754-cd1e-451a-915b-85f61ae33fb5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007475669s
STEP: Saw pod success
Aug 14 07:06:29.724: INFO: Pod "downwardapi-volume-1470b754-cd1e-451a-915b-85f61ae33fb5" satisfied condition "success or failure"
Aug 14 07:06:29.727: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod downwardapi-volume-1470b754-cd1e-451a-915b-85f61ae33fb5 container client-container: <nil>
STEP: delete the pod
Aug 14 07:06:29.749: INFO: Waiting for pod downwardapi-volume-1470b754-cd1e-451a-915b-85f61ae33fb5 to disappear
Aug 14 07:06:29.752: INFO: Pod downwardapi-volume-1470b754-cd1e-451a-915b-85f61ae33fb5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:06:29.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3865" for this suite.
Aug 14 07:06:35.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:06:35.885: INFO: namespace downward-api-3865 deletion completed in 6.129004429s
â€¢SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:06:35.885: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6373
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Aug 14 07:06:36.032: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 14 07:06:36.040: INFO: Waiting for terminating namespaces to be deleted...
Aug 14 07:06:36.044: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 before test
Aug 14 07:06:36.052: INFO: node-exporter-dzrg2 from kube-system started at 2019-08-14 06:11:41 +0000 UTC (1 container statuses recorded)
Aug 14 07:06:36.052: INFO: 	Container node-exporter ready: true, restart count 1
Aug 14 07:06:36.053: INFO: calico-node-8jgsw from kube-system started at 2019-08-14 06:11:41 +0000 UTC (1 container statuses recorded)
Aug 14 07:06:36.053: INFO: 	Container calico-node ready: true, restart count 0
Aug 14 07:06:36.053: INFO: kube-proxy-2j2fb from kube-system started at 2019-08-14 06:11:41 +0000 UTC (1 container statuses recorded)
Aug 14 07:06:36.053: INFO: 	Container kube-proxy ready: true, restart count 1
Aug 14 07:06:36.053: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx before test
Aug 14 07:06:36.067: INFO: blackbox-exporter-954dd954b-k8qsk from kube-system started at 2019-08-14 06:11:39 +0000 UTC (1 container statuses recorded)
Aug 14 07:06:36.067: INFO: 	Container blackbox-exporter ready: true, restart count 0
Aug 14 07:06:36.067: INFO: node-exporter-lscjk from kube-system started at 2019-08-14 06:11:39 +0000 UTC (1 container statuses recorded)
Aug 14 07:06:36.068: INFO: 	Container node-exporter ready: true, restart count 0
Aug 14 07:06:36.068: INFO: metrics-server-5cd447944f-dtjpr from kube-system started at 2019-08-14 06:12:09 +0000 UTC (1 container statuses recorded)
Aug 14 07:06:36.068: INFO: 	Container metrics-server ready: true, restart count 0
Aug 14 07:06:36.068: INFO: vpn-shoot-66d7fd98fb-kzprj from kube-system started at 2019-08-14 06:12:09 +0000 UTC (1 container statuses recorded)
Aug 14 07:06:36.068: INFO: 	Container vpn-shoot ready: true, restart count 0
Aug 14 07:06:36.068: INFO: coredns-85cc454dd8-lthzw from kube-system started at 2019-08-14 06:12:09 +0000 UTC (1 container statuses recorded)
Aug 14 07:06:36.068: INFO: 	Container coredns ready: true, restart count 0
Aug 14 07:06:36.068: INFO: calico-kube-controllers-5f4b46ffb5-hhmsh from kube-system started at 2019-08-14 06:12:09 +0000 UTC (1 container statuses recorded)
Aug 14 07:06:36.068: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 14 07:06:36.068: INFO: addons-kubernetes-dashboard-5c8d9945bc-dfqc7 from kube-system started at 2019-08-14 06:12:09 +0000 UTC (1 container statuses recorded)
Aug 14 07:06:36.068: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Aug 14 07:06:36.068: INFO: kube-proxy-mrtvm from kube-system started at 2019-08-14 06:11:39 +0000 UTC (1 container statuses recorded)
Aug 14 07:06:36.068: INFO: 	Container kube-proxy ready: true, restart count 1
Aug 14 07:06:36.068: INFO: coredns-85cc454dd8-275mp from kube-system started at 2019-08-14 06:12:11 +0000 UTC (1 container statuses recorded)
Aug 14 07:06:36.068: INFO: 	Container coredns ready: true, restart count 0
Aug 14 07:06:36.068: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-88d6cff74-4rp7v from kube-system started at 2019-08-14 06:12:09 +0000 UTC (1 container statuses recorded)
Aug 14 07:06:36.068: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Aug 14 07:06:36.068: INFO: calico-node-xwxqx from kube-system started at 2019-08-14 06:11:39 +0000 UTC (1 container statuses recorded)
Aug 14 07:06:36.068: INFO: 	Container calico-node ready: true, restart count 0
Aug 14 07:06:36.068: INFO: addons-nginx-ingress-controller-6496d947df-xxr56 from kube-system started at 2019-08-14 06:12:11 +0000 UTC (1 container statuses recorded)
Aug 14 07:06:36.068: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2
STEP: verifying the node has the label node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx
Aug 14 07:06:36.105: INFO: Pod addons-kubernetes-dashboard-5c8d9945bc-dfqc7 requesting resource cpu=50m on Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx
Aug 14 07:06:36.105: INFO: Pod addons-nginx-ingress-controller-6496d947df-xxr56 requesting resource cpu=100m on Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx
Aug 14 07:06:36.105: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-88d6cff74-4rp7v requesting resource cpu=0m on Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx
Aug 14 07:06:36.105: INFO: Pod blackbox-exporter-954dd954b-k8qsk requesting resource cpu=5m on Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx
Aug 14 07:06:36.105: INFO: Pod calico-kube-controllers-5f4b46ffb5-hhmsh requesting resource cpu=0m on Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx
Aug 14 07:06:36.105: INFO: Pod calico-node-8jgsw requesting resource cpu=100m on Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2
Aug 14 07:06:36.105: INFO: Pod calico-node-xwxqx requesting resource cpu=100m on Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx
Aug 14 07:06:36.105: INFO: Pod coredns-85cc454dd8-275mp requesting resource cpu=50m on Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx
Aug 14 07:06:36.105: INFO: Pod coredns-85cc454dd8-lthzw requesting resource cpu=50m on Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx
Aug 14 07:06:36.105: INFO: Pod kube-proxy-2j2fb requesting resource cpu=20m on Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2
Aug 14 07:06:36.105: INFO: Pod kube-proxy-mrtvm requesting resource cpu=20m on Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx
Aug 14 07:06:36.105: INFO: Pod metrics-server-5cd447944f-dtjpr requesting resource cpu=20m on Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx
Aug 14 07:06:36.105: INFO: Pod node-exporter-dzrg2 requesting resource cpu=5m on Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2
Aug 14 07:06:36.105: INFO: Pod node-exporter-lscjk requesting resource cpu=5m on Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx
Aug 14 07:06:36.105: INFO: Pod vpn-shoot-66d7fd98fb-kzprj requesting resource cpu=100m on Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-19381abd-cd18-47a0-af65-04003d59826a.15bab83dd1aea797], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6373/filler-pod-19381abd-cd18-47a0-af65-04003d59826a to shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-19381abd-cd18-47a0-af65-04003d59826a.15bab83df7be09ca], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-19381abd-cd18-47a0-af65-04003d59826a.15bab83dfad8d2cd], Reason = [Created], Message = [Created container filler-pod-19381abd-cd18-47a0-af65-04003d59826a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-19381abd-cd18-47a0-af65-04003d59826a.15bab83e00d9521a], Reason = [Started], Message = [Started container filler-pod-19381abd-cd18-47a0-af65-04003d59826a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fc778b72-5ed1-43f3-b750-2320d92f4ae1.15bab83dd207043b], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6373/filler-pod-fc778b72-5ed1-43f3-b750-2320d92f4ae1 to shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fc778b72-5ed1-43f3-b750-2320d92f4ae1.15bab83df88490c1], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fc778b72-5ed1-43f3-b750-2320d92f4ae1.15bab83dfc15eba2], Reason = [Created], Message = [Created container filler-pod-fc778b72-5ed1-43f3-b750-2320d92f4ae1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-fc778b72-5ed1-43f3-b750-2320d92f4ae1.15bab83e0311a849], Reason = [Started], Message = [Started container filler-pod-fc778b72-5ed1-43f3-b750-2320d92f4ae1]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15bab83e4aa5c639], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:06:39.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6373" for this suite.
Aug 14 07:06:45.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:06:45.286: INFO: namespace sched-pred-6373 deletion completed in 6.112630223s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72
â€¢SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:06:45.287: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9784
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Aug 14 07:06:45.432: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy --unix-socket=/tmp/kubectl-proxy-unix594404805/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:06:45.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9784" for this suite.
Aug 14 07:06:51.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:06:51.623: INFO: namespace kubectl-9784 deletion completed in 6.122504363s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:06:51.625: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-6163
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Aug 14 07:06:51.781: INFO: Waiting up to 5m0s for pod "var-expansion-af8a39a5-2671-4a1a-b705-54e945d2f7f7" in namespace "var-expansion-6163" to be "success or failure"
Aug 14 07:06:51.786: INFO: Pod "var-expansion-af8a39a5-2671-4a1a-b705-54e945d2f7f7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.918732ms
Aug 14 07:06:53.791: INFO: Pod "var-expansion-af8a39a5-2671-4a1a-b705-54e945d2f7f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009514787s
STEP: Saw pod success
Aug 14 07:06:53.791: INFO: Pod "var-expansion-af8a39a5-2671-4a1a-b705-54e945d2f7f7" satisfied condition "success or failure"
Aug 14 07:06:53.794: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod var-expansion-af8a39a5-2671-4a1a-b705-54e945d2f7f7 container dapi-container: <nil>
STEP: delete the pod
Aug 14 07:06:53.810: INFO: Waiting for pod var-expansion-af8a39a5-2671-4a1a-b705-54e945d2f7f7 to disappear
Aug 14 07:06:53.813: INFO: Pod var-expansion-af8a39a5-2671-4a1a-b705-54e945d2f7f7 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:06:53.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6163" for this suite.
Aug 14 07:06:59.831: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:06:59.958: INFO: namespace var-expansion-6163 deletion completed in 6.140751344s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:06:59.960: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-806
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 07:07:00.124: INFO: (0) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.147062ms)
Aug 14 07:07:00.166: INFO: (1) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 41.742468ms)
Aug 14 07:07:00.172: INFO: (2) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.851605ms)
Aug 14 07:07:00.177: INFO: (3) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.101492ms)
Aug 14 07:07:00.182: INFO: (4) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.999237ms)
Aug 14 07:07:00.188: INFO: (5) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.431304ms)
Aug 14 07:07:00.193: INFO: (6) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.283212ms)
Aug 14 07:07:00.198: INFO: (7) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.120625ms)
Aug 14 07:07:00.204: INFO: (8) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.782081ms)
Aug 14 07:07:00.210: INFO: (9) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.749193ms)
Aug 14 07:07:00.216: INFO: (10) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.254416ms)
Aug 14 07:07:00.222: INFO: (11) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.46604ms)
Aug 14 07:07:00.229: INFO: (12) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.220015ms)
Aug 14 07:07:00.235: INFO: (13) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.552773ms)
Aug 14 07:07:00.242: INFO: (14) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.758628ms)
Aug 14 07:07:00.249: INFO: (15) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.804129ms)
Aug 14 07:07:00.256: INFO: (16) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.721085ms)
Aug 14 07:07:00.262: INFO: (17) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.864215ms)
Aug 14 07:07:00.268: INFO: (18) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.901241ms)
Aug 14 07:07:00.273: INFO: (19) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.617454ms)
[AfterEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:07:00.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-806" for this suite.
Aug 14 07:07:06.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:07:06.394: INFO: namespace proxy-806 deletion completed in 6.116657365s
â€¢SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:07:06.395: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3132
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 14 07:07:12.580: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 07:07:12.584: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 07:07:14.584: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 07:07:14.588: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 07:07:16.584: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 07:07:16.588: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 07:07:18.584: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 07:07:18.593: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 07:07:20.584: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 07:07:20.589: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 07:07:22.584: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 07:07:22.589: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 07:07:24.584: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 07:07:24.589: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 07:07:26.584: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 07:07:26.589: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 07:07:28.584: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 07:07:28.589: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 07:07:30.584: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 07:07:30.589: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 14 07:07:32.584: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 14 07:07:32.589: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:07:32.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3132" for this suite.
Aug 14 07:07:54.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:07:54.770: INFO: namespace container-lifecycle-hook-3132 deletion completed in 22.165518168s
â€¢SSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:07:54.771: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-9720
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:07:56.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9720" for this suite.
Aug 14 07:08:02.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:08:03.102: INFO: namespace emptydir-wrapper-9720 deletion completed in 6.119492941s
â€¢SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:08:03.102: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9166
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 14 07:08:03.255: INFO: Waiting up to 5m0s for pod "pod-6a344fb1-6e4f-4557-ad6c-55a6fcd34cbb" in namespace "emptydir-9166" to be "success or failure"
Aug 14 07:08:03.258: INFO: Pod "pod-6a344fb1-6e4f-4557-ad6c-55a6fcd34cbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.849024ms
Aug 14 07:08:05.263: INFO: Pod "pod-6a344fb1-6e4f-4557-ad6c-55a6fcd34cbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007775762s
STEP: Saw pod success
Aug 14 07:08:05.263: INFO: Pod "pod-6a344fb1-6e4f-4557-ad6c-55a6fcd34cbb" satisfied condition "success or failure"
Aug 14 07:08:05.266: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-6a344fb1-6e4f-4557-ad6c-55a6fcd34cbb container test-container: <nil>
STEP: delete the pod
Aug 14 07:08:05.287: INFO: Waiting for pod pod-6a344fb1-6e4f-4557-ad6c-55a6fcd34cbb to disappear
Aug 14 07:08:05.289: INFO: Pod pod-6a344fb1-6e4f-4557-ad6c-55a6fcd34cbb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:08:05.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9166" for this suite.
Aug 14 07:08:11.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:08:11.415: INFO: namespace emptydir-9166 deletion completed in 6.121688904s
â€¢SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:08:11.416: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9728
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Aug 14 07:08:11.571: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9728,SelfLink:/api/v1/namespaces/watch-9728/configmaps/e2e-watch-test-configmap-a,UID:b4ae3d30-a7cf-41dc-96e3-b9b63111768a,ResourceVersion:14214,Generation:0,CreationTimestamp:2019-08-14 07:08:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 14 07:08:11.572: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9728,SelfLink:/api/v1/namespaces/watch-9728/configmaps/e2e-watch-test-configmap-a,UID:b4ae3d30-a7cf-41dc-96e3-b9b63111768a,ResourceVersion:14214,Generation:0,CreationTimestamp:2019-08-14 07:08:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Aug 14 07:08:21.580: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9728,SelfLink:/api/v1/namespaces/watch-9728/configmaps/e2e-watch-test-configmap-a,UID:b4ae3d30-a7cf-41dc-96e3-b9b63111768a,ResourceVersion:14237,Generation:0,CreationTimestamp:2019-08-14 07:08:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Aug 14 07:08:21.581: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9728,SelfLink:/api/v1/namespaces/watch-9728/configmaps/e2e-watch-test-configmap-a,UID:b4ae3d30-a7cf-41dc-96e3-b9b63111768a,ResourceVersion:14237,Generation:0,CreationTimestamp:2019-08-14 07:08:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Aug 14 07:08:31.595: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9728,SelfLink:/api/v1/namespaces/watch-9728/configmaps/e2e-watch-test-configmap-a,UID:b4ae3d30-a7cf-41dc-96e3-b9b63111768a,ResourceVersion:14261,Generation:0,CreationTimestamp:2019-08-14 07:08:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 14 07:08:31.597: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9728,SelfLink:/api/v1/namespaces/watch-9728/configmaps/e2e-watch-test-configmap-a,UID:b4ae3d30-a7cf-41dc-96e3-b9b63111768a,ResourceVersion:14261,Generation:0,CreationTimestamp:2019-08-14 07:08:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Aug 14 07:08:41.603: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9728,SelfLink:/api/v1/namespaces/watch-9728/configmaps/e2e-watch-test-configmap-a,UID:b4ae3d30-a7cf-41dc-96e3-b9b63111768a,ResourceVersion:14284,Generation:0,CreationTimestamp:2019-08-14 07:08:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 14 07:08:41.603: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9728,SelfLink:/api/v1/namespaces/watch-9728/configmaps/e2e-watch-test-configmap-a,UID:b4ae3d30-a7cf-41dc-96e3-b9b63111768a,ResourceVersion:14284,Generation:0,CreationTimestamp:2019-08-14 07:08:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Aug 14 07:08:51.609: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9728,SelfLink:/api/v1/namespaces/watch-9728/configmaps/e2e-watch-test-configmap-b,UID:4a30024e-2a1d-4674-8426-03a52b89e204,ResourceVersion:14308,Generation:0,CreationTimestamp:2019-08-14 07:08:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 14 07:08:51.609: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9728,SelfLink:/api/v1/namespaces/watch-9728/configmaps/e2e-watch-test-configmap-b,UID:4a30024e-2a1d-4674-8426-03a52b89e204,ResourceVersion:14308,Generation:0,CreationTimestamp:2019-08-14 07:08:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Aug 14 07:09:01.616: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9728,SelfLink:/api/v1/namespaces/watch-9728/configmaps/e2e-watch-test-configmap-b,UID:4a30024e-2a1d-4674-8426-03a52b89e204,ResourceVersion:14331,Generation:0,CreationTimestamp:2019-08-14 07:08:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 14 07:09:01.616: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9728,SelfLink:/api/v1/namespaces/watch-9728/configmaps/e2e-watch-test-configmap-b,UID:4a30024e-2a1d-4674-8426-03a52b89e204,ResourceVersion:14331,Generation:0,CreationTimestamp:2019-08-14 07:08:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:09:11.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9728" for this suite.
Aug 14 07:09:17.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:09:17.744: INFO: namespace watch-9728 deletion completed in 6.121958278s
â€¢SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:09:17.746: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4382
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 07:09:17.913: INFO: Create a RollingUpdate DaemonSet
Aug 14 07:09:17.918: INFO: Check that daemon pods launch on every node of the cluster
Aug 14 07:09:17.926: INFO: Number of nodes with available pods: 0
Aug 14 07:09:17.926: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 is running more than one daemon pod
Aug 14 07:09:18.937: INFO: Number of nodes with available pods: 2
Aug 14 07:09:18.937: INFO: Number of running nodes: 2, number of available pods: 2
Aug 14 07:09:18.937: INFO: Update the DaemonSet to trigger a rollout
Aug 14 07:09:18.947: INFO: Updating DaemonSet daemon-set
Aug 14 07:09:22.963: INFO: Roll back the DaemonSet before rollout is complete
Aug 14 07:09:22.969: INFO: Updating DaemonSet daemon-set
Aug 14 07:09:22.969: INFO: Make sure DaemonSet rollback is complete
Aug 14 07:09:22.973: INFO: Wrong image for pod: daemon-set-kmdzc. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 14 07:09:22.973: INFO: Pod daemon-set-kmdzc is not available
Aug 14 07:09:23.982: INFO: Wrong image for pod: daemon-set-kmdzc. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 14 07:09:23.982: INFO: Pod daemon-set-kmdzc is not available
Aug 14 07:09:24.981: INFO: Wrong image for pod: daemon-set-kmdzc. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Aug 14 07:09:24.981: INFO: Pod daemon-set-kmdzc is not available
Aug 14 07:09:25.982: INFO: Pod daemon-set-th5m9 is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4382, will wait for the garbage collector to delete the pods
Aug 14 07:09:26.054: INFO: Deleting DaemonSet.extensions daemon-set took: 6.015014ms
Aug 14 07:09:26.454: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.341709ms
Aug 14 07:09:38.859: INFO: Number of nodes with available pods: 0
Aug 14 07:09:38.859: INFO: Number of running nodes: 0, number of available pods: 0
Aug 14 07:09:38.864: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4382/daemonsets","resourceVersion":"14503"},"items":null}

Aug 14 07:09:38.867: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4382/pods","resourceVersion":"14503"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:09:38.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4382" for this suite.
Aug 14 07:09:44.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:09:44.993: INFO: namespace daemonsets-4382 deletion completed in 6.11216242s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:09:44.994: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4839
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Aug 14 07:09:45.187: INFO: Waiting up to 5m0s for pod "pod-629c754d-12da-442d-8989-626eea60b902" in namespace "emptydir-4839" to be "success or failure"
Aug 14 07:09:45.191: INFO: Pod "pod-629c754d-12da-442d-8989-626eea60b902": Phase="Pending", Reason="", readiness=false. Elapsed: 3.588137ms
Aug 14 07:09:47.195: INFO: Pod "pod-629c754d-12da-442d-8989-626eea60b902": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008131364s
STEP: Saw pod success
Aug 14 07:09:47.195: INFO: Pod "pod-629c754d-12da-442d-8989-626eea60b902" satisfied condition "success or failure"
Aug 14 07:09:47.198: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-629c754d-12da-442d-8989-626eea60b902 container test-container: <nil>
STEP: delete the pod
Aug 14 07:09:47.219: INFO: Waiting for pod pod-629c754d-12da-442d-8989-626eea60b902 to disappear
Aug 14 07:09:47.222: INFO: Pod pod-629c754d-12da-442d-8989-626eea60b902 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:09:47.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4839" for this suite.
Aug 14 07:09:53.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:09:53.341: INFO: namespace emptydir-4839 deletion completed in 6.116065584s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:09:53.342: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1686
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-1686
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 14 07:09:53.541: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 14 07:10:15.613: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.106:8080/dial?request=hostName&protocol=udp&host=100.96.1.105&port=8081&tries=1'] Namespace:pod-network-test-1686 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 07:10:15.613: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 14 07:10:20.992: INFO: Waiting for endpoints: map[]
Aug 14 07:10:20.996: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.106:8080/dial?request=hostName&protocol=udp&host=100.96.0.34&port=8081&tries=1'] Namespace:pod-network-test-1686 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 07:10:20.996: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 14 07:10:21.460: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:10:21.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1686" for this suite.
Aug 14 07:10:43.480: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:10:43.607: INFO: namespace pod-network-test-1686 deletion completed in 22.140682672s
â€¢SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:10:43.608: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6972
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 14 07:10:46.295: INFO: Successfully updated pod "labelsupdate8bbb8d04-1a1e-4700-ac93-670afadba176"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:10:50.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6972" for this suite.
Aug 14 07:11:12.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:11:12.498: INFO: namespace downward-api-6972 deletion completed in 22.163139293s
â€¢SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:11:12.499: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6560
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Aug 14 07:11:12.665: INFO: Waiting up to 5m0s for pod "pod-7f7816ba-2880-4c74-b371-40215fa0629a" in namespace "emptydir-6560" to be "success or failure"
Aug 14 07:11:12.668: INFO: Pod "pod-7f7816ba-2880-4c74-b371-40215fa0629a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.928654ms
Aug 14 07:11:14.673: INFO: Pod "pod-7f7816ba-2880-4c74-b371-40215fa0629a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007445207s
STEP: Saw pod success
Aug 14 07:11:14.673: INFO: Pod "pod-7f7816ba-2880-4c74-b371-40215fa0629a" satisfied condition "success or failure"
Aug 14 07:11:14.677: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-7f7816ba-2880-4c74-b371-40215fa0629a container test-container: <nil>
STEP: delete the pod
Aug 14 07:11:14.695: INFO: Waiting for pod pod-7f7816ba-2880-4c74-b371-40215fa0629a to disappear
Aug 14 07:11:14.698: INFO: Pod pod-7f7816ba-2880-4c74-b371-40215fa0629a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:11:14.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6560" for this suite.
Aug 14 07:11:20.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:11:20.874: INFO: namespace emptydir-6560 deletion completed in 6.170862384s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:11:20.876: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8581
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 14 07:11:21.084: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-8581'
Aug 14 07:11:21.323: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 14 07:11:21.323: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Aug 14 07:11:21.336: INFO: scanned /root for discovery docs: <nil>
Aug 14 07:11:21.336: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-8581'
Aug 14 07:11:37.113: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Aug 14 07:11:37.113: INFO: stdout: "Created e2e-test-nginx-rc-0f843f43fcee90fbf4ca83b9f37c4d58\nScaling up e2e-test-nginx-rc-0f843f43fcee90fbf4ca83b9f37c4d58 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-0f843f43fcee90fbf4ca83b9f37c4d58 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-0f843f43fcee90fbf4ca83b9f37c4d58 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Aug 14 07:11:37.113: INFO: stdout: "Created e2e-test-nginx-rc-0f843f43fcee90fbf4ca83b9f37c4d58\nScaling up e2e-test-nginx-rc-0f843f43fcee90fbf4ca83b9f37c4d58 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-0f843f43fcee90fbf4ca83b9f37c4d58 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-0f843f43fcee90fbf4ca83b9f37c4d58 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Aug 14 07:11:37.114: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-8581'
Aug 14 07:11:37.248: INFO: stderr: ""
Aug 14 07:11:37.248: INFO: stdout: "e2e-test-nginx-rc-0f843f43fcee90fbf4ca83b9f37c4d58-kkbdr "
Aug 14 07:11:37.248: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods e2e-test-nginx-rc-0f843f43fcee90fbf4ca83b9f37c4d58-kkbdr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8581'
Aug 14 07:11:37.319: INFO: stderr: ""
Aug 14 07:11:37.319: INFO: stdout: "true"
Aug 14 07:11:37.319: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods e2e-test-nginx-rc-0f843f43fcee90fbf4ca83b9f37c4d58-kkbdr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8581'
Aug 14 07:11:37.389: INFO: stderr: ""
Aug 14 07:11:37.389: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Aug 14 07:11:37.389: INFO: e2e-test-nginx-rc-0f843f43fcee90fbf4ca83b9f37c4d58-kkbdr is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Aug 14 07:11:37.390: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete rc e2e-test-nginx-rc --namespace=kubectl-8581'
Aug 14 07:11:37.468: INFO: stderr: ""
Aug 14 07:11:37.468: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:11:37.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8581" for this suite.
Aug 14 07:11:43.484: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:11:43.585: INFO: namespace kubectl-8581 deletion completed in 6.112906544s
â€¢SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:11:43.586: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7877
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:11:45.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7877" for this suite.
Aug 14 07:12:23.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:12:23.915: INFO: namespace kubelet-test-7877 deletion completed in 38.110382944s
â€¢SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:12:23.915: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4885
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4885
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 14 07:12:24.081: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 14 07:12:42.142: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.113:8080/dial?request=hostName&protocol=http&host=100.96.0.35&port=8080&tries=1'] Namespace:pod-network-test-4885 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 07:12:42.142: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 14 07:12:42.568: INFO: Waiting for endpoints: map[]
Aug 14 07:12:42.572: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.96.1.113:8080/dial?request=hostName&protocol=http&host=100.96.1.112&port=8080&tries=1'] Namespace:pod-network-test-4885 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 07:12:42.572: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 14 07:12:42.960: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:12:42.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4885" for this suite.
Aug 14 07:13:04.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:13:05.090: INFO: namespace pod-network-test-4885 deletion completed in 22.124878785s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:13:05.091: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-9750
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-9750
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9750
STEP: Deleting pre-stop pod
Aug 14 07:13:16.365: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:13:16.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9750" for this suite.
Aug 14 07:13:54.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:13:54.497: INFO: namespace prestop-9750 deletion completed in 38.121901829s
â€¢SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:13:54.499: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-9696
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0814 07:14:00.680922    3092 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 14 07:14:00.681: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:14:00.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9696" for this suite.
Aug 14 07:14:06.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:14:06.866: INFO: namespace gc-9696 deletion completed in 6.18152682s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:14:06.867: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1647
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 07:14:07.019: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f7bf751-5147-4225-96b7-0e4ac07de153" in namespace "projected-1647" to be "success or failure"
Aug 14 07:14:07.022: INFO: Pod "downwardapi-volume-3f7bf751-5147-4225-96b7-0e4ac07de153": Phase="Pending", Reason="", readiness=false. Elapsed: 3.171609ms
Aug 14 07:14:09.027: INFO: Pod "downwardapi-volume-3f7bf751-5147-4225-96b7-0e4ac07de153": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007760297s
STEP: Saw pod success
Aug 14 07:14:09.027: INFO: Pod "downwardapi-volume-3f7bf751-5147-4225-96b7-0e4ac07de153" satisfied condition "success or failure"
Aug 14 07:14:09.031: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod downwardapi-volume-3f7bf751-5147-4225-96b7-0e4ac07de153 container client-container: <nil>
STEP: delete the pod
Aug 14 07:14:09.053: INFO: Waiting for pod downwardapi-volume-3f7bf751-5147-4225-96b7-0e4ac07de153 to disappear
Aug 14 07:14:09.056: INFO: Pod downwardapi-volume-3f7bf751-5147-4225-96b7-0e4ac07de153 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:14:09.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1647" for this suite.
Aug 14 07:14:15.072: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:14:15.178: INFO: namespace projected-1647 deletion completed in 6.117858258s
â€¢SSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:14:15.178: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-432
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:14:15.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-432" for this suite.
Aug 14 07:14:21.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:14:21.527: INFO: namespace kubelet-test-432 deletion completed in 6.123040859s
â€¢SSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:14:21.527: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4265
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 07:14:21.675: INFO: Creating ReplicaSet my-hostname-basic-69dbfef3-8ef1-4126-ad7d-04a888dcc192
Aug 14 07:14:21.684: INFO: Pod name my-hostname-basic-69dbfef3-8ef1-4126-ad7d-04a888dcc192: Found 0 pods out of 1
Aug 14 07:14:26.688: INFO: Pod name my-hostname-basic-69dbfef3-8ef1-4126-ad7d-04a888dcc192: Found 1 pods out of 1
Aug 14 07:14:26.688: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-69dbfef3-8ef1-4126-ad7d-04a888dcc192" is running
Aug 14 07:14:26.691: INFO: Pod "my-hostname-basic-69dbfef3-8ef1-4126-ad7d-04a888dcc192-v4hvp" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-14 07:14:21 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-14 07:14:22 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-14 07:14:22 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-08-14 07:14:21 +0000 UTC Reason: Message:}])
Aug 14 07:14:26.691: INFO: Trying to dial the pod
Aug 14 07:14:31.784: INFO: Controller my-hostname-basic-69dbfef3-8ef1-4126-ad7d-04a888dcc192: Got expected result from replica 1 [my-hostname-basic-69dbfef3-8ef1-4126-ad7d-04a888dcc192-v4hvp]: "my-hostname-basic-69dbfef3-8ef1-4126-ad7d-04a888dcc192-v4hvp", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:14:31.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4265" for this suite.
Aug 14 07:14:37.800: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:14:37.928: INFO: namespace replicaset-4265 deletion completed in 6.139394561s
â€¢SSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:14:37.929: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7289
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 14 07:14:40.100: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:14:40.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7289" for this suite.
Aug 14 07:14:46.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:14:46.225: INFO: namespace container-runtime-7289 deletion completed in 6.112350542s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:14:46.227: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2909
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2909.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2909.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 14 07:14:59.048: INFO: DNS probes using dns-2909/dns-test-1a32bfc5-831f-4a6f-b871-2bacf1d62d19 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:14:59.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2909" for this suite.
Aug 14 07:15:05.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:15:05.233: INFO: namespace dns-2909 deletion completed in 6.169819315s
â€¢SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:15:05.235: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3652
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Aug 14 07:15:05.391: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:15:20.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3652" for this suite.
Aug 14 07:15:26.748: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:15:26.854: INFO: namespace pods-3652 deletion completed in 6.119279417s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:15:26.856: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7840
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 14 07:15:27.187: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7840'
Aug 14 07:15:27.293: INFO: stderr: ""
Aug 14 07:15:27.293: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Aug 14 07:15:27.296: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pods e2e-test-nginx-pod --namespace=kubectl-7840'
Aug 14 07:15:40.729: INFO: stderr: ""
Aug 14 07:15:40.729: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:15:40.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7840" for this suite.
Aug 14 07:15:46.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:15:46.848: INFO: namespace kubectl-7840 deletion completed in 6.114781553s
â€¢S
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:15:46.848: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3789
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Aug 14 07:15:46.998: INFO: Waiting up to 5m0s for pod "client-containers-b24c5c47-d2e4-4acf-81a7-752429336fae" in namespace "containers-3789" to be "success or failure"
Aug 14 07:15:47.001: INFO: Pod "client-containers-b24c5c47-d2e4-4acf-81a7-752429336fae": Phase="Pending", Reason="", readiness=false. Elapsed: 3.08018ms
Aug 14 07:15:49.006: INFO: Pod "client-containers-b24c5c47-d2e4-4acf-81a7-752429336fae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007654456s
STEP: Saw pod success
Aug 14 07:15:49.006: INFO: Pod "client-containers-b24c5c47-d2e4-4acf-81a7-752429336fae" satisfied condition "success or failure"
Aug 14 07:15:49.009: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod client-containers-b24c5c47-d2e4-4acf-81a7-752429336fae container test-container: <nil>
STEP: delete the pod
Aug 14 07:15:49.025: INFO: Waiting for pod client-containers-b24c5c47-d2e4-4acf-81a7-752429336fae to disappear
Aug 14 07:15:49.039: INFO: Pod client-containers-b24c5c47-d2e4-4acf-81a7-752429336fae no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:15:49.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3789" for this suite.
Aug 14 07:15:55.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:15:55.155: INFO: namespace containers-3789 deletion completed in 6.111679804s
â€¢SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:15:55.156: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-6021
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-4441
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8018
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:16:19.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6021" for this suite.
Aug 14 07:16:25.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:16:25.781: INFO: namespace namespaces-6021 deletion completed in 6.130562059s
STEP: Destroying namespace "nsdeletetest-4441" for this suite.
Aug 14 07:16:25.784: INFO: Namespace nsdeletetest-4441 was already deleted
STEP: Destroying namespace "nsdeletetest-8018" for this suite.
Aug 14 07:16:31.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:16:31.903: INFO: namespace nsdeletetest-8018 deletion completed in 6.119140158s
â€¢SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:16:31.905: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6121
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 14 07:16:32.059: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-6121'
Aug 14 07:16:32.149: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 14 07:16:32.149: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Aug 14 07:16:34.157: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete deployment e2e-test-nginx-deployment --namespace=kubectl-6121'
Aug 14 07:16:34.248: INFO: stderr: ""
Aug 14 07:16:34.248: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:16:34.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6121" for this suite.
Aug 14 07:16:40.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:16:40.381: INFO: namespace kubectl-6121 deletion completed in 6.129029515s
â€¢SSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:16:40.381: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8720
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 07:16:40.576: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 14 07:16:40.586: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 14 07:16:45.591: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Aug 14 07:16:45.591: INFO: Creating deployment "test-rolling-update-deployment"
Aug 14 07:16:45.595: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 14 07:16:45.602: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 14 07:16:47.612: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 14 07:16:47.614: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 14 07:16:47.624: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-8720,SelfLink:/apis/apps/v1/namespaces/deployment-8720/deployments/test-rolling-update-deployment,UID:24c08676-3058-446a-b7e9-92a9ec81852a,ResourceVersion:16257,Generation:1,CreationTimestamp:2019-08-14 07:16:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-08-14 07:16:45 +0000 UTC 2019-08-14 07:16:45 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-08-14 07:16:47 +0000 UTC 2019-08-14 07:16:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Aug 14 07:16:47.628: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-8720,SelfLink:/apis/apps/v1/namespaces/deployment-8720/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:9338d6d8-56c4-4d25-8326-608e708b4df9,ResourceVersion:16250,Generation:1,CreationTimestamp:2019-08-14 07:16:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 24c08676-3058-446a-b7e9-92a9ec81852a 0xc002b89157 0xc002b89158}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Aug 14 07:16:47.628: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 14 07:16:47.628: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-8720,SelfLink:/apis/apps/v1/namespaces/deployment-8720/replicasets/test-rolling-update-controller,UID:0b6c770a-4634-4532-88dd-dcec36ee1954,ResourceVersion:16256,Generation:2,CreationTimestamp:2019-08-14 07:16:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 24c08676-3058-446a-b7e9-92a9ec81852a 0xc002b89087 0xc002b89088}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 14 07:16:47.631: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-q6btm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-q6btm,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-8720,SelfLink:/api/v1/namespaces/deployment-8720/pods/test-rolling-update-deployment-79f6b9d75c-q6btm,UID:d3ec6006-68de-48ac-a7e4-5eac4c41f7bb,ResourceVersion:16249,Generation:0,CreationTimestamp:2019-08-14 07:16:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.132/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 9338d6d8-56c4-4d25-8326-608e708b4df9 0xc002b89a37 0xc002b89a38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nbkd2 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nbkd2,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-nbkd2 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002b89aa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002b89ac0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:16:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:16:47 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:16:47 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:16:45 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:100.96.1.132,StartTime:2019-08-14 07:16:45 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-08-14 07:16:46 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://ec563aa15a0f204a2a0fae6623114078a3479b7694e17339255e9184c766bf65}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:16:47.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8720" for this suite.
Aug 14 07:16:53.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:16:53.763: INFO: namespace deployment-8720 deletion completed in 6.128010897s
â€¢SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:16:53.764: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7445
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 07:16:53.918: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1e008211-5d4a-4f5f-ada2-b32ba4220789" in namespace "downward-api-7445" to be "success or failure"
Aug 14 07:16:53.922: INFO: Pod "downwardapi-volume-1e008211-5d4a-4f5f-ada2-b32ba4220789": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036328ms
Aug 14 07:16:55.927: INFO: Pod "downwardapi-volume-1e008211-5d4a-4f5f-ada2-b32ba4220789": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009219274s
STEP: Saw pod success
Aug 14 07:16:55.927: INFO: Pod "downwardapi-volume-1e008211-5d4a-4f5f-ada2-b32ba4220789" satisfied condition "success or failure"
Aug 14 07:16:55.930: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod downwardapi-volume-1e008211-5d4a-4f5f-ada2-b32ba4220789 container client-container: <nil>
STEP: delete the pod
Aug 14 07:16:55.951: INFO: Waiting for pod downwardapi-volume-1e008211-5d4a-4f5f-ada2-b32ba4220789 to disappear
Aug 14 07:16:55.954: INFO: Pod downwardapi-volume-1e008211-5d4a-4f5f-ada2-b32ba4220789 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:16:55.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7445" for this suite.
Aug 14 07:17:01.968: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:17:02.085: INFO: namespace downward-api-7445 deletion completed in 6.127627139s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:17:02.088: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6045
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Aug 14 07:17:04.258: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec pod-sharedvolume-0482d12d-d90f-46ea-a278-6aecf7a0ad27 -c busybox-main-container --namespace=emptydir-6045 -- cat /usr/share/volumeshare/shareddata.txt'
Aug 14 07:17:04.733: INFO: stderr: ""
Aug 14 07:17:04.733: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:17:04.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6045" for this suite.
Aug 14 07:17:10.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:17:10.907: INFO: namespace emptydir-6045 deletion completed in 6.169461498s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:17:10.908: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3189
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-2a4cae7c-ba49-41a0-aa07-24b8c072d3e0
STEP: Creating secret with name s-test-opt-upd-75072a5d-93d7-4ae6-a61b-41df34e10f51
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-2a4cae7c-ba49-41a0-aa07-24b8c072d3e0
STEP: Updating secret s-test-opt-upd-75072a5d-93d7-4ae6-a61b-41df34e10f51
STEP: Creating secret with name s-test-opt-create-dc9a70c6-d871-4d01-9742-dfa6e295daec
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:17:15.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3189" for this suite.
Aug 14 07:17:37.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:17:37.637: INFO: namespace projected-3189 deletion completed in 22.11657868s
â€¢S
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:17:37.637: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2861
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 07:17:37.801: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0975f60d-0329-4af1-91a1-5f8cf679e4b0" in namespace "downward-api-2861" to be "success or failure"
Aug 14 07:17:37.805: INFO: Pod "downwardapi-volume-0975f60d-0329-4af1-91a1-5f8cf679e4b0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.414493ms
Aug 14 07:17:39.810: INFO: Pod "downwardapi-volume-0975f60d-0329-4af1-91a1-5f8cf679e4b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008851136s
STEP: Saw pod success
Aug 14 07:17:39.810: INFO: Pod "downwardapi-volume-0975f60d-0329-4af1-91a1-5f8cf679e4b0" satisfied condition "success or failure"
Aug 14 07:17:39.814: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod downwardapi-volume-0975f60d-0329-4af1-91a1-5f8cf679e4b0 container client-container: <nil>
STEP: delete the pod
Aug 14 07:17:39.868: INFO: Waiting for pod downwardapi-volume-0975f60d-0329-4af1-91a1-5f8cf679e4b0 to disappear
Aug 14 07:17:39.870: INFO: Pod downwardapi-volume-0975f60d-0329-4af1-91a1-5f8cf679e4b0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:17:39.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2861" for this suite.
Aug 14 07:17:45.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:17:46.055: INFO: namespace downward-api-2861 deletion completed in 6.180935144s
â€¢SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:17:46.056: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8640
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-d6fe977d-c1a8-4969-8fa6-d6dd51b9c232
STEP: Creating a pod to test consume configMaps
Aug 14 07:17:46.213: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-850201ae-1554-485f-b806-2ab5df57a92e" in namespace "projected-8640" to be "success or failure"
Aug 14 07:17:46.216: INFO: Pod "pod-projected-configmaps-850201ae-1554-485f-b806-2ab5df57a92e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.164245ms
Aug 14 07:17:48.224: INFO: Pod "pod-projected-configmaps-850201ae-1554-485f-b806-2ab5df57a92e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010780326s
STEP: Saw pod success
Aug 14 07:17:48.224: INFO: Pod "pod-projected-configmaps-850201ae-1554-485f-b806-2ab5df57a92e" satisfied condition "success or failure"
Aug 14 07:17:48.227: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-projected-configmaps-850201ae-1554-485f-b806-2ab5df57a92e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 07:17:48.251: INFO: Waiting for pod pod-projected-configmaps-850201ae-1554-485f-b806-2ab5df57a92e to disappear
Aug 14 07:17:48.253: INFO: Pod pod-projected-configmaps-850201ae-1554-485f-b806-2ab5df57a92e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:17:48.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8640" for this suite.
Aug 14 07:17:54.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:17:54.421: INFO: namespace projected-8640 deletion completed in 6.163048533s
â€¢SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:17:54.422: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2502
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:18:15.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2502" for this suite.
Aug 14 07:18:21.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:18:21.887: INFO: namespace container-runtime-2502 deletion completed in 6.11877738s
â€¢SSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:18:21.888: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-7416
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Aug 14 07:18:26.068: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 07:18:26.071: INFO: Pod pod-with-prestop-http-hook still exists
Aug 14 07:18:28.071: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 07:18:28.076: INFO: Pod pod-with-prestop-http-hook still exists
Aug 14 07:18:30.071: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 07:18:30.076: INFO: Pod pod-with-prestop-http-hook still exists
Aug 14 07:18:32.071: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 07:18:32.076: INFO: Pod pod-with-prestop-http-hook still exists
Aug 14 07:18:34.071: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 07:18:34.076: INFO: Pod pod-with-prestop-http-hook still exists
Aug 14 07:18:36.071: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 07:18:36.076: INFO: Pod pod-with-prestop-http-hook still exists
Aug 14 07:18:38.071: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 07:18:38.076: INFO: Pod pod-with-prestop-http-hook still exists
Aug 14 07:18:40.071: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 07:18:40.076: INFO: Pod pod-with-prestop-http-hook still exists
Aug 14 07:18:42.071: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 14 07:18:42.077: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:18:42.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7416" for this suite.
Aug 14 07:19:04.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:19:04.211: INFO: namespace container-lifecycle-hook-7416 deletion completed in 22.118860945s
â€¢SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:19:04.211: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-6945
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Aug 14 07:19:04.356: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 14 07:19:04.363: INFO: Waiting for terminating namespaces to be deleted...
Aug 14 07:19:04.366: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 before test
Aug 14 07:19:04.374: INFO: node-exporter-dzrg2 from kube-system started at 2019-08-14 06:11:41 +0000 UTC (1 container statuses recorded)
Aug 14 07:19:04.374: INFO: 	Container node-exporter ready: true, restart count 1
Aug 14 07:19:04.374: INFO: kube-proxy-2j2fb from kube-system started at 2019-08-14 06:11:41 +0000 UTC (1 container statuses recorded)
Aug 14 07:19:04.374: INFO: 	Container kube-proxy ready: true, restart count 1
Aug 14 07:19:04.374: INFO: calico-node-8jgsw from kube-system started at 2019-08-14 06:11:41 +0000 UTC (1 container statuses recorded)
Aug 14 07:19:04.374: INFO: 	Container calico-node ready: true, restart count 0
Aug 14 07:19:04.374: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx before test
Aug 14 07:19:04.388: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-88d6cff74-4rp7v from kube-system started at 2019-08-14 06:12:09 +0000 UTC (1 container statuses recorded)
Aug 14 07:19:04.388: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Aug 14 07:19:04.388: INFO: calico-node-xwxqx from kube-system started at 2019-08-14 06:11:39 +0000 UTC (1 container statuses recorded)
Aug 14 07:19:04.388: INFO: 	Container calico-node ready: true, restart count 0
Aug 14 07:19:04.388: INFO: addons-nginx-ingress-controller-6496d947df-xxr56 from kube-system started at 2019-08-14 06:12:11 +0000 UTC (1 container statuses recorded)
Aug 14 07:19:04.388: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Aug 14 07:19:04.388: INFO: blackbox-exporter-954dd954b-k8qsk from kube-system started at 2019-08-14 06:11:39 +0000 UTC (1 container statuses recorded)
Aug 14 07:19:04.388: INFO: 	Container blackbox-exporter ready: true, restart count 0
Aug 14 07:19:04.388: INFO: node-exporter-lscjk from kube-system started at 2019-08-14 06:11:39 +0000 UTC (1 container statuses recorded)
Aug 14 07:19:04.388: INFO: 	Container node-exporter ready: true, restart count 0
Aug 14 07:19:04.388: INFO: metrics-server-5cd447944f-dtjpr from kube-system started at 2019-08-14 06:12:09 +0000 UTC (1 container statuses recorded)
Aug 14 07:19:04.388: INFO: 	Container metrics-server ready: true, restart count 0
Aug 14 07:19:04.388: INFO: vpn-shoot-66d7fd98fb-kzprj from kube-system started at 2019-08-14 06:12:09 +0000 UTC (1 container statuses recorded)
Aug 14 07:19:04.388: INFO: 	Container vpn-shoot ready: true, restart count 0
Aug 14 07:19:04.388: INFO: coredns-85cc454dd8-lthzw from kube-system started at 2019-08-14 06:12:09 +0000 UTC (1 container statuses recorded)
Aug 14 07:19:04.388: INFO: 	Container coredns ready: true, restart count 0
Aug 14 07:19:04.388: INFO: calico-kube-controllers-5f4b46ffb5-hhmsh from kube-system started at 2019-08-14 06:12:09 +0000 UTC (1 container statuses recorded)
Aug 14 07:19:04.388: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Aug 14 07:19:04.388: INFO: addons-kubernetes-dashboard-5c8d9945bc-dfqc7 from kube-system started at 2019-08-14 06:12:09 +0000 UTC (1 container statuses recorded)
Aug 14 07:19:04.388: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Aug 14 07:19:04.388: INFO: kube-proxy-mrtvm from kube-system started at 2019-08-14 06:11:39 +0000 UTC (1 container statuses recorded)
Aug 14 07:19:04.388: INFO: 	Container kube-proxy ready: true, restart count 1
Aug 14 07:19:04.388: INFO: coredns-85cc454dd8-275mp from kube-system started at 2019-08-14 06:12:11 +0000 UTC (1 container statuses recorded)
Aug 14 07:19:04.388: INFO: 	Container coredns ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15bab8ec0b7d74e3], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:19:05.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-6945" for this suite.
Aug 14 07:19:11.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:19:11.544: INFO: namespace sched-pred-6945 deletion completed in 6.126698577s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:19:11.548: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3193
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-c762e76e-5a67-4637-89ea-58cd1408094e
STEP: Creating a pod to test consume configMaps
Aug 14 07:19:11.703: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-66e10357-4a8f-4d88-917a-bc4aee7611fc" in namespace "projected-3193" to be "success or failure"
Aug 14 07:19:11.706: INFO: Pod "pod-projected-configmaps-66e10357-4a8f-4d88-917a-bc4aee7611fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.783259ms
Aug 14 07:19:13.712: INFO: Pod "pod-projected-configmaps-66e10357-4a8f-4d88-917a-bc4aee7611fc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009081898s
STEP: Saw pod success
Aug 14 07:19:13.712: INFO: Pod "pod-projected-configmaps-66e10357-4a8f-4d88-917a-bc4aee7611fc" satisfied condition "success or failure"
Aug 14 07:19:13.715: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-projected-configmaps-66e10357-4a8f-4d88-917a-bc4aee7611fc container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 07:19:13.733: INFO: Waiting for pod pod-projected-configmaps-66e10357-4a8f-4d88-917a-bc4aee7611fc to disappear
Aug 14 07:19:13.736: INFO: Pod pod-projected-configmaps-66e10357-4a8f-4d88-917a-bc4aee7611fc no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:19:13.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3193" for this suite.
Aug 14 07:19:19.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:19:19.863: INFO: namespace projected-3193 deletion completed in 6.122475963s
â€¢S
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:19:19.864: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-5517
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 14 07:19:22.556: INFO: Successfully updated pod "annotationupdate0d6749cb-8356-4cf6-9ff9-df43f4f26ec8"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:19:26.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5517" for this suite.
Aug 14 07:19:48.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:19:48.777: INFO: namespace downward-api-5517 deletion completed in 22.182292365s
â€¢SS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:19:48.777: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2597
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-1043829d-9856-48da-84a9-6c4f57f6e1a7
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:19:48.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2597" for this suite.
Aug 14 07:19:54.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:19:55.067: INFO: namespace secrets-2597 deletion completed in 6.122631895s
â€¢SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:19:55.068: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-6148
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 07:19:55.213: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:19:56.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6148" for this suite.
Aug 14 07:20:02.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:20:02.394: INFO: namespace custom-resource-definition-6148 deletion completed in 6.119777497s
â€¢SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:20:02.394: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-1827
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Aug 14 07:20:02.548: INFO: Waiting up to 5m0s for pod "var-expansion-9a369dbe-5034-4c2e-90d2-d195e808038c" in namespace "var-expansion-1827" to be "success or failure"
Aug 14 07:20:02.551: INFO: Pod "var-expansion-9a369dbe-5034-4c2e-90d2-d195e808038c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.879083ms
Aug 14 07:20:04.556: INFO: Pod "var-expansion-9a369dbe-5034-4c2e-90d2-d195e808038c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007912375s
STEP: Saw pod success
Aug 14 07:20:04.556: INFO: Pod "var-expansion-9a369dbe-5034-4c2e-90d2-d195e808038c" satisfied condition "success or failure"
Aug 14 07:20:04.560: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod var-expansion-9a369dbe-5034-4c2e-90d2-d195e808038c container dapi-container: <nil>
STEP: delete the pod
Aug 14 07:20:04.578: INFO: Waiting for pod var-expansion-9a369dbe-5034-4c2e-90d2-d195e808038c to disappear
Aug 14 07:20:04.581: INFO: Pod var-expansion-9a369dbe-5034-4c2e-90d2-d195e808038c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:20:04.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1827" for this suite.
Aug 14 07:20:10.599: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:20:10.709: INFO: namespace var-expansion-1827 deletion completed in 6.123462248s
â€¢SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:20:10.710: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-8339
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Aug 14 07:20:11.161: INFO: Pod name wrapped-volume-race-e57dd6ea-bb7c-416c-b7ea-27d45d70def1: Found 0 pods out of 5
Aug 14 07:20:16.170: INFO: Pod name wrapped-volume-race-e57dd6ea-bb7c-416c-b7ea-27d45d70def1: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-e57dd6ea-bb7c-416c-b7ea-27d45d70def1 in namespace emptydir-wrapper-8339, will wait for the garbage collector to delete the pods
Aug 14 07:20:16.250: INFO: Deleting ReplicationController wrapped-volume-race-e57dd6ea-bb7c-416c-b7ea-27d45d70def1 took: 7.609494ms
Aug 14 07:20:16.650: INFO: Terminating ReplicationController wrapped-volume-race-e57dd6ea-bb7c-416c-b7ea-27d45d70def1 pods took: 400.244534ms
STEP: Creating RC which spawns configmap-volume pods
Aug 14 07:21:00.872: INFO: Pod name wrapped-volume-race-6f3f5503-e6fc-47b4-809f-54e7534d53db: Found 0 pods out of 5
Aug 14 07:21:05.880: INFO: Pod name wrapped-volume-race-6f3f5503-e6fc-47b4-809f-54e7534d53db: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6f3f5503-e6fc-47b4-809f-54e7534d53db in namespace emptydir-wrapper-8339, will wait for the garbage collector to delete the pods
Aug 14 07:21:05.960: INFO: Deleting ReplicationController wrapped-volume-race-6f3f5503-e6fc-47b4-809f-54e7534d53db took: 7.337779ms
Aug 14 07:21:06.060: INFO: Terminating ReplicationController wrapped-volume-race-6f3f5503-e6fc-47b4-809f-54e7534d53db pods took: 100.430644ms
STEP: Creating RC which spawns configmap-volume pods
Aug 14 07:21:50.881: INFO: Pod name wrapped-volume-race-35c2bb52-d692-4fe2-a42c-99f0c219e564: Found 0 pods out of 5
Aug 14 07:21:55.891: INFO: Pod name wrapped-volume-race-35c2bb52-d692-4fe2-a42c-99f0c219e564: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-35c2bb52-d692-4fe2-a42c-99f0c219e564 in namespace emptydir-wrapper-8339, will wait for the garbage collector to delete the pods
Aug 14 07:21:55.971: INFO: Deleting ReplicationController wrapped-volume-race-35c2bb52-d692-4fe2-a42c-99f0c219e564 took: 6.867002ms
Aug 14 07:21:56.071: INFO: Terminating ReplicationController wrapped-volume-race-35c2bb52-d692-4fe2-a42c-99f0c219e564 pods took: 100.284902ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:22:31.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-8339" for this suite.
Aug 14 07:22:37.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:22:37.329: INFO: namespace emptydir-wrapper-8339 deletion completed in 6.115930285s
â€¢SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:22:37.329: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8926
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Aug 14 07:22:37.475: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-8926'
Aug 14 07:22:42.706: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Aug 14 07:22:42.706: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Aug 14 07:22:42.712: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete jobs e2e-test-nginx-job --namespace=kubectl-8926'
Aug 14 07:22:42.796: INFO: stderr: ""
Aug 14 07:22:42.796: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:22:42.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8926" for this suite.
Aug 14 07:23:04.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:23:04.988: INFO: namespace kubectl-8926 deletion completed in 22.188368565s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:23:04.989: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4048
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-8f3eeab8-51d6-4d2f-b20c-33917959a493
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-8f3eeab8-51d6-4d2f-b20c-33917959a493
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:23:09.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4048" for this suite.
Aug 14 07:23:31.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:23:31.407: INFO: namespace projected-4048 deletion completed in 22.118961816s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:23:31.411: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-56
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:23:35.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-56" for this suite.
Aug 14 07:23:41.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:23:41.710: INFO: namespace kubelet-test-56 deletion completed in 6.121263511s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:23:41.711: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-254
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Aug 14 07:23:51.886: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
W0814 07:23:51.886539    3092 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 14 07:23:51.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-254" for this suite.
Aug 14 07:23:57.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:23:58.017: INFO: namespace gc-254 deletion completed in 6.126995155s
â€¢S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:23:58.017: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1045
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-1045/secret-test-8783dc96-70dc-4a82-a672-d42383b99f14
STEP: Creating a pod to test consume secrets
Aug 14 07:23:58.175: INFO: Waiting up to 5m0s for pod "pod-configmaps-c185f740-14a3-4fe0-beaa-56a4786f88b0" in namespace "secrets-1045" to be "success or failure"
Aug 14 07:23:58.179: INFO: Pod "pod-configmaps-c185f740-14a3-4fe0-beaa-56a4786f88b0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.018006ms
Aug 14 07:24:00.183: INFO: Pod "pod-configmaps-c185f740-14a3-4fe0-beaa-56a4786f88b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00760525s
STEP: Saw pod success
Aug 14 07:24:00.183: INFO: Pod "pod-configmaps-c185f740-14a3-4fe0-beaa-56a4786f88b0" satisfied condition "success or failure"
Aug 14 07:24:00.187: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-configmaps-c185f740-14a3-4fe0-beaa-56a4786f88b0 container env-test: <nil>
STEP: delete the pod
Aug 14 07:24:00.207: INFO: Waiting for pod pod-configmaps-c185f740-14a3-4fe0-beaa-56a4786f88b0 to disappear
Aug 14 07:24:00.211: INFO: Pod pod-configmaps-c185f740-14a3-4fe0-beaa-56a4786f88b0 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:24:00.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1045" for this suite.
Aug 14 07:24:06.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:24:06.363: INFO: namespace secrets-1045 deletion completed in 6.148132003s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:24:06.364: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9622
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 14 07:24:06.519: INFO: Waiting up to 5m0s for pod "pod-cf4900da-3b4f-4c74-a7eb-512086bf5e1c" in namespace "emptydir-9622" to be "success or failure"
Aug 14 07:24:06.522: INFO: Pod "pod-cf4900da-3b4f-4c74-a7eb-512086bf5e1c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.200894ms
Aug 14 07:24:08.527: INFO: Pod "pod-cf4900da-3b4f-4c74-a7eb-512086bf5e1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007833518s
STEP: Saw pod success
Aug 14 07:24:08.527: INFO: Pod "pod-cf4900da-3b4f-4c74-a7eb-512086bf5e1c" satisfied condition "success or failure"
Aug 14 07:24:08.530: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-cf4900da-3b4f-4c74-a7eb-512086bf5e1c container test-container: <nil>
STEP: delete the pod
Aug 14 07:24:08.548: INFO: Waiting for pod pod-cf4900da-3b4f-4c74-a7eb-512086bf5e1c to disappear
Aug 14 07:24:08.551: INFO: Pod pod-cf4900da-3b4f-4c74-a7eb-512086bf5e1c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:24:08.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9622" for this suite.
Aug 14 07:24:14.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:24:14.673: INFO: namespace emptydir-9622 deletion completed in 6.11829846s
â€¢SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:24:14.673: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-1895
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Aug 14 07:24:14.822: INFO: Waiting up to 5m0s for pod "client-containers-f9136d02-af17-4386-b10b-a4e64cef0257" in namespace "containers-1895" to be "success or failure"
Aug 14 07:24:14.824: INFO: Pod "client-containers-f9136d02-af17-4386-b10b-a4e64cef0257": Phase="Pending", Reason="", readiness=false. Elapsed: 2.582064ms
Aug 14 07:24:16.829: INFO: Pod "client-containers-f9136d02-af17-4386-b10b-a4e64cef0257": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007329943s
STEP: Saw pod success
Aug 14 07:24:16.829: INFO: Pod "client-containers-f9136d02-af17-4386-b10b-a4e64cef0257" satisfied condition "success or failure"
Aug 14 07:24:16.832: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod client-containers-f9136d02-af17-4386-b10b-a4e64cef0257 container test-container: <nil>
STEP: delete the pod
Aug 14 07:24:16.848: INFO: Waiting for pod client-containers-f9136d02-af17-4386-b10b-a4e64cef0257 to disappear
Aug 14 07:24:16.850: INFO: Pod client-containers-f9136d02-af17-4386-b10b-a4e64cef0257 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:24:16.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1895" for this suite.
Aug 14 07:24:22.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:24:23.003: INFO: namespace containers-1895 deletion completed in 6.148945669s
â€¢SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:24:23.003: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3642
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-57d6c82c-c80f-4793-a7d2-3a0fc32e3cb8
STEP: Creating a pod to test consume configMaps
Aug 14 07:24:23.160: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c3ed46e5-6c87-4a15-830e-61ac3d71c0a5" in namespace "projected-3642" to be "success or failure"
Aug 14 07:24:23.163: INFO: Pod "pod-projected-configmaps-c3ed46e5-6c87-4a15-830e-61ac3d71c0a5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.923096ms
Aug 14 07:24:25.168: INFO: Pod "pod-projected-configmaps-c3ed46e5-6c87-4a15-830e-61ac3d71c0a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007850263s
STEP: Saw pod success
Aug 14 07:24:25.168: INFO: Pod "pod-projected-configmaps-c3ed46e5-6c87-4a15-830e-61ac3d71c0a5" satisfied condition "success or failure"
Aug 14 07:24:25.171: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-projected-configmaps-c3ed46e5-6c87-4a15-830e-61ac3d71c0a5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 07:24:25.190: INFO: Waiting for pod pod-projected-configmaps-c3ed46e5-6c87-4a15-830e-61ac3d71c0a5 to disappear
Aug 14 07:24:25.193: INFO: Pod pod-projected-configmaps-c3ed46e5-6c87-4a15-830e-61ac3d71c0a5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:24:25.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3642" for this suite.
Aug 14 07:24:31.208: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:24:31.352: INFO: namespace projected-3642 deletion completed in 6.155030909s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:24:31.353: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8575
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-43b21185-d384-44a8-85b4-1aedf48f8c4f
STEP: Creating secret with name s-test-opt-upd-57201d5c-b33b-412a-be29-f8b4710d00ae
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-43b21185-d384-44a8-85b4-1aedf48f8c4f
STEP: Updating secret s-test-opt-upd-57201d5c-b33b-412a-be29-f8b4710d00ae
STEP: Creating secret with name s-test-opt-create-63ae347d-a4f0-4d8f-b7b0-8f0206255450
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:24:38.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8575" for this suite.
Aug 14 07:25:00.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:25:00.123: INFO: namespace secrets-8575 deletion completed in 22.1186788s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:25:00.126: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-6208
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-6208
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-6208
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6208
Aug 14 07:25:00.293: INFO: Found 0 stateful pods, waiting for 1
Aug 14 07:25:10.298: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Aug 14 07:25:10.302: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 14 07:25:15.849: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 14 07:25:15.849: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 14 07:25:15.849: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 14 07:25:15.854: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 14 07:25:25.859: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 07:25:25.859: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 07:25:25.873: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999798s
Aug 14 07:25:26.877: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996743908s
Aug 14 07:25:27.882: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.991934505s
Aug 14 07:25:28.887: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.98699315s
Aug 14 07:25:29.892: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.981962548s
Aug 14 07:25:30.896: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.97719968s
Aug 14 07:25:31.901: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.972728567s
Aug 14 07:25:32.906: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.967844186s
Aug 14 07:25:33.911: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.963082794s
Aug 14 07:25:34.916: INFO: Verifying statefulset ss doesn't scale past 1 for another 957.896059ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6208
Aug 14 07:25:35.921: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:25:36.392: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 14 07:25:36.392: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 14 07:25:36.392: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 14 07:25:36.396: INFO: Found 1 stateful pods, waiting for 3
Aug 14 07:25:46.402: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 07:25:46.402: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 14 07:25:46.402: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Aug 14 07:25:46.412: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 14 07:25:46.945: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 14 07:25:46.945: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 14 07:25:46.945: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 14 07:25:46.946: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 14 07:25:47.425: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 14 07:25:47.425: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 14 07:25:47.425: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 14 07:25:47.425: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Aug 14 07:25:47.930: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Aug 14 07:25:47.930: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Aug 14 07:25:47.930: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Aug 14 07:25:47.930: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 07:25:47.934: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 14 07:25:57.947: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 07:25:57.947: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 07:25:57.947: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 14 07:25:57.962: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999562s
Aug 14 07:25:58.969: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996024201s
Aug 14 07:25:59.974: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990064918s
Aug 14 07:26:00.980: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984019168s
Aug 14 07:26:01.986: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.978889011s
Aug 14 07:26:02.993: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.972811221s
Aug 14 07:26:03.999: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.965068937s
Aug 14 07:26:05.006: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.959197138s
Aug 14 07:26:06.013: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.95184953s
Aug 14 07:26:07.018: INFO: Verifying statefulset ss doesn't scale past 3 for another 945.825657ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6208
Aug 14 07:26:08.024: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:26:08.510: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 14 07:26:08.510: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 14 07:26:08.510: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 14 07:26:08.510: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:26:08.921: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Aug 14 07:26:08.922: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Aug 14 07:26:08.922: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Aug 14 07:26:08.922: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:26:09.353: INFO: rc: 1
Aug 14 07:26:09.353: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: Internal error occurred: error executing command in container: container not running (ec209aed286188483d7db58b29057038ace38dbb4deec0deb6c041bfc7198e54)
 [] <nil> 0xc0036e1d10 exit status 1 <nil> <nil> true [0xc0023ee110 0xc0023ee170 0xc0023ee1d8] [0xc0023ee110 0xc0023ee170 0xc0023ee1d8] [0xc0023ee158 0xc0023ee1c0] [0x9d17b0 0x9d17b0] 0xc002ba0780 <nil>}:
Command stdout:

stderr:
error: Internal error occurred: error executing command in container: container not running (ec209aed286188483d7db58b29057038ace38dbb4deec0deb6c041bfc7198e54)

error:
exit status 1
Aug 14 07:26:19.354: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:26:19.572: INFO: rc: 1
Aug 14 07:26:19.572: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc002357bc0 exit status 1 <nil> <nil> true [0xc002772220 0xc002772270 0xc002772318] [0xc002772220 0xc002772270 0xc002772318] [0xc002772238 0xc0027722c8] [0x9d17b0 0x9d17b0] 0xc00367a7e0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Aug 14 07:26:29.572: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:26:34.643: INFO: rc: 1
Aug 14 07:26:34.644: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00260f380 exit status 1 <nil> <nil> true [0xc0000cf968 0xc0000cfae8 0xc0000cfc38] [0xc0000cf968 0xc0000cfae8 0xc0000cfc38] [0xc0000cfa30 0xc0000cfbf0] [0x9d17b0 0x9d17b0] 0xc0036c87e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:26:44.644: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:26:44.721: INFO: rc: 1
Aug 14 07:26:44.721: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002d92360 exit status 1 <nil> <nil> true [0xc0023ee218 0xc0023ee238 0xc0023ee2a8] [0xc0023ee218 0xc0023ee238 0xc0023ee2a8] [0xc0023ee228 0xc0023ee280] [0x9d17b0 0x9d17b0] 0xc002ba0c00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:26:54.721: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:26:54.824: INFO: rc: 1
Aug 14 07:26:54.824: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00260f920 exit status 1 <nil> <nil> true [0xc0000cfc68 0xc0000cfd50 0xc0000cfdf0] [0xc0000cfc68 0xc0000cfd50 0xc0000cfdf0] [0xc0000cfcc8 0xc0000cfdd8] [0x9d17b0 0x9d17b0] 0xc0036c8d80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:27:04.824: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:27:04.900: INFO: rc: 1
Aug 14 07:27:04.900: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001550210 exit status 1 <nil> <nil> true [0xc002772328 0xc0027723a0 0xc002772440] [0xc002772328 0xc0027723a0 0xc002772440] [0xc002772378 0xc0027723f8] [0x9d17b0 0x9d17b0] 0xc00367ac00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:27:14.900: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:27:14.991: INFO: rc: 1
Aug 14 07:27:14.991: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002d92960 exit status 1 <nil> <nil> true [0xc0023ee2c8 0xc0023ee370 0xc0023ee398] [0xc0023ee2c8 0xc0023ee370 0xc0023ee398] [0xc0023ee320 0xc0023ee388] [0x9d17b0 0x9d17b0] 0xc002ba0f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:27:24.994: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:27:25.112: INFO: rc: 1
Aug 14 07:27:25.112: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00260fef0 exit status 1 <nil> <nil> true [0xc0000cfe38 0xc0000cffe8 0xc0009b20c0] [0xc0000cfe38 0xc0000cffe8 0xc0009b20c0] [0xc0000cff50 0xc0009b2028] [0x9d17b0 0x9d17b0] 0xc0036c90e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:27:35.113: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:27:35.205: INFO: rc: 1
Aug 14 07:27:35.205: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002d92f60 exit status 1 <nil> <nil> true [0xc0023ee3c8 0xc0023ee450 0xc0023ee4f0] [0xc0023ee3c8 0xc0023ee450 0xc0023ee4f0] [0xc0023ee440 0xc0023ee4b0] [0x9d17b0 0x9d17b0] 0xc002ba13e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:27:45.206: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:27:45.304: INFO: rc: 1
Aug 14 07:27:45.304: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002d93680 exit status 1 <nil> <nil> true [0xc0023ee530 0xc0023ee5c8 0xc0023ee628] [0xc0023ee530 0xc0023ee5c8 0xc0023ee628] [0xc0023ee5a0 0xc0023ee610] [0x9d17b0 0x9d17b0] 0xc002ba16e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:27:55.305: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:27:55.425: INFO: rc: 1
Aug 14 07:27:55.425: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0036ac4e0 exit status 1 <nil> <nil> true [0xc0009b20e8 0xc0009b2390 0xc0009b2878] [0xc0009b20e8 0xc0009b2390 0xc0009b2878] [0xc0009b22b0 0xc0009b2810] [0x9d17b0 0x9d17b0] 0xc0036c93e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:28:05.426: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:28:05.514: INFO: rc: 1
Aug 14 07:28:05.514: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0036aca50 exit status 1 <nil> <nil> true [0xc0023ee670 0xc0023ee700 0xc0009b29d0] [0xc0023ee670 0xc0023ee700 0xc0009b29d0] [0xc0023ee6c8 0xc0009b2908] [0x9d17b0 0x9d17b0] 0xc0036c96e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:28:15.514: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:28:15.629: INFO: rc: 1
Aug 14 07:28:15.629: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002356ae0 exit status 1 <nil> <nil> true [0xc0009b2010 0xc0009b20e8 0xc0009b2390] [0xc0009b2010 0xc0009b20e8 0xc0009b2390] [0xc0009b20c0 0xc0009b22b0] [0x9d17b0 0x9d17b0] 0xc0017ed1a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:28:25.629: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:28:25.701: INFO: rc: 1
Aug 14 07:28:25.701: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00260ea50 exit status 1 <nil> <nil> true [0xc0000ce0c8 0xc0000cebf8 0xc0000cf160] [0xc0000ce0c8 0xc0000cebf8 0xc0000cf160] [0xc0000ceb80 0xc0000ceea8] [0x9d17b0 0x9d17b0] 0xc0036c83c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:28:35.701: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:28:35.780: INFO: rc: 1
Aug 14 07:28:35.781: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00260f050 exit status 1 <nil> <nil> true [0xc0000cf1a0 0xc0000cf790 0xc0000cfa30] [0xc0000cf1a0 0xc0000cf790 0xc0000cfa30] [0xc0000cf6e0 0xc0000cf9d8] [0x9d17b0 0x9d17b0] 0xc0036c8720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:28:45.781: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:28:45.891: INFO: rc: 1
Aug 14 07:28:45.891: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0036e05a0 exit status 1 <nil> <nil> true [0xc002772000 0xc002772030 0xc002772070] [0xc002772000 0xc002772030 0xc002772070] [0xc002772028 0xc002772060] [0x9d17b0 0x9d17b0] 0xc00367a6c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:28:55.892: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:28:55.973: INFO: rc: 1
Aug 14 07:28:55.974: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00260f620 exit status 1 <nil> <nil> true [0xc0000cfae8 0xc0000cfc38 0xc0000cfcc8] [0xc0000cfae8 0xc0000cfc38 0xc0000cfcc8] [0xc0000cfbf0 0xc0000cfc98] [0x9d17b0 0x9d17b0] 0xc0036c8cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:29:05.974: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:29:06.059: INFO: rc: 1
Aug 14 07:29:06.059: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00260fc20 exit status 1 <nil> <nil> true [0xc0000cfd50 0xc0000cfdf0 0xc0000cff50] [0xc0000cfd50 0xc0000cfdf0 0xc0000cff50] [0xc0000cfdd8 0xc0000cfe90] [0x9d17b0 0x9d17b0] 0xc0036c8fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:29:16.059: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:29:16.167: INFO: rc: 1
Aug 14 07:29:16.167: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0036acae0 exit status 1 <nil> <nil> true [0xc0023ee008 0xc0023ee038 0xc0023ee070] [0xc0023ee008 0xc0023ee038 0xc0023ee070] [0xc0023ee028 0xc0023ee060] [0x9d17b0 0x9d17b0] 0xc002ba0240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:29:26.167: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:29:26.250: INFO: rc: 1
Aug 14 07:29:26.251: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0036ad0b0 exit status 1 <nil> <nil> true [0xc0023ee0a0 0xc0023ee110 0xc0023ee170] [0xc0023ee0a0 0xc0023ee110 0xc0023ee170] [0xc0023ee0e8 0xc0023ee158] [0x9d17b0 0x9d17b0] 0xc002ba0540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:29:36.251: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:29:36.326: INFO: rc: 1
Aug 14 07:29:36.326: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023570e0 exit status 1 <nil> <nil> true [0xc0009b2480 0xc0009b2878 0xc0009b2c38] [0xc0009b2480 0xc0009b2878 0xc0009b2c38] [0xc0009b2810 0xc0009b2bd0] [0x9d17b0 0x9d17b0] 0xc000ba6120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:29:46.326: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:29:46.402: INFO: rc: 1
Aug 14 07:29:46.402: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001550270 exit status 1 <nil> <nil> true [0xc0000cffe8 0xc0001cd7b8 0xc0001cd828] [0xc0000cffe8 0xc0001cd7b8 0xc0001cd828] [0xc0001cd770 0xc0001cd818] [0x9d17b0 0x9d17b0] 0xc0036c9320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:29:56.403: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:29:56.520: INFO: rc: 1
Aug 14 07:29:56.521: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0036ad680 exit status 1 <nil> <nil> true [0xc0023ee1a0 0xc0023ee218 0xc0023ee238] [0xc0023ee1a0 0xc0023ee218 0xc0023ee238] [0xc0023ee1d8 0xc0023ee228] [0x9d17b0 0x9d17b0] 0xc002ba09c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:30:06.521: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:30:06.604: INFO: rc: 1
Aug 14 07:30:06.604: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00260e9f0 exit status 1 <nil> <nil> true [0xc0000ce0c8 0xc0000cebf8 0xc0000cf160] [0xc0000ce0c8 0xc0000cebf8 0xc0000cf160] [0xc0000ceb80 0xc0000ceea8] [0x9d17b0 0x9d17b0] 0xc0017ed1a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:30:16.605: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:30:16.689: INFO: rc: 1
Aug 14 07:30:16.689: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0036e0570 exit status 1 <nil> <nil> true [0xc0023ee008 0xc0023ee038 0xc0023ee070] [0xc0023ee008 0xc0023ee038 0xc0023ee070] [0xc0023ee028 0xc0023ee060] [0x9d17b0 0x9d17b0] 0xc002ba0240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:30:26.690: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:30:26.804: INFO: rc: 1
Aug 14 07:30:26.806: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0036e0b70 exit status 1 <nil> <nil> true [0xc0023ee0a0 0xc0023ee110 0xc0023ee170] [0xc0023ee0a0 0xc0023ee110 0xc0023ee170] [0xc0023ee0e8 0xc0023ee158] [0x9d17b0 0x9d17b0] 0xc002ba0540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:30:36.806: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:30:36.890: INFO: rc: 1
Aug 14 07:30:36.890: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00260eff0 exit status 1 <nil> <nil> true [0xc0000cf1a0 0xc0000cf790 0xc0000cfa30] [0xc0000cf1a0 0xc0000cf790 0xc0000cfa30] [0xc0000cf6e0 0xc0000cf9d8] [0x9d17b0 0x9d17b0] 0xc00367a3c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:30:46.890: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:30:46.963: INFO: rc: 1
Aug 14 07:30:46.963: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0036e1110 exit status 1 <nil> <nil> true [0xc0023ee1a0 0xc0023ee218 0xc0023ee238] [0xc0023ee1a0 0xc0023ee218 0xc0023ee238] [0xc0023ee1d8 0xc0023ee228] [0x9d17b0 0x9d17b0] 0xc002ba09c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:30:56.963: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:30:57.074: INFO: rc: 1
Aug 14 07:30:57.074: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc00260f5f0 exit status 1 <nil> <nil> true [0xc0000cfae8 0xc0000cfc38 0xc0000cfcc8] [0xc0000cfae8 0xc0000cfc38 0xc0000cfcc8] [0xc0000cfbf0 0xc0000cfc98] [0x9d17b0 0x9d17b0] 0xc00367a960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:31:07.075: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:31:07.192: INFO: rc: 1
Aug 14 07:31:07.192: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0036e16e0 exit status 1 <nil> <nil> true [0xc0023ee258 0xc0023ee2c8 0xc0023ee370] [0xc0023ee258 0xc0023ee2c8 0xc0023ee370] [0xc0023ee2a8 0xc0023ee320] [0x9d17b0 0x9d17b0] 0xc002ba0cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Aug 14 07:31:17.193: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-6208 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Aug 14 07:31:17.278: INFO: rc: 1
Aug 14 07:31:17.278: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Aug 14 07:31:17.278: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Aug 14 07:31:17.310: INFO: Deleting all statefulset in ns statefulset-6208
Aug 14 07:31:17.313: INFO: Scaling statefulset ss to 0
Aug 14 07:31:17.324: INFO: Waiting for statefulset status.replicas updated to 0
Aug 14 07:31:17.327: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:31:17.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6208" for this suite.
Aug 14 07:31:23.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:31:23.489: INFO: namespace statefulset-6208 deletion completed in 6.144316373s

â€¢ [SLOW TEST:383.363 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:31:23.490: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6943
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 14 07:31:23.652: INFO: Waiting up to 5m0s for pod "downward-api-fa7a683b-fc0a-47bd-8600-b08f3cba7991" in namespace "downward-api-6943" to be "success or failure"
Aug 14 07:31:23.655: INFO: Pod "downward-api-fa7a683b-fc0a-47bd-8600-b08f3cba7991": Phase="Pending", Reason="", readiness=false. Elapsed: 3.350153ms
Aug 14 07:31:25.661: INFO: Pod "downward-api-fa7a683b-fc0a-47bd-8600-b08f3cba7991": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008548065s
STEP: Saw pod success
Aug 14 07:31:25.661: INFO: Pod "downward-api-fa7a683b-fc0a-47bd-8600-b08f3cba7991" satisfied condition "success or failure"
Aug 14 07:31:25.664: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod downward-api-fa7a683b-fc0a-47bd-8600-b08f3cba7991 container dapi-container: <nil>
STEP: delete the pod
Aug 14 07:31:25.687: INFO: Waiting for pod downward-api-fa7a683b-fc0a-47bd-8600-b08f3cba7991 to disappear
Aug 14 07:31:25.691: INFO: Pod downward-api-fa7a683b-fc0a-47bd-8600-b08f3cba7991 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:31:25.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6943" for this suite.
Aug 14 07:31:31.709: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:31:31.869: INFO: namespace downward-api-6943 deletion completed in 6.173234341s
â€¢SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:31:31.870: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6679
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-3d066827-c802-4e60-a991-beb6336e74da
STEP: Creating a pod to test consume configMaps
Aug 14 07:31:32.025: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e1f4354b-39f0-440c-b48e-10f90ec655e0" in namespace "projected-6679" to be "success or failure"
Aug 14 07:31:32.028: INFO: Pod "pod-projected-configmaps-e1f4354b-39f0-440c-b48e-10f90ec655e0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.194851ms
Aug 14 07:31:34.034: INFO: Pod "pod-projected-configmaps-e1f4354b-39f0-440c-b48e-10f90ec655e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008729508s
Aug 14 07:31:36.038: INFO: Pod "pod-projected-configmaps-e1f4354b-39f0-440c-b48e-10f90ec655e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013443733s
STEP: Saw pod success
Aug 14 07:31:36.038: INFO: Pod "pod-projected-configmaps-e1f4354b-39f0-440c-b48e-10f90ec655e0" satisfied condition "success or failure"
Aug 14 07:31:36.042: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-projected-configmaps-e1f4354b-39f0-440c-b48e-10f90ec655e0 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 07:31:36.063: INFO: Waiting for pod pod-projected-configmaps-e1f4354b-39f0-440c-b48e-10f90ec655e0 to disappear
Aug 14 07:31:36.066: INFO: Pod pod-projected-configmaps-e1f4354b-39f0-440c-b48e-10f90ec655e0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:31:36.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6679" for this suite.
Aug 14 07:31:42.084: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:31:42.197: INFO: namespace projected-6679 deletion completed in 6.125053839s
â€¢SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:31:42.197: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7648
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 14 07:31:44.887: INFO: Successfully updated pod "labelsupdate6df6557f-236d-495d-aee2-d87cef12e749"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:31:46.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7648" for this suite.
Aug 14 07:32:08.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:32:09.088: INFO: namespace projected-7648 deletion completed in 22.172817647s
â€¢SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:32:09.088: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4162
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 14 07:32:11.768: INFO: Successfully updated pod "pod-update-activedeadlineseconds-397de3a3-65f2-40df-8391-7e86dd65be6f"
Aug 14 07:32:11.768: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-397de3a3-65f2-40df-8391-7e86dd65be6f" in namespace "pods-4162" to be "terminated due to deadline exceeded"
Aug 14 07:32:11.771: INFO: Pod "pod-update-activedeadlineseconds-397de3a3-65f2-40df-8391-7e86dd65be6f": Phase="Running", Reason="", readiness=true. Elapsed: 2.849756ms
Aug 14 07:32:13.777: INFO: Pod "pod-update-activedeadlineseconds-397de3a3-65f2-40df-8391-7e86dd65be6f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008363052s
Aug 14 07:32:15.782: INFO: Pod "pod-update-activedeadlineseconds-397de3a3-65f2-40df-8391-7e86dd65be6f": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.013256251s
Aug 14 07:32:15.782: INFO: Pod "pod-update-activedeadlineseconds-397de3a3-65f2-40df-8391-7e86dd65be6f" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:32:15.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4162" for this suite.
Aug 14 07:32:21.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:32:21.917: INFO: namespace pods-4162 deletion completed in 6.129684263s
â€¢SSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:32:21.917: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3266
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 07:32:22.062: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:32:24.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3266" for this suite.
Aug 14 07:33:02.317: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:33:02.431: INFO: namespace pods-3266 deletion completed in 38.125353502s
â€¢SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:33:02.432: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7196
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Aug 14 07:33:02.580: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-7196'
Aug 14 07:33:02.921: INFO: stderr: ""
Aug 14 07:33:02.921: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 14 07:33:03.927: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 07:33:03.927: INFO: Found 0 / 1
Aug 14 07:33:04.927: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 07:33:04.927: INFO: Found 1 / 1
Aug 14 07:33:04.927: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Aug 14 07:33:04.931: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 07:33:04.931: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 14 07:33:04.931: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config patch pod redis-master-f4pnw --namespace=kubectl-7196 -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 14 07:33:05.024: INFO: stderr: ""
Aug 14 07:33:05.024: INFO: stdout: "pod/redis-master-f4pnw patched\n"
STEP: checking annotations
Aug 14 07:33:05.028: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 07:33:05.028: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:33:05.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7196" for this suite.
Aug 14 07:33:27.048: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:33:27.167: INFO: namespace kubectl-7196 deletion completed in 22.133567966s
â€¢SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:33:27.168: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2841
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-c9eae0ac-367e-4880-a5c6-0ba912f7747d
STEP: Creating a pod to test consume configMaps
Aug 14 07:33:27.331: INFO: Waiting up to 5m0s for pod "pod-configmaps-3b9764f3-4b3d-4376-bd69-4a709153780f" in namespace "configmap-2841" to be "success or failure"
Aug 14 07:33:27.335: INFO: Pod "pod-configmaps-3b9764f3-4b3d-4376-bd69-4a709153780f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.595938ms
Aug 14 07:33:29.341: INFO: Pod "pod-configmaps-3b9764f3-4b3d-4376-bd69-4a709153780f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009759052s
STEP: Saw pod success
Aug 14 07:33:29.341: INFO: Pod "pod-configmaps-3b9764f3-4b3d-4376-bd69-4a709153780f" satisfied condition "success or failure"
Aug 14 07:33:29.345: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-configmaps-3b9764f3-4b3d-4376-bd69-4a709153780f container configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 07:33:29.364: INFO: Waiting for pod pod-configmaps-3b9764f3-4b3d-4376-bd69-4a709153780f to disappear
Aug 14 07:33:29.367: INFO: Pod pod-configmaps-3b9764f3-4b3d-4376-bd69-4a709153780f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:33:29.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2841" for this suite.
Aug 14 07:33:35.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:33:35.517: INFO: namespace configmap-2841 deletion completed in 6.144930446s
â€¢SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:33:35.518: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-1956
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 07:33:35.664: INFO: Creating deployment "nginx-deployment"
Aug 14 07:33:35.670: INFO: Waiting for observed generation 1
Aug 14 07:33:37.679: INFO: Waiting for all required pods to come up
Aug 14 07:33:37.685: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Aug 14 07:33:39.696: INFO: Waiting for deployment "nginx-deployment" to complete
Aug 14 07:33:39.704: INFO: Updating deployment "nginx-deployment" with a non-existent image
Aug 14 07:33:39.713: INFO: Updating deployment nginx-deployment
Aug 14 07:33:39.713: INFO: Waiting for observed generation 2
Aug 14 07:33:41.722: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 14 07:33:41.725: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 14 07:33:41.729: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Aug 14 07:33:41.742: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 14 07:33:41.742: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 14 07:33:41.745: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Aug 14 07:33:41.751: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Aug 14 07:33:41.751: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Aug 14 07:33:41.758: INFO: Updating deployment nginx-deployment
Aug 14 07:33:41.758: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Aug 14 07:33:41.768: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 14 07:33:43.778: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 14 07:33:43.784: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-1956,SelfLink:/apis/apps/v1/namespaces/deployment-1956/deployments/nginx-deployment,UID:740f97ea-e61b-4410-a649-388a313858b7,ResourceVersion:20224,Generation:3,CreationTimestamp:2019-08-14 07:33:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-08-14 07:33:41 +0000 UTC 2019-08-14 07:33:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-08-14 07:33:41 +0000 UTC 2019-08-14 07:33:35 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Aug 14 07:33:43.789: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-1956,SelfLink:/apis/apps/v1/namespaces/deployment-1956/replicasets/nginx-deployment-55fb7cb77f,UID:6dbdea5f-e1b6-44c6-8f2b-351cb4cbafdc,ResourceVersion:20217,Generation:3,CreationTimestamp:2019-08-14 07:33:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 740f97ea-e61b-4410-a649-388a313858b7 0xc0035d1a37 0xc0035d1a38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 14 07:33:43.789: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Aug 14 07:33:43.789: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-1956,SelfLink:/apis/apps/v1/namespaces/deployment-1956/replicasets/nginx-deployment-7b8c6f4498,UID:35224d23-3cdf-481c-8b2f-7b3cfd6a134d,ResourceVersion:20223,Generation:3,CreationTimestamp:2019-08-14 07:33:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 740f97ea-e61b-4410-a649-388a313858b7 0xc0035d1b07 0xc0035d1b08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Aug 14 07:33:43.799: INFO: Pod "nginx-deployment-55fb7cb77f-48kxf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-48kxf,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-55fb7cb77f-48kxf,UID:12dbf638-dc5f-4891-a396-24eb7b8bbe92,ResourceVersion:20238,Generation:0,CreationTimestamp:2019-08-14 07:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.189/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 6dbdea5f-e1b6-44c6-8f2b-351cb4cbafdc 0xc002de7af7 0xc002de7af8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002de7b60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002de7b80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-08-14 07:33:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.799: INFO: Pod "nginx-deployment-55fb7cb77f-68lmh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-68lmh,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-55fb7cb77f-68lmh,UID:b4e750e2-4223-44b6-8647-17c5de8e108a,ResourceVersion:20150,Generation:0,CreationTimestamp:2019-08-14 07:33:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.46/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 6dbdea5f-e1b6-44c6-8f2b-351cb4cbafdc 0xc002de7c60 0xc002de7c61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002de7cd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002de7cf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.14,PodIP:,StartTime:2019-08-14 07:33:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.799: INFO: Pod "nginx-deployment-55fb7cb77f-8c5s4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-8c5s4,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-55fb7cb77f-8c5s4,UID:da904996-cc0b-421b-a9ef-9fd9093d899d,ResourceVersion:20153,Generation:0,CreationTimestamp:2019-08-14 07:33:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.186/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 6dbdea5f-e1b6-44c6-8f2b-351cb4cbafdc 0xc002de7e10 0xc002de7e11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002de7e80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002de7ea0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-08-14 07:33:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.800: INFO: Pod "nginx-deployment-55fb7cb77f-8wnds" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-8wnds,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-55fb7cb77f-8wnds,UID:de3ffa22-47d0-4f8d-96f8-756f01cf6ec3,ResourceVersion:20152,Generation:0,CreationTimestamp:2019-08-14 07:33:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.47/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 6dbdea5f-e1b6-44c6-8f2b-351cb4cbafdc 0xc002de7f80 0xc002de7f81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002de7ff0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021ce010}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.14,PodIP:,StartTime:2019-08-14 07:33:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.800: INFO: Pod "nginx-deployment-55fb7cb77f-976gc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-976gc,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-55fb7cb77f-976gc,UID:c9560214-3153-477d-b994-2bf987cfee9d,ResourceVersion:20210,Generation:0,CreationTimestamp:2019-08-14 07:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 6dbdea5f-e1b6-44c6-8f2b-351cb4cbafdc 0xc0021ce0e0 0xc0021ce0e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021ce150} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021ce170}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.14,PodIP:,StartTime:2019-08-14 07:33:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.800: INFO: Pod "nginx-deployment-55fb7cb77f-9prjx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-9prjx,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-55fb7cb77f-9prjx,UID:d70beb85-d45e-4543-84e8-3c29fad31f9f,ResourceVersion:20241,Generation:0,CreationTimestamp:2019-08-14 07:33:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.185/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 6dbdea5f-e1b6-44c6-8f2b-351cb4cbafdc 0xc0021ce250 0xc0021ce251}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021ce2c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021ce2e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:100.96.1.185,StartTime:2019-08-14 07:33:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.800: INFO: Pod "nginx-deployment-55fb7cb77f-9qpz7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-9qpz7,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-55fb7cb77f-9qpz7,UID:fb3873cc-381e-431f-a5a5-27e6311ffd73,ResourceVersion:20235,Generation:0,CreationTimestamp:2019-08-14 07:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.48/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 6dbdea5f-e1b6-44c6-8f2b-351cb4cbafdc 0xc0021ce3e0 0xc0021ce3e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021ce450} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021ce470}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.14,PodIP:,StartTime:2019-08-14 07:33:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.800: INFO: Pod "nginx-deployment-55fb7cb77f-bvzxn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-bvzxn,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-55fb7cb77f-bvzxn,UID:b97b5661-cbd8-42ee-a11c-ff892aeb605d,ResourceVersion:20225,Generation:0,CreationTimestamp:2019-08-14 07:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 6dbdea5f-e1b6-44c6-8f2b-351cb4cbafdc 0xc0021ce540 0xc0021ce541}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021ce5b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021ce5d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.14,PodIP:,StartTime:2019-08-14 07:33:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.801: INFO: Pod "nginx-deployment-55fb7cb77f-hkjcd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-hkjcd,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-55fb7cb77f-hkjcd,UID:c381f0d4-30cb-4bc3-9c9c-cd5086dd8199,ResourceVersion:20230,Generation:0,CreationTimestamp:2019-08-14 07:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 6dbdea5f-e1b6-44c6-8f2b-351cb4cbafdc 0xc0021ce6a0 0xc0021ce6a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021ce710} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021ce730}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.14,PodIP:,StartTime:2019-08-14 07:33:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.801: INFO: Pod "nginx-deployment-55fb7cb77f-kcz95" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-kcz95,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-55fb7cb77f-kcz95,UID:cc643d81-202b-485e-b070-3a691d88b58f,ResourceVersion:20213,Generation:0,CreationTimestamp:2019-08-14 07:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 6dbdea5f-e1b6-44c6-8f2b-351cb4cbafdc 0xc0021ce890 0xc0021ce891}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021ce940} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021ce960}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-08-14 07:33:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.801: INFO: Pod "nginx-deployment-55fb7cb77f-rdpm9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-rdpm9,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-55fb7cb77f-rdpm9,UID:82deab68-b135-464f-91c6-239e30c7c629,ResourceVersion:20228,Generation:0,CreationTimestamp:2019-08-14 07:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 6dbdea5f-e1b6-44c6-8f2b-351cb4cbafdc 0xc0021ceb00 0xc0021ceb01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021cec70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021cec90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.14,PodIP:,StartTime:2019-08-14 07:33:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.801: INFO: Pod "nginx-deployment-55fb7cb77f-s4qms" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-s4qms,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-55fb7cb77f-s4qms,UID:61c363cc-c4d5-47e9-a237-b889ed85e522,ResourceVersion:20155,Generation:0,CreationTimestamp:2019-08-14 07:33:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.187/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 6dbdea5f-e1b6-44c6-8f2b-351cb4cbafdc 0xc0021cf070 0xc0021cf071}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021cf390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021cf3f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:39 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-08-14 07:33:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.802: INFO: Pod "nginx-deployment-55fb7cb77f-wr4s4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-wr4s4,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-55fb7cb77f-wr4s4,UID:9dca9d83-fdae-40a2-bf58-a06f264c36e3,ResourceVersion:20221,Generation:0,CreationTimestamp:2019-08-14 07:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 6dbdea5f-e1b6-44c6-8f2b-351cb4cbafdc 0xc0021cf660 0xc0021cf661}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021cf6d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021cf6f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-08-14 07:33:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.802: INFO: Pod "nginx-deployment-7b8c6f4498-4fz46" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4fz46,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-7b8c6f4498-4fz46,UID:22052d09-705a-4c9c-9565-a276581a8bb0,ResourceVersion:20226,Generation:0,CreationTimestamp:2019-08-14 07:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 35224d23-3cdf-481c-8b2f-7b3cfd6a134d 0xc0021cf7c0 0xc0021cf7c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021cf820} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021cf840}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.14,PodIP:,StartTime:2019-08-14 07:33:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.802: INFO: Pod "nginx-deployment-7b8c6f4498-6sskl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-6sskl,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-7b8c6f4498-6sskl,UID:1730227f-0919-4d3f-b74a-16ca9f043b63,ResourceVersion:20203,Generation:0,CreationTimestamp:2019-08-14 07:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 35224d23-3cdf-481c-8b2f-7b3cfd6a134d 0xc0021cf900 0xc0021cf901}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021cf960} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021cf980}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-08-14 07:33:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.802: INFO: Pod "nginx-deployment-7b8c6f4498-8jt87" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8jt87,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-7b8c6f4498-8jt87,UID:3ac3870a-7f25-43de-83b4-f3ce646a1bfa,ResourceVersion:20240,Generation:0,CreationTimestamp:2019-08-14 07:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.50/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 35224d23-3cdf-481c-8b2f-7b3cfd6a134d 0xc0021cfa50 0xc0021cfa51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021cfab0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021cfad0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.14,PodIP:,StartTime:2019-08-14 07:33:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.802: INFO: Pod "nginx-deployment-7b8c6f4498-8qr4m" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8qr4m,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-7b8c6f4498-8qr4m,UID:3c17c205-e37a-4ff6-9da4-bf59a6a1d82a,ResourceVersion:20080,Generation:0,CreationTimestamp:2019-08-14 07:33:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.44/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 35224d23-3cdf-481c-8b2f-7b3cfd6a134d 0xc0021cfba0 0xc0021cfba1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021cfc00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021cfc20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:35 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.14,PodIP:100.96.0.44,StartTime:2019-08-14 07:33:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-14 07:33:37 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://960ea235cce785885c5491ff3329779456fff90262cd384f3cb15f07937d4909}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.803: INFO: Pod "nginx-deployment-7b8c6f4498-8tmhq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8tmhq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-7b8c6f4498-8tmhq,UID:bb2ad20f-4527-44a4-beda-14d1e03d689b,ResourceVersion:20216,Generation:0,CreationTimestamp:2019-08-14 07:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 35224d23-3cdf-481c-8b2f-7b3cfd6a134d 0xc0021cfcf0 0xc0021cfcf1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021cfd50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021cfd70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-08-14 07:33:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.803: INFO: Pod "nginx-deployment-7b8c6f4498-8wmgv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8wmgv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-7b8c6f4498-8wmgv,UID:cef002bf-8c52-4365-834a-80e4f156ef02,ResourceVersion:20094,Generation:0,CreationTimestamp:2019-08-14 07:33:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.180/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 35224d23-3cdf-481c-8b2f-7b3cfd6a134d 0xc0021cfe40 0xc0021cfe41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021cfea0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0021cfec0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:35 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:100.96.1.180,StartTime:2019-08-14 07:33:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-14 07:33:36 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://307d0c325e7354b8fcbc9f348d321ff9a0a852da7514645d98081875e8de05a8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.803: INFO: Pod "nginx-deployment-7b8c6f4498-f9m9q" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-f9m9q,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-7b8c6f4498-f9m9q,UID:d9a8d282-c477-46fd-9360-7693c4948b4b,ResourceVersion:20227,Generation:0,CreationTimestamp:2019-08-14 07:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 35224d23-3cdf-481c-8b2f-7b3cfd6a134d 0xc0021cff90 0xc0021cff91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0021cfff0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00310c010}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.14,PodIP:,StartTime:2019-08-14 07:33:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.803: INFO: Pod "nginx-deployment-7b8c6f4498-fcq2w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fcq2w,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-7b8c6f4498-fcq2w,UID:b7fbcff9-425e-4fb2-91df-8eadf4981169,ResourceVersion:20237,Generation:0,CreationTimestamp:2019-08-14 07:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.49/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 35224d23-3cdf-481c-8b2f-7b3cfd6a134d 0xc00310c0e0 0xc00310c0e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00310c140} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00310c160}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.14,PodIP:,StartTime:2019-08-14 07:33:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.804: INFO: Pod "nginx-deployment-7b8c6f4498-hwzk4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-hwzk4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-7b8c6f4498-hwzk4,UID:ea2c1c6d-69c0-4d77-9ff8-b082ec1bdd92,ResourceVersion:20243,Generation:0,CreationTimestamp:2019-08-14 07:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.191/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 35224d23-3cdf-481c-8b2f-7b3cfd6a134d 0xc00310c230 0xc00310c231}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00310c290} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00310c2b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-08-14 07:33:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.804: INFO: Pod "nginx-deployment-7b8c6f4498-jmrnl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-jmrnl,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-7b8c6f4498-jmrnl,UID:62551115-3516-4f3b-9fc9-fb75d4801f3d,ResourceVersion:20229,Generation:0,CreationTimestamp:2019-08-14 07:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 35224d23-3cdf-481c-8b2f-7b3cfd6a134d 0xc00310c370 0xc00310c371}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00310c3d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00310c3f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.14,PodIP:,StartTime:2019-08-14 07:33:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.804: INFO: Pod "nginx-deployment-7b8c6f4498-kfpf2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-kfpf2,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-7b8c6f4498-kfpf2,UID:8c18ed12-5a77-4d1b-854f-e7c4eccd2d7c,ResourceVersion:20107,Generation:0,CreationTimestamp:2019-08-14 07:33:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.181/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 35224d23-3cdf-481c-8b2f-7b3cfd6a134d 0xc00310c4c0 0xc00310c4c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00310c520} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00310c540}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:35 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:100.96.1.181,StartTime:2019-08-14 07:33:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-14 07:33:37 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://d4284963c25941ab729b5e0a97119d408039d5394eb21dcaaf9e178358591823}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.804: INFO: Pod "nginx-deployment-7b8c6f4498-kw6k9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-kw6k9,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-7b8c6f4498-kw6k9,UID:3caa76e0-c155-4b4c-8115-915df2560713,ResourceVersion:20239,Generation:0,CreationTimestamp:2019-08-14 07:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.190/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 35224d23-3cdf-481c-8b2f-7b3cfd6a134d 0xc00310c620 0xc00310c621}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00310c680} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00310c6a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-08-14 07:33:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.805: INFO: Pod "nginx-deployment-7b8c6f4498-m6w6v" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-m6w6v,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-7b8c6f4498-m6w6v,UID:c2025ade-eed2-4ece-99fb-432816ec69ec,ResourceVersion:20100,Generation:0,CreationTimestamp:2019-08-14 07:33:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.183/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 35224d23-3cdf-481c-8b2f-7b3cfd6a134d 0xc00310c770 0xc00310c771}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00310c7d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00310c7f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:35 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:100.96.1.183,StartTime:2019-08-14 07:33:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-14 07:33:37 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://89a14cd1d3b7434c1139b7a118130053f8d964cc6c710f850c19e26037b36c75}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.805: INFO: Pod "nginx-deployment-7b8c6f4498-mswdq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-mswdq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-7b8c6f4498-mswdq,UID:86255686-8936-48ef-9c72-748138c69a39,ResourceVersion:20201,Generation:0,CreationTimestamp:2019-08-14 07:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 35224d23-3cdf-481c-8b2f-7b3cfd6a134d 0xc00310c8c0 0xc00310c8c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00310c920} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00310c940}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.14,PodIP:,StartTime:2019-08-14 07:33:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.805: INFO: Pod "nginx-deployment-7b8c6f4498-mzmfv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-mzmfv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-7b8c6f4498-mzmfv,UID:358c6165-1cc0-4e0d-b561-3ee0e4722024,ResourceVersion:20233,Generation:0,CreationTimestamp:2019-08-14 07:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.188/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 35224d23-3cdf-481c-8b2f-7b3cfd6a134d 0xc00310ca10 0xc00310ca11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00310ca70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00310ca90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-08-14 07:33:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.805: INFO: Pod "nginx-deployment-7b8c6f4498-n688t" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-n688t,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-7b8c6f4498-n688t,UID:f109172d-a909-4ec5-85b6-ef03fc645407,ResourceVersion:20086,Generation:0,CreationTimestamp:2019-08-14 07:33:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.42/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 35224d23-3cdf-481c-8b2f-7b3cfd6a134d 0xc00310cb60 0xc00310cb61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00310cbc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00310cbe0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:35 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.14,PodIP:100.96.0.42,StartTime:2019-08-14 07:33:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-14 07:33:36 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://e7eb5203bf945d22fdc1780782717f318f0fe04fc07390889cabd44efc44992a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.805: INFO: Pod "nginx-deployment-7b8c6f4498-nbz4p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-nbz4p,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-7b8c6f4498-nbz4p,UID:1a9d452a-7cbf-4f8e-bd1a-be713dca519e,ResourceVersion:20211,Generation:0,CreationTimestamp:2019-08-14 07:33:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 35224d23-3cdf-481c-8b2f-7b3cfd6a134d 0xc00310ccb0 0xc00310ccb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00310cd10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00310cd30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:41 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-08-14 07:33:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.806: INFO: Pod "nginx-deployment-7b8c6f4498-tb29l" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-tb29l,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-7b8c6f4498-tb29l,UID:703eab3e-1f79-4eeb-810d-364d390e5f47,ResourceVersion:20083,Generation:0,CreationTimestamp:2019-08-14 07:33:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.0.43/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 35224d23-3cdf-481c-8b2f-7b3cfd6a134d 0xc00310ce00 0xc00310ce01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00310ce60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00310ce80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:35 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.14,PodIP:100.96.0.43,StartTime:2019-08-14 07:33:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-14 07:33:37 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://8f130917cc04fc46406dcd6a15e3a5082f9374f5303cf40731bebf2e9913687e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.806: INFO: Pod "nginx-deployment-7b8c6f4498-w7ng7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-w7ng7,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-7b8c6f4498-w7ng7,UID:81420042-83e9-4380-832c-5734f968aad7,ResourceVersion:20091,Generation:0,CreationTimestamp:2019-08-14 07:33:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.179/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 35224d23-3cdf-481c-8b2f-7b3cfd6a134d 0xc00310cf60 0xc00310cf61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00310cfc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00310cfe0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:35 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:100.96.1.179,StartTime:2019-08-14 07:33:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-14 07:33:36 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://9e508d7543ea4a5ee5914ca672ac913e1ecc412c2ef830e198d25397dd7f129a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Aug 14 07:33:43.806: INFO: Pod "nginx-deployment-7b8c6f4498-xdfn6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xdfn6,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1956,SelfLink:/api/v1/namespaces/deployment-1956/pods/nginx-deployment-7b8c6f4498-xdfn6,UID:019bf63c-e7aa-4d1b-8c06-ecc5b65dbe30,ResourceVersion:20096,Generation:0,CreationTimestamp:2019-08-14 07:33:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.96.1.182/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 35224d23-3cdf-481c-8b2f-7b3cfd6a134d 0xc00310d0c0 0xc00310d0c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-qdn7b {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-qdn7b,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-qdn7b true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00310d120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00310d140}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:33:35 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:100.96.1.182,StartTime:2019-08-14 07:33:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-08-14 07:33:37 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://599bc9a3842cd0cdb025c31b97b63de2613dffaa00b7b41e6c7edec64b2512b5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:33:43.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1956" for this suite.
Aug 14 07:33:49.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:33:49.938: INFO: namespace deployment-1956 deletion completed in 6.126876502s
â€¢SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:33:49.939: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4597
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 07:33:50.100: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-4597'
Aug 14 07:33:55.349: INFO: stderr: ""
Aug 14 07:33:55.349: INFO: stdout: "replicationcontroller/redis-master created\n"
Aug 14 07:33:55.349: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-4597'
Aug 14 07:33:55.595: INFO: stderr: ""
Aug 14 07:33:55.595: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 14 07:33:56.601: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 07:33:56.601: INFO: Found 0 / 1
Aug 14 07:33:57.601: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 07:33:57.601: INFO: Found 1 / 1
Aug 14 07:33:57.601: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 14 07:33:57.605: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 07:33:57.605: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 14 07:33:57.605: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe pod redis-master-fmd2z --namespace=kubectl-4597'
Aug 14 07:33:57.703: INFO: stderr: ""
Aug 14 07:33:57.703: INFO: stdout: "Name:           redis-master-fmd2z\nNamespace:      kubectl-4597\nPriority:       0\nNode:           shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2/10.250.0.8\nStart Time:     Wed, 14 Aug 2019 07:33:55 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    cni.projectcalico.org/podIP: 100.96.1.197/32\n                kubernetes.io/psp: e2e-test-privileged-psp\nStatus:         Running\nIP:             100.96.1.197\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://a23339717e2b6e568c9122d10bdc55c83454f9235aa7478e2e0ed0c91be13af6\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 14 Aug 2019 07:33:56 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-lr5k9 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-lr5k9:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-lr5k9\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                         Message\n  ----    ------     ----  ----                                                         -------\n  Normal  Scheduled  2s    default-scheduler                                            Successfully assigned kubectl-4597/redis-master-fmd2z to shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2\n  Normal  Pulled     2s    kubelet, shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Created container redis-master\n  Normal  Started    1s    kubelet, shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2  Started container redis-master\n"
Aug 14 07:33:57.704: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe rc redis-master --namespace=kubectl-4597'
Aug 14 07:33:57.802: INFO: stderr: ""
Aug 14 07:33:57.802: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-4597\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-fmd2z\n"
Aug 14 07:33:57.802: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe service redis-master --namespace=kubectl-4597'
Aug 14 07:33:57.905: INFO: stderr: ""
Aug 14 07:33:57.905: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-4597\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.68.8.235\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.96.1.197:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 14 07:33:57.910: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2'
Aug 14 07:33:58.015: INFO: stderr: ""
Aug 14 07:33:58.015: INFO: stdout: "Name:               shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=ecf69421-6238-4433-a0a0-70bb24198e09\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/zone=rot_1_1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/role=node\n                    worker.garden.sapcloud.io/group=cpu-worker\n                    worker.gardener.cloud/pool=cpu-worker\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.250.0.8/19\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.104.65.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 14 Aug 2019 06:11:41 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 14 Aug 2019 06:12:20 +0000   Wed, 14 Aug 2019 06:12:20 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 14 Aug 2019 07:33:53 +0000   Wed, 14 Aug 2019 06:11:41 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 14 Aug 2019 07:33:53 +0000   Wed, 14 Aug 2019 06:11:41 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 14 Aug 2019 07:33:53 +0000   Wed, 14 Aug 2019 06:11:41 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 14 Aug 2019 07:33:53 +0000   Wed, 14 Aug 2019 06:12:21 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.250.0.8\nCapacity:\n attachable-volumes-cinder:  256\n cpu:                        2\n ephemeral-storage:          38216108Ki\n hugepages-1Gi:              0\n hugepages-2Mi:              0\n memory:                     4039624Ki\n pods:                       110\nAllocatable:\n attachable-volumes-cinder:  256\n cpu:                        1920m\n ephemeral-storage:          37176629834\n hugepages-1Gi:              0\n hugepages-2Mi:              0\n memory:                     2856004401\n pods:                       110\nSystem Info:\n Machine ID:                 f0114ec86ba74c58b86a14f043c2651d\n System UUID:                f0114ec8-6ba7-4c58-b86a-14f043c2651d\n Boot ID:                    21f30103-8652-47e5-a90f-b0eb57948071\n Kernel Version:             4.19.56-coreos-r1\n OS Image:                   Container Linux by CoreOS 2135.6.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.3\n Kubelet Version:            v1.15.2\n Kube-Proxy Version:         v1.15.2\nPodCIDR:                     100.96.1.0/24\nProviderID:                  openstack:///f0114ec8-6ba7-4c58-b86a-14f043c2651d\nNon-terminated Pods:         (4 in total)\n  Namespace                  Name                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                   ------------  ----------  ---------------  -------------  ---\n  kube-system                calico-node-8jgsw      100m (5%)     500m (26%)  100Mi (3%)       700Mi (25%)    82m\n  kube-system                kube-proxy-2j2fb       20m (1%)      0 (0%)      64Mi (2%)        0 (0%)         82m\n  kube-system                node-exporter-dzrg2    5m (0%)       25m (1%)    10Mi (0%)        100Mi (3%)     82m\n  kubectl-4597               redis-master-fmd2z     0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                   Requests    Limits\n  --------                   --------    ------\n  cpu                        125m (6%)   525m (27%)\n  memory                     174Mi (6%)  800Mi (29%)\n  ephemeral-storage          0 (0%)      0 (0%)\n  attachable-volumes-cinder  0           0\nEvents:                      <none>\n"
Aug 14 07:33:58.015: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe namespace kubectl-4597'
Aug 14 07:33:58.100: INFO: stderr: ""
Aug 14 07:33:58.100: INFO: stdout: "Name:         kubectl-4597\nLabels:       e2e-framework=kubectl\n              e2e-run=a80b271f-2ac5-4304-af3e-63f31fe4a4be\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:33:58.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4597" for this suite.
Aug 14 07:34:20.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:34:20.233: INFO: namespace kubectl-4597 deletion completed in 22.12923414s
â€¢SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:34:20.234: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6306
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-2870
STEP: Creating secret with name secret-test-4fc27ebd-b6b3-434f-bdb4-b137c9ba627b
STEP: Creating a pod to test consume secrets
Aug 14 07:34:20.533: INFO: Waiting up to 5m0s for pod "pod-secrets-e4f4d0e6-2a8f-4ad3-86a3-2d4cdaa14260" in namespace "secrets-6306" to be "success or failure"
Aug 14 07:34:20.538: INFO: Pod "pod-secrets-e4f4d0e6-2a8f-4ad3-86a3-2d4cdaa14260": Phase="Pending", Reason="", readiness=false. Elapsed: 4.935078ms
Aug 14 07:34:22.543: INFO: Pod "pod-secrets-e4f4d0e6-2a8f-4ad3-86a3-2d4cdaa14260": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009785947s
STEP: Saw pod success
Aug 14 07:34:22.543: INFO: Pod "pod-secrets-e4f4d0e6-2a8f-4ad3-86a3-2d4cdaa14260" satisfied condition "success or failure"
Aug 14 07:34:22.546: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-secrets-e4f4d0e6-2a8f-4ad3-86a3-2d4cdaa14260 container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 07:34:22.564: INFO: Waiting for pod pod-secrets-e4f4d0e6-2a8f-4ad3-86a3-2d4cdaa14260 to disappear
Aug 14 07:34:22.567: INFO: Pod pod-secrets-e4f4d0e6-2a8f-4ad3-86a3-2d4cdaa14260 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:34:22.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6306" for this suite.
Aug 14 07:34:28.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:34:28.699: INFO: namespace secrets-6306 deletion completed in 6.127690245s
STEP: Destroying namespace "secret-namespace-2870" for this suite.
Aug 14 07:34:34.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:34:34.840: INFO: namespace secret-namespace-2870 deletion completed in 6.141078075s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:34:34.841: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3578
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-aba9f7ea-7272-4c37-86b3-869b4ce60978
STEP: Creating a pod to test consume configMaps
Aug 14 07:34:34.999: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-72f954e0-4754-448b-a10e-1e6425bbbed5" in namespace "projected-3578" to be "success or failure"
Aug 14 07:34:35.003: INFO: Pod "pod-projected-configmaps-72f954e0-4754-448b-a10e-1e6425bbbed5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.437337ms
Aug 14 07:34:37.008: INFO: Pod "pod-projected-configmaps-72f954e0-4754-448b-a10e-1e6425bbbed5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008060213s
STEP: Saw pod success
Aug 14 07:34:37.008: INFO: Pod "pod-projected-configmaps-72f954e0-4754-448b-a10e-1e6425bbbed5" satisfied condition "success or failure"
Aug 14 07:34:37.011: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-projected-configmaps-72f954e0-4754-448b-a10e-1e6425bbbed5 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 07:34:37.028: INFO: Waiting for pod pod-projected-configmaps-72f954e0-4754-448b-a10e-1e6425bbbed5 to disappear
Aug 14 07:34:37.030: INFO: Pod pod-projected-configmaps-72f954e0-4754-448b-a10e-1e6425bbbed5 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:34:37.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3578" for this suite.
Aug 14 07:34:43.046: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:34:43.195: INFO: namespace projected-3578 deletion completed in 6.16109554s
â€¢S
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:34:43.196: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3298
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-3298/configmap-test-b2d1fa82-cdba-47f9-9465-9f7de9cf93e5
STEP: Creating a pod to test consume configMaps
Aug 14 07:34:43.351: INFO: Waiting up to 5m0s for pod "pod-configmaps-22681fdd-8bb7-4c8d-939b-d508cabf5df0" in namespace "configmap-3298" to be "success or failure"
Aug 14 07:34:43.354: INFO: Pod "pod-configmaps-22681fdd-8bb7-4c8d-939b-d508cabf5df0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.021894ms
Aug 14 07:34:45.358: INFO: Pod "pod-configmaps-22681fdd-8bb7-4c8d-939b-d508cabf5df0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007901233s
STEP: Saw pod success
Aug 14 07:34:45.359: INFO: Pod "pod-configmaps-22681fdd-8bb7-4c8d-939b-d508cabf5df0" satisfied condition "success or failure"
Aug 14 07:34:45.363: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-configmaps-22681fdd-8bb7-4c8d-939b-d508cabf5df0 container env-test: <nil>
STEP: delete the pod
Aug 14 07:34:45.382: INFO: Waiting for pod pod-configmaps-22681fdd-8bb7-4c8d-939b-d508cabf5df0 to disappear
Aug 14 07:34:45.386: INFO: Pod pod-configmaps-22681fdd-8bb7-4c8d-939b-d508cabf5df0 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:34:45.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3298" for this suite.
Aug 14 07:34:51.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:34:51.524: INFO: namespace configmap-3298 deletion completed in 6.129421716s
â€¢
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:34:51.524: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6998
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 14 07:34:51.695: INFO: Waiting up to 5m0s for pod "pod-31fd8830-63be-4355-984b-e17e9eedc2c7" in namespace "emptydir-6998" to be "success or failure"
Aug 14 07:34:51.707: INFO: Pod "pod-31fd8830-63be-4355-984b-e17e9eedc2c7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.423008ms
Aug 14 07:34:53.713: INFO: Pod "pod-31fd8830-63be-4355-984b-e17e9eedc2c7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017010709s
STEP: Saw pod success
Aug 14 07:34:53.713: INFO: Pod "pod-31fd8830-63be-4355-984b-e17e9eedc2c7" satisfied condition "success or failure"
Aug 14 07:34:53.717: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-31fd8830-63be-4355-984b-e17e9eedc2c7 container test-container: <nil>
STEP: delete the pod
Aug 14 07:34:53.733: INFO: Waiting for pod pod-31fd8830-63be-4355-984b-e17e9eedc2c7 to disappear
Aug 14 07:34:53.736: INFO: Pod pod-31fd8830-63be-4355-984b-e17e9eedc2c7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:34:53.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6998" for this suite.
Aug 14 07:34:59.754: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:34:59.914: INFO: namespace emptydir-6998 deletion completed in 6.172439489s
â€¢SSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:34:59.915: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-3763
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Aug 14 07:35:03.117: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:35:04.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3763" for this suite.
Aug 14 07:35:26.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:35:26.269: INFO: namespace replicaset-3763 deletion completed in 22.130317377s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:35:26.271: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2354
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 14 07:35:26.423: INFO: Waiting up to 5m0s for pod "downward-api-b2129269-4f90-44bc-a97a-d93e32bb4eaf" in namespace "downward-api-2354" to be "success or failure"
Aug 14 07:35:26.426: INFO: Pod "downward-api-b2129269-4f90-44bc-a97a-d93e32bb4eaf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.908501ms
Aug 14 07:35:28.431: INFO: Pod "downward-api-b2129269-4f90-44bc-a97a-d93e32bb4eaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007768258s
STEP: Saw pod success
Aug 14 07:35:28.431: INFO: Pod "downward-api-b2129269-4f90-44bc-a97a-d93e32bb4eaf" satisfied condition "success or failure"
Aug 14 07:35:28.434: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod downward-api-b2129269-4f90-44bc-a97a-d93e32bb4eaf container dapi-container: <nil>
STEP: delete the pod
Aug 14 07:35:28.460: INFO: Waiting for pod downward-api-b2129269-4f90-44bc-a97a-d93e32bb4eaf to disappear
Aug 14 07:35:28.464: INFO: Pod downward-api-b2129269-4f90-44bc-a97a-d93e32bb4eaf no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:35:28.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2354" for this suite.
Aug 14 07:35:34.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:35:34.594: INFO: namespace downward-api-2354 deletion completed in 6.125627095s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:35:34.597: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3736
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Aug 14 07:35:37.289: INFO: Successfully updated pod "annotationupdatedb96733a-5c48-43ec-b9b6-c50a4006331c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:35:39.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3736" for this suite.
Aug 14 07:36:01.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:36:01.453: INFO: namespace projected-3736 deletion completed in 22.139084081s
â€¢SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:36:01.454: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8671
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Aug 14 07:36:01.618: INFO: Waiting up to 5m0s for pod "pod-5ed50c59-adfa-42a1-a757-d354e9e3419e" in namespace "emptydir-8671" to be "success or failure"
Aug 14 07:36:01.624: INFO: Pod "pod-5ed50c59-adfa-42a1-a757-d354e9e3419e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.088086ms
Aug 14 07:36:03.629: INFO: Pod "pod-5ed50c59-adfa-42a1-a757-d354e9e3419e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011197188s
STEP: Saw pod success
Aug 14 07:36:03.629: INFO: Pod "pod-5ed50c59-adfa-42a1-a757-d354e9e3419e" satisfied condition "success or failure"
Aug 14 07:36:03.633: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-5ed50c59-adfa-42a1-a757-d354e9e3419e container test-container: <nil>
STEP: delete the pod
Aug 14 07:36:03.650: INFO: Waiting for pod pod-5ed50c59-adfa-42a1-a757-d354e9e3419e to disappear
Aug 14 07:36:03.653: INFO: Pod pod-5ed50c59-adfa-42a1-a757-d354e9e3419e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:36:03.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8671" for this suite.
Aug 14 07:36:09.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:36:09.789: INFO: namespace emptydir-8671 deletion completed in 6.130994202s
â€¢SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:36:09.789: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6298
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 07:36:09.941: INFO: Waiting up to 5m0s for pod "downwardapi-volume-be4269eb-d83d-40a9-9c57-bf49105f3491" in namespace "downward-api-6298" to be "success or failure"
Aug 14 07:36:09.944: INFO: Pod "downwardapi-volume-be4269eb-d83d-40a9-9c57-bf49105f3491": Phase="Pending", Reason="", readiness=false. Elapsed: 2.713873ms
Aug 14 07:36:11.949: INFO: Pod "downwardapi-volume-be4269eb-d83d-40a9-9c57-bf49105f3491": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008053215s
STEP: Saw pod success
Aug 14 07:36:11.949: INFO: Pod "downwardapi-volume-be4269eb-d83d-40a9-9c57-bf49105f3491" satisfied condition "success or failure"
Aug 14 07:36:11.952: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod downwardapi-volume-be4269eb-d83d-40a9-9c57-bf49105f3491 container client-container: <nil>
STEP: delete the pod
Aug 14 07:36:11.969: INFO: Waiting for pod downwardapi-volume-be4269eb-d83d-40a9-9c57-bf49105f3491 to disappear
Aug 14 07:36:11.972: INFO: Pod downwardapi-volume-be4269eb-d83d-40a9-9c57-bf49105f3491 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:36:11.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6298" for this suite.
Aug 14 07:36:17.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:36:18.107: INFO: namespace downward-api-6298 deletion completed in 6.131038101s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:36:18.108: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8441
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 14 07:36:18.261: INFO: Waiting up to 5m0s for pod "downward-api-c9a716e7-4a2e-4003-bf49-183885e363de" in namespace "downward-api-8441" to be "success or failure"
Aug 14 07:36:18.265: INFO: Pod "downward-api-c9a716e7-4a2e-4003-bf49-183885e363de": Phase="Pending", Reason="", readiness=false. Elapsed: 4.361269ms
Aug 14 07:36:20.271: INFO: Pod "downward-api-c9a716e7-4a2e-4003-bf49-183885e363de": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009435791s
STEP: Saw pod success
Aug 14 07:36:20.271: INFO: Pod "downward-api-c9a716e7-4a2e-4003-bf49-183885e363de" satisfied condition "success or failure"
Aug 14 07:36:20.275: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod downward-api-c9a716e7-4a2e-4003-bf49-183885e363de container dapi-container: <nil>
STEP: delete the pod
Aug 14 07:36:20.293: INFO: Waiting for pod downward-api-c9a716e7-4a2e-4003-bf49-183885e363de to disappear
Aug 14 07:36:20.296: INFO: Pod downward-api-c9a716e7-4a2e-4003-bf49-183885e363de no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:36:20.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8441" for this suite.
Aug 14 07:36:26.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:36:26.463: INFO: namespace downward-api-8441 deletion completed in 6.161600476s
â€¢SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:36:26.463: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7689
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-d6d10fb9-dfd3-4562-875c-b712e8d8a898
STEP: Creating a pod to test consume secrets
Aug 14 07:36:26.620: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b897c6aa-01de-47f6-b92f-c863b8acb843" in namespace "projected-7689" to be "success or failure"
Aug 14 07:36:26.624: INFO: Pod "pod-projected-secrets-b897c6aa-01de-47f6-b92f-c863b8acb843": Phase="Pending", Reason="", readiness=false. Elapsed: 3.755358ms
Aug 14 07:36:28.630: INFO: Pod "pod-projected-secrets-b897c6aa-01de-47f6-b92f-c863b8acb843": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009898159s
STEP: Saw pod success
Aug 14 07:36:28.630: INFO: Pod "pod-projected-secrets-b897c6aa-01de-47f6-b92f-c863b8acb843" satisfied condition "success or failure"
Aug 14 07:36:28.633: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-projected-secrets-b897c6aa-01de-47f6-b92f-c863b8acb843 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 14 07:36:28.653: INFO: Waiting for pod pod-projected-secrets-b897c6aa-01de-47f6-b92f-c863b8acb843 to disappear
Aug 14 07:36:28.656: INFO: Pod pod-projected-secrets-b897c6aa-01de-47f6-b92f-c863b8acb843 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:36:28.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7689" for this suite.
Aug 14 07:36:34.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:36:34.801: INFO: namespace projected-7689 deletion completed in 6.139965405s
â€¢S
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:36:34.801: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9833
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-933b4367-61fb-4e17-b827-092ae1048a98
STEP: Creating a pod to test consume configMaps
Aug 14 07:36:34.961: INFO: Waiting up to 5m0s for pod "pod-configmaps-bec76371-259b-46ca-8b57-52ea1adadfa4" in namespace "configmap-9833" to be "success or failure"
Aug 14 07:36:34.965: INFO: Pod "pod-configmaps-bec76371-259b-46ca-8b57-52ea1adadfa4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.557401ms
Aug 14 07:36:36.971: INFO: Pod "pod-configmaps-bec76371-259b-46ca-8b57-52ea1adadfa4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01049892s
STEP: Saw pod success
Aug 14 07:36:36.972: INFO: Pod "pod-configmaps-bec76371-259b-46ca-8b57-52ea1adadfa4" satisfied condition "success or failure"
Aug 14 07:36:36.975: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-configmaps-bec76371-259b-46ca-8b57-52ea1adadfa4 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 07:36:36.993: INFO: Waiting for pod pod-configmaps-bec76371-259b-46ca-8b57-52ea1adadfa4 to disappear
Aug 14 07:36:36.996: INFO: Pod pod-configmaps-bec76371-259b-46ca-8b57-52ea1adadfa4 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:36:36.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9833" for this suite.
Aug 14 07:36:43.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:36:43.170: INFO: namespace configmap-9833 deletion completed in 6.169243534s
â€¢SSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:36:43.170: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-1428
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Aug 14 07:36:43.331: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1428,SelfLink:/api/v1/namespaces/watch-1428/configmaps/e2e-watch-test-watch-closed,UID:4ee47536-de47-44c5-893b-acc002183052,ResourceVersion:21153,Generation:0,CreationTimestamp:2019-08-14 07:36:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Aug 14 07:36:43.331: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1428,SelfLink:/api/v1/namespaces/watch-1428/configmaps/e2e-watch-test-watch-closed,UID:4ee47536-de47-44c5-893b-acc002183052,ResourceVersion:21154,Generation:0,CreationTimestamp:2019-08-14 07:36:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Aug 14 07:36:43.345: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1428,SelfLink:/api/v1/namespaces/watch-1428/configmaps/e2e-watch-test-watch-closed,UID:4ee47536-de47-44c5-893b-acc002183052,ResourceVersion:21155,Generation:0,CreationTimestamp:2019-08-14 07:36:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Aug 14 07:36:43.345: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-1428,SelfLink:/api/v1/namespaces/watch-1428/configmaps/e2e-watch-test-watch-closed,UID:4ee47536-de47-44c5-893b-acc002183052,ResourceVersion:21156,Generation:0,CreationTimestamp:2019-08-14 07:36:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:36:43.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1428" for this suite.
Aug 14 07:36:49.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:36:49.474: INFO: namespace watch-1428 deletion completed in 6.124522206s
â€¢SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:36:49.475: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3927
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Aug 14 07:36:49.620: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-3927'
Aug 14 07:36:49.861: INFO: stderr: ""
Aug 14 07:36:49.861: INFO: stdout: "pod/pause created\n"
Aug 14 07:36:49.861: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 14 07:36:49.861: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3927" to be "running and ready"
Aug 14 07:36:49.864: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.13613ms
Aug 14 07:36:51.868: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.007317665s
Aug 14 07:36:51.868: INFO: Pod "pause" satisfied condition "running and ready"
Aug 14 07:36:51.868: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Aug 14 07:36:51.868: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config label pods pause testing-label=testing-label-value --namespace=kubectl-3927'
Aug 14 07:36:51.942: INFO: stderr: ""
Aug 14 07:36:51.942: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Aug 14 07:36:51.942: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-3927'
Aug 14 07:36:52.018: INFO: stderr: ""
Aug 14 07:36:52.018: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Aug 14 07:36:52.019: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config label pods pause testing-label- --namespace=kubectl-3927'
Aug 14 07:36:52.106: INFO: stderr: ""
Aug 14 07:36:52.106: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Aug 14 07:36:52.106: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-3927'
Aug 14 07:36:52.176: INFO: stderr: ""
Aug 14 07:36:52.176: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Aug 14 07:36:52.176: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-3927'
Aug 14 07:36:52.261: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 07:36:52.261: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 14 07:36:52.261: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=pause --no-headers --namespace=kubectl-3927'
Aug 14 07:36:52.346: INFO: stderr: "No resources found.\n"
Aug 14 07:36:52.346: INFO: stdout: ""
Aug 14 07:36:52.346: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=pause --namespace=kubectl-3927 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 14 07:36:52.418: INFO: stderr: ""
Aug 14 07:36:52.418: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:36:52.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3927" for this suite.
Aug 14 07:36:58.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:36:58.597: INFO: namespace kubectl-3927 deletion completed in 6.172650562s
â€¢SSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:36:58.597: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-2237
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-qnz6k in namespace proxy-2237
I0814 07:36:58.769466    3092 runners.go:180] Created replication controller with name: proxy-service-qnz6k, namespace: proxy-2237, replica count: 1
I0814 07:36:59.820281    3092 runners.go:180] proxy-service-qnz6k Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 07:37:00.820517    3092 runners.go:180] proxy-service-qnz6k Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0814 07:37:01.820753    3092 runners.go:180] proxy-service-qnz6k Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0814 07:37:02.821029    3092 runners.go:180] proxy-service-qnz6k Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 14 07:37:02.825: INFO: setup took 4.069414421s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Aug 14 07:37:02.838: INFO: (0) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">... (200; 12.659622ms)
Aug 14 07:37:02.839: INFO: (0) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 13.406413ms)
Aug 14 07:37:02.839: INFO: (0) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/rewriteme">test</a> (200; 13.536497ms)
Aug 14 07:37:02.839: INFO: (0) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">test<... (200; 13.521222ms)
Aug 14 07:37:02.843: INFO: (0) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 17.715697ms)
Aug 14 07:37:02.843: INFO: (0) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname2/proxy/: bar (200; 17.744583ms)
Aug 14 07:37:02.843: INFO: (0) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname1/proxy/: foo (200; 17.841432ms)
Aug 14 07:37:02.843: INFO: (0) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname2/proxy/: bar (200; 17.795663ms)
Aug 14 07:37:02.843: INFO: (0) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 17.839195ms)
Aug 14 07:37:02.843: INFO: (0) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 18.27304ms)
Aug 14 07:37:02.843: INFO: (0) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname1/proxy/: foo (200; 18.191437ms)
Aug 14 07:37:02.844: INFO: (0) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:460/proxy/: tls baz (200; 19.549622ms)
Aug 14 07:37:02.846: INFO: (0) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:462/proxy/: tls qux (200; 20.925574ms)
Aug 14 07:37:02.846: INFO: (0) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname1/proxy/: tls baz (200; 21.07787ms)
Aug 14 07:37:02.846: INFO: (0) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/tlsrewritem... (200; 20.910306ms)
Aug 14 07:37:02.858: INFO: (0) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname2/proxy/: tls qux (200; 32.426157ms)
Aug 14 07:37:02.868: INFO: (1) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:460/proxy/: tls baz (200; 9.888406ms)
Aug 14 07:37:02.868: INFO: (1) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/rewriteme">test</a> (200; 9.87499ms)
Aug 14 07:37:02.868: INFO: (1) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">test<... (200; 9.925802ms)
Aug 14 07:37:02.868: INFO: (1) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 10.004813ms)
Aug 14 07:37:02.868: INFO: (1) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/tlsrewritem... (200; 10.050407ms)
Aug 14 07:37:02.868: INFO: (1) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 10.074997ms)
Aug 14 07:37:02.868: INFO: (1) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 9.895433ms)
Aug 14 07:37:02.868: INFO: (1) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:462/proxy/: tls qux (200; 10.008237ms)
Aug 14 07:37:02.868: INFO: (1) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">... (200; 9.935193ms)
Aug 14 07:37:02.868: INFO: (1) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 10.076463ms)
Aug 14 07:37:02.868: INFO: (1) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname2/proxy/: tls qux (200; 10.347569ms)
Aug 14 07:37:02.869: INFO: (1) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname1/proxy/: foo (200; 11.798712ms)
Aug 14 07:37:02.870: INFO: (1) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname2/proxy/: bar (200; 11.920044ms)
Aug 14 07:37:02.870: INFO: (1) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname1/proxy/: tls baz (200; 11.821306ms)
Aug 14 07:37:02.870: INFO: (1) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname2/proxy/: bar (200; 11.908589ms)
Aug 14 07:37:02.870: INFO: (1) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname1/proxy/: foo (200; 12.273562ms)
Aug 14 07:37:02.879: INFO: (2) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 8.473418ms)
Aug 14 07:37:02.879: INFO: (2) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 8.567423ms)
Aug 14 07:37:02.879: INFO: (2) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:462/proxy/: tls qux (200; 8.647813ms)
Aug 14 07:37:02.879: INFO: (2) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 8.519395ms)
Aug 14 07:37:02.879: INFO: (2) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">test<... (200; 8.610514ms)
Aug 14 07:37:02.879: INFO: (2) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/tlsrewritem... (200; 8.520992ms)
Aug 14 07:37:02.879: INFO: (2) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">... (200; 8.594416ms)
Aug 14 07:37:02.879: INFO: (2) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 8.767963ms)
Aug 14 07:37:02.879: INFO: (2) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname1/proxy/: tls baz (200; 8.865308ms)
Aug 14 07:37:02.879: INFO: (2) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname2/proxy/: tls qux (200; 8.733742ms)
Aug 14 07:37:02.879: INFO: (2) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:460/proxy/: tls baz (200; 8.8859ms)
Aug 14 07:37:02.879: INFO: (2) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/rewriteme">test</a> (200; 8.914487ms)
Aug 14 07:37:02.880: INFO: (2) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname2/proxy/: bar (200; 9.466651ms)
Aug 14 07:37:02.881: INFO: (2) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname1/proxy/: foo (200; 10.410355ms)
Aug 14 07:37:02.881: INFO: (2) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname2/proxy/: bar (200; 10.405582ms)
Aug 14 07:37:02.881: INFO: (2) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname1/proxy/: foo (200; 10.556808ms)
Aug 14 07:37:02.886: INFO: (3) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 4.967208ms)
Aug 14 07:37:02.886: INFO: (3) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/rewriteme">test</a> (200; 5.428234ms)
Aug 14 07:37:02.887: INFO: (3) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/tlsrewritem... (200; 6.506715ms)
Aug 14 07:37:02.887: INFO: (3) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:460/proxy/: tls baz (200; 6.462581ms)
Aug 14 07:37:02.887: INFO: (3) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">test<... (200; 6.551827ms)
Aug 14 07:37:02.887: INFO: (3) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 6.61726ms)
Aug 14 07:37:02.887: INFO: (3) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 6.593894ms)
Aug 14 07:37:02.887: INFO: (3) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">... (200; 6.737222ms)
Aug 14 07:37:02.887: INFO: (3) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 6.653221ms)
Aug 14 07:37:02.888: INFO: (3) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname2/proxy/: bar (200; 7.452606ms)
Aug 14 07:37:02.888: INFO: (3) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname1/proxy/: tls baz (200; 7.395187ms)
Aug 14 07:37:02.889: INFO: (3) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:462/proxy/: tls qux (200; 7.699975ms)
Aug 14 07:37:02.889: INFO: (3) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname1/proxy/: foo (200; 8.24961ms)
Aug 14 07:37:02.889: INFO: (3) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname2/proxy/: tls qux (200; 8.304881ms)
Aug 14 07:37:02.889: INFO: (3) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname1/proxy/: foo (200; 8.401758ms)
Aug 14 07:37:02.889: INFO: (3) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname2/proxy/: bar (200; 8.321332ms)
Aug 14 07:37:02.896: INFO: (4) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">test<... (200; 6.600088ms)
Aug 14 07:37:02.896: INFO: (4) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/rewriteme">test</a> (200; 6.703404ms)
Aug 14 07:37:02.896: INFO: (4) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:462/proxy/: tls qux (200; 6.627594ms)
Aug 14 07:37:02.896: INFO: (4) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:460/proxy/: tls baz (200; 6.821692ms)
Aug 14 07:37:02.896: INFO: (4) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 6.590278ms)
Aug 14 07:37:02.896: INFO: (4) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname1/proxy/: tls baz (200; 6.925757ms)
Aug 14 07:37:02.896: INFO: (4) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 6.865881ms)
Aug 14 07:37:02.896: INFO: (4) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">... (200; 6.950404ms)
Aug 14 07:37:02.896: INFO: (4) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 6.940034ms)
Aug 14 07:37:02.896: INFO: (4) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 6.954293ms)
Aug 14 07:37:02.896: INFO: (4) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/tlsrewritem... (200; 7.017532ms)
Aug 14 07:37:02.896: INFO: (4) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname2/proxy/: tls qux (200; 6.977028ms)
Aug 14 07:37:02.897: INFO: (4) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname2/proxy/: bar (200; 7.597941ms)
Aug 14 07:37:02.898: INFO: (4) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname1/proxy/: foo (200; 8.411602ms)
Aug 14 07:37:02.898: INFO: (4) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname2/proxy/: bar (200; 8.411234ms)
Aug 14 07:37:02.898: INFO: (4) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname1/proxy/: foo (200; 8.889324ms)
Aug 14 07:37:02.905: INFO: (5) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 6.475461ms)
Aug 14 07:37:02.905: INFO: (5) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:462/proxy/: tls qux (200; 7.053356ms)
Aug 14 07:37:02.905: INFO: (5) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 6.976033ms)
Aug 14 07:37:02.905: INFO: (5) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:460/proxy/: tls baz (200; 6.883989ms)
Aug 14 07:37:02.906: INFO: (5) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/rewriteme">test</a> (200; 7.389045ms)
Aug 14 07:37:02.906: INFO: (5) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/tlsrewritem... (200; 7.48137ms)
Aug 14 07:37:02.906: INFO: (5) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">... (200; 7.544545ms)
Aug 14 07:37:02.906: INFO: (5) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">test<... (200; 7.743394ms)
Aug 14 07:37:02.906: INFO: (5) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 7.660207ms)
Aug 14 07:37:02.906: INFO: (5) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 7.988521ms)
Aug 14 07:37:02.906: INFO: (5) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname2/proxy/: bar (200; 8.081762ms)
Aug 14 07:37:02.907: INFO: (5) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname2/proxy/: tls qux (200; 8.251091ms)
Aug 14 07:37:02.907: INFO: (5) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname2/proxy/: bar (200; 8.927435ms)
Aug 14 07:37:02.907: INFO: (5) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname1/proxy/: foo (200; 8.935357ms)
Aug 14 07:37:02.907: INFO: (5) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname1/proxy/: tls baz (200; 9.062216ms)
Aug 14 07:37:02.907: INFO: (5) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname1/proxy/: foo (200; 8.899296ms)
Aug 14 07:37:02.913: INFO: (6) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 5.229268ms)
Aug 14 07:37:02.913: INFO: (6) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 5.593078ms)
Aug 14 07:37:02.913: INFO: (6) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 5.845296ms)
Aug 14 07:37:02.913: INFO: (6) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">... (200; 5.815528ms)
Aug 14 07:37:02.913: INFO: (6) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:462/proxy/: tls qux (200; 5.030757ms)
Aug 14 07:37:02.914: INFO: (6) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">test<... (200; 5.283757ms)
Aug 14 07:37:02.914: INFO: (6) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 5.822539ms)
Aug 14 07:37:02.914: INFO: (6) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/tlsrewritem... (200; 6.147206ms)
Aug 14 07:37:02.914: INFO: (6) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/rewriteme">test</a> (200; 5.630563ms)
Aug 14 07:37:02.914: INFO: (6) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:460/proxy/: tls baz (200; 5.897395ms)
Aug 14 07:37:02.914: INFO: (6) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname1/proxy/: foo (200; 6.010722ms)
Aug 14 07:37:02.914: INFO: (6) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname1/proxy/: foo (200; 6.512746ms)
Aug 14 07:37:02.914: INFO: (6) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname2/proxy/: tls qux (200; 6.506331ms)
Aug 14 07:37:02.915: INFO: (6) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname2/proxy/: bar (200; 6.899184ms)
Aug 14 07:37:02.916: INFO: (6) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname2/proxy/: bar (200; 7.693091ms)
Aug 14 07:37:02.920: INFO: (6) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname1/proxy/: tls baz (200; 11.680003ms)
Aug 14 07:37:02.926: INFO: (7) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 5.711119ms)
Aug 14 07:37:02.927: INFO: (7) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">... (200; 6.543699ms)
Aug 14 07:37:02.927: INFO: (7) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:462/proxy/: tls qux (200; 6.903889ms)
Aug 14 07:37:02.927: INFO: (7) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">test<... (200; 6.85442ms)
Aug 14 07:37:02.927: INFO: (7) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 6.87316ms)
Aug 14 07:37:02.927: INFO: (7) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 7.15342ms)
Aug 14 07:37:02.928: INFO: (7) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/tlsrewritem... (200; 7.19308ms)
Aug 14 07:37:02.928: INFO: (7) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/rewriteme">test</a> (200; 7.236448ms)
Aug 14 07:37:02.928: INFO: (7) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:460/proxy/: tls baz (200; 7.375908ms)
Aug 14 07:37:02.928: INFO: (7) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 7.264833ms)
Aug 14 07:37:02.928: INFO: (7) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname1/proxy/: foo (200; 7.557552ms)
Aug 14 07:37:02.929: INFO: (7) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname2/proxy/: bar (200; 9.125345ms)
Aug 14 07:37:02.929: INFO: (7) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname2/proxy/: tls qux (200; 9.209288ms)
Aug 14 07:37:02.929: INFO: (7) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname2/proxy/: bar (200; 9.159271ms)
Aug 14 07:37:02.929: INFO: (7) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname1/proxy/: foo (200; 9.3417ms)
Aug 14 07:37:02.929: INFO: (7) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname1/proxy/: tls baz (200; 9.283619ms)
Aug 14 07:37:02.938: INFO: (8) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/rewriteme">test</a> (200; 7.940544ms)
Aug 14 07:37:02.938: INFO: (8) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">... (200; 7.746561ms)
Aug 14 07:37:02.938: INFO: (8) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:460/proxy/: tls baz (200; 8.215555ms)
Aug 14 07:37:02.938: INFO: (8) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname1/proxy/: tls baz (200; 8.011335ms)
Aug 14 07:37:02.938: INFO: (8) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/tlsrewritem... (200; 7.721723ms)
Aug 14 07:37:02.938: INFO: (8) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">test<... (200; 7.816607ms)
Aug 14 07:37:02.938: INFO: (8) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname2/proxy/: tls qux (200; 8.21089ms)
Aug 14 07:37:02.938: INFO: (8) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:462/proxy/: tls qux (200; 8.265886ms)
Aug 14 07:37:02.938: INFO: (8) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 8.374383ms)
Aug 14 07:37:02.938: INFO: (8) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 8.275319ms)
Aug 14 07:37:02.938: INFO: (8) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname2/proxy/: bar (200; 8.521694ms)
Aug 14 07:37:02.938: INFO: (8) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname1/proxy/: foo (200; 8.635474ms)
Aug 14 07:37:02.939: INFO: (8) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 9.243767ms)
Aug 14 07:37:02.939: INFO: (8) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname1/proxy/: foo (200; 9.523791ms)
Aug 14 07:37:02.939: INFO: (8) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname2/proxy/: bar (200; 9.458891ms)
Aug 14 07:37:02.941: INFO: (8) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 10.814219ms)
Aug 14 07:37:02.947: INFO: (9) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 6.064025ms)
Aug 14 07:37:02.948: INFO: (9) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 7.207136ms)
Aug 14 07:37:02.948: INFO: (9) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:462/proxy/: tls qux (200; 7.414722ms)
Aug 14 07:37:02.948: INFO: (9) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/tlsrewritem... (200; 7.483128ms)
Aug 14 07:37:02.951: INFO: (9) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/rewriteme">test</a> (200; 10.180078ms)
Aug 14 07:37:02.951: INFO: (9) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 10.030099ms)
Aug 14 07:37:02.951: INFO: (9) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname1/proxy/: foo (200; 10.115761ms)
Aug 14 07:37:02.951: INFO: (9) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname2/proxy/: bar (200; 10.240999ms)
Aug 14 07:37:02.951: INFO: (9) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 10.267839ms)
Aug 14 07:37:02.951: INFO: (9) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">test<... (200; 10.182749ms)
Aug 14 07:37:02.951: INFO: (9) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname2/proxy/: tls qux (200; 10.223547ms)
Aug 14 07:37:02.951: INFO: (9) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">... (200; 10.350892ms)
Aug 14 07:37:02.951: INFO: (9) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname1/proxy/: tls baz (200; 10.156666ms)
Aug 14 07:37:02.951: INFO: (9) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:460/proxy/: tls baz (200; 10.170184ms)
Aug 14 07:37:02.952: INFO: (9) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname2/proxy/: bar (200; 11.150812ms)
Aug 14 07:37:02.952: INFO: (9) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname1/proxy/: foo (200; 11.296736ms)
Aug 14 07:37:02.959: INFO: (10) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">... (200; 6.440964ms)
Aug 14 07:37:02.960: INFO: (10) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 7.359479ms)
Aug 14 07:37:02.960: INFO: (10) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:460/proxy/: tls baz (200; 7.775403ms)
Aug 14 07:37:02.960: INFO: (10) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 7.907867ms)
Aug 14 07:37:02.961: INFO: (10) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/rewriteme">test</a> (200; 7.989159ms)
Aug 14 07:37:02.961: INFO: (10) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/tlsrewritem... (200; 8.294131ms)
Aug 14 07:37:02.961: INFO: (10) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 8.252288ms)
Aug 14 07:37:02.961: INFO: (10) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:462/proxy/: tls qux (200; 7.897774ms)
Aug 14 07:37:02.961: INFO: (10) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">test<... (200; 8.084207ms)
Aug 14 07:37:02.961: INFO: (10) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 7.86218ms)
Aug 14 07:37:02.961: INFO: (10) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname1/proxy/: tls baz (200; 8.459579ms)
Aug 14 07:37:02.961: INFO: (10) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname2/proxy/: tls qux (200; 8.147176ms)
Aug 14 07:37:02.961: INFO: (10) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname1/proxy/: foo (200; 8.300191ms)
Aug 14 07:37:02.962: INFO: (10) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname2/proxy/: bar (200; 9.464909ms)
Aug 14 07:37:02.962: INFO: (10) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname2/proxy/: bar (200; 9.473886ms)
Aug 14 07:37:02.962: INFO: (10) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname1/proxy/: foo (200; 9.7708ms)
Aug 14 07:37:02.974: INFO: (11) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/tlsrewritem... (200; 11.329262ms)
Aug 14 07:37:02.974: INFO: (11) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:462/proxy/: tls qux (200; 11.854492ms)
Aug 14 07:37:02.975: INFO: (11) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:460/proxy/: tls baz (200; 12.164011ms)
Aug 14 07:37:02.975: INFO: (11) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname2/proxy/: bar (200; 12.126217ms)
Aug 14 07:37:02.975: INFO: (11) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">... (200; 12.28691ms)
Aug 14 07:37:02.975: INFO: (11) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 12.373956ms)
Aug 14 07:37:02.975: INFO: (11) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 12.517156ms)
Aug 14 07:37:02.975: INFO: (11) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">test<... (200; 12.487601ms)
Aug 14 07:37:02.975: INFO: (11) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname2/proxy/: tls qux (200; 12.535319ms)
Aug 14 07:37:02.975: INFO: (11) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname1/proxy/: tls baz (200; 12.638038ms)
Aug 14 07:37:02.975: INFO: (11) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 12.747956ms)
Aug 14 07:37:02.975: INFO: (11) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/rewriteme">test</a> (200; 13.062228ms)
Aug 14 07:37:02.977: INFO: (11) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 14.361629ms)
Aug 14 07:37:02.977: INFO: (11) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname2/proxy/: bar (200; 14.39326ms)
Aug 14 07:37:02.977: INFO: (11) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname1/proxy/: foo (200; 14.530962ms)
Aug 14 07:37:02.977: INFO: (11) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname1/proxy/: foo (200; 14.830892ms)
Aug 14 07:37:02.993: INFO: (12) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/rewriteme">test</a> (200; 15.733347ms)
Aug 14 07:37:03.018: INFO: (12) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">... (200; 40.742534ms)
Aug 14 07:37:03.020: INFO: (12) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 42.057032ms)
Aug 14 07:37:03.020: INFO: (12) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 41.843665ms)
Aug 14 07:37:03.020: INFO: (12) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 43.122622ms)
Aug 14 07:37:03.021: INFO: (12) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/tlsrewritem... (200; 43.916316ms)
Aug 14 07:37:03.021: INFO: (12) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:460/proxy/: tls baz (200; 43.558837ms)
Aug 14 07:37:03.021: INFO: (12) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname1/proxy/: tls baz (200; 43.385723ms)
Aug 14 07:37:03.021: INFO: (12) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 43.615534ms)
Aug 14 07:37:03.022: INFO: (12) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">test<... (200; 44.816972ms)
Aug 14 07:37:03.022: INFO: (12) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:462/proxy/: tls qux (200; 44.282827ms)
Aug 14 07:37:03.022: INFO: (12) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname2/proxy/: tls qux (200; 45.01812ms)
Aug 14 07:37:03.027: INFO: (12) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname2/proxy/: bar (200; 49.422281ms)
Aug 14 07:37:03.027: INFO: (12) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname1/proxy/: foo (200; 49.198079ms)
Aug 14 07:37:03.027: INFO: (12) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname1/proxy/: foo (200; 49.790866ms)
Aug 14 07:37:03.027: INFO: (12) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname2/proxy/: bar (200; 49.655905ms)
Aug 14 07:37:03.040: INFO: (13) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">... (200; 12.531215ms)
Aug 14 07:37:03.040: INFO: (13) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">test<... (200; 12.355052ms)
Aug 14 07:37:03.040: INFO: (13) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/rewriteme">test</a> (200; 12.619665ms)
Aug 14 07:37:03.040: INFO: (13) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 12.407821ms)
Aug 14 07:37:03.040: INFO: (13) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname2/proxy/: bar (200; 12.532619ms)
Aug 14 07:37:03.040: INFO: (13) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 12.551634ms)
Aug 14 07:37:03.040: INFO: (13) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:460/proxy/: tls baz (200; 12.844839ms)
Aug 14 07:37:03.040: INFO: (13) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/tlsrewritem... (200; 13.115145ms)
Aug 14 07:37:03.040: INFO: (13) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:462/proxy/: tls qux (200; 12.94726ms)
Aug 14 07:37:03.042: INFO: (13) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 14.230809ms)
Aug 14 07:37:03.042: INFO: (13) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname1/proxy/: foo (200; 15.00587ms)
Aug 14 07:37:03.044: INFO: (13) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 16.789391ms)
Aug 14 07:37:03.118: INFO: (13) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname1/proxy/: tls baz (200; 90.161135ms)
Aug 14 07:37:03.118: INFO: (13) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname2/proxy/: tls qux (200; 90.290492ms)
Aug 14 07:37:03.119: INFO: (13) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname2/proxy/: bar (200; 91.592919ms)
Aug 14 07:37:03.119: INFO: (13) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname1/proxy/: foo (200; 91.490741ms)
Aug 14 07:37:03.132: INFO: (14) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 13.089273ms)
Aug 14 07:37:03.132: INFO: (14) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 13.374183ms)
Aug 14 07:37:03.133: INFO: (14) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname2/proxy/: tls qux (200; 13.430997ms)
Aug 14 07:37:03.133: INFO: (14) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 13.483701ms)
Aug 14 07:37:03.133: INFO: (14) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 13.950204ms)
Aug 14 07:37:03.133: INFO: (14) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/rewriteme">test</a> (200; 13.910393ms)
Aug 14 07:37:03.133: INFO: (14) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/tlsrewritem... (200; 13.92801ms)
Aug 14 07:37:03.133: INFO: (14) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:462/proxy/: tls qux (200; 13.898489ms)
Aug 14 07:37:03.133: INFO: (14) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:460/proxy/: tls baz (200; 13.918236ms)
Aug 14 07:37:03.133: INFO: (14) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">... (200; 14.021184ms)
Aug 14 07:37:03.133: INFO: (14) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">test<... (200; 14.080236ms)
Aug 14 07:37:03.133: INFO: (14) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname1/proxy/: tls baz (200; 13.900286ms)
Aug 14 07:37:03.217: INFO: (14) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname2/proxy/: bar (200; 98.108605ms)
Aug 14 07:37:03.217: INFO: (14) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname2/proxy/: bar (200; 97.875983ms)
Aug 14 07:37:03.217: INFO: (14) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname1/proxy/: foo (200; 98.137627ms)
Aug 14 07:37:03.217: INFO: (14) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname1/proxy/: foo (200; 98.014218ms)
Aug 14 07:37:03.233: INFO: (15) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 15.233638ms)
Aug 14 07:37:03.233: INFO: (15) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/tlsrewritem... (200; 15.797444ms)
Aug 14 07:37:03.233: INFO: (15) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:460/proxy/: tls baz (200; 15.810327ms)
Aug 14 07:37:03.233: INFO: (15) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/rewriteme">test</a> (200; 15.98389ms)
Aug 14 07:37:03.233: INFO: (15) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname1/proxy/: tls baz (200; 15.791371ms)
Aug 14 07:37:03.233: INFO: (15) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 15.986824ms)
Aug 14 07:37:03.233: INFO: (15) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 15.863913ms)
Aug 14 07:37:03.233: INFO: (15) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">test<... (200; 15.971198ms)
Aug 14 07:37:03.233: INFO: (15) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 15.955381ms)
Aug 14 07:37:03.233: INFO: (15) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">... (200; 15.929273ms)
Aug 14 07:37:03.233: INFO: (15) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:462/proxy/: tls qux (200; 16.040098ms)
Aug 14 07:37:03.233: INFO: (15) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname2/proxy/: tls qux (200; 16.000546ms)
Aug 14 07:37:03.235: INFO: (15) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname2/proxy/: bar (200; 17.47712ms)
Aug 14 07:37:03.235: INFO: (15) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname2/proxy/: bar (200; 17.533348ms)
Aug 14 07:37:03.235: INFO: (15) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname1/proxy/: foo (200; 17.632558ms)
Aug 14 07:37:03.235: INFO: (15) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname1/proxy/: foo (200; 17.433374ms)
Aug 14 07:37:03.246: INFO: (16) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 10.623265ms)
Aug 14 07:37:03.246: INFO: (16) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">... (200; 10.618153ms)
Aug 14 07:37:03.246: INFO: (16) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/tlsrewritem... (200; 11.083809ms)
Aug 14 07:37:03.246: INFO: (16) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">test<... (200; 10.815457ms)
Aug 14 07:37:03.246: INFO: (16) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:462/proxy/: tls qux (200; 10.803816ms)
Aug 14 07:37:03.246: INFO: (16) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:460/proxy/: tls baz (200; 11.042614ms)
Aug 14 07:37:03.246: INFO: (16) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/rewriteme">test</a> (200; 11.344805ms)
Aug 14 07:37:03.246: INFO: (16) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 10.994997ms)
Aug 14 07:37:03.248: INFO: (16) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname1/proxy/: foo (200; 12.706908ms)
Aug 14 07:37:03.248: INFO: (16) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname1/proxy/: tls baz (200; 12.767789ms)
Aug 14 07:37:03.248: INFO: (16) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname2/proxy/: bar (200; 12.883139ms)
Aug 14 07:37:03.248: INFO: (16) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname1/proxy/: foo (200; 12.801109ms)
Aug 14 07:37:03.248: INFO: (16) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 13.271606ms)
Aug 14 07:37:03.249: INFO: (16) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname2/proxy/: tls qux (200; 13.232023ms)
Aug 14 07:37:03.289: INFO: (16) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 53.530281ms)
Aug 14 07:37:03.289: INFO: (16) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname2/proxy/: bar (200; 53.771815ms)
Aug 14 07:37:03.297: INFO: (17) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">test<... (200; 7.097079ms)
Aug 14 07:37:03.297: INFO: (17) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 7.375674ms)
Aug 14 07:37:03.297: INFO: (17) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 7.160788ms)
Aug 14 07:37:03.297: INFO: (17) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/tlsrewritem... (200; 7.391891ms)
Aug 14 07:37:03.297: INFO: (17) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 7.496494ms)
Aug 14 07:37:03.317: INFO: (17) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/rewriteme">test</a> (200; 27.397989ms)
Aug 14 07:37:03.317: INFO: (17) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:460/proxy/: tls baz (200; 27.949598ms)
Aug 14 07:37:03.317: INFO: (17) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname2/proxy/: bar (200; 27.661983ms)
Aug 14 07:37:03.317: INFO: (17) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:462/proxy/: tls qux (200; 28.416292ms)
Aug 14 07:37:03.317: INFO: (17) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname1/proxy/: tls baz (200; 28.052111ms)
Aug 14 07:37:03.318: INFO: (17) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname2/proxy/: tls qux (200; 28.332273ms)
Aug 14 07:37:03.318: INFO: (17) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname1/proxy/: foo (200; 28.259477ms)
Aug 14 07:37:03.318: INFO: (17) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname1/proxy/: foo (200; 28.709812ms)
Aug 14 07:37:03.318: INFO: (17) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname2/proxy/: bar (200; 28.315484ms)
Aug 14 07:37:03.324: INFO: (17) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 34.999729ms)
Aug 14 07:37:03.340: INFO: (17) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">... (200; 50.643291ms)
Aug 14 07:37:03.348: INFO: (18) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 7.707108ms)
Aug 14 07:37:03.349: INFO: (18) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">test<... (200; 8.274752ms)
Aug 14 07:37:03.349: INFO: (18) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">... (200; 8.256205ms)
Aug 14 07:37:03.350: INFO: (18) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:462/proxy/: tls qux (200; 9.255256ms)
Aug 14 07:37:03.350: INFO: (18) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 9.440863ms)
Aug 14 07:37:03.350: INFO: (18) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 9.603802ms)
Aug 14 07:37:03.350: INFO: (18) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname2/proxy/: tls qux (200; 9.656325ms)
Aug 14 07:37:03.350: INFO: (18) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname1/proxy/: tls baz (200; 9.679112ms)
Aug 14 07:37:03.351: INFO: (18) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:460/proxy/: tls baz (200; 9.951384ms)
Aug 14 07:37:03.351: INFO: (18) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 9.990747ms)
Aug 14 07:37:03.351: INFO: (18) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/rewriteme">test</a> (200; 10.28549ms)
Aug 14 07:37:03.351: INFO: (18) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/tlsrewritem... (200; 10.321169ms)
Aug 14 07:37:03.351: INFO: (18) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname2/proxy/: bar (200; 10.314871ms)
Aug 14 07:37:03.351: INFO: (18) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname1/proxy/: foo (200; 10.376188ms)
Aug 14 07:37:03.351: INFO: (18) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname2/proxy/: bar (200; 10.491287ms)
Aug 14 07:37:03.351: INFO: (18) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname1/proxy/: foo (200; 10.440615ms)
Aug 14 07:37:03.358: INFO: (19) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 6.405096ms)
Aug 14 07:37:03.358: INFO: (19) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">... (200; 6.781488ms)
Aug 14 07:37:03.358: INFO: (19) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:460/proxy/: tls baz (200; 7.020155ms)
Aug 14 07:37:03.358: INFO: (19) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j/proxy/rewriteme">test</a> (200; 6.626294ms)
Aug 14 07:37:03.359: INFO: (19) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:462/proxy/: tls qux (200; 8.019414ms)
Aug 14 07:37:03.359: INFO: (19) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:1080/proxy/rewriteme">test<... (200; 7.149823ms)
Aug 14 07:37:03.359: INFO: (19) /api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/: <a href="/api/v1/namespaces/proxy-2237/pods/https:proxy-service-qnz6k-dmt6j:443/proxy/tlsrewritem... (200; 8.256862ms)
Aug 14 07:37:03.359: INFO: (19) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname2/proxy/: bar (200; 8.011053ms)
Aug 14 07:37:03.360: INFO: (19) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname1/proxy/: tls baz (200; 7.807862ms)
Aug 14 07:37:03.360: INFO: (19) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:162/proxy/: bar (200; 8.082768ms)
Aug 14 07:37:03.360: INFO: (19) /api/v1/namespaces/proxy-2237/pods/http:proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 7.779551ms)
Aug 14 07:37:03.360: INFO: (19) /api/v1/namespaces/proxy-2237/pods/proxy-service-qnz6k-dmt6j:160/proxy/: foo (200; 8.201938ms)
Aug 14 07:37:03.362: INFO: (19) /api/v1/namespaces/proxy-2237/services/https:proxy-service-qnz6k:tlsportname2/proxy/: tls qux (200; 10.364313ms)
Aug 14 07:37:03.362: INFO: (19) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname2/proxy/: bar (200; 10.12986ms)
Aug 14 07:37:03.362: INFO: (19) /api/v1/namespaces/proxy-2237/services/http:proxy-service-qnz6k:portname1/proxy/: foo (200; 9.975982ms)
Aug 14 07:37:03.362: INFO: (19) /api/v1/namespaces/proxy-2237/services/proxy-service-qnz6k:portname1/proxy/: foo (200; 10.55321ms)
STEP: deleting ReplicationController proxy-service-qnz6k in namespace proxy-2237, will wait for the garbage collector to delete the pods
Aug 14 07:37:03.422: INFO: Deleting ReplicationController proxy-service-qnz6k took: 6.784011ms
Aug 14 07:37:03.522: INFO: Terminating ReplicationController proxy-service-qnz6k pods took: 100.281573ms
[AfterEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:37:10.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2237" for this suite.
Aug 14 07:37:16.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:37:16.983: INFO: namespace proxy-2237 deletion completed in 6.123328804s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:37:16.985: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-2242
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Aug 14 07:37:47.677: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:37:47.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0814 07:37:47.677482    3092 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-2242" for this suite.
Aug 14 07:37:53.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:37:53.859: INFO: namespace gc-2242 deletion completed in 6.177772892s
â€¢SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:37:53.862: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1259
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Aug 14 07:37:54.082: INFO: namespace kubectl-1259
Aug 14 07:37:54.083: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-1259'
Aug 14 07:37:54.326: INFO: stderr: ""
Aug 14 07:37:54.326: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Aug 14 07:37:55.333: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 07:37:55.333: INFO: Found 0 / 1
Aug 14 07:37:56.332: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 07:37:56.332: INFO: Found 1 / 1
Aug 14 07:37:56.332: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 14 07:37:56.335: INFO: Selector matched 1 pods for map[app:redis]
Aug 14 07:37:56.335: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 14 07:37:56.335: INFO: wait on redis-master startup in kubectl-1259 
Aug 14 07:37:56.335: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-hshnp redis-master --namespace=kubectl-1259'
Aug 14 07:37:56.424: INFO: stderr: ""
Aug 14 07:37:56.424: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 14 Aug 07:37:55.172 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 14 Aug 07:37:55.172 # Server started, Redis version 3.2.12\n1:M 14 Aug 07:37:55.172 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 14 Aug 07:37:55.172 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Aug 14 07:37:56.424: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-1259'
Aug 14 07:37:56.523: INFO: stderr: ""
Aug 14 07:37:56.523: INFO: stdout: "service/rm2 exposed\n"
Aug 14 07:37:56.526: INFO: Service rm2 in namespace kubectl-1259 found.
STEP: exposing service
Aug 14 07:37:58.537: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-1259'
Aug 14 07:37:58.630: INFO: stderr: ""
Aug 14 07:37:58.630: INFO: stdout: "service/rm3 exposed\n"
Aug 14 07:37:58.634: INFO: Service rm3 in namespace kubectl-1259 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:38:00.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1259" for this suite.
Aug 14 07:38:22.663: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:38:22.782: INFO: namespace kubectl-1259 deletion completed in 22.133168168s
â€¢SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:38:22.782: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-704
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 14 07:38:24.960: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:38:24.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-704" for this suite.
Aug 14 07:38:30.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:38:31.115: INFO: namespace container-runtime-704 deletion completed in 6.135376284s
â€¢SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:38:31.116: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7635
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Aug 14 07:38:31.273: INFO: Waiting up to 5m0s for pod "pod-ff73dd6a-f260-48d1-8eda-4d6c157e0744" in namespace "emptydir-7635" to be "success or failure"
Aug 14 07:38:31.276: INFO: Pod "pod-ff73dd6a-f260-48d1-8eda-4d6c157e0744": Phase="Pending", Reason="", readiness=false. Elapsed: 3.598774ms
Aug 14 07:38:33.281: INFO: Pod "pod-ff73dd6a-f260-48d1-8eda-4d6c157e0744": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00830824s
STEP: Saw pod success
Aug 14 07:38:33.281: INFO: Pod "pod-ff73dd6a-f260-48d1-8eda-4d6c157e0744" satisfied condition "success or failure"
Aug 14 07:38:33.285: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-ff73dd6a-f260-48d1-8eda-4d6c157e0744 container test-container: <nil>
STEP: delete the pod
Aug 14 07:38:33.302: INFO: Waiting for pod pod-ff73dd6a-f260-48d1-8eda-4d6c157e0744 to disappear
Aug 14 07:38:33.305: INFO: Pod pod-ff73dd6a-f260-48d1-8eda-4d6c157e0744 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:38:33.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7635" for this suite.
Aug 14 07:38:39.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:38:39.451: INFO: namespace emptydir-7635 deletion completed in 6.141110021s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:38:39.452: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8730
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-373f9b85-3d88-4bc1-837c-a9a60c22c81a
STEP: Creating a pod to test consume secrets
Aug 14 07:38:39.614: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-687a4628-55a9-4508-84ee-a1cfca55b245" in namespace "projected-8730" to be "success or failure"
Aug 14 07:38:39.620: INFO: Pod "pod-projected-secrets-687a4628-55a9-4508-84ee-a1cfca55b245": Phase="Pending", Reason="", readiness=false. Elapsed: 6.435317ms
Aug 14 07:38:41.626: INFO: Pod "pod-projected-secrets-687a4628-55a9-4508-84ee-a1cfca55b245": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012027384s
STEP: Saw pod success
Aug 14 07:38:41.626: INFO: Pod "pod-projected-secrets-687a4628-55a9-4508-84ee-a1cfca55b245" satisfied condition "success or failure"
Aug 14 07:38:41.630: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-projected-secrets-687a4628-55a9-4508-84ee-a1cfca55b245 container projected-secret-volume-test: <nil>
STEP: delete the pod
Aug 14 07:38:41.649: INFO: Waiting for pod pod-projected-secrets-687a4628-55a9-4508-84ee-a1cfca55b245 to disappear
Aug 14 07:38:41.652: INFO: Pod pod-projected-secrets-687a4628-55a9-4508-84ee-a1cfca55b245 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:38:41.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8730" for this suite.
Aug 14 07:38:47.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:38:47.794: INFO: namespace projected-8730 deletion completed in 6.13759575s
â€¢SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:38:47.795: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7465
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 14 07:38:47.949: INFO: Waiting up to 5m0s for pod "pod-fa0d6544-95ea-4cbd-a13e-616fec8b6bcd" in namespace "emptydir-7465" to be "success or failure"
Aug 14 07:38:47.952: INFO: Pod "pod-fa0d6544-95ea-4cbd-a13e-616fec8b6bcd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.889058ms
Aug 14 07:38:49.957: INFO: Pod "pod-fa0d6544-95ea-4cbd-a13e-616fec8b6bcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008714625s
STEP: Saw pod success
Aug 14 07:38:49.958: INFO: Pod "pod-fa0d6544-95ea-4cbd-a13e-616fec8b6bcd" satisfied condition "success or failure"
Aug 14 07:38:49.961: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-fa0d6544-95ea-4cbd-a13e-616fec8b6bcd container test-container: <nil>
STEP: delete the pod
Aug 14 07:38:49.979: INFO: Waiting for pod pod-fa0d6544-95ea-4cbd-a13e-616fec8b6bcd to disappear
Aug 14 07:38:49.983: INFO: Pod pod-fa0d6544-95ea-4cbd-a13e-616fec8b6bcd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:38:49.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7465" for this suite.
Aug 14 07:38:56.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:38:56.159: INFO: namespace emptydir-7465 deletion completed in 6.171497636s
â€¢SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:38:56.159: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9461
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Aug 14 07:38:56.311: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config cluster-info'
Aug 14 07:38:56.391: INFO: stderr: ""
Aug 14 07:38:56.391: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:38:56.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9461" for this suite.
Aug 14 07:39:02.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:39:02.541: INFO: namespace kubectl-9461 deletion completed in 6.145489542s
â€¢
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:39:02.541: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-9915
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 07:39:02.707: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Aug 14 07:39:02.715: INFO: Number of nodes with available pods: 0
Aug 14 07:39:02.715: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Aug 14 07:39:02.731: INFO: Number of nodes with available pods: 0
Aug 14 07:39:02.731: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 is running more than one daemon pod
Aug 14 07:39:03.741: INFO: Number of nodes with available pods: 0
Aug 14 07:39:03.741: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 is running more than one daemon pod
Aug 14 07:39:04.737: INFO: Number of nodes with available pods: 1
Aug 14 07:39:04.737: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Aug 14 07:39:04.756: INFO: Number of nodes with available pods: 1
Aug 14 07:39:04.756: INFO: Number of running nodes: 0, number of available pods: 1
Aug 14 07:39:05.760: INFO: Number of nodes with available pods: 0
Aug 14 07:39:05.760: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Aug 14 07:39:05.771: INFO: Number of nodes with available pods: 0
Aug 14 07:39:05.771: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 is running more than one daemon pod
Aug 14 07:39:06.777: INFO: Number of nodes with available pods: 0
Aug 14 07:39:06.777: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 is running more than one daemon pod
Aug 14 07:39:07.777: INFO: Number of nodes with available pods: 0
Aug 14 07:39:07.777: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 is running more than one daemon pod
Aug 14 07:39:08.776: INFO: Number of nodes with available pods: 0
Aug 14 07:39:08.776: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 is running more than one daemon pod
Aug 14 07:39:09.777: INFO: Number of nodes with available pods: 0
Aug 14 07:39:09.777: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 is running more than one daemon pod
Aug 14 07:39:10.776: INFO: Number of nodes with available pods: 0
Aug 14 07:39:10.776: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 is running more than one daemon pod
Aug 14 07:39:11.777: INFO: Number of nodes with available pods: 0
Aug 14 07:39:11.777: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 is running more than one daemon pod
Aug 14 07:39:12.777: INFO: Number of nodes with available pods: 1
Aug 14 07:39:12.777: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9915, will wait for the garbage collector to delete the pods
Aug 14 07:39:12.845: INFO: Deleting DaemonSet.extensions daemon-set took: 6.52798ms
Aug 14 07:39:12.945: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.311731ms
Aug 14 07:39:20.750: INFO: Number of nodes with available pods: 0
Aug 14 07:39:20.751: INFO: Number of running nodes: 0, number of available pods: 0
Aug 14 07:39:20.755: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9915/daemonsets","resourceVersion":"21836"},"items":null}

Aug 14 07:39:20.758: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9915/pods","resourceVersion":"21836"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:39:20.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9915" for this suite.
Aug 14 07:39:26.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:39:26.924: INFO: namespace daemonsets-9915 deletion completed in 6.141685914s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:39:26.927: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6005
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-bdc4b120-7084-4d4f-a7ef-97c07ea57489
STEP: Creating configMap with name cm-test-opt-upd-9c107793-2cc3-4973-9f41-361b8b01fc10
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-bdc4b120-7084-4d4f-a7ef-97c07ea57489
STEP: Updating configmap cm-test-opt-upd-9c107793-2cc3-4973-9f41-361b8b01fc10
STEP: Creating configMap with name cm-test-opt-create-670636b3-d414-45d7-9bbb-ea8da1ca9ad2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:39:31.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6005" for this suite.
Aug 14 07:39:53.554: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:39:53.677: INFO: namespace configmap-6005 deletion completed in 22.135307437s
â€¢SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:39:53.678: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-5224
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Aug 14 07:39:53.851: INFO: Number of nodes with available pods: 0
Aug 14 07:39:53.851: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 is running more than one daemon pod
Aug 14 07:39:54.861: INFO: Number of nodes with available pods: 0
Aug 14 07:39:54.861: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 is running more than one daemon pod
Aug 14 07:39:55.861: INFO: Number of nodes with available pods: 2
Aug 14 07:39:55.861: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Aug 14 07:39:55.882: INFO: Number of nodes with available pods: 1
Aug 14 07:39:55.882: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 is running more than one daemon pod
Aug 14 07:39:56.892: INFO: Number of nodes with available pods: 1
Aug 14 07:39:56.892: INFO: Node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 is running more than one daemon pod
Aug 14 07:39:57.892: INFO: Number of nodes with available pods: 2
Aug 14 07:39:57.892: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5224, will wait for the garbage collector to delete the pods
Aug 14 07:39:57.958: INFO: Deleting DaemonSet.extensions daemon-set took: 5.888443ms
Aug 14 07:39:58.358: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.23818ms
Aug 14 07:40:10.762: INFO: Number of nodes with available pods: 0
Aug 14 07:40:10.762: INFO: Number of running nodes: 0, number of available pods: 0
Aug 14 07:40:10.765: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-5224/daemonsets","resourceVersion":"22051"},"items":null}

Aug 14 07:40:10.768: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-5224/pods","resourceVersion":"22051"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:40:10.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5224" for this suite.
Aug 14 07:40:16.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:40:16.906: INFO: namespace daemonsets-5224 deletion completed in 6.122964341s
â€¢SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:40:16.907: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9520
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-3933fef8-2b51-48ec-97a7-f5ac89e4a7a2
STEP: Creating a pod to test consume secrets
Aug 14 07:40:17.063: INFO: Waiting up to 5m0s for pod "pod-secrets-2a637b07-0daa-4b07-9fd6-e9f2e38a731e" in namespace "secrets-9520" to be "success or failure"
Aug 14 07:40:17.067: INFO: Pod "pod-secrets-2a637b07-0daa-4b07-9fd6-e9f2e38a731e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.70254ms
Aug 14 07:40:19.071: INFO: Pod "pod-secrets-2a637b07-0daa-4b07-9fd6-e9f2e38a731e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00844383s
STEP: Saw pod success
Aug 14 07:40:19.071: INFO: Pod "pod-secrets-2a637b07-0daa-4b07-9fd6-e9f2e38a731e" satisfied condition "success or failure"
Aug 14 07:40:19.074: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-secrets-2a637b07-0daa-4b07-9fd6-e9f2e38a731e container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 07:40:19.095: INFO: Waiting for pod pod-secrets-2a637b07-0daa-4b07-9fd6-e9f2e38a731e to disappear
Aug 14 07:40:19.098: INFO: Pod pod-secrets-2a637b07-0daa-4b07-9fd6-e9f2e38a731e no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:40:19.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9520" for this suite.
Aug 14 07:40:25.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:40:25.235: INFO: namespace secrets-9520 deletion completed in 6.133169449s
â€¢SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:40:25.236: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1575
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0814 07:40:35.456930    3092 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 14 07:40:35.457: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:40:35.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1575" for this suite.
Aug 14 07:40:41.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:40:41.591: INFO: namespace gc-1575 deletion completed in 6.129190243s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:40:41.595: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3784
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 07:40:41.754: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bf4861cf-8d02-41a9-8a7e-22ae051eb0b0" in namespace "downward-api-3784" to be "success or failure"
Aug 14 07:40:41.759: INFO: Pod "downwardapi-volume-bf4861cf-8d02-41a9-8a7e-22ae051eb0b0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.984759ms
Aug 14 07:40:43.765: INFO: Pod "downwardapi-volume-bf4861cf-8d02-41a9-8a7e-22ae051eb0b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010659509s
STEP: Saw pod success
Aug 14 07:40:43.765: INFO: Pod "downwardapi-volume-bf4861cf-8d02-41a9-8a7e-22ae051eb0b0" satisfied condition "success or failure"
Aug 14 07:40:43.770: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod downwardapi-volume-bf4861cf-8d02-41a9-8a7e-22ae051eb0b0 container client-container: <nil>
STEP: delete the pod
Aug 14 07:40:43.789: INFO: Waiting for pod downwardapi-volume-bf4861cf-8d02-41a9-8a7e-22ae051eb0b0 to disappear
Aug 14 07:40:43.792: INFO: Pod downwardapi-volume-bf4861cf-8d02-41a9-8a7e-22ae051eb0b0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:40:43.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3784" for this suite.
Aug 14 07:40:49.811: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:40:49.929: INFO: namespace downward-api-3784 deletion completed in 6.131727901s
â€¢SSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:40:49.930: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3672
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Aug 14 07:40:50.083: INFO: Waiting up to 5m0s for pod "pod-c5e4f8ad-aeac-4f4d-9552-52438c942d53" in namespace "emptydir-3672" to be "success or failure"
Aug 14 07:40:50.088: INFO: Pod "pod-c5e4f8ad-aeac-4f4d-9552-52438c942d53": Phase="Pending", Reason="", readiness=false. Elapsed: 4.911917ms
Aug 14 07:40:52.093: INFO: Pod "pod-c5e4f8ad-aeac-4f4d-9552-52438c942d53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010157956s
STEP: Saw pod success
Aug 14 07:40:52.093: INFO: Pod "pod-c5e4f8ad-aeac-4f4d-9552-52438c942d53" satisfied condition "success or failure"
Aug 14 07:40:52.097: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-c5e4f8ad-aeac-4f4d-9552-52438c942d53 container test-container: <nil>
STEP: delete the pod
Aug 14 07:40:52.115: INFO: Waiting for pod pod-c5e4f8ad-aeac-4f4d-9552-52438c942d53 to disappear
Aug 14 07:40:52.118: INFO: Pod pod-c5e4f8ad-aeac-4f4d-9552-52438c942d53 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:40:52.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3672" for this suite.
Aug 14 07:40:58.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:40:58.251: INFO: namespace emptydir-3672 deletion completed in 6.127363121s
â€¢SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:40:58.252: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-54
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 07:40:58.411: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b074df0b-0fd6-40c1-a76b-e3eb75c1d3f9" in namespace "projected-54" to be "success or failure"
Aug 14 07:40:58.417: INFO: Pod "downwardapi-volume-b074df0b-0fd6-40c1-a76b-e3eb75c1d3f9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.89258ms
Aug 14 07:41:00.422: INFO: Pod "downwardapi-volume-b074df0b-0fd6-40c1-a76b-e3eb75c1d3f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011136338s
STEP: Saw pod success
Aug 14 07:41:00.422: INFO: Pod "downwardapi-volume-b074df0b-0fd6-40c1-a76b-e3eb75c1d3f9" satisfied condition "success or failure"
Aug 14 07:41:00.426: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod downwardapi-volume-b074df0b-0fd6-40c1-a76b-e3eb75c1d3f9 container client-container: <nil>
STEP: delete the pod
Aug 14 07:41:00.458: INFO: Waiting for pod downwardapi-volume-b074df0b-0fd6-40c1-a76b-e3eb75c1d3f9 to disappear
Aug 14 07:41:00.462: INFO: Pod downwardapi-volume-b074df0b-0fd6-40c1-a76b-e3eb75c1d3f9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:41:00.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-54" for this suite.
Aug 14 07:41:06.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:41:06.659: INFO: namespace projected-54 deletion completed in 6.191909972s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:41:06.662: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5082
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Aug 14 07:41:06.828: INFO: Waiting up to 5m0s for pod "pod-25c7fcaf-91f9-4a29-9662-476be52199bd" in namespace "emptydir-5082" to be "success or failure"
Aug 14 07:41:06.836: INFO: Pod "pod-25c7fcaf-91f9-4a29-9662-476be52199bd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.981965ms
Aug 14 07:41:08.840: INFO: Pod "pod-25c7fcaf-91f9-4a29-9662-476be52199bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012750343s
STEP: Saw pod success
Aug 14 07:41:08.840: INFO: Pod "pod-25c7fcaf-91f9-4a29-9662-476be52199bd" satisfied condition "success or failure"
Aug 14 07:41:08.843: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-25c7fcaf-91f9-4a29-9662-476be52199bd container test-container: <nil>
STEP: delete the pod
Aug 14 07:41:08.863: INFO: Waiting for pod pod-25c7fcaf-91f9-4a29-9662-476be52199bd to disappear
Aug 14 07:41:08.867: INFO: Pod pod-25c7fcaf-91f9-4a29-9662-476be52199bd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:41:08.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5082" for this suite.
Aug 14 07:41:14.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:41:15.010: INFO: namespace emptydir-5082 deletion completed in 6.135969458s
â€¢SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:41:15.011: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-5340
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5340.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5340.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5340.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5340.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5340.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-5340.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 14 07:41:19.821: INFO: DNS probes using dns-5340/dns-test-d732da7f-c851-4e1c-b9a9-49b697ac0fb8 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:41:19.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5340" for this suite.
Aug 14 07:41:25.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:41:25.967: INFO: namespace dns-5340 deletion completed in 6.131061364s
â€¢SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:41:25.968: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-2395
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-1cce0459-bf18-4e91-975b-a36a78b83f49 in namespace container-probe-2395
Aug 14 07:41:28.134: INFO: Started pod busybox-1cce0459-bf18-4e91-975b-a36a78b83f49 in namespace container-probe-2395
STEP: checking the pod's current state and verifying that restartCount is present
Aug 14 07:41:28.137: INFO: Initial restart count of pod busybox-1cce0459-bf18-4e91-975b-a36a78b83f49 is 0
Aug 14 07:42:18.285: INFO: Restart count of pod container-probe-2395/busybox-1cce0459-bf18-4e91-975b-a36a78b83f49 is now 1 (50.148103043s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:42:18.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2395" for this suite.
Aug 14 07:42:24.309: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:42:24.434: INFO: namespace container-probe-2395 deletion completed in 6.136045458s
â€¢SSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:42:24.434: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-4359
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Aug 14 07:42:24.591: INFO: Waiting up to 5m0s for pod "var-expansion-9c6e3401-64b4-47bc-af81-a9bff5400fa4" in namespace "var-expansion-4359" to be "success or failure"
Aug 14 07:42:24.595: INFO: Pod "var-expansion-9c6e3401-64b4-47bc-af81-a9bff5400fa4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.22654ms
Aug 14 07:42:26.600: INFO: Pod "var-expansion-9c6e3401-64b4-47bc-af81-a9bff5400fa4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00859622s
STEP: Saw pod success
Aug 14 07:42:26.600: INFO: Pod "var-expansion-9c6e3401-64b4-47bc-af81-a9bff5400fa4" satisfied condition "success or failure"
Aug 14 07:42:26.604: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod var-expansion-9c6e3401-64b4-47bc-af81-a9bff5400fa4 container dapi-container: <nil>
STEP: delete the pod
Aug 14 07:42:26.627: INFO: Waiting for pod var-expansion-9c6e3401-64b4-47bc-af81-a9bff5400fa4 to disappear
Aug 14 07:42:26.630: INFO: Pod var-expansion-9c6e3401-64b4-47bc-af81-a9bff5400fa4 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:42:26.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4359" for this suite.
Aug 14 07:42:32.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:42:32.775: INFO: namespace var-expansion-4359 deletion completed in 6.140921162s
â€¢SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:42:32.776: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1853
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-1853
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 14 07:42:32.931: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 14 07:42:55.014: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.0.65:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1853 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 07:42:55.014: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 14 07:42:55.450: INFO: Found all expected endpoints: [netserver-0]
Aug 14 07:42:55.454: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.96.1.238:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1853 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 07:42:55.454: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 14 07:42:55.839: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:42:55.839: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1853" for this suite.
Aug 14 07:43:17.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:43:17.983: INFO: namespace pod-network-test-1853 deletion completed in 22.13792041s
â€¢SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:43:17.983: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7714
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 07:43:18.157: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"42cacf0a-9c67-454d-a50f-03e3f49b5c59", Controller:(*bool)(0xc0020a2dfa), BlockOwnerDeletion:(*bool)(0xc0020a2dfb)}}
Aug 14 07:43:18.162: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"b9ed03e7-06cd-41ca-abb8-b76aa2c5328b", Controller:(*bool)(0xc002d6e53e), BlockOwnerDeletion:(*bool)(0xc002d6e53f)}}
Aug 14 07:43:18.166: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"7afbe61c-82cc-4a21-ae6c-e1ac37758759", Controller:(*bool)(0xc001a01866), BlockOwnerDeletion:(*bool)(0xc001a01867)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:43:23.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7714" for this suite.
Aug 14 07:43:29.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:43:29.335: INFO: namespace gc-7714 deletion completed in 6.154006854s
â€¢SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:43:29.337: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-2756
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-fxbl
STEP: Creating a pod to test atomic-volume-subpath
Aug 14 07:43:29.499: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-fxbl" in namespace "subpath-2756" to be "success or failure"
Aug 14 07:43:29.502: INFO: Pod "pod-subpath-test-downwardapi-fxbl": Phase="Pending", Reason="", readiness=false. Elapsed: 3.20488ms
Aug 14 07:43:31.507: INFO: Pod "pod-subpath-test-downwardapi-fxbl": Phase="Running", Reason="", readiness=true. Elapsed: 2.008074959s
Aug 14 07:43:33.512: INFO: Pod "pod-subpath-test-downwardapi-fxbl": Phase="Running", Reason="", readiness=true. Elapsed: 4.012662458s
Aug 14 07:43:35.517: INFO: Pod "pod-subpath-test-downwardapi-fxbl": Phase="Running", Reason="", readiness=true. Elapsed: 6.017810582s
Aug 14 07:43:37.522: INFO: Pod "pod-subpath-test-downwardapi-fxbl": Phase="Running", Reason="", readiness=true. Elapsed: 8.023080372s
Aug 14 07:43:39.528: INFO: Pod "pod-subpath-test-downwardapi-fxbl": Phase="Running", Reason="", readiness=true. Elapsed: 10.029249226s
Aug 14 07:43:41.534: INFO: Pod "pod-subpath-test-downwardapi-fxbl": Phase="Running", Reason="", readiness=true. Elapsed: 12.034801407s
Aug 14 07:43:43.539: INFO: Pod "pod-subpath-test-downwardapi-fxbl": Phase="Running", Reason="", readiness=true. Elapsed: 14.040242508s
Aug 14 07:43:45.544: INFO: Pod "pod-subpath-test-downwardapi-fxbl": Phase="Running", Reason="", readiness=true. Elapsed: 16.045229475s
Aug 14 07:43:47.549: INFO: Pod "pod-subpath-test-downwardapi-fxbl": Phase="Running", Reason="", readiness=true. Elapsed: 18.049955979s
Aug 14 07:43:49.554: INFO: Pod "pod-subpath-test-downwardapi-fxbl": Phase="Running", Reason="", readiness=true. Elapsed: 20.054762651s
Aug 14 07:43:51.559: INFO: Pod "pod-subpath-test-downwardapi-fxbl": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.059958992s
STEP: Saw pod success
Aug 14 07:43:51.559: INFO: Pod "pod-subpath-test-downwardapi-fxbl" satisfied condition "success or failure"
Aug 14 07:43:51.562: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-subpath-test-downwardapi-fxbl container test-container-subpath-downwardapi-fxbl: <nil>
STEP: delete the pod
Aug 14 07:43:51.580: INFO: Waiting for pod pod-subpath-test-downwardapi-fxbl to disappear
Aug 14 07:43:51.583: INFO: Pod pod-subpath-test-downwardapi-fxbl no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-fxbl
Aug 14 07:43:51.583: INFO: Deleting pod "pod-subpath-test-downwardapi-fxbl" in namespace "subpath-2756"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:43:51.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2756" for this suite.
Aug 14 07:43:57.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:43:57.775: INFO: namespace subpath-2756 deletion completed in 6.182823305s
â€¢SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:43:57.776: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-3827
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Aug 14 07:44:01.986: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 07:44:01.989: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 07:44:03.989: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 07:44:03.995: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 07:44:05.994: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 07:44:05.998: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 07:44:07.993: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 07:44:08.000: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 07:44:09.990: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 07:44:09.995: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 07:44:11.990: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 07:44:12.001: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 07:44:13.990: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 07:44:13.995: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 07:44:15.990: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 07:44:15.995: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 07:44:17.991: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 07:44:17.996: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 07:44:19.990: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 07:44:19.995: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 07:44:21.990: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 07:44:21.996: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 07:44:23.990: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 07:44:23.995: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 07:44:25.990: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 07:44:25.995: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 07:44:27.993: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 07:44:27.998: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 07:44:29.990: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 07:44:29.995: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 14 07:44:31.990: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 14 07:44:31.994: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:44:31.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3827" for this suite.
Aug 14 07:44:54.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:44:54.131: INFO: namespace container-lifecycle-hook-3827 deletion completed in 22.13201012s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:44:54.132: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9200
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 07:44:54.278: INFO: Creating deployment "test-recreate-deployment"
Aug 14 07:44:54.283: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 14 07:44:54.293: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 14 07:44:56.302: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 14 07:44:56.305: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 14 07:44:56.313: INFO: Updating deployment test-recreate-deployment
Aug 14 07:44:56.313: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Aug 14 07:44:56.361: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-9200,SelfLink:/apis/apps/v1/namespaces/deployment-9200/deployments/test-recreate-deployment,UID:8cf474f7-61eb-4fe2-874e-ca7232496319,ResourceVersion:23252,Generation:2,CreationTimestamp:2019-08-14 07:44:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-08-14 07:44:56 +0000 UTC 2019-08-14 07:44:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-08-14 07:44:56 +0000 UTC 2019-08-14 07:44:54 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Aug 14 07:44:56.366: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-9200,SelfLink:/apis/apps/v1/namespaces/deployment-9200/replicasets/test-recreate-deployment-5c8c9cc69d,UID:ca9f071b-3c3b-476c-ab62-b26396a24a49,ResourceVersion:23251,Generation:1,CreationTimestamp:2019-08-14 07:44:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 8cf474f7-61eb-4fe2-874e-ca7232496319 0xc0005c3247 0xc0005c3248}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 14 07:44:56.366: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 14 07:44:56.366: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-9200,SelfLink:/apis/apps/v1/namespaces/deployment-9200/replicasets/test-recreate-deployment-6df85df6b9,UID:c7de4be7-eb58-49b4-af1e-375a2de0e747,ResourceVersion:23243,Generation:2,CreationTimestamp:2019-08-14 07:44:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 8cf474f7-61eb-4fe2-874e-ca7232496319 0xc0005c33f7 0xc0005c33f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Aug 14 07:44:56.371: INFO: Pod "test-recreate-deployment-5c8c9cc69d-wtwx4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-wtwx4,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-9200,SelfLink:/api/v1/namespaces/deployment-9200/pods/test-recreate-deployment-5c8c9cc69d-wtwx4,UID:8b420ef3-bc22-4223-9524-73357a2a7542,ResourceVersion:23253,Generation:0,CreationTimestamp:2019-08-14 07:44:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d ca9f071b-3c3b-476c-ab62-b26396a24a49 0xc002d6e597 0xc002d6e598}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-4sl2p {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4sl2p,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4sl2p true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002d6e600} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002d6e620}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:44:56 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:44:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:44:56 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-08-14 07:44:56 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.8,PodIP:,StartTime:2019-08-14 07:44:56 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:44:56.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9200" for this suite.
Aug 14 07:45:02.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:45:02.509: INFO: namespace deployment-9200 deletion completed in 6.133313004s
â€¢SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:45:02.509: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7989
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Aug 14 07:45:02.666: INFO: Waiting up to 5m0s for pod "downward-api-31ea8971-4a67-48d2-b4ee-d509a50f4b16" in namespace "downward-api-7989" to be "success or failure"
Aug 14 07:45:02.672: INFO: Pod "downward-api-31ea8971-4a67-48d2-b4ee-d509a50f4b16": Phase="Pending", Reason="", readiness=false. Elapsed: 5.639215ms
Aug 14 07:45:04.677: INFO: Pod "downward-api-31ea8971-4a67-48d2-b4ee-d509a50f4b16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011161548s
STEP: Saw pod success
Aug 14 07:45:04.677: INFO: Pod "downward-api-31ea8971-4a67-48d2-b4ee-d509a50f4b16" satisfied condition "success or failure"
Aug 14 07:45:04.681: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod downward-api-31ea8971-4a67-48d2-b4ee-d509a50f4b16 container dapi-container: <nil>
STEP: delete the pod
Aug 14 07:45:04.701: INFO: Waiting for pod downward-api-31ea8971-4a67-48d2-b4ee-d509a50f4b16 to disappear
Aug 14 07:45:04.705: INFO: Pod downward-api-31ea8971-4a67-48d2-b4ee-d509a50f4b16 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:45:04.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7989" for this suite.
Aug 14 07:45:10.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:45:10.952: INFO: namespace downward-api-7989 deletion completed in 6.243218471s
â€¢SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:45:10.954: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4137
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 07:45:11.108: INFO: Waiting up to 5m0s for pod "downwardapi-volume-734cdbc1-5a42-4f00-a87e-11789f150c30" in namespace "projected-4137" to be "success or failure"
Aug 14 07:45:11.111: INFO: Pod "downwardapi-volume-734cdbc1-5a42-4f00-a87e-11789f150c30": Phase="Pending", Reason="", readiness=false. Elapsed: 3.004287ms
Aug 14 07:45:13.117: INFO: Pod "downwardapi-volume-734cdbc1-5a42-4f00-a87e-11789f150c30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009032344s
STEP: Saw pod success
Aug 14 07:45:13.117: INFO: Pod "downwardapi-volume-734cdbc1-5a42-4f00-a87e-11789f150c30" satisfied condition "success or failure"
Aug 14 07:45:13.120: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod downwardapi-volume-734cdbc1-5a42-4f00-a87e-11789f150c30 container client-container: <nil>
STEP: delete the pod
Aug 14 07:45:13.138: INFO: Waiting for pod downwardapi-volume-734cdbc1-5a42-4f00-a87e-11789f150c30 to disappear
Aug 14 07:45:13.141: INFO: Pod downwardapi-volume-734cdbc1-5a42-4f00-a87e-11789f150c30 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:45:13.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4137" for this suite.
Aug 14 07:45:19.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:45:19.280: INFO: namespace projected-4137 deletion completed in 6.133620556s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:45:19.281: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-4931
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-4931
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Aug 14 07:45:19.428: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Aug 14 07:45:43.510: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.0.66 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4931 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 07:45:43.510: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 14 07:45:44.893: INFO: Found all expected endpoints: [netserver-0]
Aug 14 07:45:44.898: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.96.1.247 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4931 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 07:45:44.898: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 14 07:45:46.275: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:45:46.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4931" for this suite.
Aug 14 07:46:08.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:46:08.415: INFO: namespace pod-network-test-4931 deletion completed in 22.13416205s
â€¢SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:46:08.415: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-679
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-898caed6-6ea2-4e6f-a3aa-7a0edc219d92
STEP: Creating a pod to test consume secrets
Aug 14 07:46:08.577: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cb6d3169-bdff-48fb-84f2-1bc21558ae39" in namespace "projected-679" to be "success or failure"
Aug 14 07:46:08.582: INFO: Pod "pod-projected-secrets-cb6d3169-bdff-48fb-84f2-1bc21558ae39": Phase="Pending", Reason="", readiness=false. Elapsed: 4.651802ms
Aug 14 07:46:10.587: INFO: Pod "pod-projected-secrets-cb6d3169-bdff-48fb-84f2-1bc21558ae39": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010282056s
STEP: Saw pod success
Aug 14 07:46:10.588: INFO: Pod "pod-projected-secrets-cb6d3169-bdff-48fb-84f2-1bc21558ae39" satisfied condition "success or failure"
Aug 14 07:46:10.592: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-projected-secrets-cb6d3169-bdff-48fb-84f2-1bc21558ae39 container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 07:46:10.611: INFO: Waiting for pod pod-projected-secrets-cb6d3169-bdff-48fb-84f2-1bc21558ae39 to disappear
Aug 14 07:46:10.615: INFO: Pod pod-projected-secrets-cb6d3169-bdff-48fb-84f2-1bc21558ae39 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:46:10.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-679" for this suite.
Aug 14 07:46:16.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:46:16.742: INFO: namespace projected-679 deletion completed in 6.122182472s
â€¢SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:46:16.743: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-8550
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 07:46:16.900: INFO: (0) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.999709ms)
Aug 14 07:46:16.939: INFO: (1) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 39.47843ms)
Aug 14 07:46:16.944: INFO: (2) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.842436ms)
Aug 14 07:46:16.949: INFO: (3) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.362573ms)
Aug 14 07:46:16.955: INFO: (4) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.973772ms)
Aug 14 07:46:16.960: INFO: (5) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.053384ms)
Aug 14 07:46:16.965: INFO: (6) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.807599ms)
Aug 14 07:46:16.971: INFO: (7) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.992452ms)
Aug 14 07:46:16.978: INFO: (8) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.704367ms)
Aug 14 07:46:16.984: INFO: (9) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.587907ms)
Aug 14 07:46:16.990: INFO: (10) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.672764ms)
Aug 14 07:46:16.996: INFO: (11) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.837358ms)
Aug 14 07:46:17.001: INFO: (12) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.321173ms)
Aug 14 07:46:17.007: INFO: (13) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.94897ms)
Aug 14 07:46:17.013: INFO: (14) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.415802ms)
Aug 14 07:46:17.019: INFO: (15) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.717205ms)
Aug 14 07:46:17.024: INFO: (16) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.015709ms)
Aug 14 07:46:17.029: INFO: (17) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.908709ms)
Aug 14 07:46:17.034: INFO: (18) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.973564ms)
Aug 14 07:46:17.039: INFO: (19) /api/v1/nodes/shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.874221ms)
[AfterEach] version v1
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:46:17.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8550" for this suite.
Aug 14 07:46:23.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:46:23.167: INFO: namespace proxy-8550 deletion completed in 6.124078441s
â€¢SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:46:23.167: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-311
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Aug 14 07:46:23.317: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Aug 14 07:46:23.317: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-311'
Aug 14 07:46:23.619: INFO: stderr: ""
Aug 14 07:46:23.619: INFO: stdout: "service/redis-slave created\n"
Aug 14 07:46:23.620: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Aug 14 07:46:23.620: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-311'
Aug 14 07:46:23.841: INFO: stderr: ""
Aug 14 07:46:23.841: INFO: stdout: "service/redis-master created\n"
Aug 14 07:46:23.841: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 14 07:46:23.841: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-311'
Aug 14 07:46:24.068: INFO: stderr: ""
Aug 14 07:46:24.068: INFO: stdout: "service/frontend created\n"
Aug 14 07:46:24.068: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Aug 14 07:46:24.068: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-311'
Aug 14 07:46:24.238: INFO: stderr: ""
Aug 14 07:46:24.238: INFO: stdout: "deployment.apps/frontend created\n"
Aug 14 07:46:24.238: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 14 07:46:24.238: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-311'
Aug 14 07:46:24.445: INFO: stderr: ""
Aug 14 07:46:24.445: INFO: stdout: "deployment.apps/redis-master created\n"
Aug 14 07:46:24.445: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Aug 14 07:46:24.445: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-311'
Aug 14 07:46:24.593: INFO: stderr: ""
Aug 14 07:46:24.593: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Aug 14 07:46:24.593: INFO: Waiting for all frontend pods to be Running.
Aug 14 07:46:39.644: INFO: Waiting for frontend to serve content.
Aug 14 07:46:44.709: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Aug 14 07:46:49.802: INFO: Trying to add a new entry to the guestbook.
Aug 14 07:46:49.930: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Aug 14 07:46:50.058: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-311'
Aug 14 07:46:50.201: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 07:46:50.202: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Aug 14 07:46:50.202: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-311'
Aug 14 07:46:50.307: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 07:46:50.307: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 14 07:46:50.308: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-311'
Aug 14 07:46:50.409: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 07:46:50.409: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 14 07:46:50.409: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-311'
Aug 14 07:46:50.485: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 07:46:50.485: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Aug 14 07:46:50.485: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-311'
Aug 14 07:46:50.561: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 07:46:50.561: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Aug 14 07:46:50.561: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-311'
Aug 14 07:46:50.636: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 14 07:46:50.636: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:46:50.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-311" for this suite.
Aug 14 07:47:32.653: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:47:32.787: INFO: namespace kubectl-311 deletion completed in 42.146125717s
â€¢SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:47:32.788: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9087
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-w9qw
STEP: Creating a pod to test atomic-volume-subpath
Aug 14 07:47:32.951: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-w9qw" in namespace "subpath-9087" to be "success or failure"
Aug 14 07:47:32.954: INFO: Pod "pod-subpath-test-configmap-w9qw": Phase="Pending", Reason="", readiness=false. Elapsed: 3.140708ms
Aug 14 07:47:34.959: INFO: Pod "pod-subpath-test-configmap-w9qw": Phase="Running", Reason="", readiness=true. Elapsed: 2.008058441s
Aug 14 07:47:36.965: INFO: Pod "pod-subpath-test-configmap-w9qw": Phase="Running", Reason="", readiness=true. Elapsed: 4.013901806s
Aug 14 07:47:38.970: INFO: Pod "pod-subpath-test-configmap-w9qw": Phase="Running", Reason="", readiness=true. Elapsed: 6.01924085s
Aug 14 07:47:40.975: INFO: Pod "pod-subpath-test-configmap-w9qw": Phase="Running", Reason="", readiness=true. Elapsed: 8.024348556s
Aug 14 07:47:42.981: INFO: Pod "pod-subpath-test-configmap-w9qw": Phase="Running", Reason="", readiness=true. Elapsed: 10.029447385s
Aug 14 07:47:44.985: INFO: Pod "pod-subpath-test-configmap-w9qw": Phase="Running", Reason="", readiness=true. Elapsed: 12.034382487s
Aug 14 07:47:46.991: INFO: Pod "pod-subpath-test-configmap-w9qw": Phase="Running", Reason="", readiness=true. Elapsed: 14.040000436s
Aug 14 07:47:48.997: INFO: Pod "pod-subpath-test-configmap-w9qw": Phase="Running", Reason="", readiness=true. Elapsed: 16.046092077s
Aug 14 07:47:51.003: INFO: Pod "pod-subpath-test-configmap-w9qw": Phase="Running", Reason="", readiness=true. Elapsed: 18.051472192s
Aug 14 07:47:53.008: INFO: Pod "pod-subpath-test-configmap-w9qw": Phase="Running", Reason="", readiness=true. Elapsed: 20.056673498s
Aug 14 07:47:55.013: INFO: Pod "pod-subpath-test-configmap-w9qw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.061541852s
STEP: Saw pod success
Aug 14 07:47:55.013: INFO: Pod "pod-subpath-test-configmap-w9qw" satisfied condition "success or failure"
Aug 14 07:47:55.016: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-subpath-test-configmap-w9qw container test-container-subpath-configmap-w9qw: <nil>
STEP: delete the pod
Aug 14 07:47:55.043: INFO: Waiting for pod pod-subpath-test-configmap-w9qw to disappear
Aug 14 07:47:55.046: INFO: Pod pod-subpath-test-configmap-w9qw no longer exists
STEP: Deleting pod pod-subpath-test-configmap-w9qw
Aug 14 07:47:55.046: INFO: Deleting pod "pod-subpath-test-configmap-w9qw" in namespace "subpath-9087"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:47:55.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9087" for this suite.
Aug 14 07:48:01.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:48:01.221: INFO: namespace subpath-9087 deletion completed in 6.166115314s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:48:01.225: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8450
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Aug 14 07:48:41.413: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
W0814 07:48:41.413599    3092 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Aug 14 07:48:41.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8450" for this suite.
Aug 14 07:48:47.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:48:47.587: INFO: namespace gc-8450 deletion completed in 6.169276373s
â€¢SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:48:47.587: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1528
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Aug 14 07:48:47.740: INFO: Waiting up to 5m0s for pod "pod-aa3c258b-dfbd-4357-9707-8410861ebd4b" in namespace "emptydir-1528" to be "success or failure"
Aug 14 07:48:47.743: INFO: Pod "pod-aa3c258b-dfbd-4357-9707-8410861ebd4b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.686506ms
Aug 14 07:48:49.749: INFO: Pod "pod-aa3c258b-dfbd-4357-9707-8410861ebd4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00801921s
STEP: Saw pod success
Aug 14 07:48:49.749: INFO: Pod "pod-aa3c258b-dfbd-4357-9707-8410861ebd4b" satisfied condition "success or failure"
Aug 14 07:48:49.752: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-aa3c258b-dfbd-4357-9707-8410861ebd4b container test-container: <nil>
STEP: delete the pod
Aug 14 07:48:49.774: INFO: Waiting for pod pod-aa3c258b-dfbd-4357-9707-8410861ebd4b to disappear
Aug 14 07:48:49.776: INFO: Pod pod-aa3c258b-dfbd-4357-9707-8410861ebd4b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:48:49.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1528" for this suite.
Aug 14 07:48:55.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:48:55.948: INFO: namespace emptydir-1528 deletion completed in 6.16637315s
â€¢SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:48:55.949: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1492
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Aug 14 07:48:56.101: INFO: Waiting up to 5m0s for pod "pod-bb9097fc-7600-47eb-aeba-e3ca09ab2a68" in namespace "emptydir-1492" to be "success or failure"
Aug 14 07:48:56.105: INFO: Pod "pod-bb9097fc-7600-47eb-aeba-e3ca09ab2a68": Phase="Pending", Reason="", readiness=false. Elapsed: 3.355373ms
Aug 14 07:48:58.110: INFO: Pod "pod-bb9097fc-7600-47eb-aeba-e3ca09ab2a68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008141896s
STEP: Saw pod success
Aug 14 07:48:58.110: INFO: Pod "pod-bb9097fc-7600-47eb-aeba-e3ca09ab2a68" satisfied condition "success or failure"
Aug 14 07:48:58.113: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-bb9097fc-7600-47eb-aeba-e3ca09ab2a68 container test-container: <nil>
STEP: delete the pod
Aug 14 07:48:58.130: INFO: Waiting for pod pod-bb9097fc-7600-47eb-aeba-e3ca09ab2a68 to disappear
Aug 14 07:48:58.133: INFO: Pod pod-bb9097fc-7600-47eb-aeba-e3ca09ab2a68 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:48:58.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1492" for this suite.
Aug 14 07:49:04.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:49:04.304: INFO: namespace emptydir-1492 deletion completed in 6.166555763s
â€¢SS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:49:04.304: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-5870
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Aug 14 07:49:04.588: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-5870" to be "success or failure"
Aug 14 07:49:04.591: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.502352ms
Aug 14 07:49:06.595: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00785371s
STEP: Saw pod success
Aug 14 07:49:06.596: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Aug 14 07:49:06.599: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Aug 14 07:49:06.615: INFO: Waiting for pod pod-host-path-test to disappear
Aug 14 07:49:06.619: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:49:06.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-5870" for this suite.
Aug 14 07:49:12.635: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:49:12.794: INFO: namespace hostpath-5870 deletion completed in 6.170707167s
â€¢SSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:49:12.794: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-77
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:49:15.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-77" for this suite.
Aug 14 07:49:38.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:49:38.159: INFO: namespace replication-controller-77 deletion completed in 22.177428153s
â€¢SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:49:38.159: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2963
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Aug 14 07:49:38.301: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 14 07:49:38.309: INFO: Waiting for terminating namespaces to be deleted...
Aug 14 07:49:38.312: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 before test
Aug 14 07:49:38.320: INFO: node-exporter-dzrg2 from kube-system started at 2019-08-14 06:11:41 +0000 UTC (1 container statuses recorded)
Aug 14 07:49:38.320: INFO: 	Container node-exporter ready: true, restart count 1
Aug 14 07:49:38.320: INFO: kube-proxy-2j2fb from kube-system started at 2019-08-14 06:11:41 +0000 UTC (1 container statuses recorded)
Aug 14 07:49:38.320: INFO: 	Container kube-proxy ready: true, restart count 1
Aug 14 07:49:38.320: INFO: calico-node-8jgsw from kube-system started at 2019-08-14 06:11:41 +0000 UTC (1 container statuses recorded)
Aug 14 07:49:38.320: INFO: 	Container calico-node ready: true, restart count 0
Aug 14 07:49:38.320: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-stfbx before test
Aug 14 07:49:38.334: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-88d6cff74-4rp7v from kube-system started at 2019-08-14 06:12:09 +0000 UTC (1 container statuses recorded)
Aug 14 07:49:38.334: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Aug 14 07:49:38.334: INFO: addons-kubernetes-dashboard-5c8d9945bc-dfqc7 from kube-system started at 2019-08-14 06:12:09 +0000 UTC (1 container statuses recorded)
Aug 14 07:49:38.334: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Aug 14 07:49:38.334: INFO: node-exporter-lscjk from kube-system started at 2019-08-14 06:11:39 +0000 UTC (1 container statuses recorded)
Aug 14 07:49:38.334: INFO: 	Container node-exporter ready: true, restart count 0
Aug 14 07:49:38.334: INFO: metrics-server-5cd447944f-dtjpr from kube-system started at 2019-08-14 06:12:09 +0000 UTC (1 container statuses recorded)
Aug 14 07:49:38.334: INFO: 	Container metrics-server ready: true, restart count 0
Aug 14 07:49:38.334: INFO: coredns-85cc454dd8-lthzw from kube-system started at 2019-08-14 06:12:09 +0000 UTC (1 container statuses recorded)
Aug 14 07:49:38.334: INFO: 	Container coredns ready: true, restart count 0
Aug 14 07:49:38.334: INFO: kube-proxy-mrtvm from kube-system started at 2019-08-14 06:11:39 +0000 UTC (1 container statuses recorded)
Aug 14 07:49:38.334: INFO: 	Container kube-proxy ready: true, restart count 1
Aug 14 07:49:38.334: INFO: coredns-85cc454dd8-275mp from kube-system started at 2019-08-14 06:12:11 +0000 UTC (1 container statuses recorded)
Aug 14 07:49:38.334: INFO: 	Container coredns ready: true, restart count 0
Aug 14 07:49:38.334: INFO: calico-node-xwxqx from kube-system started at 2019-08-14 06:11:39 +0000 UTC (1 container statuses recorded)
Aug 14 07:49:38.334: INFO: 	Container calico-node ready: true, restart count 0
Aug 14 07:49:38.334: INFO: addons-nginx-ingress-controller-6496d947df-xxr56 from kube-system started at 2019-08-14 06:12:11 +0000 UTC (1 container statuses recorded)
Aug 14 07:49:38.334: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Aug 14 07:49:38.334: INFO: blackbox-exporter-954dd954b-k8qsk from kube-system started at 2019-08-14 06:11:39 +0000 UTC (1 container statuses recorded)
Aug 14 07:49:38.334: INFO: 	Container blackbox-exporter ready: true, restart count 0
Aug 14 07:49:38.334: INFO: vpn-shoot-66d7fd98fb-kzprj from kube-system started at 2019-08-14 06:12:09 +0000 UTC (1 container statuses recorded)
Aug 14 07:49:38.334: INFO: 	Container vpn-shoot ready: true, restart count 0
Aug 14 07:49:38.334: INFO: calico-kube-controllers-5f4b46ffb5-hhmsh from kube-system started at 2019-08-14 06:12:09 +0000 UTC (1 container statuses recorded)
Aug 14 07:49:38.334: INFO: 	Container calico-kube-controllers ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-9fce7af7-2ed1-4968-89e4-267c6799b1a3 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-9fce7af7-2ed1-4968-89e4-267c6799b1a3 off the node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-9fce7af7-2ed1-4968-89e4-267c6799b1a3
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:49:42.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2963" for this suite.
Aug 14 07:50:10.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:50:10.579: INFO: namespace sched-pred-2963 deletion completed in 28.174188224s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72
â€¢
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:50:10.580: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8686
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-89c183e1-6569-4899-a30d-c9910f09fd7d in namespace container-probe-8686
Aug 14 07:50:12.757: INFO: Started pod test-webserver-89c183e1-6569-4899-a30d-c9910f09fd7d in namespace container-probe-8686
STEP: checking the pod's current state and verifying that restartCount is present
Aug 14 07:50:12.760: INFO: Initial restart count of pod test-webserver-89c183e1-6569-4899-a30d-c9910f09fd7d is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:54:13.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8686" for this suite.
Aug 14 07:54:19.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:54:19.558: INFO: namespace container-probe-8686 deletion completed in 6.133163308s
â€¢SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:54:19.560: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4837
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-4ba7fbcb-e697-4880-a56f-9fe74f1442ff
STEP: Creating a pod to test consume configMaps
Aug 14 07:54:19.720: INFO: Waiting up to 5m0s for pod "pod-configmaps-5e14b0ec-511b-4cb4-a804-174fa55c8ab2" in namespace "configmap-4837" to be "success or failure"
Aug 14 07:54:19.724: INFO: Pod "pod-configmaps-5e14b0ec-511b-4cb4-a804-174fa55c8ab2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.621895ms
Aug 14 07:54:21.729: INFO: Pod "pod-configmaps-5e14b0ec-511b-4cb4-a804-174fa55c8ab2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008307264s
STEP: Saw pod success
Aug 14 07:54:21.729: INFO: Pod "pod-configmaps-5e14b0ec-511b-4cb4-a804-174fa55c8ab2" satisfied condition "success or failure"
Aug 14 07:54:21.733: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-configmaps-5e14b0ec-511b-4cb4-a804-174fa55c8ab2 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 07:54:21.758: INFO: Waiting for pod pod-configmaps-5e14b0ec-511b-4cb4-a804-174fa55c8ab2 to disappear
Aug 14 07:54:21.761: INFO: Pod pod-configmaps-5e14b0ec-511b-4cb4-a804-174fa55c8ab2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:54:21.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4837" for this suite.
Aug 14 07:54:27.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:54:27.909: INFO: namespace configmap-4837 deletion completed in 6.143068366s
â€¢SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:54:27.910: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-9182
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Aug 14 07:54:46.078: INFO: Container started at 2019-08-14 07:54:28 +0000 UTC, pod became ready at 2019-08-14 07:54:44 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:54:46.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9182" for this suite.
Aug 14 07:55:08.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:55:08.217: INFO: namespace container-probe-9182 deletion completed in 22.132664771s
â€¢SSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:55:08.218: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-4234
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:55:10.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4234" for this suite.
Aug 14 07:55:48.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:55:48.537: INFO: namespace kubelet-test-4234 deletion completed in 38.136017254s
â€¢SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:55:48.537: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2953
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-1b99e0c9-0aff-49a3-9802-3f799bb1d1df
STEP: Creating a pod to test consume configMaps
Aug 14 07:55:48.692: INFO: Waiting up to 5m0s for pod "pod-configmaps-7766f86e-b921-4ed1-b0db-f7c5f4fdf748" in namespace "configmap-2953" to be "success or failure"
Aug 14 07:55:48.697: INFO: Pod "pod-configmaps-7766f86e-b921-4ed1-b0db-f7c5f4fdf748": Phase="Pending", Reason="", readiness=false. Elapsed: 5.869843ms
Aug 14 07:55:50.705: INFO: Pod "pod-configmaps-7766f86e-b921-4ed1-b0db-f7c5f4fdf748": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01372888s
STEP: Saw pod success
Aug 14 07:55:50.705: INFO: Pod "pod-configmaps-7766f86e-b921-4ed1-b0db-f7c5f4fdf748" satisfied condition "success or failure"
Aug 14 07:55:50.709: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-configmaps-7766f86e-b921-4ed1-b0db-f7c5f4fdf748 container configmap-volume-test: <nil>
STEP: delete the pod
Aug 14 07:55:50.729: INFO: Waiting for pod pod-configmaps-7766f86e-b921-4ed1-b0db-f7c5f4fdf748 to disappear
Aug 14 07:55:50.738: INFO: Pod pod-configmaps-7766f86e-b921-4ed1-b0db-f7c5f4fdf748 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:55:50.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2953" for this suite.
Aug 14 07:55:56.755: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:55:56.888: INFO: namespace configmap-2953 deletion completed in 6.145280657s
â€¢SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:55:56.889: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7225
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Aug 14 07:55:59.566: INFO: Successfully updated pod "pod-update-d3a85af5-fca9-4c25-b854-4eecb42304dc"
STEP: verifying the updated pod is in kubernetes
Aug 14 07:55:59.575: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:55:59.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7225" for this suite.
Aug 14 07:56:21.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:56:21.719: INFO: namespace pods-7225 deletion completed in 22.138940227s
â€¢SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:56:21.719: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1665
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-57985cee-8ae6-4422-922a-42e45faff16c
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-57985cee-8ae6-4422-922a-42e45faff16c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:56:25.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1665" for this suite.
Aug 14 07:56:48.009: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:56:48.122: INFO: namespace configmap-1665 deletion completed in 22.12720355s
â€¢SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:56:48.123: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9217
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Aug 14 07:56:50.309: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:56:50.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9217" for this suite.
Aug 14 07:57:44.337: INFO: Error while waiting for namespace to be terminated: Get https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com/api/v1/namespaces/container-runtime-9217: unexpected EOF
Aug 14 07:57:46.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:57:51.546: INFO: namespace container-runtime-9217 deletion completed in 1m1.219956195s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:57:51.550: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3294
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Aug 14 07:57:53.744: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Aug 14 07:58:13.833: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:58:13.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3294" for this suite.
Aug 14 07:58:19.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:58:19.982: INFO: namespace pods-3294 deletion completed in 6.13978034s
â€¢SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:58:19.983: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2210
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Aug 14 07:58:20.133: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm-0eqz5.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-2210 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Aug 14 07:58:22.196: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Aug 14 07:58:22.196: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:58:24.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2210" for this suite.
Aug 14 07:58:30.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:58:30.384: INFO: namespace kubectl-2210 deletion completed in 6.171994061s
â€¢SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:58:30.384: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6606
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 07:58:30.541: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f9391375-7791-4c66-9315-d312fae15894" in namespace "downward-api-6606" to be "success or failure"
Aug 14 07:58:30.544: INFO: Pod "downwardapi-volume-f9391375-7791-4c66-9315-d312fae15894": Phase="Pending", Reason="", readiness=false. Elapsed: 3.319619ms
Aug 14 07:58:32.549: INFO: Pod "downwardapi-volume-f9391375-7791-4c66-9315-d312fae15894": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008074426s
STEP: Saw pod success
Aug 14 07:58:32.549: INFO: Pod "downwardapi-volume-f9391375-7791-4c66-9315-d312fae15894" satisfied condition "success or failure"
Aug 14 07:58:32.552: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod downwardapi-volume-f9391375-7791-4c66-9315-d312fae15894 container client-container: <nil>
STEP: delete the pod
Aug 14 07:58:32.572: INFO: Waiting for pod downwardapi-volume-f9391375-7791-4c66-9315-d312fae15894 to disappear
Aug 14 07:58:32.575: INFO: Pod downwardapi-volume-f9391375-7791-4c66-9315-d312fae15894 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:58:32.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6606" for this suite.
Aug 14 07:58:38.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:58:38.713: INFO: namespace downward-api-6606 deletion completed in 6.132808384s
â€¢SSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:58:38.713: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2420
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2420.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2420.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2420.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2420.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2420.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2420.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2420.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2420.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2420.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2420.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2420.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 10.34.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.34.10_udp@PTR;check="$$(dig +tcp +noall +answer +search 10.34.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.34.10_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2420.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2420.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2420.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2420.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2420.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2420.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2420.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2420.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2420.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2420.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2420.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 10.34.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.34.10_udp@PTR;check="$$(dig +tcp +noall +answer +search 10.34.68.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.68.34.10_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Aug 14 07:58:41.007: INFO: Unable to read wheezy_udp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:41.048: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:41.053: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:41.060: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:41.564: INFO: Unable to read jessie_udp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:41.570: INFO: Unable to read jessie_tcp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:41.575: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:41.579: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:42.038: INFO: Lookups using dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9 failed for: [wheezy_udp@dns-test-service.dns-2420.svc.cluster.local wheezy_tcp@dns-test-service.dns-2420.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local jessie_udp@dns-test-service.dns-2420.svc.cluster.local jessie_tcp@dns-test-service.dns-2420.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local]

Aug 14 07:58:47.045: INFO: Unable to read wheezy_udp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:47.087: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:47.094: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:47.099: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:47.599: INFO: Unable to read jessie_udp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:47.605: INFO: Unable to read jessie_tcp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:47.611: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:47.616: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:48.082: INFO: Lookups using dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9 failed for: [wheezy_udp@dns-test-service.dns-2420.svc.cluster.local wheezy_tcp@dns-test-service.dns-2420.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local jessie_udp@dns-test-service.dns-2420.svc.cluster.local jessie_tcp@dns-test-service.dns-2420.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local]

Aug 14 07:58:52.046: INFO: Unable to read wheezy_udp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:52.051: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:52.056: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:52.061: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:52.547: INFO: Unable to read jessie_udp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:52.553: INFO: Unable to read jessie_tcp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:52.559: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:52.564: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:53.025: INFO: Lookups using dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9 failed for: [wheezy_udp@dns-test-service.dns-2420.svc.cluster.local wheezy_tcp@dns-test-service.dns-2420.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local jessie_udp@dns-test-service.dns-2420.svc.cluster.local jessie_tcp@dns-test-service.dns-2420.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local]

Aug 14 07:58:57.045: INFO: Unable to read wheezy_udp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:57.087: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:57.092: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:57.097: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:57.583: INFO: Unable to read jessie_udp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:57.589: INFO: Unable to read jessie_tcp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:57.594: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:57.598: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:58:58.042: INFO: Lookups using dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9 failed for: [wheezy_udp@dns-test-service.dns-2420.svc.cluster.local wheezy_tcp@dns-test-service.dns-2420.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local jessie_udp@dns-test-service.dns-2420.svc.cluster.local jessie_tcp@dns-test-service.dns-2420.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local]

Aug 14 07:59:02.046: INFO: Unable to read wheezy_udp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:59:02.052: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:59:02.058: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:59:02.064: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:59:02.560: INFO: Unable to read jessie_udp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:59:02.567: INFO: Unable to read jessie_tcp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:59:02.575: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:59:02.581: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:59:02.970: INFO: Lookups using dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9 failed for: [wheezy_udp@dns-test-service.dns-2420.svc.cluster.local wheezy_tcp@dns-test-service.dns-2420.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local jessie_udp@dns-test-service.dns-2420.svc.cluster.local jessie_tcp@dns-test-service.dns-2420.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local]

Aug 14 07:59:07.045: INFO: Unable to read wheezy_udp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:59:07.051: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:59:07.057: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:59:07.073: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:59:07.580: INFO: Unable to read jessie_udp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:59:07.585: INFO: Unable to read jessie_tcp@dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:59:07.590: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:59:07.595: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local from pod dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9: the server could not find the requested resource (get pods dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9)
Aug 14 07:59:08.042: INFO: Lookups using dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9 failed for: [wheezy_udp@dns-test-service.dns-2420.svc.cluster.local wheezy_tcp@dns-test-service.dns-2420.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local jessie_udp@dns-test-service.dns-2420.svc.cluster.local jessie_tcp@dns-test-service.dns-2420.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2420.svc.cluster.local]

Aug 14 07:59:13.578: INFO: DNS probes using dns-2420/dns-test-c8d2e7b5-0362-4012-bb06-71db2f1036e9 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:59:13.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2420" for this suite.
Aug 14 07:59:19.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:59:19.751: INFO: namespace dns-2420 deletion completed in 6.139335693s
â€¢SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:59:19.753: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-6747
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Aug 14 07:59:19.906: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:59:23.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6747" for this suite.
Aug 14 07:59:45.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:59:45.259: INFO: namespace init-container-6747 deletion completed in 22.176873993s
â€¢S
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:59:45.260: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-3403
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-aad941e1-d7c4-4ba1-bb7d-a049cec3ffa0
STEP: Creating a pod to test consume secrets
Aug 14 07:59:45.421: INFO: Waiting up to 5m0s for pod "pod-secrets-cd27e9c8-dc0a-4672-a933-58eea98c2e1a" in namespace "secrets-3403" to be "success or failure"
Aug 14 07:59:45.425: INFO: Pod "pod-secrets-cd27e9c8-dc0a-4672-a933-58eea98c2e1a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.766601ms
Aug 14 07:59:47.429: INFO: Pod "pod-secrets-cd27e9c8-dc0a-4672-a933-58eea98c2e1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008339691s
STEP: Saw pod success
Aug 14 07:59:47.429: INFO: Pod "pod-secrets-cd27e9c8-dc0a-4672-a933-58eea98c2e1a" satisfied condition "success or failure"
Aug 14 07:59:47.433: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-secrets-cd27e9c8-dc0a-4672-a933-58eea98c2e1a container secret-env-test: <nil>
STEP: delete the pod
Aug 14 07:59:47.451: INFO: Waiting for pod pod-secrets-cd27e9c8-dc0a-4672-a933-58eea98c2e1a to disappear
Aug 14 07:59:47.454: INFO: Pod pod-secrets-cd27e9c8-dc0a-4672-a933-58eea98c2e1a no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:59:47.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3403" for this suite.
Aug 14 07:59:53.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 07:59:53.636: INFO: namespace secrets-3403 deletion completed in 6.177191667s
â€¢SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 07:59:53.637: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7849
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Aug 14 07:59:53.802: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c98bbdcd-155c-4781-a5f4-85eeff0d14b7" in namespace "projected-7849" to be "success or failure"
Aug 14 07:59:53.806: INFO: Pod "downwardapi-volume-c98bbdcd-155c-4781-a5f4-85eeff0d14b7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.038513ms
Aug 14 07:59:55.811: INFO: Pod "downwardapi-volume-c98bbdcd-155c-4781-a5f4-85eeff0d14b7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009146806s
STEP: Saw pod success
Aug 14 07:59:55.811: INFO: Pod "downwardapi-volume-c98bbdcd-155c-4781-a5f4-85eeff0d14b7" satisfied condition "success or failure"
Aug 14 07:59:55.815: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod downwardapi-volume-c98bbdcd-155c-4781-a5f4-85eeff0d14b7 container client-container: <nil>
STEP: delete the pod
Aug 14 07:59:55.834: INFO: Waiting for pod downwardapi-volume-c98bbdcd-155c-4781-a5f4-85eeff0d14b7 to disappear
Aug 14 07:59:55.837: INFO: Pod downwardapi-volume-c98bbdcd-155c-4781-a5f4-85eeff0d14b7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 07:59:55.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7849" for this suite.
Aug 14 08:00:01.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 08:00:01.979: INFO: namespace projected-7849 deletion completed in 6.137817522s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 08:00:01.982: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-686
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-741ecb29-8f72-4d39-8408-666be52038c4
STEP: Creating configMap with name cm-test-opt-upd-bf0c46a0-2891-4446-8d92-6654208cd156
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-741ecb29-8f72-4d39-8408-666be52038c4
STEP: Updating configmap cm-test-opt-upd-bf0c46a0-2891-4446-8d92-6654208cd156
STEP: Creating configMap with name cm-test-opt-create-6061cbb0-f578-4926-8038-abd47b34203e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 08:00:06.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-686" for this suite.
Aug 14 08:00:28.605: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 08:00:28.719: INFO: namespace projected-686 deletion completed in 22.126463611s
â€¢SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 08:00:28.719: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9009
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-7368f3c9-b412-4280-a072-0cce89f184a2
STEP: Creating a pod to test consume secrets
Aug 14 08:00:28.879: INFO: Waiting up to 5m0s for pod "pod-secrets-0243c6c4-a8c5-47bc-8bb3-0e3fa6b84551" in namespace "secrets-9009" to be "success or failure"
Aug 14 08:00:28.885: INFO: Pod "pod-secrets-0243c6c4-a8c5-47bc-8bb3-0e3fa6b84551": Phase="Pending", Reason="", readiness=false. Elapsed: 5.994974ms
Aug 14 08:00:30.889: INFO: Pod "pod-secrets-0243c6c4-a8c5-47bc-8bb3-0e3fa6b84551": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010593423s
STEP: Saw pod success
Aug 14 08:00:30.890: INFO: Pod "pod-secrets-0243c6c4-a8c5-47bc-8bb3-0e3fa6b84551" satisfied condition "success or failure"
Aug 14 08:00:30.893: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-secrets-0243c6c4-a8c5-47bc-8bb3-0e3fa6b84551 container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 08:00:30.946: INFO: Waiting for pod pod-secrets-0243c6c4-a8c5-47bc-8bb3-0e3fa6b84551 to disappear
Aug 14 08:00:30.950: INFO: Pod pod-secrets-0243c6c4-a8c5-47bc-8bb3-0e3fa6b84551 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 08:00:30.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9009" for this suite.
Aug 14 08:00:36.969: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 08:00:37.104: INFO: namespace secrets-9009 deletion completed in 6.147506862s
â€¢SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 08:00:37.104: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-5534
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Aug 14 08:00:41.289: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5534 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 08:00:41.289: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 14 08:00:46.715: INFO: Exec stderr: ""
Aug 14 08:00:46.715: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5534 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 08:00:46.715: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 14 08:00:47.205: INFO: Exec stderr: ""
Aug 14 08:00:47.205: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5534 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 08:00:47.205: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 14 08:00:47.594: INFO: Exec stderr: ""
Aug 14 08:00:47.594: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5534 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 08:00:47.594: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 14 08:00:48.034: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Aug 14 08:00:48.034: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5534 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 08:00:48.034: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 14 08:00:48.530: INFO: Exec stderr: ""
Aug 14 08:00:48.530: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5534 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 08:00:48.530: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 14 08:00:48.968: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Aug 14 08:00:48.968: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5534 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 08:00:48.968: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 14 08:00:49.360: INFO: Exec stderr: ""
Aug 14 08:00:49.360: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5534 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 08:00:49.360: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 14 08:00:49.760: INFO: Exec stderr: ""
Aug 14 08:00:49.760: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5534 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 08:00:49.760: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 14 08:00:50.143: INFO: Exec stderr: ""
Aug 14 08:00:50.144: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5534 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Aug 14 08:00:50.144: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Aug 14 08:00:50.503: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 08:00:50.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5534" for this suite.
Aug 14 08:01:28.522: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 08:01:28.647: INFO: namespace e2e-kubelet-etc-hosts-5534 deletion completed in 38.137332479s
â€¢SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Aug 14 08:01:28.649: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-6150
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-faaf0709-2d8b-480e-88ff-be2ebb62fa65
STEP: Creating a pod to test consume secrets
Aug 14 08:01:28.818: INFO: Waiting up to 5m0s for pod "pod-secrets-366a476d-4e5f-4ac8-83f1-05e27702d243" in namespace "secrets-6150" to be "success or failure"
Aug 14 08:01:28.825: INFO: Pod "pod-secrets-366a476d-4e5f-4ac8-83f1-05e27702d243": Phase="Pending", Reason="", readiness=false. Elapsed: 6.519171ms
Aug 14 08:01:30.829: INFO: Pod "pod-secrets-366a476d-4e5f-4ac8-83f1-05e27702d243": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010998149s
STEP: Saw pod success
Aug 14 08:01:30.829: INFO: Pod "pod-secrets-366a476d-4e5f-4ac8-83f1-05e27702d243" satisfied condition "success or failure"
Aug 14 08:01:30.832: INFO: Trying to get logs from node shoot--it--tm-0eqz5-cpu-worker-z1-868656cfc4-r74d2 pod pod-secrets-366a476d-4e5f-4ac8-83f1-05e27702d243 container secret-volume-test: <nil>
STEP: delete the pod
Aug 14 08:01:30.853: INFO: Waiting for pod pod-secrets-366a476d-4e5f-4ac8-83f1-05e27702d243 to disappear
Aug 14 08:01:30.856: INFO: Pod pod-secrets-366a476d-4e5f-4ac8-83f1-05e27702d243 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.2-beta.0.7+f6278300bebbb7/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Aug 14 08:01:30.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6150" for this suite.
Aug 14 08:01:36.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Aug 14 08:01:37.047: INFO: namespace secrets-6150 deletion completed in 6.185825305s
â€¢SSSSSSSSSSSSSSSSSSSSAug 14 08:01:37.048: INFO: Running AfterSuite actions on all nodes
Aug 14 08:01:37.048: INFO: Running AfterSuite actions on node 1
Aug 14 08:01:37.048: INFO: Skipping dumping logs from cluster

Ran 212 of 4413 Specs in 5712.620 seconds
SUCCESS! -- 212 Passed | 0 Failed | 0 Flaked | 0 Pending | 4201 Skipped
PASS

Ginkgo ran 1 suite in 1h35m45.655913239s
Test Suite Passed
