Conformance test: not doing test setup.
I1129 12:22:07.831266    4828 e2e.go:243] Starting e2e run "8865765b-a34c-45dc-988d-6af5cf09a23c" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1575030125 - Will randomize all specs
Will run 215 of 4413 specs

Nov 29 12:22:54.205: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Deleting namespaces
STEP: Waiting for namespaces to vanish
I1129 12:22:54.274567    4828 e2e.go:98] Waiting for deletion of the following namespaces: []
Nov 29 12:22:56.279: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Nov 29 12:22:56.289: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Nov 29 12:22:56.325: INFO: 20 / 20 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Nov 29 12:22:56.325: INFO: expected 12 pod replicas in namespace 'kube-system', 12 are Running and Ready.
Nov 29 12:22:56.326: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Nov 29 12:22:56.337: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Nov 29 12:22:56.337: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Nov 29 12:22:56.337: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-exporter' (0 seconds elapsed)
Nov 29 12:22:56.337: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'node-problem-detector' (0 seconds elapsed)
Nov 29 12:22:56.337: INFO: e2e test version: v1.15.6
Nov 29 12:22:56.339: INFO: kube-apiserver version: v1.15.6
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:22:56.339: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
Nov 29 12:22:56.380: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Nov 29 12:22:56.395: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-72
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 29 12:23:04.687: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 29 12:23:04.691: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 29 12:23:06.691: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 29 12:23:06.696: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 29 12:23:08.691: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 29 12:23:08.695: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 29 12:23:10.691: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 29 12:23:10.696: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 29 12:23:12.691: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 29 12:23:12.696: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 29 12:23:14.691: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 29 12:23:14.695: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 29 12:23:16.691: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 29 12:23:16.696: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 29 12:23:18.691: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 29 12:23:18.696: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 29 12:23:20.691: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 29 12:23:20.696: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 29 12:23:22.691: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 29 12:23:22.696: INFO: Pod pod-with-poststart-exec-hook still exists
Nov 29 12:23:24.691: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Nov 29 12:23:24.696: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:23:24.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-72" for this suite.
Nov 29 12:23:46.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:23:46.835: INFO: namespace container-lifecycle-hook-72 deletion completed in 22.135596325s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:23:46.836: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4783
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Nov 29 12:23:46.986: INFO: Waiting up to 5m0s for pod "downward-api-b4dcea8e-3a8c-4944-95f4-dea1ecf2d62c" in namespace "downward-api-4783" to be "success or failure"
Nov 29 12:23:46.989: INFO: Pod "downward-api-b4dcea8e-3a8c-4944-95f4-dea1ecf2d62c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.772134ms
Nov 29 12:23:48.994: INFO: Pod "downward-api-b4dcea8e-3a8c-4944-95f4-dea1ecf2d62c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008857914s
Nov 29 12:23:51.000: INFO: Pod "downward-api-b4dcea8e-3a8c-4944-95f4-dea1ecf2d62c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013942621s
STEP: Saw pod success
Nov 29 12:23:51.000: INFO: Pod "downward-api-b4dcea8e-3a8c-4944-95f4-dea1ecf2d62c" satisfied condition "success or failure"
Nov 29 12:23:51.003: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod downward-api-b4dcea8e-3a8c-4944-95f4-dea1ecf2d62c container dapi-container: <nil>
STEP: delete the pod
Nov 29 12:23:51.026: INFO: Waiting for pod downward-api-b4dcea8e-3a8c-4944-95f4-dea1ecf2d62c to disappear
Nov 29 12:23:51.029: INFO: Pod downward-api-b4dcea8e-3a8c-4944-95f4-dea1ecf2d62c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:23:51.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4783" for this suite.
Nov 29 12:23:57.050: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:23:57.179: INFO: namespace downward-api-4783 deletion completed in 6.140952272s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:23:57.179: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3238
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:24:57.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3238" for this suite.
Nov 29 12:25:19.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:25:19.475: INFO: namespace container-probe-3238 deletion completed in 22.134462204s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:25:19.475: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7011
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1292
STEP: creating an rc
Nov 29 12:25:19.619: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-7011'
Nov 29 12:25:19.890: INFO: stderr: ""
Nov 29 12:25:19.890: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Nov 29 12:25:20.895: INFO: Selector matched 1 pods for map[app:redis]
Nov 29 12:25:20.895: INFO: Found 0 / 1
Nov 29 12:25:21.895: INFO: Selector matched 1 pods for map[app:redis]
Nov 29 12:25:21.895: INFO: Found 0 / 1
Nov 29 12:25:22.899: INFO: Selector matched 1 pods for map[app:redis]
Nov 29 12:25:22.899: INFO: Found 1 / 1
Nov 29 12:25:22.899: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 29 12:25:22.902: INFO: Selector matched 1 pods for map[app:redis]
Nov 29 12:25:22.902: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Nov 29 12:25:22.902: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-mwx49 redis-master --namespace=kubectl-7011'
Nov 29 12:25:23.080: INFO: stderr: ""
Nov 29 12:25:23.080: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 29 Nov 12:25:21.994 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 29 Nov 12:25:21.994 # Server started, Redis version 3.2.12\n1:M 29 Nov 12:25:21.994 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 29 Nov 12:25:21.994 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Nov 29 12:25:23.080: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-mwx49 redis-master --namespace=kubectl-7011 --tail=1'
Nov 29 12:25:23.175: INFO: stderr: ""
Nov 29 12:25:23.175: INFO: stdout: "1:M 29 Nov 12:25:21.994 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Nov 29 12:25:23.175: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-mwx49 redis-master --namespace=kubectl-7011 --limit-bytes=1'
Nov 29 12:25:23.263: INFO: stderr: ""
Nov 29 12:25:23.263: INFO: stdout: " "
STEP: exposing timestamps
Nov 29 12:25:23.263: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-mwx49 redis-master --namespace=kubectl-7011 --tail=1 --timestamps'
Nov 29 12:25:23.362: INFO: stderr: ""
Nov 29 12:25:23.362: INFO: stdout: "2019-11-29T12:25:21.994725271Z 1:M 29 Nov 12:25:21.994 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Nov 29 12:25:25.862: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-mwx49 redis-master --namespace=kubectl-7011 --since=1s'
Nov 29 12:25:25.947: INFO: stderr: ""
Nov 29 12:25:25.947: INFO: stdout: ""
Nov 29 12:25:25.947: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-mwx49 redis-master --namespace=kubectl-7011 --since=24h'
Nov 29 12:25:26.035: INFO: stderr: ""
Nov 29 12:25:26.036: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 29 Nov 12:25:21.994 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 29 Nov 12:25:21.994 # Server started, Redis version 3.2.12\n1:M 29 Nov 12:25:21.994 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 29 Nov 12:25:21.994 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
STEP: using delete to clean up resources
Nov 29 12:25:26.036: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-7011'
Nov 29 12:25:26.109: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 12:25:26.109: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Nov 29 12:25:26.109: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=nginx --no-headers --namespace=kubectl-7011'
Nov 29 12:25:26.191: INFO: stderr: "No resources found.\n"
Nov 29 12:25:26.191: INFO: stdout: ""
Nov 29 12:25:26.191: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=nginx --namespace=kubectl-7011 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 29 12:25:26.260: INFO: stderr: ""
Nov 29 12:25:26.260: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:25:26.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7011" for this suite.
Nov 29 12:25:48.280: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:25:48.404: INFO: namespace kubectl-7011 deletion completed in 22.138022074s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:25:48.406: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4603
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-37314f17-6207-4129-b46c-cff95bda8b0f in namespace container-probe-4603
Nov 29 12:25:50.575: INFO: Started pod test-webserver-37314f17-6207-4129-b46c-cff95bda8b0f in namespace container-probe-4603
STEP: checking the pod's current state and verifying that restartCount is present
Nov 29 12:25:50.579: INFO: Initial restart count of pod test-webserver-37314f17-6207-4129-b46c-cff95bda8b0f is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:29:51.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4603" for this suite.
Nov 29 12:29:57.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:29:57.394: INFO: namespace container-probe-4603 deletion completed in 6.130845026s
•SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:29:57.394: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9088
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-n74h5 in namespace proxy-9088
I1129 12:29:57.635329    4828 runners.go:180] Created replication controller with name: proxy-service-n74h5, namespace: proxy-9088, replica count: 1
I1129 12:29:58.686325    4828 runners.go:180] proxy-service-n74h5 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1129 12:29:59.686567    4828 runners.go:180] proxy-service-n74h5 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1129 12:30:00.686781    4828 runners.go:180] proxy-service-n74h5 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1129 12:30:01.687005    4828 runners.go:180] proxy-service-n74h5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1129 12:30:02.687240    4828 runners.go:180] proxy-service-n74h5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1129 12:30:03.687442    4828 runners.go:180] proxy-service-n74h5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1129 12:30:04.687626    4828 runners.go:180] proxy-service-n74h5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1129 12:30:05.687854    4828 runners.go:180] proxy-service-n74h5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1129 12:30:06.688039    4828 runners.go:180] proxy-service-n74h5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1129 12:30:07.688278    4828 runners.go:180] proxy-service-n74h5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1129 12:30:08.688535    4828 runners.go:180] proxy-service-n74h5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1129 12:30:09.688858    4828 runners.go:180] proxy-service-n74h5 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1129 12:30:10.689084    4828 runners.go:180] proxy-service-n74h5 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 29 12:30:10.693: INFO: setup took 13.07421941s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Nov 29 12:30:10.704: INFO: (0) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:162/proxy/: bar (200; 10.511255ms)
Nov 29 12:30:10.704: INFO: (0) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname1/proxy/: foo (200; 10.422097ms)
Nov 29 12:30:10.704: INFO: (0) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:160/proxy/: foo (200; 10.415464ms)
Nov 29 12:30:10.704: INFO: (0) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:160/proxy/: foo (200; 10.566722ms)
Nov 29 12:30:10.708: INFO: (0) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/rewriteme">test<... (200; 14.471672ms)
Nov 29 12:30:10.708: INFO: (0) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/rewriteme">... (200; 14.796055ms)
Nov 29 12:30:10.712: INFO: (0) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname2/proxy/: bar (200; 17.946616ms)
Nov 29 12:30:10.712: INFO: (0) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/rewriteme">test</a> (200; 18.058307ms)
Nov 29 12:30:10.712: INFO: (0) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname2/proxy/: bar (200; 18.1061ms)
Nov 29 12:30:10.712: INFO: (0) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:162/proxy/: bar (200; 18.385377ms)
Nov 29 12:30:10.712: INFO: (0) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname1/proxy/: foo (200; 18.128556ms)
Nov 29 12:30:10.714: INFO: (0) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/tlsrewritem... (200; 20.776429ms)
Nov 29 12:30:10.714: INFO: (0) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname2/proxy/: tls qux (200; 20.976525ms)
Nov 29 12:30:10.715: INFO: (0) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname1/proxy/: tls baz (200; 21.231431ms)
Nov 29 12:30:10.716: INFO: (0) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:462/proxy/: tls qux (200; 22.398064ms)
Nov 29 12:30:10.724: INFO: (0) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:460/proxy/: tls baz (200; 30.706091ms)
Nov 29 12:30:10.733: INFO: (1) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/rewriteme">test<... (200; 8.636003ms)
Nov 29 12:30:10.733: INFO: (1) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:160/proxy/: foo (200; 8.750501ms)
Nov 29 12:30:10.734: INFO: (1) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/tlsrewritem... (200; 8.64746ms)
Nov 29 12:30:10.734: INFO: (1) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:162/proxy/: bar (200; 8.840891ms)
Nov 29 12:30:10.734: INFO: (1) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/rewriteme">... (200; 8.843218ms)
Nov 29 12:30:10.734: INFO: (1) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:462/proxy/: tls qux (200; 8.702432ms)
Nov 29 12:30:10.734: INFO: (1) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:162/proxy/: bar (200; 8.750645ms)
Nov 29 12:30:10.734: INFO: (1) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/rewriteme">test</a> (200; 8.947824ms)
Nov 29 12:30:10.734: INFO: (1) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:160/proxy/: foo (200; 8.845283ms)
Nov 29 12:30:10.734: INFO: (1) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:460/proxy/: tls baz (200; 9.128273ms)
Nov 29 12:30:10.736: INFO: (1) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname1/proxy/: tls baz (200; 10.757269ms)
Nov 29 12:30:10.736: INFO: (1) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname1/proxy/: foo (200; 10.964928ms)
Nov 29 12:30:10.736: INFO: (1) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname2/proxy/: bar (200; 11.179735ms)
Nov 29 12:30:10.736: INFO: (1) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname2/proxy/: tls qux (200; 10.765808ms)
Nov 29 12:30:10.736: INFO: (1) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname1/proxy/: foo (200; 10.834867ms)
Nov 29 12:30:10.736: INFO: (1) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname2/proxy/: bar (200; 10.959802ms)
Nov 29 12:30:10.745: INFO: (2) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:460/proxy/: tls baz (200; 8.714709ms)
Nov 29 12:30:10.745: INFO: (2) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:462/proxy/: tls qux (200; 8.807251ms)
Nov 29 12:30:10.745: INFO: (2) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:160/proxy/: foo (200; 8.808263ms)
Nov 29 12:30:10.748: INFO: (2) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/rewriteme">test</a> (200; 11.586669ms)
Nov 29 12:30:10.751: INFO: (2) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/tlsrewritem... (200; 15.172339ms)
Nov 29 12:30:10.751: INFO: (2) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:160/proxy/: foo (200; 15.105652ms)
Nov 29 12:30:10.751: INFO: (2) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:162/proxy/: bar (200; 15.557491ms)
Nov 29 12:30:10.753: INFO: (2) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/rewriteme">test<... (200; 16.493017ms)
Nov 29 12:30:10.753: INFO: (2) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/rewriteme">... (200; 16.565908ms)
Nov 29 12:30:10.753: INFO: (2) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:162/proxy/: bar (200; 17.046735ms)
Nov 29 12:30:10.753: INFO: (2) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname1/proxy/: tls baz (200; 17.215091ms)
Nov 29 12:30:10.790: INFO: (2) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname2/proxy/: tls qux (200; 54.455032ms)
Nov 29 12:30:10.790: INFO: (2) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname1/proxy/: foo (200; 54.499956ms)
Nov 29 12:30:10.790: INFO: (2) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname2/proxy/: bar (200; 54.428515ms)
Nov 29 12:30:10.790: INFO: (2) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname2/proxy/: bar (200; 54.589758ms)
Nov 29 12:30:10.790: INFO: (2) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname1/proxy/: foo (200; 54.414029ms)
Nov 29 12:30:10.797: INFO: (3) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/rewriteme">test<... (200; 6.278615ms)
Nov 29 12:30:10.797: INFO: (3) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:162/proxy/: bar (200; 6.378479ms)
Nov 29 12:30:10.797: INFO: (3) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:160/proxy/: foo (200; 6.459182ms)
Nov 29 12:30:10.797: INFO: (3) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:162/proxy/: bar (200; 6.556248ms)
Nov 29 12:30:10.797: INFO: (3) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/rewriteme">test</a> (200; 6.516042ms)
Nov 29 12:30:10.797: INFO: (3) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:460/proxy/: tls baz (200; 6.54257ms)
Nov 29 12:30:10.797: INFO: (3) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:462/proxy/: tls qux (200; 6.579448ms)
Nov 29 12:30:10.797: INFO: (3) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/tlsrewritem... (200; 6.664803ms)
Nov 29 12:30:10.797: INFO: (3) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:160/proxy/: foo (200; 6.667748ms)
Nov 29 12:30:10.798: INFO: (3) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/rewriteme">... (200; 7.26396ms)
Nov 29 12:30:10.798: INFO: (3) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname2/proxy/: tls qux (200; 7.434652ms)
Nov 29 12:30:10.798: INFO: (3) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname1/proxy/: tls baz (200; 7.539006ms)
Nov 29 12:30:10.799: INFO: (3) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname1/proxy/: foo (200; 7.944579ms)
Nov 29 12:30:10.799: INFO: (3) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname2/proxy/: bar (200; 7.96809ms)
Nov 29 12:30:10.799: INFO: (3) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname2/proxy/: bar (200; 8.606343ms)
Nov 29 12:30:10.799: INFO: (3) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname1/proxy/: foo (200; 8.755999ms)
Nov 29 12:30:10.807: INFO: (4) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:460/proxy/: tls baz (200; 7.291257ms)
Nov 29 12:30:10.807: INFO: (4) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:462/proxy/: tls qux (200; 7.363088ms)
Nov 29 12:30:10.807: INFO: (4) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/rewriteme">test</a> (200; 7.722172ms)
Nov 29 12:30:10.807: INFO: (4) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/tlsrewritem... (200; 7.707169ms)
Nov 29 12:30:10.807: INFO: (4) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:160/proxy/: foo (200; 7.943447ms)
Nov 29 12:30:10.808: INFO: (4) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:162/proxy/: bar (200; 8.114553ms)
Nov 29 12:30:10.808: INFO: (4) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/rewriteme">... (200; 8.059256ms)
Nov 29 12:30:10.808: INFO: (4) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:162/proxy/: bar (200; 8.456726ms)
Nov 29 12:30:10.808: INFO: (4) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:160/proxy/: foo (200; 8.294076ms)
Nov 29 12:30:10.808: INFO: (4) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/rewriteme">test<... (200; 8.482901ms)
Nov 29 12:30:10.808: INFO: (4) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname2/proxy/: tls qux (200; 8.541222ms)
Nov 29 12:30:10.808: INFO: (4) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname1/proxy/: tls baz (200; 8.662417ms)
Nov 29 12:30:10.809: INFO: (4) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname2/proxy/: bar (200; 9.379521ms)
Nov 29 12:30:10.810: INFO: (4) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname1/proxy/: foo (200; 10.202638ms)
Nov 29 12:30:10.810: INFO: (4) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname2/proxy/: bar (200; 10.451342ms)
Nov 29 12:30:10.810: INFO: (4) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname1/proxy/: foo (200; 10.534096ms)
Nov 29 12:30:10.819: INFO: (5) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/rewriteme">test</a> (200; 9.064458ms)
Nov 29 12:30:10.819: INFO: (5) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:160/proxy/: foo (200; 9.160768ms)
Nov 29 12:30:10.819: INFO: (5) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:160/proxy/: foo (200; 9.29189ms)
Nov 29 12:30:10.819: INFO: (5) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/rewriteme">test<... (200; 9.202525ms)
Nov 29 12:30:10.819: INFO: (5) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/rewriteme">... (200; 9.234831ms)
Nov 29 12:30:10.820: INFO: (5) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname2/proxy/: bar (200; 9.85944ms)
Nov 29 12:30:10.820: INFO: (5) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:162/proxy/: bar (200; 9.77138ms)
Nov 29 12:30:10.821: INFO: (5) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:462/proxy/: tls qux (200; 10.63677ms)
Nov 29 12:30:10.821: INFO: (5) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:460/proxy/: tls baz (200; 11.00048ms)
Nov 29 12:30:10.821: INFO: (5) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/tlsrewritem... (200; 11.071366ms)
Nov 29 12:30:10.821: INFO: (5) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname2/proxy/: tls qux (200; 11.030286ms)
Nov 29 12:30:10.821: INFO: (5) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname1/proxy/: tls baz (200; 11.016778ms)
Nov 29 12:30:10.822: INFO: (5) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname2/proxy/: bar (200; 11.970713ms)
Nov 29 12:30:10.822: INFO: (5) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname1/proxy/: foo (200; 12.184817ms)
Nov 29 12:30:10.822: INFO: (5) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname1/proxy/: foo (200; 12.181433ms)
Nov 29 12:30:10.822: INFO: (5) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:162/proxy/: bar (200; 12.179168ms)
Nov 29 12:30:10.831: INFO: (6) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:160/proxy/: foo (200; 7.988347ms)
Nov 29 12:30:10.831: INFO: (6) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/rewriteme">test<... (200; 8.181059ms)
Nov 29 12:30:10.831: INFO: (6) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/tlsrewritem... (200; 7.724233ms)
Nov 29 12:30:10.831: INFO: (6) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/rewriteme">test</a> (200; 8.051272ms)
Nov 29 12:30:10.832: INFO: (6) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:460/proxy/: tls baz (200; 8.723271ms)
Nov 29 12:30:10.832: INFO: (6) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname2/proxy/: bar (200; 9.113059ms)
Nov 29 12:30:10.832: INFO: (6) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname2/proxy/: tls qux (200; 8.290697ms)
Nov 29 12:30:10.832: INFO: (6) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname1/proxy/: foo (200; 9.221822ms)
Nov 29 12:30:10.832: INFO: (6) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/rewriteme">... (200; 9.349893ms)
Nov 29 12:30:10.832: INFO: (6) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:462/proxy/: tls qux (200; 8.647131ms)
Nov 29 12:30:10.832: INFO: (6) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:162/proxy/: bar (200; 9.040503ms)
Nov 29 12:30:10.832: INFO: (6) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname1/proxy/: tls baz (200; 8.502533ms)
Nov 29 12:30:10.833: INFO: (6) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname1/proxy/: foo (200; 10.164406ms)
Nov 29 12:30:10.837: INFO: (6) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:162/proxy/: bar (200; 13.441866ms)
Nov 29 12:30:10.837: INFO: (6) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:160/proxy/: foo (200; 13.503441ms)
Nov 29 12:30:10.837: INFO: (6) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname2/proxy/: bar (200; 13.289738ms)
Nov 29 12:30:10.848: INFO: (7) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:160/proxy/: foo (200; 11.310646ms)
Nov 29 12:30:10.848: INFO: (7) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:460/proxy/: tls baz (200; 11.258029ms)
Nov 29 12:30:10.848: INFO: (7) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/tlsrewritem... (200; 11.251414ms)
Nov 29 12:30:10.848: INFO: (7) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:162/proxy/: bar (200; 11.064491ms)
Nov 29 12:30:10.848: INFO: (7) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:160/proxy/: foo (200; 11.167252ms)
Nov 29 12:30:10.848: INFO: (7) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/rewriteme">... (200; 11.321641ms)
Nov 29 12:30:10.848: INFO: (7) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:462/proxy/: tls qux (200; 11.702415ms)
Nov 29 12:30:10.848: INFO: (7) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:162/proxy/: bar (200; 11.559801ms)
Nov 29 12:30:10.852: INFO: (7) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname1/proxy/: tls baz (200; 15.24856ms)
Nov 29 12:30:10.874: INFO: (7) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname2/proxy/: bar (200; 36.904213ms)
Nov 29 12:30:10.874: INFO: (7) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname1/proxy/: foo (200; 36.754489ms)
Nov 29 12:30:10.874: INFO: (7) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname2/proxy/: tls qux (200; 36.860832ms)
Nov 29 12:30:10.874: INFO: (7) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/rewriteme">test<... (200; 36.793486ms)
Nov 29 12:30:10.874: INFO: (7) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname1/proxy/: foo (200; 36.826599ms)
Nov 29 12:30:10.874: INFO: (7) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname2/proxy/: bar (200; 36.811229ms)
Nov 29 12:30:10.874: INFO: (7) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/rewriteme">test</a> (200; 37.118743ms)
Nov 29 12:30:10.884: INFO: (8) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname2/proxy/: bar (200; 9.76103ms)
Nov 29 12:30:10.884: INFO: (8) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:160/proxy/: foo (200; 9.839748ms)
Nov 29 12:30:10.884: INFO: (8) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/rewriteme">test<... (200; 9.986437ms)
Nov 29 12:30:10.884: INFO: (8) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:162/proxy/: bar (200; 9.977659ms)
Nov 29 12:30:10.884: INFO: (8) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/rewriteme">test</a> (200; 9.767318ms)
Nov 29 12:30:10.884: INFO: (8) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/rewriteme">... (200; 9.877536ms)
Nov 29 12:30:10.884: INFO: (8) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:460/proxy/: tls baz (200; 9.949132ms)
Nov 29 12:30:10.884: INFO: (8) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/tlsrewritem... (200; 10.001002ms)
Nov 29 12:30:10.884: INFO: (8) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname1/proxy/: foo (200; 10.344322ms)
Nov 29 12:30:10.885: INFO: (8) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:162/proxy/: bar (200; 10.583063ms)
Nov 29 12:30:10.885: INFO: (8) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:462/proxy/: tls qux (200; 10.466247ms)
Nov 29 12:30:10.885: INFO: (8) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname2/proxy/: tls qux (200; 10.664506ms)
Nov 29 12:30:10.885: INFO: (8) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname1/proxy/: tls baz (200; 10.659644ms)
Nov 29 12:30:10.885: INFO: (8) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname2/proxy/: bar (200; 11.323974ms)
Nov 29 12:30:10.886: INFO: (8) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:160/proxy/: foo (200; 11.593137ms)
Nov 29 12:30:10.886: INFO: (8) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname1/proxy/: foo (200; 11.611208ms)
Nov 29 12:30:10.894: INFO: (9) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/rewriteme">test<... (200; 7.638245ms)
Nov 29 12:30:10.894: INFO: (9) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:162/proxy/: bar (200; 8.273986ms)
Nov 29 12:30:10.894: INFO: (9) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:160/proxy/: foo (200; 8.471536ms)
Nov 29 12:30:10.894: INFO: (9) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:460/proxy/: tls baz (200; 8.352778ms)
Nov 29 12:30:10.894: INFO: (9) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:162/proxy/: bar (200; 8.211777ms)
Nov 29 12:30:10.895: INFO: (9) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/rewriteme">test</a> (200; 8.445529ms)
Nov 29 12:30:10.895: INFO: (9) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname1/proxy/: foo (200; 8.578838ms)
Nov 29 12:30:10.895: INFO: (9) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/tlsrewritem... (200; 8.578916ms)
Nov 29 12:30:10.895: INFO: (9) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/rewriteme">... (200; 8.55354ms)
Nov 29 12:30:10.895: INFO: (9) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:462/proxy/: tls qux (200; 8.714537ms)
Nov 29 12:30:10.895: INFO: (9) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname1/proxy/: tls baz (200; 8.977079ms)
Nov 29 12:30:10.896: INFO: (9) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname2/proxy/: bar (200; 9.759974ms)
Nov 29 12:30:10.896: INFO: (9) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname2/proxy/: bar (200; 9.817953ms)
Nov 29 12:30:10.896: INFO: (9) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname2/proxy/: tls qux (200; 9.747152ms)
Nov 29 12:30:10.896: INFO: (9) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:160/proxy/: foo (200; 9.897694ms)
Nov 29 12:30:10.899: INFO: (9) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname1/proxy/: foo (200; 12.414747ms)
Nov 29 12:30:10.908: INFO: (10) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:160/proxy/: foo (200; 8.711323ms)
Nov 29 12:30:10.908: INFO: (10) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:162/proxy/: bar (200; 8.876111ms)
Nov 29 12:30:10.908: INFO: (10) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/rewriteme">test<... (200; 8.80195ms)
Nov 29 12:30:10.908: INFO: (10) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:160/proxy/: foo (200; 8.995824ms)
Nov 29 12:30:10.908: INFO: (10) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/rewriteme">test</a> (200; 8.792124ms)
Nov 29 12:30:10.908: INFO: (10) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/rewriteme">... (200; 9.184488ms)
Nov 29 12:30:10.908: INFO: (10) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname2/proxy/: tls qux (200; 9.118789ms)
Nov 29 12:30:10.908: INFO: (10) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:462/proxy/: tls qux (200; 9.336338ms)
Nov 29 12:30:10.908: INFO: (10) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:162/proxy/: bar (200; 9.506673ms)
Nov 29 12:30:10.908: INFO: (10) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/tlsrewritem... (200; 9.199274ms)
Nov 29 12:30:10.908: INFO: (10) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:460/proxy/: tls baz (200; 9.274388ms)
Nov 29 12:30:10.908: INFO: (10) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname1/proxy/: tls baz (200; 9.2823ms)
Nov 29 12:30:10.908: INFO: (10) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname1/proxy/: foo (200; 9.51766ms)
Nov 29 12:30:10.909: INFO: (10) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname1/proxy/: foo (200; 9.893792ms)
Nov 29 12:30:10.909: INFO: (10) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname2/proxy/: bar (200; 10.035191ms)
Nov 29 12:30:10.909: INFO: (10) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname2/proxy/: bar (200; 10.194156ms)
Nov 29 12:30:10.916: INFO: (11) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:462/proxy/: tls qux (200; 6.439606ms)
Nov 29 12:30:10.916: INFO: (11) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname1/proxy/: tls baz (200; 6.952192ms)
Nov 29 12:30:10.916: INFO: (11) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:160/proxy/: foo (200; 6.254222ms)
Nov 29 12:30:10.916: INFO: (11) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/rewriteme">... (200; 6.819353ms)
Nov 29 12:30:10.916: INFO: (11) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/tlsrewritem... (200; 6.615987ms)
Nov 29 12:30:10.916: INFO: (11) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:160/proxy/: foo (200; 6.861661ms)
Nov 29 12:30:10.916: INFO: (11) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:460/proxy/: tls baz (200; 7.370754ms)
Nov 29 12:30:10.916: INFO: (11) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname2/proxy/: tls qux (200; 7.086139ms)
Nov 29 12:30:10.916: INFO: (11) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname2/proxy/: bar (200; 7.205526ms)
Nov 29 12:30:10.916: INFO: (11) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:162/proxy/: bar (200; 6.807246ms)
Nov 29 12:30:10.917: INFO: (11) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/rewriteme">test</a> (200; 6.522127ms)
Nov 29 12:30:10.917: INFO: (11) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/rewriteme">test<... (200; 7.237007ms)
Nov 29 12:30:10.918: INFO: (11) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname1/proxy/: foo (200; 8.15516ms)
Nov 29 12:30:10.918: INFO: (11) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:162/proxy/: bar (200; 8.008681ms)
Nov 29 12:30:10.918: INFO: (11) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname2/proxy/: bar (200; 8.117397ms)
Nov 29 12:30:10.918: INFO: (11) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname1/proxy/: foo (200; 7.929526ms)
Nov 29 12:30:10.927: INFO: (12) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:162/proxy/: bar (200; 8.331045ms)
Nov 29 12:30:10.927: INFO: (12) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:162/proxy/: bar (200; 7.601517ms)
Nov 29 12:30:10.927: INFO: (12) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:160/proxy/: foo (200; 7.655304ms)
Nov 29 12:30:10.927: INFO: (12) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:160/proxy/: foo (200; 8.181103ms)
Nov 29 12:30:10.927: INFO: (12) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname1/proxy/: tls baz (200; 8.618796ms)
Nov 29 12:30:10.927: INFO: (12) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/rewriteme">test<... (200; 8.526536ms)
Nov 29 12:30:10.927: INFO: (12) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/rewriteme">... (200; 7.833019ms)
Nov 29 12:30:10.927: INFO: (12) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/rewriteme">test</a> (200; 8.002737ms)
Nov 29 12:30:10.927: INFO: (12) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:460/proxy/: tls baz (200; 8.159319ms)
Nov 29 12:30:10.927: INFO: (12) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/tlsrewritem... (200; 7.8426ms)
Nov 29 12:30:10.928: INFO: (12) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname1/proxy/: foo (200; 8.999768ms)
Nov 29 12:30:10.928: INFO: (12) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname2/proxy/: bar (200; 9.25138ms)
Nov 29 12:30:10.928: INFO: (12) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname1/proxy/: foo (200; 9.342248ms)
Nov 29 12:30:10.928: INFO: (12) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname2/proxy/: bar (200; 8.301845ms)
Nov 29 12:30:10.928: INFO: (12) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname2/proxy/: tls qux (200; 8.945034ms)
Nov 29 12:30:10.928: INFO: (12) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:462/proxy/: tls qux (200; 8.387154ms)
Nov 29 12:30:10.935: INFO: (13) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:460/proxy/: tls baz (200; 6.815726ms)
Nov 29 12:30:10.935: INFO: (13) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:162/proxy/: bar (200; 7.007716ms)
Nov 29 12:30:10.935: INFO: (13) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/rewriteme">test</a> (200; 6.846221ms)
Nov 29 12:30:10.935: INFO: (13) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/rewriteme">... (200; 6.973102ms)
Nov 29 12:30:10.935: INFO: (13) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname2/proxy/: bar (200; 7.320682ms)
Nov 29 12:30:10.935: INFO: (13) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname1/proxy/: foo (200; 7.546879ms)
Nov 29 12:30:10.935: INFO: (13) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/tlsrewritem... (200; 7.401648ms)
Nov 29 12:30:10.935: INFO: (13) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/rewriteme">test<... (200; 7.385052ms)
Nov 29 12:30:10.935: INFO: (13) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:160/proxy/: foo (200; 7.394221ms)
Nov 29 12:30:10.936: INFO: (13) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:462/proxy/: tls qux (200; 7.889611ms)
Nov 29 12:30:10.936: INFO: (13) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname1/proxy/: tls baz (200; 7.75957ms)
Nov 29 12:30:10.936: INFO: (13) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname2/proxy/: tls qux (200; 7.762451ms)
Nov 29 12:30:10.936: INFO: (13) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:162/proxy/: bar (200; 8.044869ms)
Nov 29 12:30:10.937: INFO: (13) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname2/proxy/: bar (200; 8.779845ms)
Nov 29 12:30:10.937: INFO: (13) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname1/proxy/: foo (200; 9.317713ms)
Nov 29 12:30:10.937: INFO: (13) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:160/proxy/: foo (200; 9.492582ms)
Nov 29 12:30:10.947: INFO: (14) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:160/proxy/: foo (200; 9.67389ms)
Nov 29 12:30:10.947: INFO: (14) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:160/proxy/: foo (200; 9.802752ms)
Nov 29 12:30:10.947: INFO: (14) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:162/proxy/: bar (200; 9.906901ms)
Nov 29 12:30:10.947: INFO: (14) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:462/proxy/: tls qux (200; 9.789914ms)
Nov 29 12:30:10.947: INFO: (14) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname2/proxy/: bar (200; 10.05605ms)
Nov 29 12:30:10.947: INFO: (14) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:460/proxy/: tls baz (200; 9.831789ms)
Nov 29 12:30:10.947: INFO: (14) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/rewriteme">... (200; 9.822698ms)
Nov 29 12:30:10.947: INFO: (14) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/tlsrewritem... (200; 9.870642ms)
Nov 29 12:30:10.947: INFO: (14) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/rewriteme">test<... (200; 9.958748ms)
Nov 29 12:30:10.947: INFO: (14) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/rewriteme">test</a> (200; 9.972732ms)
Nov 29 12:30:10.949: INFO: (14) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname2/proxy/: bar (200; 11.132786ms)
Nov 29 12:30:10.949: INFO: (14) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname1/proxy/: foo (200; 11.190173ms)
Nov 29 12:30:10.949: INFO: (14) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:162/proxy/: bar (200; 11.249713ms)
Nov 29 12:30:10.949: INFO: (14) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname1/proxy/: tls baz (200; 11.370239ms)
Nov 29 12:30:10.949: INFO: (14) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname2/proxy/: tls qux (200; 11.274773ms)
Nov 29 12:30:10.949: INFO: (14) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname1/proxy/: foo (200; 11.793844ms)
Nov 29 12:30:10.958: INFO: (15) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:462/proxy/: tls qux (200; 8.067508ms)
Nov 29 12:30:10.958: INFO: (15) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/rewriteme">... (200; 8.041862ms)
Nov 29 12:30:10.958: INFO: (15) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:162/proxy/: bar (200; 8.066572ms)
Nov 29 12:30:10.958: INFO: (15) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname1/proxy/: foo (200; 8.105737ms)
Nov 29 12:30:10.958: INFO: (15) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:162/proxy/: bar (200; 8.172856ms)
Nov 29 12:30:10.958: INFO: (15) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/tlsrewritem... (200; 8.329386ms)
Nov 29 12:30:10.958: INFO: (15) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/rewriteme">test</a> (200; 8.216368ms)
Nov 29 12:30:10.958: INFO: (15) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:460/proxy/: tls baz (200; 8.509846ms)
Nov 29 12:30:10.958: INFO: (15) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:160/proxy/: foo (200; 8.228946ms)
Nov 29 12:30:10.959: INFO: (15) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname2/proxy/: tls qux (200; 8.902797ms)
Nov 29 12:30:10.959: INFO: (15) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname1/proxy/: tls baz (200; 9.224838ms)
Nov 29 12:30:10.959: INFO: (15) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/rewriteme">test<... (200; 9.222075ms)
Nov 29 12:30:10.960: INFO: (15) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname2/proxy/: bar (200; 10.226086ms)
Nov 29 12:30:10.960: INFO: (15) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:160/proxy/: foo (200; 10.357046ms)
Nov 29 12:30:10.960: INFO: (15) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname2/proxy/: bar (200; 10.383521ms)
Nov 29 12:30:10.960: INFO: (15) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname1/proxy/: foo (200; 10.229437ms)
Nov 29 12:30:10.982: INFO: (16) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/rewriteme">test</a> (200; 21.436055ms)
Nov 29 12:30:10.982: INFO: (16) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/rewriteme">test<... (200; 21.647919ms)
Nov 29 12:30:10.982: INFO: (16) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname1/proxy/: foo (200; 21.788219ms)
Nov 29 12:30:10.982: INFO: (16) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname2/proxy/: bar (200; 21.745743ms)
Nov 29 12:30:10.983: INFO: (16) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/rewriteme">... (200; 22.194208ms)
Nov 29 12:30:10.983: INFO: (16) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname2/proxy/: tls qux (200; 22.490087ms)
Nov 29 12:30:10.983: INFO: (16) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:462/proxy/: tls qux (200; 22.858933ms)
Nov 29 12:30:10.985: INFO: (16) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:460/proxy/: tls baz (200; 24.367295ms)
Nov 29 12:30:10.985: INFO: (16) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname2/proxy/: bar (200; 24.476524ms)
Nov 29 12:30:10.985: INFO: (16) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:160/proxy/: foo (200; 24.387577ms)
Nov 29 12:30:10.985: INFO: (16) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname1/proxy/: tls baz (200; 24.517608ms)
Nov 29 12:30:10.985: INFO: (16) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/tlsrewritem... (200; 24.620453ms)
Nov 29 12:30:10.986: INFO: (16) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:162/proxy/: bar (200; 25.819269ms)
Nov 29 12:30:10.990: INFO: (16) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname1/proxy/: foo (200; 30.022059ms)
Nov 29 12:30:10.990: INFO: (16) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:160/proxy/: foo (200; 29.970576ms)
Nov 29 12:30:10.993: INFO: (16) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:162/proxy/: bar (200; 32.603566ms)
Nov 29 12:30:11.003: INFO: (17) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:160/proxy/: foo (200; 9.726941ms)
Nov 29 12:30:11.006: INFO: (17) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:460/proxy/: tls baz (200; 12.519373ms)
Nov 29 12:30:11.006: INFO: (17) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/tlsrewritem... (200; 12.411686ms)
Nov 29 12:30:11.006: INFO: (17) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:162/proxy/: bar (200; 12.888585ms)
Nov 29 12:30:11.006: INFO: (17) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:160/proxy/: foo (200; 12.010334ms)
Nov 29 12:30:11.006: INFO: (17) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/rewriteme">... (200; 12.075059ms)
Nov 29 12:30:11.009: INFO: (17) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:462/proxy/: tls qux (200; 15.305378ms)
Nov 29 12:30:11.009: INFO: (17) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/rewriteme">test<... (200; 15.016912ms)
Nov 29 12:30:11.009: INFO: (17) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname1/proxy/: tls baz (200; 15.240392ms)
Nov 29 12:30:11.009: INFO: (17) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname2/proxy/: bar (200; 15.262916ms)
Nov 29 12:30:11.009: INFO: (17) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname2/proxy/: tls qux (200; 15.170862ms)
Nov 29 12:30:11.009: INFO: (17) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:162/proxy/: bar (200; 15.428603ms)
Nov 29 12:30:11.009: INFO: (17) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/rewriteme">test</a> (200; 15.610342ms)
Nov 29 12:30:11.009: INFO: (17) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname1/proxy/: foo (200; 15.081546ms)
Nov 29 12:30:11.010: INFO: (17) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname1/proxy/: foo (200; 16.554855ms)
Nov 29 12:30:11.013: INFO: (17) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname2/proxy/: bar (200; 19.377719ms)
Nov 29 12:30:11.021: INFO: (18) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:162/proxy/: bar (200; 7.254167ms)
Nov 29 12:30:11.021: INFO: (18) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:160/proxy/: foo (200; 7.310013ms)
Nov 29 12:30:11.021: INFO: (18) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/rewriteme">test</a> (200; 7.109843ms)
Nov 29 12:30:11.021: INFO: (18) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/tlsrewritem... (200; 7.174743ms)
Nov 29 12:30:11.021: INFO: (18) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname2/proxy/: tls qux (200; 7.521463ms)
Nov 29 12:30:11.024: INFO: (18) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:162/proxy/: bar (200; 9.934635ms)
Nov 29 12:30:11.024: INFO: (18) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/rewriteme">test<... (200; 9.997861ms)
Nov 29 12:30:11.024: INFO: (18) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:160/proxy/: foo (200; 9.809802ms)
Nov 29 12:30:11.024: INFO: (18) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/rewriteme">... (200; 10.245423ms)
Nov 29 12:30:11.025: INFO: (18) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname1/proxy/: foo (200; 10.980656ms)
Nov 29 12:30:11.025: INFO: (18) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:462/proxy/: tls qux (200; 11.44735ms)
Nov 29 12:30:11.025: INFO: (18) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname1/proxy/: tls baz (200; 11.514806ms)
Nov 29 12:30:11.025: INFO: (18) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:460/proxy/: tls baz (200; 11.434762ms)
Nov 29 12:30:11.025: INFO: (18) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname2/proxy/: bar (200; 10.854305ms)
Nov 29 12:30:11.026: INFO: (18) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname2/proxy/: bar (200; 12.231654ms)
Nov 29 12:30:11.026: INFO: (18) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname1/proxy/: foo (200; 11.629744ms)
Nov 29 12:30:11.035: INFO: (19) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:160/proxy/: foo (200; 8.584932ms)
Nov 29 12:30:11.035: INFO: (19) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname2/proxy/: bar (200; 8.827828ms)
Nov 29 12:30:11.035: INFO: (19) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:1080/proxy/rewriteme">... (200; 8.658852ms)
Nov 29 12:30:11.035: INFO: (19) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:462/proxy/: tls qux (200; 8.77097ms)
Nov 29 12:30:11.035: INFO: (19) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:460/proxy/: tls baz (200; 9.066473ms)
Nov 29 12:30:11.035: INFO: (19) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:160/proxy/: foo (200; 9.102802ms)
Nov 29 12:30:11.035: INFO: (19) /api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/https:proxy-service-n74h5-96dns:443/proxy/tlsrewritem... (200; 9.244897ms)
Nov 29 12:30:11.035: INFO: (19) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns/proxy/rewriteme">test</a> (200; 9.367964ms)
Nov 29 12:30:11.035: INFO: (19) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:162/proxy/: bar (200; 9.290142ms)
Nov 29 12:30:11.035: INFO: (19) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname2/proxy/: tls qux (200; 9.281835ms)
Nov 29 12:30:11.035: INFO: (19) /api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/: <a href="/api/v1/namespaces/proxy-9088/pods/proxy-service-n74h5-96dns:1080/proxy/rewriteme">test<... (200; 9.285954ms)
Nov 29 12:30:11.035: INFO: (19) /api/v1/namespaces/proxy-9088/services/https:proxy-service-n74h5:tlsportname1/proxy/: tls baz (200; 9.347708ms)
Nov 29 12:30:11.037: INFO: (19) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname2/proxy/: bar (200; 10.552027ms)
Nov 29 12:30:11.037: INFO: (19) /api/v1/namespaces/proxy-9088/services/proxy-service-n74h5:portname1/proxy/: foo (200; 10.62558ms)
Nov 29 12:30:11.037: INFO: (19) /api/v1/namespaces/proxy-9088/services/http:proxy-service-n74h5:portname1/proxy/: foo (200; 11.344097ms)
Nov 29 12:30:11.037: INFO: (19) /api/v1/namespaces/proxy-9088/pods/http:proxy-service-n74h5-96dns:162/proxy/: bar (200; 11.454406ms)
STEP: deleting ReplicationController proxy-service-n74h5 in namespace proxy-9088, will wait for the garbage collector to delete the pods
Nov 29 12:30:11.099: INFO: Deleting ReplicationController proxy-service-n74h5 took: 6.460735ms
Nov 29 12:30:11.600: INFO: Terminating ReplicationController proxy-service-n74h5 pods took: 500.26195ms
[AfterEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:30:19.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9088" for this suite.
Nov 29 12:30:25.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:30:25.144: INFO: namespace proxy-9088 deletion completed in 6.138203134s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:30:25.145: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7743
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Nov 29 12:31:05.326: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:31:05.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1129 12:31:05.326274    4828 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-7743" for this suite.
Nov 29 12:31:11.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:31:11.468: INFO: namespace gc-7743 deletion completed in 6.138314594s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:31:11.469: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9348
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Nov 29 12:31:11.625: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-9348'
Nov 29 12:31:12.131: INFO: stderr: ""
Nov 29 12:31:12.131: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 29 12:31:12.131: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9348'
Nov 29 12:31:12.210: INFO: stderr: ""
Nov 29 12:31:12.210: INFO: stdout: "update-demo-nautilus-7bw27 update-demo-nautilus-7wms5 "
Nov 29 12:31:12.210: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-7bw27 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9348'
Nov 29 12:31:12.294: INFO: stderr: ""
Nov 29 12:31:12.294: INFO: stdout: ""
Nov 29 12:31:12.294: INFO: update-demo-nautilus-7bw27 is created but not running
Nov 29 12:31:17.295: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9348'
Nov 29 12:31:17.372: INFO: stderr: ""
Nov 29 12:31:17.372: INFO: stdout: "update-demo-nautilus-7bw27 update-demo-nautilus-7wms5 "
Nov 29 12:31:17.372: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-7bw27 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9348'
Nov 29 12:31:17.438: INFO: stderr: ""
Nov 29 12:31:17.438: INFO: stdout: "true"
Nov 29 12:31:17.438: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-7bw27 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9348'
Nov 29 12:31:17.509: INFO: stderr: ""
Nov 29 12:31:17.509: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 29 12:31:17.509: INFO: validating pod update-demo-nautilus-7bw27
Nov 29 12:31:17.597: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 29 12:31:17.597: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 29 12:31:17.597: INFO: update-demo-nautilus-7bw27 is verified up and running
Nov 29 12:31:17.598: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-7wms5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9348'
Nov 29 12:31:17.678: INFO: stderr: ""
Nov 29 12:31:17.678: INFO: stdout: "true"
Nov 29 12:31:17.678: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-7wms5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9348'
Nov 29 12:31:17.750: INFO: stderr: ""
Nov 29 12:31:17.750: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 29 12:31:17.750: INFO: validating pod update-demo-nautilus-7wms5
Nov 29 12:31:17.836: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 29 12:31:17.836: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 29 12:31:17.836: INFO: update-demo-nautilus-7wms5 is verified up and running
STEP: using delete to clean up resources
Nov 29 12:31:17.837: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-9348'
Nov 29 12:31:17.923: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 12:31:17.923: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 29 12:31:17.923: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9348'
Nov 29 12:31:18.012: INFO: stderr: "No resources found.\n"
Nov 29 12:31:18.012: INFO: stdout: ""
Nov 29 12:31:18.012: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-9348 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 29 12:31:18.087: INFO: stderr: ""
Nov 29 12:31:18.087: INFO: stdout: "update-demo-nautilus-7bw27\nupdate-demo-nautilus-7wms5\n"
Nov 29 12:31:18.587: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-9348'
Nov 29 12:31:18.669: INFO: stderr: "No resources found.\n"
Nov 29 12:31:18.669: INFO: stdout: ""
Nov 29 12:31:18.669: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-9348 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 29 12:31:18.746: INFO: stderr: ""
Nov 29 12:31:18.746: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:31:18.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9348" for this suite.
Nov 29 12:31:40.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:31:40.892: INFO: namespace kubectl-9348 deletion completed in 22.139805082s
•SSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:31:40.892: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-5208
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-1773
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-699
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:31:47.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5208" for this suite.
Nov 29 12:31:53.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:31:53.531: INFO: namespace namespaces-5208 deletion completed in 6.163715581s
STEP: Destroying namespace "nsdeletetest-1773" for this suite.
Nov 29 12:31:53.534: INFO: Namespace nsdeletetest-1773 was already deleted
STEP: Destroying namespace "nsdeletetest-699" for this suite.
Nov 29 12:31:59.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:31:59.672: INFO: namespace nsdeletetest-699 deletion completed in 6.138414016s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:31:59.673: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2691
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-7c4ad8ee-b316-445d-95f4-a4ad76343fff
Nov 29 12:31:59.829: INFO: Pod name my-hostname-basic-7c4ad8ee-b316-445d-95f4-a4ad76343fff: Found 0 pods out of 1
Nov 29 12:32:04.834: INFO: Pod name my-hostname-basic-7c4ad8ee-b316-445d-95f4-a4ad76343fff: Found 1 pods out of 1
Nov 29 12:32:04.834: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-7c4ad8ee-b316-445d-95f4-a4ad76343fff" are running
Nov 29 12:32:04.837: INFO: Pod "my-hostname-basic-7c4ad8ee-b316-445d-95f4-a4ad76343fff-phltw" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-29 12:31:59 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-29 12:32:02 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-29 12:32:02 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-29 12:31:59 +0000 UTC Reason: Message:}])
Nov 29 12:32:04.838: INFO: Trying to dial the pod
Nov 29 12:32:09.936: INFO: Controller my-hostname-basic-7c4ad8ee-b316-445d-95f4-a4ad76343fff: Got expected result from replica 1 [my-hostname-basic-7c4ad8ee-b316-445d-95f4-a4ad76343fff-phltw]: "my-hostname-basic-7c4ad8ee-b316-445d-95f4-a4ad76343fff-phltw", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:32:09.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2691" for this suite.
Nov 29 12:32:15.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:32:16.070: INFO: namespace replication-controller-2691 deletion completed in 6.12841156s
•
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:32:16.070: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-5572
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Nov 29 12:32:16.231: INFO: Pod name pod-release: Found 0 pods out of 1
Nov 29 12:32:21.236: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:32:21.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5572" for this suite.
Nov 29 12:32:27.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:32:27.433: INFO: namespace replication-controller-5572 deletion completed in 6.176497647s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:32:27.433: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3342
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-81d4caf6-123e-47be-be3e-590c630c52ad in namespace container-probe-3342
Nov 29 12:32:29.593: INFO: Started pod busybox-81d4caf6-123e-47be-be3e-590c630c52ad in namespace container-probe-3342
STEP: checking the pod's current state and verifying that restartCount is present
Nov 29 12:32:29.597: INFO: Initial restart count of pod busybox-81d4caf6-123e-47be-be3e-590c630c52ad is 0
Nov 29 12:33:17.724: INFO: Restart count of pod container-probe-3342/busybox-81d4caf6-123e-47be-be3e-590c630c52ad is now 1 (48.127623094s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:33:17.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3342" for this suite.
Nov 29 12:33:23.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:33:23.879: INFO: namespace container-probe-3342 deletion completed in 6.141422029s
•SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:33:23.880: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9000
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-f936050b-1ddc-45ac-a18b-7b0dd1b54ce8
STEP: Creating a pod to test consume configMaps
Nov 29 12:33:24.038: INFO: Waiting up to 5m0s for pod "pod-configmaps-35a041ac-a78d-4b25-95b4-e7ff68e8b046" in namespace "configmap-9000" to be "success or failure"
Nov 29 12:33:24.049: INFO: Pod "pod-configmaps-35a041ac-a78d-4b25-95b4-e7ff68e8b046": Phase="Pending", Reason="", readiness=false. Elapsed: 11.094132ms
Nov 29 12:33:26.054: INFO: Pod "pod-configmaps-35a041ac-a78d-4b25-95b4-e7ff68e8b046": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016110452s
Nov 29 12:33:28.059: INFO: Pod "pod-configmaps-35a041ac-a78d-4b25-95b4-e7ff68e8b046": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021124229s
STEP: Saw pod success
Nov 29 12:33:28.060: INFO: Pod "pod-configmaps-35a041ac-a78d-4b25-95b4-e7ff68e8b046" satisfied condition "success or failure"
Nov 29 12:33:28.063: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-configmaps-35a041ac-a78d-4b25-95b4-e7ff68e8b046 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 12:33:28.086: INFO: Waiting for pod pod-configmaps-35a041ac-a78d-4b25-95b4-e7ff68e8b046 to disappear
Nov 29 12:33:28.090: INFO: Pod pod-configmaps-35a041ac-a78d-4b25-95b4-e7ff68e8b046 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:33:28.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9000" for this suite.
Nov 29 12:33:34.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:33:34.229: INFO: namespace configmap-9000 deletion completed in 6.133538076s
•SS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:33:34.229: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8378
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Nov 29 12:33:36.918: INFO: Successfully updated pod "labelsupdate1383d919-68bb-4e9d-87cf-1c774b2e5f56"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:33:38.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8378" for this suite.
Nov 29 12:34:00.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:34:01.082: INFO: namespace downward-api-8378 deletion completed in 22.133371949s
•SSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:34:01.082: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-1119
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-1119
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 29 12:34:01.234: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Nov 29 12:34:17.302: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.0.27:8080/dial?request=hostName&protocol=http&host=100.64.1.16&port=8080&tries=1'] Namespace:pod-network-test-1119 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 12:34:17.302: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Nov 29 12:34:17.761: INFO: Waiting for endpoints: map[]
Nov 29 12:34:17.764: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.0.27:8080/dial?request=hostName&protocol=http&host=100.64.0.26&port=8080&tries=1'] Namespace:pod-network-test-1119 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 12:34:17.764: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Nov 29 12:34:18.367: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:34:18.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1119" for this suite.
Nov 29 12:34:40.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:34:40.521: INFO: namespace pod-network-test-1119 deletion completed in 22.145939454s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:34:40.521: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2193
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-10380c98-8b77-42dd-b9cd-8142ff41656d
STEP: Creating a pod to test consume configMaps
Nov 29 12:34:40.685: INFO: Waiting up to 5m0s for pod "pod-configmaps-408e7ac3-c82b-445e-ae81-4566654d6412" in namespace "configmap-2193" to be "success or failure"
Nov 29 12:34:40.689: INFO: Pod "pod-configmaps-408e7ac3-c82b-445e-ae81-4566654d6412": Phase="Pending", Reason="", readiness=false. Elapsed: 4.325635ms
Nov 29 12:34:42.694: INFO: Pod "pod-configmaps-408e7ac3-c82b-445e-ae81-4566654d6412": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00863441s
STEP: Saw pod success
Nov 29 12:34:42.694: INFO: Pod "pod-configmaps-408e7ac3-c82b-445e-ae81-4566654d6412" satisfied condition "success or failure"
Nov 29 12:34:42.697: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-configmaps-408e7ac3-c82b-445e-ae81-4566654d6412 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 12:34:42.716: INFO: Waiting for pod pod-configmaps-408e7ac3-c82b-445e-ae81-4566654d6412 to disappear
Nov 29 12:34:42.719: INFO: Pod pod-configmaps-408e7ac3-c82b-445e-ae81-4566654d6412 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:34:42.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2193" for this suite.
Nov 29 12:34:48.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:34:48.853: INFO: namespace configmap-2193 deletion completed in 6.129035521s
•S
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:34:48.853: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3946
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Nov 29 12:34:49.004: INFO: Waiting up to 5m0s for pod "downward-api-3a40d564-f014-4861-8715-4c9af24d70be" in namespace "downward-api-3946" to be "success or failure"
Nov 29 12:34:49.008: INFO: Pod "downward-api-3a40d564-f014-4861-8715-4c9af24d70be": Phase="Pending", Reason="", readiness=false. Elapsed: 3.642584ms
Nov 29 12:34:51.013: INFO: Pod "downward-api-3a40d564-f014-4861-8715-4c9af24d70be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008405646s
STEP: Saw pod success
Nov 29 12:34:51.013: INFO: Pod "downward-api-3a40d564-f014-4861-8715-4c9af24d70be" satisfied condition "success or failure"
Nov 29 12:34:51.017: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod downward-api-3a40d564-f014-4861-8715-4c9af24d70be container dapi-container: <nil>
STEP: delete the pod
Nov 29 12:34:51.038: INFO: Waiting for pod downward-api-3a40d564-f014-4861-8715-4c9af24d70be to disappear
Nov 29 12:34:51.042: INFO: Pod downward-api-3a40d564-f014-4861-8715-4c9af24d70be no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:34:51.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3946" for this suite.
Nov 29 12:34:57.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:34:57.175: INFO: namespace downward-api-3946 deletion completed in 6.127170985s
•SSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:34:57.175: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-8111
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8111
I1129 12:34:57.331569    4828 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8111, replica count: 1
I1129 12:34:58.381982    4828 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1129 12:34:59.382221    4828 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Nov 29 12:34:59.494: INFO: Created: latency-svc-2gf6g
Nov 29 12:34:59.497: INFO: Got endpoints: latency-svc-2gf6g [15.313735ms]
Nov 29 12:34:59.508: INFO: Created: latency-svc-4xgwl
Nov 29 12:34:59.511: INFO: Got endpoints: latency-svc-4xgwl [13.221334ms]
Nov 29 12:34:59.516: INFO: Created: latency-svc-2v4wl
Nov 29 12:34:59.519: INFO: Got endpoints: latency-svc-2v4wl [21.200986ms]
Nov 29 12:34:59.522: INFO: Created: latency-svc-9vn2q
Nov 29 12:34:59.525: INFO: Got endpoints: latency-svc-9vn2q [27.285157ms]
Nov 29 12:34:59.529: INFO: Created: latency-svc-jt2fq
Nov 29 12:34:59.532: INFO: Got endpoints: latency-svc-jt2fq [33.813463ms]
Nov 29 12:34:59.536: INFO: Created: latency-svc-2t9pv
Nov 29 12:34:59.542: INFO: Got endpoints: latency-svc-2t9pv [44.288884ms]
Nov 29 12:34:59.543: INFO: Created: latency-svc-w4fm4
Nov 29 12:34:59.545: INFO: Got endpoints: latency-svc-w4fm4 [46.81637ms]
Nov 29 12:34:59.550: INFO: Created: latency-svc-pts9r
Nov 29 12:34:59.552: INFO: Got endpoints: latency-svc-pts9r [54.274126ms]
Nov 29 12:34:59.557: INFO: Created: latency-svc-qmh6x
Nov 29 12:34:59.560: INFO: Got endpoints: latency-svc-qmh6x [62.620799ms]
Nov 29 12:34:59.565: INFO: Created: latency-svc-w9x4k
Nov 29 12:34:59.571: INFO: Got endpoints: latency-svc-w9x4k [73.304454ms]
Nov 29 12:34:59.571: INFO: Created: latency-svc-wbn59
Nov 29 12:34:59.573: INFO: Got endpoints: latency-svc-wbn59 [75.570972ms]
Nov 29 12:34:59.578: INFO: Created: latency-svc-2klhj
Nov 29 12:34:59.583: INFO: Created: latency-svc-svsxd
Nov 29 12:34:59.584: INFO: Got endpoints: latency-svc-2klhj [85.919136ms]
Nov 29 12:34:59.589: INFO: Got endpoints: latency-svc-svsxd [91.389537ms]
Nov 29 12:34:59.593: INFO: Created: latency-svc-sxxfv
Nov 29 12:34:59.598: INFO: Got endpoints: latency-svc-sxxfv [99.659525ms]
Nov 29 12:34:59.601: INFO: Created: latency-svc-gxkmh
Nov 29 12:34:59.604: INFO: Got endpoints: latency-svc-gxkmh [105.892007ms]
Nov 29 12:34:59.622: INFO: Created: latency-svc-j62zx
Nov 29 12:34:59.625: INFO: Got endpoints: latency-svc-j62zx [127.304918ms]
Nov 29 12:34:59.630: INFO: Created: latency-svc-d5sst
Nov 29 12:34:59.634: INFO: Got endpoints: latency-svc-d5sst [123.168457ms]
Nov 29 12:34:59.640: INFO: Created: latency-svc-pfhb7
Nov 29 12:34:59.647: INFO: Got endpoints: latency-svc-pfhb7 [128.701637ms]
Nov 29 12:34:59.647: INFO: Created: latency-svc-gc4mp
Nov 29 12:34:59.654: INFO: Created: latency-svc-tvp8j
Nov 29 12:34:59.654: INFO: Got endpoints: latency-svc-gc4mp [128.891463ms]
Nov 29 12:34:59.660: INFO: Got endpoints: latency-svc-tvp8j [128.714784ms]
Nov 29 12:34:59.661: INFO: Created: latency-svc-sgm6n
Nov 29 12:34:59.664: INFO: Got endpoints: latency-svc-sgm6n [122.462614ms]
Nov 29 12:34:59.668: INFO: Created: latency-svc-d2lmt
Nov 29 12:34:59.678: INFO: Created: latency-svc-6w6m9
Nov 29 12:34:59.678: INFO: Got endpoints: latency-svc-d2lmt [133.277102ms]
Nov 29 12:34:59.685: INFO: Got endpoints: latency-svc-6w6m9 [132.273368ms]
Nov 29 12:34:59.685: INFO: Created: latency-svc-vxmts
Nov 29 12:34:59.691: INFO: Got endpoints: latency-svc-vxmts [130.46156ms]
Nov 29 12:34:59.691: INFO: Created: latency-svc-kl2ps
Nov 29 12:34:59.697: INFO: Got endpoints: latency-svc-kl2ps [125.579662ms]
Nov 29 12:34:59.697: INFO: Created: latency-svc-ggrqs
Nov 29 12:34:59.700: INFO: Got endpoints: latency-svc-ggrqs [126.508655ms]
Nov 29 12:34:59.705: INFO: Created: latency-svc-7bkgp
Nov 29 12:34:59.707: INFO: Got endpoints: latency-svc-7bkgp [123.714064ms]
Nov 29 12:34:59.712: INFO: Created: latency-svc-bv6xn
Nov 29 12:34:59.720: INFO: Got endpoints: latency-svc-bv6xn [131.199979ms]
Nov 29 12:34:59.721: INFO: Created: latency-svc-bvvtc
Nov 29 12:34:59.728: INFO: Got endpoints: latency-svc-bvvtc [130.717791ms]
Nov 29 12:34:59.731: INFO: Created: latency-svc-wc92h
Nov 29 12:34:59.734: INFO: Got endpoints: latency-svc-wc92h [129.785989ms]
Nov 29 12:34:59.738: INFO: Created: latency-svc-6wh67
Nov 29 12:34:59.741: INFO: Got endpoints: latency-svc-6wh67 [115.490629ms]
Nov 29 12:34:59.745: INFO: Created: latency-svc-c4gmj
Nov 29 12:34:59.750: INFO: Got endpoints: latency-svc-c4gmj [115.786694ms]
Nov 29 12:34:59.752: INFO: Created: latency-svc-p68ln
Nov 29 12:34:59.755: INFO: Got endpoints: latency-svc-p68ln [14.448464ms]
Nov 29 12:34:59.759: INFO: Created: latency-svc-tt9dw
Nov 29 12:34:59.764: INFO: Created: latency-svc-qcnx2
Nov 29 12:34:59.764: INFO: Got endpoints: latency-svc-tt9dw [116.392223ms]
Nov 29 12:34:59.771: INFO: Got endpoints: latency-svc-qcnx2 [116.940162ms]
Nov 29 12:34:59.771: INFO: Created: latency-svc-m9cw2
Nov 29 12:34:59.778: INFO: Created: latency-svc-bscvc
Nov 29 12:34:59.785: INFO: Created: latency-svc-bvdmz
Nov 29 12:34:59.791: INFO: Created: latency-svc-z2bh8
Nov 29 12:34:59.800: INFO: Got endpoints: latency-svc-m9cw2 [139.388539ms]
Nov 29 12:34:59.800: INFO: Created: latency-svc-zd654
Nov 29 12:34:59.807: INFO: Created: latency-svc-8kcj7
Nov 29 12:34:59.815: INFO: Created: latency-svc-7gnwb
Nov 29 12:34:59.823: INFO: Created: latency-svc-xtngj
Nov 29 12:34:59.828: INFO: Created: latency-svc-jsp8s
Nov 29 12:34:59.838: INFO: Created: latency-svc-8rnmr
Nov 29 12:34:59.843: INFO: Created: latency-svc-5tkfq
Nov 29 12:34:59.849: INFO: Got endpoints: latency-svc-bscvc [185.030563ms]
Nov 29 12:34:59.856: INFO: Created: latency-svc-4cl9r
Nov 29 12:34:59.862: INFO: Created: latency-svc-ztgm6
Nov 29 12:34:59.871: INFO: Created: latency-svc-x955q
Nov 29 12:34:59.877: INFO: Created: latency-svc-fpm4w
Nov 29 12:34:59.884: INFO: Created: latency-svc-hxr5n
Nov 29 12:34:59.896: INFO: Created: latency-svc-rn8mv
Nov 29 12:34:59.898: INFO: Got endpoints: latency-svc-bvdmz [219.598939ms]
Nov 29 12:34:59.908: INFO: Created: latency-svc-c2hzv
Nov 29 12:34:59.948: INFO: Got endpoints: latency-svc-z2bh8 [263.557748ms]
Nov 29 12:34:59.959: INFO: Created: latency-svc-74nsq
Nov 29 12:34:59.998: INFO: Got endpoints: latency-svc-zd654 [307.234368ms]
Nov 29 12:35:00.011: INFO: Created: latency-svc-4wqm7
Nov 29 12:35:00.048: INFO: Got endpoints: latency-svc-8kcj7 [351.331438ms]
Nov 29 12:35:00.059: INFO: Created: latency-svc-6rc92
Nov 29 12:35:00.097: INFO: Got endpoints: latency-svc-7gnwb [397.665289ms]
Nov 29 12:35:00.109: INFO: Created: latency-svc-pnvlm
Nov 29 12:35:00.151: INFO: Got endpoints: latency-svc-xtngj [443.054369ms]
Nov 29 12:35:00.162: INFO: Created: latency-svc-bm9ks
Nov 29 12:35:00.198: INFO: Got endpoints: latency-svc-jsp8s [477.458515ms]
Nov 29 12:35:00.209: INFO: Created: latency-svc-b49k7
Nov 29 12:35:00.248: INFO: Got endpoints: latency-svc-8rnmr [519.746237ms]
Nov 29 12:35:00.261: INFO: Created: latency-svc-cnf8n
Nov 29 12:35:00.297: INFO: Got endpoints: latency-svc-5tkfq [563.78662ms]
Nov 29 12:35:00.308: INFO: Created: latency-svc-58lh2
Nov 29 12:35:00.348: INFO: Got endpoints: latency-svc-4cl9r [598.051004ms]
Nov 29 12:35:00.361: INFO: Created: latency-svc-hv6vg
Nov 29 12:35:00.398: INFO: Got endpoints: latency-svc-ztgm6 [642.685286ms]
Nov 29 12:35:00.408: INFO: Created: latency-svc-r8d2p
Nov 29 12:35:00.448: INFO: Got endpoints: latency-svc-x955q [684.095354ms]
Nov 29 12:35:00.458: INFO: Created: latency-svc-2knjw
Nov 29 12:35:00.498: INFO: Got endpoints: latency-svc-fpm4w [727.10058ms]
Nov 29 12:35:00.508: INFO: Created: latency-svc-rkp86
Nov 29 12:35:00.549: INFO: Got endpoints: latency-svc-hxr5n [749.070386ms]
Nov 29 12:35:00.559: INFO: Created: latency-svc-8c5kq
Nov 29 12:35:00.598: INFO: Got endpoints: latency-svc-rn8mv [747.99519ms]
Nov 29 12:35:00.609: INFO: Created: latency-svc-hd7cg
Nov 29 12:35:00.648: INFO: Got endpoints: latency-svc-c2hzv [750.330256ms]
Nov 29 12:35:00.659: INFO: Created: latency-svc-25482
Nov 29 12:35:00.697: INFO: Got endpoints: latency-svc-74nsq [749.113334ms]
Nov 29 12:35:00.711: INFO: Created: latency-svc-49fwv
Nov 29 12:35:00.748: INFO: Got endpoints: latency-svc-4wqm7 [750.239135ms]
Nov 29 12:35:00.759: INFO: Created: latency-svc-cpcx5
Nov 29 12:35:00.797: INFO: Got endpoints: latency-svc-6rc92 [749.299949ms]
Nov 29 12:35:00.807: INFO: Created: latency-svc-fq8f7
Nov 29 12:35:00.848: INFO: Got endpoints: latency-svc-pnvlm [750.45223ms]
Nov 29 12:35:00.858: INFO: Created: latency-svc-2xw84
Nov 29 12:35:00.898: INFO: Got endpoints: latency-svc-bm9ks [747.242478ms]
Nov 29 12:35:00.910: INFO: Created: latency-svc-59zg4
Nov 29 12:35:00.948: INFO: Got endpoints: latency-svc-b49k7 [749.895879ms]
Nov 29 12:35:00.959: INFO: Created: latency-svc-8hb4z
Nov 29 12:35:00.998: INFO: Got endpoints: latency-svc-cnf8n [749.924206ms]
Nov 29 12:35:01.009: INFO: Created: latency-svc-5hn6j
Nov 29 12:35:01.051: INFO: Got endpoints: latency-svc-58lh2 [753.800094ms]
Nov 29 12:35:01.065: INFO: Created: latency-svc-4ltzv
Nov 29 12:35:01.098: INFO: Got endpoints: latency-svc-hv6vg [750.145419ms]
Nov 29 12:35:01.109: INFO: Created: latency-svc-mdvw6
Nov 29 12:35:01.147: INFO: Got endpoints: latency-svc-r8d2p [749.271168ms]
Nov 29 12:35:01.157: INFO: Created: latency-svc-q7htd
Nov 29 12:35:01.197: INFO: Got endpoints: latency-svc-2knjw [749.178077ms]
Nov 29 12:35:01.207: INFO: Created: latency-svc-bs8gg
Nov 29 12:35:01.248: INFO: Got endpoints: latency-svc-rkp86 [749.43566ms]
Nov 29 12:35:01.258: INFO: Created: latency-svc-cw7q5
Nov 29 12:35:01.298: INFO: Got endpoints: latency-svc-8c5kq [748.634413ms]
Nov 29 12:35:01.309: INFO: Created: latency-svc-md8j7
Nov 29 12:35:01.348: INFO: Got endpoints: latency-svc-hd7cg [750.61868ms]
Nov 29 12:35:01.358: INFO: Created: latency-svc-r67h5
Nov 29 12:35:01.401: INFO: Got endpoints: latency-svc-25482 [753.145066ms]
Nov 29 12:35:01.413: INFO: Created: latency-svc-7269p
Nov 29 12:35:01.448: INFO: Got endpoints: latency-svc-49fwv [750.856482ms]
Nov 29 12:35:01.460: INFO: Created: latency-svc-8sgmx
Nov 29 12:35:01.498: INFO: Got endpoints: latency-svc-cpcx5 [749.344477ms]
Nov 29 12:35:01.511: INFO: Created: latency-svc-lfvbg
Nov 29 12:35:01.547: INFO: Got endpoints: latency-svc-fq8f7 [750.023903ms]
Nov 29 12:35:01.558: INFO: Created: latency-svc-772dk
Nov 29 12:35:01.598: INFO: Got endpoints: latency-svc-2xw84 [749.810593ms]
Nov 29 12:35:01.610: INFO: Created: latency-svc-cvx92
Nov 29 12:35:01.648: INFO: Got endpoints: latency-svc-59zg4 [749.996027ms]
Nov 29 12:35:01.660: INFO: Created: latency-svc-bv5m8
Nov 29 12:35:01.698: INFO: Got endpoints: latency-svc-8hb4z [750.176699ms]
Nov 29 12:35:01.709: INFO: Created: latency-svc-zgcjw
Nov 29 12:35:01.748: INFO: Got endpoints: latency-svc-5hn6j [749.765904ms]
Nov 29 12:35:01.758: INFO: Created: latency-svc-h5sl8
Nov 29 12:35:01.798: INFO: Got endpoints: latency-svc-4ltzv [746.598059ms]
Nov 29 12:35:01.808: INFO: Created: latency-svc-bhvz5
Nov 29 12:35:01.847: INFO: Got endpoints: latency-svc-mdvw6 [749.130971ms]
Nov 29 12:35:01.858: INFO: Created: latency-svc-9tcl6
Nov 29 12:35:01.897: INFO: Got endpoints: latency-svc-q7htd [750.188274ms]
Nov 29 12:35:01.909: INFO: Created: latency-svc-984tw
Nov 29 12:35:01.947: INFO: Got endpoints: latency-svc-bs8gg [750.04522ms]
Nov 29 12:35:01.957: INFO: Created: latency-svc-5kzjt
Nov 29 12:35:01.998: INFO: Got endpoints: latency-svc-cw7q5 [750.080543ms]
Nov 29 12:35:02.011: INFO: Created: latency-svc-8wxnk
Nov 29 12:35:02.048: INFO: Got endpoints: latency-svc-md8j7 [750.32591ms]
Nov 29 12:35:02.058: INFO: Created: latency-svc-7hfsf
Nov 29 12:35:02.097: INFO: Got endpoints: latency-svc-r67h5 [748.718057ms]
Nov 29 12:35:02.107: INFO: Created: latency-svc-bsw8f
Nov 29 12:35:02.147: INFO: Got endpoints: latency-svc-7269p [746.124557ms]
Nov 29 12:35:02.158: INFO: Created: latency-svc-xkb5n
Nov 29 12:35:02.198: INFO: Got endpoints: latency-svc-8sgmx [749.235924ms]
Nov 29 12:35:02.208: INFO: Created: latency-svc-snck2
Nov 29 12:35:02.248: INFO: Got endpoints: latency-svc-lfvbg [750.169211ms]
Nov 29 12:35:02.259: INFO: Created: latency-svc-bzzdc
Nov 29 12:35:02.297: INFO: Got endpoints: latency-svc-772dk [749.947369ms]
Nov 29 12:35:02.309: INFO: Created: latency-svc-slk54
Nov 29 12:35:02.347: INFO: Got endpoints: latency-svc-cvx92 [748.794351ms]
Nov 29 12:35:02.358: INFO: Created: latency-svc-7gh9p
Nov 29 12:35:02.399: INFO: Got endpoints: latency-svc-bv5m8 [750.60952ms]
Nov 29 12:35:02.410: INFO: Created: latency-svc-kkh89
Nov 29 12:35:02.449: INFO: Got endpoints: latency-svc-zgcjw [750.546384ms]
Nov 29 12:35:02.463: INFO: Created: latency-svc-mgk55
Nov 29 12:35:02.498: INFO: Got endpoints: latency-svc-h5sl8 [750.107649ms]
Nov 29 12:35:02.509: INFO: Created: latency-svc-fxdml
Nov 29 12:35:02.547: INFO: Got endpoints: latency-svc-bhvz5 [749.151861ms]
Nov 29 12:35:02.565: INFO: Created: latency-svc-6jvkq
Nov 29 12:35:02.597: INFO: Got endpoints: latency-svc-9tcl6 [749.62378ms]
Nov 29 12:35:02.608: INFO: Created: latency-svc-f8mln
Nov 29 12:35:02.648: INFO: Got endpoints: latency-svc-984tw [750.253575ms]
Nov 29 12:35:02.658: INFO: Created: latency-svc-pph6q
Nov 29 12:35:02.697: INFO: Got endpoints: latency-svc-5kzjt [749.879703ms]
Nov 29 12:35:02.708: INFO: Created: latency-svc-s4qbg
Nov 29 12:35:02.748: INFO: Got endpoints: latency-svc-8wxnk [750.289134ms]
Nov 29 12:35:02.758: INFO: Created: latency-svc-9qct5
Nov 29 12:35:02.799: INFO: Got endpoints: latency-svc-7hfsf [750.602129ms]
Nov 29 12:35:02.809: INFO: Created: latency-svc-fmx2p
Nov 29 12:35:02.848: INFO: Got endpoints: latency-svc-bsw8f [750.379307ms]
Nov 29 12:35:02.860: INFO: Created: latency-svc-dshbd
Nov 29 12:35:02.898: INFO: Got endpoints: latency-svc-xkb5n [750.140724ms]
Nov 29 12:35:02.908: INFO: Created: latency-svc-qqv7g
Nov 29 12:35:02.948: INFO: Got endpoints: latency-svc-snck2 [750.411385ms]
Nov 29 12:35:02.959: INFO: Created: latency-svc-5q9t8
Nov 29 12:35:02.998: INFO: Got endpoints: latency-svc-bzzdc [749.850855ms]
Nov 29 12:35:03.008: INFO: Created: latency-svc-d88dd
Nov 29 12:35:03.048: INFO: Got endpoints: latency-svc-slk54 [750.395033ms]
Nov 29 12:35:03.060: INFO: Created: latency-svc-l6hw6
Nov 29 12:35:03.098: INFO: Got endpoints: latency-svc-7gh9p [750.15656ms]
Nov 29 12:35:03.108: INFO: Created: latency-svc-wvzhh
Nov 29 12:35:03.148: INFO: Got endpoints: latency-svc-kkh89 [748.93632ms]
Nov 29 12:35:03.161: INFO: Created: latency-svc-p74bj
Nov 29 12:35:03.197: INFO: Got endpoints: latency-svc-mgk55 [748.550579ms]
Nov 29 12:35:03.207: INFO: Created: latency-svc-qzjpf
Nov 29 12:35:03.247: INFO: Got endpoints: latency-svc-fxdml [749.20356ms]
Nov 29 12:35:03.258: INFO: Created: latency-svc-2qscd
Nov 29 12:35:03.299: INFO: Got endpoints: latency-svc-6jvkq [751.438798ms]
Nov 29 12:35:03.310: INFO: Created: latency-svc-ldnc9
Nov 29 12:35:03.348: INFO: Got endpoints: latency-svc-f8mln [750.631611ms]
Nov 29 12:35:03.357: INFO: Created: latency-svc-qjt6n
Nov 29 12:35:03.397: INFO: Got endpoints: latency-svc-pph6q [749.765772ms]
Nov 29 12:35:03.408: INFO: Created: latency-svc-56bcw
Nov 29 12:35:03.447: INFO: Got endpoints: latency-svc-s4qbg [749.61889ms]
Nov 29 12:35:03.458: INFO: Created: latency-svc-hd9f7
Nov 29 12:35:03.500: INFO: Got endpoints: latency-svc-9qct5 [751.371476ms]
Nov 29 12:35:03.510: INFO: Created: latency-svc-vhclq
Nov 29 12:35:03.548: INFO: Got endpoints: latency-svc-fmx2p [748.954572ms]
Nov 29 12:35:03.558: INFO: Created: latency-svc-27862
Nov 29 12:35:03.602: INFO: Got endpoints: latency-svc-dshbd [754.406752ms]
Nov 29 12:35:03.615: INFO: Created: latency-svc-kxp4p
Nov 29 12:35:03.649: INFO: Got endpoints: latency-svc-qqv7g [750.845498ms]
Nov 29 12:35:03.660: INFO: Created: latency-svc-sljjt
Nov 29 12:35:03.698: INFO: Got endpoints: latency-svc-5q9t8 [749.47202ms]
Nov 29 12:35:03.711: INFO: Created: latency-svc-vqzfg
Nov 29 12:35:03.748: INFO: Got endpoints: latency-svc-d88dd [749.909536ms]
Nov 29 12:35:03.759: INFO: Created: latency-svc-zcpjw
Nov 29 12:35:03.798: INFO: Got endpoints: latency-svc-l6hw6 [749.909288ms]
Nov 29 12:35:03.809: INFO: Created: latency-svc-k56g9
Nov 29 12:35:03.848: INFO: Got endpoints: latency-svc-wvzhh [749.936204ms]
Nov 29 12:35:03.858: INFO: Created: latency-svc-7pqf4
Nov 29 12:35:03.898: INFO: Got endpoints: latency-svc-p74bj [749.878684ms]
Nov 29 12:35:03.908: INFO: Created: latency-svc-mlktv
Nov 29 12:35:03.947: INFO: Got endpoints: latency-svc-qzjpf [750.004679ms]
Nov 29 12:35:03.958: INFO: Created: latency-svc-mhmm7
Nov 29 12:35:03.997: INFO: Got endpoints: latency-svc-2qscd [749.956031ms]
Nov 29 12:35:04.009: INFO: Created: latency-svc-t7w2p
Nov 29 12:35:04.048: INFO: Got endpoints: latency-svc-ldnc9 [748.826284ms]
Nov 29 12:35:04.059: INFO: Created: latency-svc-v5fts
Nov 29 12:35:04.098: INFO: Got endpoints: latency-svc-qjt6n [750.044449ms]
Nov 29 12:35:04.108: INFO: Created: latency-svc-5cpzn
Nov 29 12:35:04.148: INFO: Got endpoints: latency-svc-56bcw [750.237486ms]
Nov 29 12:35:04.163: INFO: Created: latency-svc-tpfxw
Nov 29 12:35:04.197: INFO: Got endpoints: latency-svc-hd9f7 [750.066888ms]
Nov 29 12:35:04.207: INFO: Created: latency-svc-zjz5x
Nov 29 12:35:04.248: INFO: Got endpoints: latency-svc-vhclq [747.961454ms]
Nov 29 12:35:04.260: INFO: Created: latency-svc-zflcf
Nov 29 12:35:04.299: INFO: Got endpoints: latency-svc-27862 [750.914397ms]
Nov 29 12:35:04.310: INFO: Created: latency-svc-zk2jj
Nov 29 12:35:04.348: INFO: Got endpoints: latency-svc-kxp4p [745.657252ms]
Nov 29 12:35:04.358: INFO: Created: latency-svc-85j5v
Nov 29 12:35:04.399: INFO: Got endpoints: latency-svc-sljjt [750.322528ms]
Nov 29 12:35:04.410: INFO: Created: latency-svc-ccdz7
Nov 29 12:35:04.448: INFO: Got endpoints: latency-svc-vqzfg [750.319333ms]
Nov 29 12:35:04.460: INFO: Created: latency-svc-shh5k
Nov 29 12:35:04.498: INFO: Got endpoints: latency-svc-zcpjw [749.859072ms]
Nov 29 12:35:04.508: INFO: Created: latency-svc-m46jf
Nov 29 12:35:04.547: INFO: Got endpoints: latency-svc-k56g9 [749.376466ms]
Nov 29 12:35:04.569: INFO: Created: latency-svc-zw8cd
Nov 29 12:35:04.598: INFO: Got endpoints: latency-svc-7pqf4 [750.236854ms]
Nov 29 12:35:04.609: INFO: Created: latency-svc-bmccj
Nov 29 12:35:04.648: INFO: Got endpoints: latency-svc-mlktv [749.871046ms]
Nov 29 12:35:04.661: INFO: Created: latency-svc-vnhlr
Nov 29 12:35:04.697: INFO: Got endpoints: latency-svc-mhmm7 [749.803856ms]
Nov 29 12:35:04.708: INFO: Created: latency-svc-m8jzb
Nov 29 12:35:04.748: INFO: Got endpoints: latency-svc-t7w2p [750.602955ms]
Nov 29 12:35:04.759: INFO: Created: latency-svc-js447
Nov 29 12:35:04.798: INFO: Got endpoints: latency-svc-v5fts [750.019967ms]
Nov 29 12:35:04.810: INFO: Created: latency-svc-v7mz6
Nov 29 12:35:04.847: INFO: Got endpoints: latency-svc-5cpzn [749.626517ms]
Nov 29 12:35:04.858: INFO: Created: latency-svc-68gbr
Nov 29 12:35:04.898: INFO: Got endpoints: latency-svc-tpfxw [749.587238ms]
Nov 29 12:35:04.909: INFO: Created: latency-svc-25qzr
Nov 29 12:35:04.952: INFO: Got endpoints: latency-svc-zjz5x [754.660477ms]
Nov 29 12:35:04.968: INFO: Created: latency-svc-9zzkf
Nov 29 12:35:04.999: INFO: Got endpoints: latency-svc-zflcf [751.549141ms]
Nov 29 12:35:05.012: INFO: Created: latency-svc-6w4p7
Nov 29 12:35:05.048: INFO: Got endpoints: latency-svc-zk2jj [749.09384ms]
Nov 29 12:35:05.059: INFO: Created: latency-svc-s25gf
Nov 29 12:35:05.097: INFO: Got endpoints: latency-svc-85j5v [749.199496ms]
Nov 29 12:35:05.109: INFO: Created: latency-svc-d9688
Nov 29 12:35:05.148: INFO: Got endpoints: latency-svc-ccdz7 [748.462928ms]
Nov 29 12:35:05.157: INFO: Created: latency-svc-tmpwv
Nov 29 12:35:05.201: INFO: Got endpoints: latency-svc-shh5k [752.473499ms]
Nov 29 12:35:05.210: INFO: Created: latency-svc-vhgwq
Nov 29 12:35:05.251: INFO: Got endpoints: latency-svc-m46jf [752.646364ms]
Nov 29 12:35:05.265: INFO: Created: latency-svc-85rz9
Nov 29 12:35:05.304: INFO: Got endpoints: latency-svc-zw8cd [756.809059ms]
Nov 29 12:35:05.321: INFO: Created: latency-svc-l57fg
Nov 29 12:35:05.350: INFO: Got endpoints: latency-svc-bmccj [751.759368ms]
Nov 29 12:35:05.369: INFO: Created: latency-svc-dpm5p
Nov 29 12:35:05.398: INFO: Got endpoints: latency-svc-vnhlr [750.291859ms]
Nov 29 12:35:05.408: INFO: Created: latency-svc-c2bmd
Nov 29 12:35:05.448: INFO: Got endpoints: latency-svc-m8jzb [750.8917ms]
Nov 29 12:35:05.460: INFO: Created: latency-svc-sqz6z
Nov 29 12:35:05.500: INFO: Got endpoints: latency-svc-js447 [751.531555ms]
Nov 29 12:35:05.511: INFO: Created: latency-svc-kjszl
Nov 29 12:35:05.549: INFO: Got endpoints: latency-svc-v7mz6 [750.809495ms]
Nov 29 12:35:05.560: INFO: Created: latency-svc-h6qcj
Nov 29 12:35:05.598: INFO: Got endpoints: latency-svc-68gbr [750.29329ms]
Nov 29 12:35:05.610: INFO: Created: latency-svc-bmdnx
Nov 29 12:35:05.647: INFO: Got endpoints: latency-svc-25qzr [749.893663ms]
Nov 29 12:35:05.659: INFO: Created: latency-svc-cd2kg
Nov 29 12:35:05.698: INFO: Got endpoints: latency-svc-9zzkf [745.99296ms]
Nov 29 12:35:05.711: INFO: Created: latency-svc-xgfl9
Nov 29 12:35:05.748: INFO: Got endpoints: latency-svc-6w4p7 [748.629885ms]
Nov 29 12:35:05.759: INFO: Created: latency-svc-z6hfm
Nov 29 12:35:05.797: INFO: Got endpoints: latency-svc-s25gf [749.279983ms]
Nov 29 12:35:05.808: INFO: Created: latency-svc-zlbpv
Nov 29 12:35:05.848: INFO: Got endpoints: latency-svc-d9688 [751.077217ms]
Nov 29 12:35:05.858: INFO: Created: latency-svc-fnjrj
Nov 29 12:35:05.898: INFO: Got endpoints: latency-svc-tmpwv [750.26585ms]
Nov 29 12:35:05.908: INFO: Created: latency-svc-jf54c
Nov 29 12:35:05.947: INFO: Got endpoints: latency-svc-vhgwq [746.857931ms]
Nov 29 12:35:05.958: INFO: Created: latency-svc-gv8ml
Nov 29 12:35:05.998: INFO: Got endpoints: latency-svc-85rz9 [747.512991ms]
Nov 29 12:35:06.011: INFO: Created: latency-svc-cfkrs
Nov 29 12:35:06.048: INFO: Got endpoints: latency-svc-l57fg [743.854183ms]
Nov 29 12:35:06.059: INFO: Created: latency-svc-zk4c4
Nov 29 12:35:06.098: INFO: Got endpoints: latency-svc-dpm5p [747.883284ms]
Nov 29 12:35:06.108: INFO: Created: latency-svc-nr7lc
Nov 29 12:35:06.147: INFO: Got endpoints: latency-svc-c2bmd [749.24296ms]
Nov 29 12:35:06.158: INFO: Created: latency-svc-56t7n
Nov 29 12:35:06.197: INFO: Got endpoints: latency-svc-sqz6z [748.777697ms]
Nov 29 12:35:06.207: INFO: Created: latency-svc-r77t4
Nov 29 12:35:06.247: INFO: Got endpoints: latency-svc-kjszl [747.666233ms]
Nov 29 12:35:06.258: INFO: Created: latency-svc-stjnv
Nov 29 12:35:06.297: INFO: Got endpoints: latency-svc-h6qcj [748.380654ms]
Nov 29 12:35:06.307: INFO: Created: latency-svc-jfbpw
Nov 29 12:35:06.347: INFO: Got endpoints: latency-svc-bmdnx [749.385625ms]
Nov 29 12:35:06.356: INFO: Created: latency-svc-kh87b
Nov 29 12:35:06.398: INFO: Got endpoints: latency-svc-cd2kg [750.130277ms]
Nov 29 12:35:06.409: INFO: Created: latency-svc-rgmct
Nov 29 12:35:06.448: INFO: Got endpoints: latency-svc-xgfl9 [749.47354ms]
Nov 29 12:35:06.458: INFO: Created: latency-svc-58dh7
Nov 29 12:35:06.497: INFO: Got endpoints: latency-svc-z6hfm [749.422157ms]
Nov 29 12:35:06.508: INFO: Created: latency-svc-qlc7b
Nov 29 12:35:06.550: INFO: Got endpoints: latency-svc-zlbpv [752.588522ms]
Nov 29 12:35:06.560: INFO: Created: latency-svc-fbh88
Nov 29 12:35:06.598: INFO: Got endpoints: latency-svc-fnjrj [749.366326ms]
Nov 29 12:35:06.609: INFO: Created: latency-svc-7ls5w
Nov 29 12:35:06.647: INFO: Got endpoints: latency-svc-jf54c [749.253123ms]
Nov 29 12:35:06.659: INFO: Created: latency-svc-gh5hp
Nov 29 12:35:06.698: INFO: Got endpoints: latency-svc-gv8ml [750.085005ms]
Nov 29 12:35:06.711: INFO: Created: latency-svc-5ghwp
Nov 29 12:35:06.748: INFO: Got endpoints: latency-svc-cfkrs [749.657413ms]
Nov 29 12:35:06.760: INFO: Created: latency-svc-8d7xs
Nov 29 12:35:06.798: INFO: Got endpoints: latency-svc-zk4c4 [749.387437ms]
Nov 29 12:35:06.807: INFO: Created: latency-svc-r4ckb
Nov 29 12:35:06.848: INFO: Got endpoints: latency-svc-nr7lc [750.083551ms]
Nov 29 12:35:06.858: INFO: Created: latency-svc-m7q7t
Nov 29 12:35:06.898: INFO: Got endpoints: latency-svc-56t7n [750.517156ms]
Nov 29 12:35:06.909: INFO: Created: latency-svc-82jcf
Nov 29 12:35:06.948: INFO: Got endpoints: latency-svc-r77t4 [750.48ms]
Nov 29 12:35:06.957: INFO: Created: latency-svc-dtrqc
Nov 29 12:35:06.998: INFO: Got endpoints: latency-svc-stjnv [750.928063ms]
Nov 29 12:35:07.010: INFO: Created: latency-svc-6dwvf
Nov 29 12:35:07.048: INFO: Got endpoints: latency-svc-jfbpw [750.882369ms]
Nov 29 12:35:07.062: INFO: Created: latency-svc-tsbzd
Nov 29 12:35:07.097: INFO: Got endpoints: latency-svc-kh87b [749.549181ms]
Nov 29 12:35:07.107: INFO: Created: latency-svc-wmgjw
Nov 29 12:35:07.148: INFO: Got endpoints: latency-svc-rgmct [750.357181ms]
Nov 29 12:35:07.159: INFO: Created: latency-svc-nx95h
Nov 29 12:35:07.198: INFO: Got endpoints: latency-svc-58dh7 [750.725548ms]
Nov 29 12:35:07.208: INFO: Created: latency-svc-qh2f7
Nov 29 12:35:07.249: INFO: Got endpoints: latency-svc-qlc7b [751.216888ms]
Nov 29 12:35:07.277: INFO: Created: latency-svc-h5vfn
Nov 29 12:35:07.297: INFO: Got endpoints: latency-svc-fbh88 [747.27251ms]
Nov 29 12:35:07.308: INFO: Created: latency-svc-vsqjt
Nov 29 12:35:07.347: INFO: Got endpoints: latency-svc-7ls5w [749.405366ms]
Nov 29 12:35:07.398: INFO: Got endpoints: latency-svc-gh5hp [750.442368ms]
Nov 29 12:35:07.448: INFO: Got endpoints: latency-svc-5ghwp [749.987282ms]
Nov 29 12:35:07.497: INFO: Got endpoints: latency-svc-8d7xs [748.992207ms]
Nov 29 12:35:07.548: INFO: Got endpoints: latency-svc-r4ckb [749.994041ms]
Nov 29 12:35:07.597: INFO: Got endpoints: latency-svc-m7q7t [749.51433ms]
Nov 29 12:35:07.647: INFO: Got endpoints: latency-svc-82jcf [749.416893ms]
Nov 29 12:35:07.699: INFO: Got endpoints: latency-svc-dtrqc [750.778127ms]
Nov 29 12:35:07.748: INFO: Got endpoints: latency-svc-6dwvf [749.242673ms]
Nov 29 12:35:07.798: INFO: Got endpoints: latency-svc-tsbzd [750.051222ms]
Nov 29 12:35:07.848: INFO: Got endpoints: latency-svc-wmgjw [751.051424ms]
Nov 29 12:35:07.900: INFO: Got endpoints: latency-svc-nx95h [752.228912ms]
Nov 29 12:35:07.948: INFO: Got endpoints: latency-svc-qh2f7 [750.008398ms]
Nov 29 12:35:07.998: INFO: Got endpoints: latency-svc-h5vfn [748.875277ms]
Nov 29 12:35:08.048: INFO: Got endpoints: latency-svc-vsqjt [750.251535ms]
Nov 29 12:35:08.048: INFO: Latencies: [13.221334ms 14.448464ms 21.200986ms 27.285157ms 33.813463ms 44.288884ms 46.81637ms 54.274126ms 62.620799ms 73.304454ms 75.570972ms 85.919136ms 91.389537ms 99.659525ms 105.892007ms 115.490629ms 115.786694ms 116.392223ms 116.940162ms 122.462614ms 123.168457ms 123.714064ms 125.579662ms 126.508655ms 127.304918ms 128.701637ms 128.714784ms 128.891463ms 129.785989ms 130.46156ms 130.717791ms 131.199979ms 132.273368ms 133.277102ms 139.388539ms 185.030563ms 219.598939ms 263.557748ms 307.234368ms 351.331438ms 397.665289ms 443.054369ms 477.458515ms 519.746237ms 563.78662ms 598.051004ms 642.685286ms 684.095354ms 727.10058ms 743.854183ms 745.657252ms 745.99296ms 746.124557ms 746.598059ms 746.857931ms 747.242478ms 747.27251ms 747.512991ms 747.666233ms 747.883284ms 747.961454ms 747.99519ms 748.380654ms 748.462928ms 748.550579ms 748.629885ms 748.634413ms 748.718057ms 748.777697ms 748.794351ms 748.826284ms 748.875277ms 748.93632ms 748.954572ms 748.992207ms 749.070386ms 749.09384ms 749.113334ms 749.130971ms 749.151861ms 749.178077ms 749.199496ms 749.20356ms 749.235924ms 749.242673ms 749.24296ms 749.253123ms 749.271168ms 749.279983ms 749.299949ms 749.344477ms 749.366326ms 749.376466ms 749.385625ms 749.387437ms 749.405366ms 749.416893ms 749.422157ms 749.43566ms 749.47202ms 749.47354ms 749.51433ms 749.549181ms 749.587238ms 749.61889ms 749.62378ms 749.626517ms 749.657413ms 749.765772ms 749.765904ms 749.803856ms 749.810593ms 749.850855ms 749.859072ms 749.871046ms 749.878684ms 749.879703ms 749.893663ms 749.895879ms 749.909288ms 749.909536ms 749.924206ms 749.936204ms 749.947369ms 749.956031ms 749.987282ms 749.994041ms 749.996027ms 750.004679ms 750.008398ms 750.019967ms 750.023903ms 750.044449ms 750.04522ms 750.051222ms 750.066888ms 750.080543ms 750.083551ms 750.085005ms 750.107649ms 750.130277ms 750.140724ms 750.145419ms 750.15656ms 750.169211ms 750.176699ms 750.188274ms 750.236854ms 750.237486ms 750.239135ms 750.251535ms 750.253575ms 750.26585ms 750.289134ms 750.291859ms 750.29329ms 750.319333ms 750.322528ms 750.32591ms 750.330256ms 750.357181ms 750.379307ms 750.395033ms 750.411385ms 750.442368ms 750.45223ms 750.48ms 750.517156ms 750.546384ms 750.602129ms 750.602955ms 750.60952ms 750.61868ms 750.631611ms 750.725548ms 750.778127ms 750.809495ms 750.845498ms 750.856482ms 750.882369ms 750.8917ms 750.914397ms 750.928063ms 751.051424ms 751.077217ms 751.216888ms 751.371476ms 751.438798ms 751.531555ms 751.549141ms 751.759368ms 752.228912ms 752.473499ms 752.588522ms 752.646364ms 753.145066ms 753.800094ms 754.406752ms 754.660477ms 756.809059ms]
Nov 29 12:35:08.048: INFO: 50 %ile: 749.47354ms
Nov 29 12:35:08.048: INFO: 90 %ile: 750.8917ms
Nov 29 12:35:08.048: INFO: 99 %ile: 754.660477ms
Nov 29 12:35:08.048: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:35:08.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8111" for this suite.
Nov 29 12:35:22.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:35:22.186: INFO: namespace svc-latency-8111 deletion completed in 14.133159467s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:35:22.186: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8688
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-79582956-49d2-4cf4-a362-7d7b5a276ca2
STEP: Creating a pod to test consume configMaps
Nov 29 12:35:22.340: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a6d1f9f9-2502-4567-8ad5-8677fb8a351f" in namespace "projected-8688" to be "success or failure"
Nov 29 12:35:22.344: INFO: Pod "pod-projected-configmaps-a6d1f9f9-2502-4567-8ad5-8677fb8a351f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.664743ms
Nov 29 12:35:24.348: INFO: Pod "pod-projected-configmaps-a6d1f9f9-2502-4567-8ad5-8677fb8a351f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008241462s
STEP: Saw pod success
Nov 29 12:35:24.348: INFO: Pod "pod-projected-configmaps-a6d1f9f9-2502-4567-8ad5-8677fb8a351f" satisfied condition "success or failure"
Nov 29 12:35:24.351: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-projected-configmaps-a6d1f9f9-2502-4567-8ad5-8677fb8a351f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 12:35:24.371: INFO: Waiting for pod pod-projected-configmaps-a6d1f9f9-2502-4567-8ad5-8677fb8a351f to disappear
Nov 29 12:35:24.374: INFO: Pod pod-projected-configmaps-a6d1f9f9-2502-4567-8ad5-8677fb8a351f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:35:24.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8688" for this suite.
Nov 29 12:35:30.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:35:30.516: INFO: namespace projected-8688 deletion completed in 6.137176494s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:35:30.517: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7869
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 29 12:35:30.678: INFO: Waiting up to 5m0s for pod "downwardapi-volume-98e66a3b-2d28-4d8a-9b70-4f1c49c4b687" in namespace "downward-api-7869" to be "success or failure"
Nov 29 12:35:30.683: INFO: Pod "downwardapi-volume-98e66a3b-2d28-4d8a-9b70-4f1c49c4b687": Phase="Pending", Reason="", readiness=false. Elapsed: 5.189451ms
Nov 29 12:35:32.691: INFO: Pod "downwardapi-volume-98e66a3b-2d28-4d8a-9b70-4f1c49c4b687": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012573402s
STEP: Saw pod success
Nov 29 12:35:32.691: INFO: Pod "downwardapi-volume-98e66a3b-2d28-4d8a-9b70-4f1c49c4b687" satisfied condition "success or failure"
Nov 29 12:35:32.695: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod downwardapi-volume-98e66a3b-2d28-4d8a-9b70-4f1c49c4b687 container client-container: <nil>
STEP: delete the pod
Nov 29 12:35:32.715: INFO: Waiting for pod downwardapi-volume-98e66a3b-2d28-4d8a-9b70-4f1c49c4b687 to disappear
Nov 29 12:35:32.718: INFO: Pod downwardapi-volume-98e66a3b-2d28-4d8a-9b70-4f1c49c4b687 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:35:32.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7869" for this suite.
Nov 29 12:35:38.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:35:38.898: INFO: namespace downward-api-7869 deletion completed in 6.175519434s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:35:38.899: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5998
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Nov 29 12:35:39.046: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config api-versions'
Nov 29 12:35:39.319: INFO: stderr: ""
Nov 29 12:35:39.319: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncert.gardener.cloud/v1alpha1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:35:39.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5998" for this suite.
Nov 29 12:35:45.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:35:45.454: INFO: namespace kubectl-5998 deletion completed in 6.129288609s
•SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:35:45.454: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1241
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-j8pr
STEP: Creating a pod to test atomic-volume-subpath
Nov 29 12:35:45.611: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-j8pr" in namespace "subpath-1241" to be "success or failure"
Nov 29 12:35:45.616: INFO: Pod "pod-subpath-test-secret-j8pr": Phase="Pending", Reason="", readiness=false. Elapsed: 5.014187ms
Nov 29 12:35:47.621: INFO: Pod "pod-subpath-test-secret-j8pr": Phase="Running", Reason="", readiness=true. Elapsed: 2.009989909s
Nov 29 12:35:49.625: INFO: Pod "pod-subpath-test-secret-j8pr": Phase="Running", Reason="", readiness=true. Elapsed: 4.014047982s
Nov 29 12:35:51.630: INFO: Pod "pod-subpath-test-secret-j8pr": Phase="Running", Reason="", readiness=true. Elapsed: 6.018739458s
Nov 29 12:35:53.634: INFO: Pod "pod-subpath-test-secret-j8pr": Phase="Running", Reason="", readiness=true. Elapsed: 8.023535219s
Nov 29 12:35:55.638: INFO: Pod "pod-subpath-test-secret-j8pr": Phase="Running", Reason="", readiness=true. Elapsed: 10.027459862s
Nov 29 12:35:57.643: INFO: Pod "pod-subpath-test-secret-j8pr": Phase="Running", Reason="", readiness=true. Elapsed: 12.03222397s
Nov 29 12:35:59.648: INFO: Pod "pod-subpath-test-secret-j8pr": Phase="Running", Reason="", readiness=true. Elapsed: 14.037036516s
Nov 29 12:36:01.653: INFO: Pod "pod-subpath-test-secret-j8pr": Phase="Running", Reason="", readiness=true. Elapsed: 16.041806137s
Nov 29 12:36:03.657: INFO: Pod "pod-subpath-test-secret-j8pr": Phase="Running", Reason="", readiness=true. Elapsed: 18.046010444s
Nov 29 12:36:05.663: INFO: Pod "pod-subpath-test-secret-j8pr": Phase="Running", Reason="", readiness=true. Elapsed: 20.051622617s
Nov 29 12:36:07.667: INFO: Pod "pod-subpath-test-secret-j8pr": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.056334054s
STEP: Saw pod success
Nov 29 12:36:07.667: INFO: Pod "pod-subpath-test-secret-j8pr" satisfied condition "success or failure"
Nov 29 12:36:07.670: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-subpath-test-secret-j8pr container test-container-subpath-secret-j8pr: <nil>
STEP: delete the pod
Nov 29 12:36:07.689: INFO: Waiting for pod pod-subpath-test-secret-j8pr to disappear
Nov 29 12:36:07.692: INFO: Pod pod-subpath-test-secret-j8pr no longer exists
STEP: Deleting pod pod-subpath-test-secret-j8pr
Nov 29 12:36:07.692: INFO: Deleting pod "pod-subpath-test-secret-j8pr" in namespace "subpath-1241"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:36:07.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1241" for this suite.
Nov 29 12:36:13.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:36:13.851: INFO: namespace subpath-1241 deletion completed in 6.144510172s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:36:13.852: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-1503
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-1503
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-1503
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1503
Nov 29 12:36:14.011: INFO: Found 0 stateful pods, waiting for 1
Nov 29 12:36:24.015: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Nov 29 12:36:24.019: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 29 12:36:24.586: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov 29 12:36:24.587: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 29 12:36:24.587: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 29 12:36:24.591: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 29 12:36:34.603: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 12:36:34.603: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 12:36:34.619: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999756s
Nov 29 12:36:35.624: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995042853s
Nov 29 12:36:36.629: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.989910243s
Nov 29 12:36:37.634: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.984845668s
Nov 29 12:36:38.639: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.979925503s
Nov 29 12:36:39.644: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.974819893s
Nov 29 12:36:40.649: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.970052226s
Nov 29 12:36:41.658: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.964780984s
Nov 29 12:36:42.662: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.956606964s
Nov 29 12:36:43.668: INFO: Verifying statefulset ss doesn't scale past 1 for another 951.999619ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1503
Nov 29 12:36:44.673: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:36:45.146: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Nov 29 12:36:45.147: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 29 12:36:45.147: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov 29 12:36:45.152: INFO: Found 1 stateful pods, waiting for 3
Nov 29 12:36:55.157: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 12:36:55.157: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 12:36:55.157: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Nov 29 12:36:55.164: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 29 12:36:55.666: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov 29 12:36:55.666: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 29 12:36:55.666: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 29 12:36:55.666: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 29 12:36:56.150: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov 29 12:36:56.150: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 29 12:36:56.150: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 29 12:36:56.150: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 29 12:36:56.626: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov 29 12:36:56.626: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 29 12:36:56.626: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 29 12:36:56.626: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 12:36:56.630: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Nov 29 12:37:06.638: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 12:37:06.638: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 12:37:06.638: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 12:37:06.649: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999782s
Nov 29 12:37:07.655: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996060472s
Nov 29 12:37:08.661: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990034107s
Nov 29 12:37:09.667: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.98431661s
Nov 29 12:37:10.673: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.978719066s
Nov 29 12:37:11.678: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.972311458s
Nov 29 12:37:12.683: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.967436575s
Nov 29 12:37:13.689: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.961840465s
Nov 29 12:37:14.695: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.956120186s
Nov 29 12:37:15.700: INFO: Verifying statefulset ss doesn't scale past 3 for another 950.667516ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1503
Nov 29 12:37:16.705: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:37:17.199: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Nov 29 12:37:17.199: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 29 12:37:17.199: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov 29 12:37:17.199: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:37:17.692: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Nov 29 12:37:17.692: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 29 12:37:17.692: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov 29 12:37:17.692: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:37:18.094: INFO: rc: 1
Nov 29 12:37:18.094: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: Internal error occurred: error executing command in container: container not running (81c04bcc798d9426c96dca4466dcd1050ffa71c20bee3952ede09d0b1e96f21b)
 [] <nil> 0xc001778a20 exit status 1 <nil> <nil> true [0xc00233e1c0 0xc00233e220 0xc00233e350] [0xc00233e1c0 0xc00233e220 0xc00233e350] [0xc00233e208 0xc00233e2e0] [0xba6c10 0xba6c10] 0xc001f4ef00 <nil>}:
Command stdout:

stderr:
error: Internal error occurred: error executing command in container: container not running (81c04bcc798d9426c96dca4466dcd1050ffa71c20bee3952ede09d0b1e96f21b)

error:
exit status 1
Nov 29 12:37:28.094: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:37:28.295: INFO: rc: 1
Nov 29 12:37:28.295: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0011ac510 exit status 1 <nil> <nil> true [0xc00289a320 0xc00289a3b0 0xc00289a3d0] [0xc00289a320 0xc00289a3b0 0xc00289a3d0] [0xc00289a390 0xc00289a3c0] [0xba6c10 0xba6c10] 0xc002e752c0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Nov 29 12:37:38.296: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:37:38.403: INFO: rc: 1
Nov 29 12:37:38.403: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001779080 exit status 1 <nil> <nil> true [0xc00233e380 0xc00233e420 0xc00233e458] [0xc00233e380 0xc00233e420 0xc00233e458] [0xc00233e408 0xc00233e438] [0xba6c10 0xba6c10] 0xc001f4f200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:37:48.404: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:37:48.482: INFO: rc: 1
Nov 29 12:37:48.482: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0011acb40 exit status 1 <nil> <nil> true [0xc00289a3e0 0xc00289a408 0xc00289a438] [0xc00289a3e0 0xc00289a408 0xc00289a438] [0xc00289a400 0xc00289a418] [0xba6c10 0xba6c10] 0xc002e755c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:37:58.483: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:37:58.557: INFO: rc: 1
Nov 29 12:37:58.557: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0011ad200 exit status 1 <nil> <nil> true [0xc00289a468 0xc00289a4c8 0xc00289a528] [0xc00289a468 0xc00289a4c8 0xc00289a528] [0xc00289a4b0 0xc00289a500] [0xba6c10 0xba6c10] 0xc002e758c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:38:08.557: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:38:08.684: INFO: rc: 1
Nov 29 12:38:08.684: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0011ad860 exit status 1 <nil> <nil> true [0xc00289a560 0xc00289a5c0 0xc00289a5f8] [0xc00289a560 0xc00289a5c0 0xc00289a5f8] [0xc00289a5a0 0xc00289a5e8] [0xba6c10 0xba6c10] 0xc002e75bc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:38:18.685: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:38:18.896: INFO: rc: 1
Nov 29 12:38:18.896: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0011adf20 exit status 1 <nil> <nil> true [0xc00289a628 0xc00289a688 0xc00289a6c0] [0xc00289a628 0xc00289a688 0xc00289a6c0] [0xc00289a680 0xc00289a6a0] [0xba6c10 0xba6c10] 0xc002e75ec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:38:28.896: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:38:28.972: INFO: rc: 1
Nov 29 12:38:28.972: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002710e40 exit status 1 <nil> <nil> true [0xc0000116a8 0xc000011fe0 0xc0013ec0c8] [0xc0000116a8 0xc000011fe0 0xc0013ec0c8] [0xc000011a88 0xc0013ec070] [0xba6c10 0xba6c10] 0xc001822420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:38:38.973: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:38:39.094: INFO: rc: 1
Nov 29 12:38:39.094: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c18ff0 exit status 1 <nil> <nil> true [0xc0002134b8 0xc000213748 0xc000213ab0] [0xc0002134b8 0xc000213748 0xc000213ab0] [0xc0002136f0 0xc000213938] [0xba6c10 0xba6c10] 0xc00273e6c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:38:49.094: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:38:49.172: INFO: rc: 1
Nov 29 12:38:49.173: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002711470 exit status 1 <nil> <nil> true [0xc0013ec120 0xc0013ec218 0xc0013ec548] [0xc0013ec120 0xc0013ec218 0xc0013ec548] [0xc0013ec150 0xc0013ec3f8] [0xba6c10 0xba6c10] 0xc001822720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:38:59.173: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:38:59.253: INFO: rc: 1
Nov 29 12:38:59.254: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002711a70 exit status 1 <nil> <nil> true [0xc0013ec6a8 0xc0013ec948 0xc0013ecbd8] [0xc0013ec6a8 0xc0013ec948 0xc0013ecbd8] [0xc0013ec7f8 0xc0013ecaf8] [0xba6c10 0xba6c10] 0xc001822ba0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:39:09.254: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:39:09.331: INFO: rc: 1
Nov 29 12:39:09.331: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b365d0 exit status 1 <nil> <nil> true [0xc0003a22f0 0xc0003a27a0 0xc0003a2f58] [0xc0003a22f0 0xc0003a27a0 0xc0003a2f58] [0xc0003a2680 0xc0003a28e0] [0xba6c10 0xba6c10] 0xc001f4e240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:39:19.340: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:39:19.448: INFO: rc: 1
Nov 29 12:39:19.450: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b36c30 exit status 1 <nil> <nil> true [0xc0003a3540 0xc0003a3b10 0xc0003a3bc8] [0xc0003a3540 0xc0003a3b10 0xc0003a3bc8] [0xc0003a3820 0xc0003a3bb0] [0xba6c10 0xba6c10] 0xc001f4e660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:39:29.453: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:39:29.535: INFO: rc: 1
Nov 29 12:39:29.535: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002af2630 exit status 1 <nil> <nil> true [0xc000010990 0xc000010ef0 0xc0000116a8] [0xc000010990 0xc000010ef0 0xc0000116a8] [0xc000010e68 0xc0000112e8] [0xba6c10 0xba6c10] 0xc00273e240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:39:39.536: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:39:39.616: INFO: rc: 1
Nov 29 12:39:39.616: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b37260 exit status 1 <nil> <nil> true [0xc0003a3ec8 0xc00233e0c8 0xc00233e158] [0xc0003a3ec8 0xc00233e0c8 0xc00233e158] [0xc00233e0b0 0xc00233e100] [0xba6c10 0xba6c10] 0xc001f4eae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:39:49.616: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:39:49.895: INFO: rc: 1
Nov 29 12:39:49.895: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002bfa630 exit status 1 <nil> <nil> true [0xc000213128 0xc000213210 0xc000213370] [0xc000213128 0xc000213210 0xc000213370] [0xc0002131e0 0xc000213338] [0xba6c10 0xba6c10] 0xc001822240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:39:59.895: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:39:59.975: INFO: rc: 1
Nov 29 12:39:59.976: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002af2c60 exit status 1 <nil> <nil> true [0xc000011848 0xc0013ec020 0xc0013ec120] [0xc000011848 0xc0013ec020 0xc0013ec120] [0xc000011fe0 0xc0013ec0c8] [0xba6c10 0xba6c10] 0xc00273e6c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:40:09.976: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:40:10.056: INFO: rc: 1
Nov 29 12:40:10.056: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002bfac30 exit status 1 <nil> <nil> true [0xc000213398 0xc0002134b8 0xc000213748] [0xc000213398 0xc0002134b8 0xc000213748] [0xc000213430 0xc0002136f0] [0xba6c10 0xba6c10] 0xc001822540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:40:20.056: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:40:20.202: INFO: rc: 1
Nov 29 12:40:20.202: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002bfb260 exit status 1 <nil> <nil> true [0xc000213820 0xc000213af8 0xc000213d38] [0xc000213820 0xc000213af8 0xc000213d38] [0xc000213ab0 0xc000213ce0] [0xba6c10 0xba6c10] 0xc001822cc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:40:30.202: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:40:30.625: INFO: rc: 1
Nov 29 12:40:30.625: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001778630 exit status 1 <nil> <nil> true [0xc00289a000 0xc00289a030 0xc00289a080] [0xc00289a000 0xc00289a030 0xc00289a080] [0xc00289a020 0xc00289a060] [0xba6c10 0xba6c10] 0xc002e74240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:40:40.626: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:40:40.703: INFO: rc: 1
Nov 29 12:40:40.703: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002af32c0 exit status 1 <nil> <nil> true [0xc0013ec130 0xc0013ec350 0xc0013ecc08] [0xc0013ec130 0xc0013ec350 0xc0013ecc08] [0xc0013ec218 0xc0013ec548] [0xba6c10 0xba6c10] 0xc00273e9c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:40:50.703: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:40:50.776: INFO: rc: 1
Nov 29 12:40:50.776: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002af38c0 exit status 1 <nil> <nil> true [0xc0013ecc38 0xc0013ecef8 0xc0013ed0d8] [0xc0013ecc38 0xc0013ecef8 0xc0013ed0d8] [0xc0013ece90 0xc0013ed098] [0xba6c10 0xba6c10] 0xc00273ecc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:41:00.777: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:41:01.051: INFO: rc: 1
Nov 29 12:41:01.051: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002af2600 exit status 1 <nil> <nil> true [0xc000010b18 0xc000011030 0xc000011848] [0xc000010b18 0xc000011030 0xc000011848] [0xc000010ef0 0xc0000116a8] [0xba6c10 0xba6c10] 0xc00273e240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:41:11.051: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:41:11.128: INFO: rc: 1
Nov 29 12:41:11.128: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b36600 exit status 1 <nil> <nil> true [0xc0013ec020 0xc0013ec120 0xc0013ec218] [0xc0013ec020 0xc0013ec120 0xc0013ec218] [0xc0013ec0c8 0xc0013ec150] [0xba6c10 0xba6c10] 0xc001f4e240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:41:21.128: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:41:21.202: INFO: rc: 1
Nov 29 12:41:21.203: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b36c60 exit status 1 <nil> <nil> true [0xc0013ec350 0xc0013ec650 0xc0013ec7f8] [0xc0013ec350 0xc0013ec650 0xc0013ec7f8] [0xc0013ec548 0xc0013ec758] [0xba6c10 0xba6c10] 0xc001f4e660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:41:31.203: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:41:31.320: INFO: rc: 1
Nov 29 12:41:31.320: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b37710 exit status 1 <nil> <nil> true [0xc0013ec948 0xc0013ecbd8 0xc0013ecd48] [0xc0013ec948 0xc0013ecbd8 0xc0013ecd48] [0xc0013ecaf8 0xc0013ecc38] [0xba6c10 0xba6c10] 0xc001f4eae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:41:41.321: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:41:41.401: INFO: rc: 1
Nov 29 12:41:41.401: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002af2cc0 exit status 1 <nil> <nil> true [0xc000011a88 0xc0003a2460 0xc0003a2830] [0xc000011a88 0xc0003a2460 0xc0003a2830] [0xc0003a22f0 0xc0003a27a0] [0xba6c10 0xba6c10] 0xc00273e6c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:41:51.402: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:41:51.476: INFO: rc: 1
Nov 29 12:41:51.476: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002bfa5d0 exit status 1 <nil> <nil> true [0xc00233e098 0xc00233e0e8 0xc00233e178] [0xc00233e098 0xc00233e0e8 0xc00233e178] [0xc00233e0c8 0xc00233e158] [0xba6c10 0xba6c10] 0xc002e74240 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:42:01.476: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:42:01.607: INFO: rc: 1
Nov 29 12:42:01.607: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002bfac60 exit status 1 <nil> <nil> true [0xc00233e188 0xc00233e1e8 0xc00233e290] [0xc00233e188 0xc00233e1e8 0xc00233e290] [0xc00233e1c0 0xc00233e220] [0xba6c10 0xba6c10] 0xc002e746c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:42:11.607: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:42:11.684: INFO: rc: 1
Nov 29 12:42:11.685: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc001b37d40 exit status 1 <nil> <nil> true [0xc0013ece90 0xc0013ed098 0xc0013ed130] [0xc0013ece90 0xc0013ed098 0xc0013ed130] [0xc0013ed040 0xc0013ed110] [0xba6c10 0xba6c10] 0xc001f4ede0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 12:42:21.685: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-1503 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 12:42:21.761: INFO: rc: 1
Nov 29 12:42:21.761: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Nov 29 12:42:21.761: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Nov 29 12:42:21.788: INFO: Deleting all statefulset in ns statefulset-1503
Nov 29 12:42:21.792: INFO: Scaling statefulset ss to 0
Nov 29 12:42:21.802: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 12:42:21.805: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:42:21.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1503" for this suite.
Nov 29 12:42:27.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:42:27.953: INFO: namespace statefulset-1503 deletion completed in 6.130729705s

• [SLOW TEST:374.102 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:42:27.954: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-6200
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 29 12:42:28.125: INFO: Waiting up to 5m0s for pod "pod-18c57c0a-59a4-4df2-8c0d-e426db7b968b" in namespace "emptydir-6200" to be "success or failure"
Nov 29 12:42:28.130: INFO: Pod "pod-18c57c0a-59a4-4df2-8c0d-e426db7b968b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.001723ms
Nov 29 12:42:30.134: INFO: Pod "pod-18c57c0a-59a4-4df2-8c0d-e426db7b968b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009222907s
STEP: Saw pod success
Nov 29 12:42:30.134: INFO: Pod "pod-18c57c0a-59a4-4df2-8c0d-e426db7b968b" satisfied condition "success or failure"
Nov 29 12:42:30.138: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-18c57c0a-59a4-4df2-8c0d-e426db7b968b container test-container: <nil>
STEP: delete the pod
Nov 29 12:42:30.159: INFO: Waiting for pod pod-18c57c0a-59a4-4df2-8c0d-e426db7b968b to disappear
Nov 29 12:42:30.162: INFO: Pod pod-18c57c0a-59a4-4df2-8c0d-e426db7b968b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:42:30.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6200" for this suite.
Nov 29 12:42:36.179: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:42:36.295: INFO: namespace emptydir-6200 deletion completed in 6.128260658s
•SSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:42:36.295: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-4968
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4968, will wait for the garbage collector to delete the pods
Nov 29 12:42:38.507: INFO: Deleting Job.batch foo took: 6.28696ms
Nov 29 12:42:38.608: INFO: Terminating Job.batch foo pods took: 100.230295ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:43:19.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4968" for this suite.
Nov 29 12:43:25.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:43:25.147: INFO: namespace job-4968 deletion completed in 6.128303076s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:43:25.148: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7263
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Nov 29 12:43:25.302: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:43:25.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7263" for this suite.
Nov 29 12:43:31.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:43:31.529: INFO: namespace kubectl-7263 deletion completed in 6.126105205s
•SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:43:31.530: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7555
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 29 12:43:31.673: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-7555'
Nov 29 12:43:31.895: INFO: stderr: ""
Nov 29 12:43:31.895: INFO: stdout: "replicationcontroller/redis-master created\n"
Nov 29 12:43:31.895: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-7555'
Nov 29 12:43:32.061: INFO: stderr: ""
Nov 29 12:43:32.061: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Nov 29 12:43:33.066: INFO: Selector matched 1 pods for map[app:redis]
Nov 29 12:43:33.067: INFO: Found 1 / 1
Nov 29 12:43:33.067: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 29 12:43:33.070: INFO: Selector matched 1 pods for map[app:redis]
Nov 29 12:43:33.070: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 29 12:43:33.070: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe pod redis-master-rcm5j --namespace=kubectl-7555'
Nov 29 12:43:33.157: INFO: stderr: ""
Nov 29 12:43:33.157: INFO: stdout: "Name:           redis-master-rcm5j\nNamespace:      kubectl-7555\nPriority:       0\nNode:           shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj/10.250.0.10\nStart Time:     Fri, 29 Nov 2019 12:43:31 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    cni.projectcalico.org/podIP: 100.64.0.39/32\n                kubernetes.io/psp: e2e-test-privileged-psp\nStatus:         Running\nIP:             100.64.0.39\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://0e080ea6c341343f8e2036e8ab3045b562609a0e8307a4a7870a431cbdb2977c\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 29 Nov 2019 12:43:32 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-k4l8k (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-k4l8k:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-k4l8k\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                               Message\n  ----    ------     ----  ----                                                               -------\n  Normal  Scheduled  2s    default-scheduler                                                  Successfully assigned kubectl-7555/redis-master-rcm5j to shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj\n  Normal  Pulled     1s    kubelet, shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj  Created container redis-master\n  Normal  Started    1s    kubelet, shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj  Started container redis-master\n"
Nov 29 12:43:33.158: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe rc redis-master --namespace=kubectl-7555'
Nov 29 12:43:33.246: INFO: stderr: ""
Nov 29 12:43:33.246: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-7555\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-rcm5j\n"
Nov 29 12:43:33.246: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe service redis-master --namespace=kubectl-7555'
Nov 29 12:43:33.327: INFO: stderr: ""
Nov 29 12:43:33.327: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-7555\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                100.111.240.236\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         100.64.0.39:6379\nSession Affinity:  None\nEvents:            <none>\n"
Nov 29 12:43:33.332: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x'
Nov 29 12:43:33.442: INFO: stderr: ""
Nov 29 12:43:33.442: INFO: stdout: "Name:               shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=ecf69421-6238-4433-a0a0-70bb24198e09\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/zone=rot_1_1\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x\n                    kubernetes.io/os=linux\n                    node.kubernetes.io/role=node\n                    worker.garden.sapcloud.io/group=coreos-xhmxyljw\n                    worker.gardener.cloud/pool=coreos-xhmxyljw\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.250.0.11/19\n                    projectcalico.org/IPv4IPIPTunnelAddr: 100.64.1.1\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 29 Nov 2019 12:14:04 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                          Status  LastHeartbeatTime                 LastTransitionTime                Reason                          Message\n  ----                          ------  -----------------                 ------------------                ------                          -------\n  KernelDeadlock                False   Fri, 29 Nov 2019 12:42:50 +0000   Fri, 29 Nov 2019 12:14:33 +0000   KernelHasNoDeadlock             kernel has no deadlock\n  ReadonlyFilesystem            False   Fri, 29 Nov 2019 12:42:50 +0000   Fri, 29 Nov 2019 12:14:33 +0000   FilesystemIsNotReadOnly         Filesystem is not read-only\n  CorruptDockerOverlay2         False   Fri, 29 Nov 2019 12:42:50 +0000   Fri, 29 Nov 2019 12:14:33 +0000   NoCorruptDockerOverlay2         docker overlay2 is functioning properly\n  FrequentUnregisterNetDevice   False   Fri, 29 Nov 2019 12:42:50 +0000   Fri, 29 Nov 2019 12:14:33 +0000   NoFrequentUnregisterNetDevice   node is functioning properly\n  FrequentKubeletRestart        False   Fri, 29 Nov 2019 12:42:50 +0000   Fri, 29 Nov 2019 12:14:33 +0000   NoFrequentKubeletRestart        kubelet is functioning properly\n  FrequentDockerRestart         False   Fri, 29 Nov 2019 12:42:50 +0000   Fri, 29 Nov 2019 12:14:33 +0000   NoFrequentDockerRestart         docker is functioning properly\n  FrequentContainerdRestart     False   Fri, 29 Nov 2019 12:42:50 +0000   Fri, 29 Nov 2019 12:14:33 +0000   NoFrequentContainerdRestart     containerd is functioning properly\n  NetworkUnavailable            False   Fri, 29 Nov 2019 12:14:21 +0000   Fri, 29 Nov 2019 12:14:21 +0000   CalicoIsUp                      Calico is running on this node\n  MemoryPressure                False   Fri, 29 Nov 2019 12:43:26 +0000   Fri, 29 Nov 2019 12:14:04 +0000   KubeletHasSufficientMemory      kubelet has sufficient memory available\n  DiskPressure                  False   Fri, 29 Nov 2019 12:43:26 +0000   Fri, 29 Nov 2019 12:14:04 +0000   KubeletHasNoDiskPressure        kubelet has no disk pressure\n  PIDPressure                   False   Fri, 29 Nov 2019 12:43:26 +0000   Fri, 29 Nov 2019 12:14:04 +0000   KubeletHasSufficientPID         kubelet has sufficient PID available\n  Ready                         True    Fri, 29 Nov 2019 12:43:26 +0000   Fri, 29 Nov 2019 12:14:14 +0000   KubeletReady                    kubelet is posting ready status\nAddresses:\n  InternalIP:  10.250.0.11\nCapacity:\n attachable-volumes-cinder:  256\n cpu:                        2\n ephemeral-storage:          38216108Ki\n hugepages-1Gi:              0\n hugepages-2Mi:              0\n memory:                     4039624Ki\n pods:                       110\nAllocatable:\n attachable-volumes-cinder:  256\n cpu:                        1920m\n ephemeral-storage:          37176629834\n hugepages-1Gi:              0\n hugepages-2Mi:              0\n memory:                     2856004401\n pods:                       110\nSystem Info:\n Machine ID:                 84cfe050fc4548a2985a982484030753\n System UUID:                84cfe050-fc45-48a2-985a-982484030753\n Boot ID:                    c493a420-b44f-462c-85cf-d1d1da17331d\n Kernel Version:             4.19.56-coreos-r1\n OS Image:                   Container Linux by CoreOS 2135.6.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.3\n Kubelet Version:            v1.15.6\n Kube-Proxy Version:         v1.15.6\nPodCIDR:                     100.64.1.0/24\nProviderID:                  openstack:///84cfe050-fc45-48a2-985a-982484030753\nNon-terminated Pods:         (14 in total)\n  Namespace                  Name                                                              CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                              ------------  ----------  ---------------  -------------  ---\n  kube-system                addons-kubernetes-dashboard-5c8d9945bc-cn9cj                      50m (2%)      100m (5%)   50Mi (1%)        256Mi (9%)     31m\n  kube-system                addons-nginx-ingress-controller-8468678b64-6zjss                  100m (5%)     2 (104%)    100Mi (3%)       1Gi (37%)      31m\n  kube-system                addons-nginx-ingress-nginx-ingress-k8s-backend-88d6cff74-67jk5    0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m\n  kube-system                calico-kube-controllers-5d785bc598-x2vsh                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m\n  kube-system                calico-node-nqnzx                                                 100m (5%)     500m (26%)  100Mi (3%)       700Mi (25%)    29m\n  kube-system                calico-typha-horizontal-autoscaler-554dfbfdd7-htv8l               10m (0%)      10m (0%)    0 (0%)           0 (0%)         31m\n  kube-system                calico-typha-vertical-autoscaler-656557779f-qhsh7                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m\n  kube-system                coredns-858b686868-cd7fh                                          50m (2%)      100m (5%)   15Mi (0%)        100Mi (3%)     30m\n  kube-system                coredns-858b686868-mnrpm                                          50m (2%)      100m (5%)   15Mi (0%)        100Mi (3%)     31m\n  kube-system                kube-proxy-xhmk5                                                  20m (1%)      0 (0%)      64Mi (2%)        0 (0%)         29m\n  kube-system                metrics-server-5d767db8b7-nk95s                                   20m (1%)      80m (4%)    100Mi (3%)       400Mi (14%)    31m\n  kube-system                node-exporter-zhlwf                                               5m (0%)       25m (1%)    10Mi (0%)        100Mi (3%)     29m\n  kube-system                node-problem-detector-v46z5                                       20m (1%)      200m (10%)  20Mi (0%)        100Mi (3%)     29m\n  kube-system                vpn-shoot-78776d6d84-gttn4                                        100m (5%)     1 (52%)     100Mi (3%)       1000Mi (36%)   31m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource                   Requests     Limits\n  --------                   --------     ------\n  cpu                        525m (27%)   4115m (214%)\n  memory                     574Mi (21%)  3780Mi (138%)\n  ephemeral-storage          0 (0%)       0 (0%)\n  attachable-volumes-cinder  0            0\nEvents:\n  Type     Reason                   Age                From                                                                       Message\n  ----     ------                   ----               ----                                                                       -------\n  Normal   NodeHasSufficientMemory  29m (x8 over 29m)  kubelet, shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x          Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    29m (x8 over 29m)  kubelet, shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x          Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x status is now: NodeHasNoDiskPressure\n  Normal   Starting                 29m                kube-proxy, shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x       Starting kube-proxy.\n  Warning  DockerStart              28m (x3 over 28m)  systemd-monitor, shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x  Starting Docker Application Container Engine...\n"
Nov 29 12:43:33.442: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config describe namespace kubectl-7555'
Nov 29 12:43:33.529: INFO: stderr: ""
Nov 29 12:43:33.529: INFO: stdout: "Name:         kubectl-7555\nLabels:       e2e-framework=kubectl\n              e2e-run=8865765b-a34c-45dc-988d-6af5cf09a23c\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:43:33.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7555" for this suite.
Nov 29 12:43:55.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:43:55.671: INFO: namespace kubectl-7555 deletion completed in 22.136621406s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:43:55.671: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7122
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-75df9847-8814-48c5-a26d-c8407d7392b7
STEP: Creating a pod to test consume secrets
Nov 29 12:43:55.835: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5475f6fa-2a45-4604-9779-d71779fb44fa" in namespace "projected-7122" to be "success or failure"
Nov 29 12:43:55.841: INFO: Pod "pod-projected-secrets-5475f6fa-2a45-4604-9779-d71779fb44fa": Phase="Pending", Reason="", readiness=false. Elapsed: 6.224565ms
Nov 29 12:43:57.845: INFO: Pod "pod-projected-secrets-5475f6fa-2a45-4604-9779-d71779fb44fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010513914s
STEP: Saw pod success
Nov 29 12:43:57.845: INFO: Pod "pod-projected-secrets-5475f6fa-2a45-4604-9779-d71779fb44fa" satisfied condition "success or failure"
Nov 29 12:43:57.848: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-projected-secrets-5475f6fa-2a45-4604-9779-d71779fb44fa container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 29 12:43:57.866: INFO: Waiting for pod pod-projected-secrets-5475f6fa-2a45-4604-9779-d71779fb44fa to disappear
Nov 29 12:43:57.868: INFO: Pod pod-projected-secrets-5475f6fa-2a45-4604-9779-d71779fb44fa no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:43:57.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7122" for this suite.
Nov 29 12:44:03.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:44:04.005: INFO: namespace projected-7122 deletion completed in 6.13195478s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:44:04.005: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-3230
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Nov 29 12:44:04.148: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:44:08.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3230" for this suite.
Nov 29 12:44:30.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:44:30.537: INFO: namespace init-container-3230 deletion completed in 22.182142364s
•SSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:44:30.537: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2879
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:44:55.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2879" for this suite.
Nov 29 12:45:01.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:45:02.056: INFO: namespace container-runtime-2879 deletion completed in 6.133952238s
•SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:45:02.056: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3388
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Nov 29 12:45:32.738: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1129 12:45:32.738820    4828 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:45:32.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3388" for this suite.
Nov 29 12:45:38.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:45:38.868: INFO: namespace gc-3388 deletion completed in 6.125854755s
•SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:45:38.868: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7423
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1456
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Nov 29 12:45:39.016: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-7423'
Nov 29 12:45:39.101: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov 29 12:45:39.101: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Nov 29 12:45:39.111: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-rv2xm]
Nov 29 12:45:39.111: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-rv2xm" in namespace "kubectl-7423" to be "running and ready"
Nov 29 12:45:39.115: INFO: Pod "e2e-test-nginx-rc-rv2xm": Phase="Pending", Reason="", readiness=false. Elapsed: 3.880916ms
Nov 29 12:45:41.119: INFO: Pod "e2e-test-nginx-rc-rv2xm": Phase="Running", Reason="", readiness=true. Elapsed: 2.008644627s
Nov 29 12:45:41.119: INFO: Pod "e2e-test-nginx-rc-rv2xm" satisfied condition "running and ready"
Nov 29 12:45:41.119: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-rv2xm]
Nov 29 12:45:41.119: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs rc/e2e-test-nginx-rc --namespace=kubectl-7423'
Nov 29 12:45:41.277: INFO: stderr: ""
Nov 29 12:45:41.277: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1461
Nov 29 12:45:41.277: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete rc e2e-test-nginx-rc --namespace=kubectl-7423'
Nov 29 12:45:41.353: INFO: stderr: ""
Nov 29 12:45:41.353: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:45:41.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7423" for this suite.
Nov 29 12:45:47.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:45:47.493: INFO: namespace kubectl-7423 deletion completed in 6.135556755s
•SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:45:47.494: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7250
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 29 12:45:47.646: INFO: Waiting up to 5m0s for pod "downwardapi-volume-fd58ccac-9494-495b-ad7a-f8529fdd3bb8" in namespace "downward-api-7250" to be "success or failure"
Nov 29 12:45:47.649: INFO: Pod "downwardapi-volume-fd58ccac-9494-495b-ad7a-f8529fdd3bb8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.269345ms
Nov 29 12:45:49.654: INFO: Pod "downwardapi-volume-fd58ccac-9494-495b-ad7a-f8529fdd3bb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007835019s
STEP: Saw pod success
Nov 29 12:45:49.654: INFO: Pod "downwardapi-volume-fd58ccac-9494-495b-ad7a-f8529fdd3bb8" satisfied condition "success or failure"
Nov 29 12:45:49.657: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod downwardapi-volume-fd58ccac-9494-495b-ad7a-f8529fdd3bb8 container client-container: <nil>
STEP: delete the pod
Nov 29 12:45:49.683: INFO: Waiting for pod downwardapi-volume-fd58ccac-9494-495b-ad7a-f8529fdd3bb8 to disappear
Nov 29 12:45:49.686: INFO: Pod downwardapi-volume-fd58ccac-9494-495b-ad7a-f8529fdd3bb8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:45:49.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7250" for this suite.
Nov 29 12:45:55.702: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:45:55.825: INFO: namespace downward-api-7250 deletion completed in 6.133894378s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:45:55.825: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-65
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-vrrj
STEP: Creating a pod to test atomic-volume-subpath
Nov 29 12:45:55.990: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-vrrj" in namespace "subpath-65" to be "success or failure"
Nov 29 12:45:55.993: INFO: Pod "pod-subpath-test-projected-vrrj": Phase="Pending", Reason="", readiness=false. Elapsed: 2.891957ms
Nov 29 12:45:58.002: INFO: Pod "pod-subpath-test-projected-vrrj": Phase="Running", Reason="", readiness=true. Elapsed: 2.011566826s
Nov 29 12:46:00.007: INFO: Pod "pod-subpath-test-projected-vrrj": Phase="Running", Reason="", readiness=true. Elapsed: 4.016803202s
Nov 29 12:46:02.012: INFO: Pod "pod-subpath-test-projected-vrrj": Phase="Running", Reason="", readiness=true. Elapsed: 6.021709344s
Nov 29 12:46:04.016: INFO: Pod "pod-subpath-test-projected-vrrj": Phase="Running", Reason="", readiness=true. Elapsed: 8.026156083s
Nov 29 12:46:06.021: INFO: Pod "pod-subpath-test-projected-vrrj": Phase="Running", Reason="", readiness=true. Elapsed: 10.031081732s
Nov 29 12:46:08.026: INFO: Pod "pod-subpath-test-projected-vrrj": Phase="Running", Reason="", readiness=true. Elapsed: 12.035747862s
Nov 29 12:46:10.031: INFO: Pod "pod-subpath-test-projected-vrrj": Phase="Running", Reason="", readiness=true. Elapsed: 14.040445964s
Nov 29 12:46:12.041: INFO: Pod "pod-subpath-test-projected-vrrj": Phase="Running", Reason="", readiness=true. Elapsed: 16.050517439s
Nov 29 12:46:14.046: INFO: Pod "pod-subpath-test-projected-vrrj": Phase="Running", Reason="", readiness=true. Elapsed: 18.055673149s
Nov 29 12:46:16.050: INFO: Pod "pod-subpath-test-projected-vrrj": Phase="Running", Reason="", readiness=true. Elapsed: 20.060099684s
Nov 29 12:46:18.055: INFO: Pod "pod-subpath-test-projected-vrrj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.065206327s
STEP: Saw pod success
Nov 29 12:46:18.055: INFO: Pod "pod-subpath-test-projected-vrrj" satisfied condition "success or failure"
Nov 29 12:46:18.058: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-subpath-test-projected-vrrj container test-container-subpath-projected-vrrj: <nil>
STEP: delete the pod
Nov 29 12:46:18.080: INFO: Waiting for pod pod-subpath-test-projected-vrrj to disappear
Nov 29 12:46:18.083: INFO: Pod pod-subpath-test-projected-vrrj no longer exists
STEP: Deleting pod pod-subpath-test-projected-vrrj
Nov 29 12:46:18.083: INFO: Deleting pod "pod-subpath-test-projected-vrrj" in namespace "subpath-65"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:46:18.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-65" for this suite.
Nov 29 12:46:24.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:46:24.223: INFO: namespace subpath-65 deletion completed in 6.132559985s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:46:24.224: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2101
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-4554ab8d-a591-4715-af4d-3595e1d796b6
STEP: Creating a pod to test consume configMaps
Nov 29 12:46:24.384: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-aeadda7c-73d7-48b5-8835-81b883ab85c0" in namespace "projected-2101" to be "success or failure"
Nov 29 12:46:24.389: INFO: Pod "pod-projected-configmaps-aeadda7c-73d7-48b5-8835-81b883ab85c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.980957ms
Nov 29 12:46:26.394: INFO: Pod "pod-projected-configmaps-aeadda7c-73d7-48b5-8835-81b883ab85c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009862796s
STEP: Saw pod success
Nov 29 12:46:26.394: INFO: Pod "pod-projected-configmaps-aeadda7c-73d7-48b5-8835-81b883ab85c0" satisfied condition "success or failure"
Nov 29 12:46:26.397: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-projected-configmaps-aeadda7c-73d7-48b5-8835-81b883ab85c0 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 12:46:26.415: INFO: Waiting for pod pod-projected-configmaps-aeadda7c-73d7-48b5-8835-81b883ab85c0 to disappear
Nov 29 12:46:26.417: INFO: Pod pod-projected-configmaps-aeadda7c-73d7-48b5-8835-81b883ab85c0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:46:26.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2101" for this suite.
Nov 29 12:46:32.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:46:32.551: INFO: namespace projected-2101 deletion completed in 6.128826592s
•S
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:46:32.551: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8503
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Nov 29 12:46:32.702: INFO: Waiting up to 5m0s for pod "var-expansion-cea260d2-7c9e-450b-adbe-45d093ed0f40" in namespace "var-expansion-8503" to be "success or failure"
Nov 29 12:46:32.705: INFO: Pod "var-expansion-cea260d2-7c9e-450b-adbe-45d093ed0f40": Phase="Pending", Reason="", readiness=false. Elapsed: 3.164574ms
Nov 29 12:46:34.709: INFO: Pod "var-expansion-cea260d2-7c9e-450b-adbe-45d093ed0f40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007078101s
STEP: Saw pod success
Nov 29 12:46:34.709: INFO: Pod "var-expansion-cea260d2-7c9e-450b-adbe-45d093ed0f40" satisfied condition "success or failure"
Nov 29 12:46:34.712: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod var-expansion-cea260d2-7c9e-450b-adbe-45d093ed0f40 container dapi-container: <nil>
STEP: delete the pod
Nov 29 12:46:34.731: INFO: Waiting for pod var-expansion-cea260d2-7c9e-450b-adbe-45d093ed0f40 to disappear
Nov 29 12:46:34.734: INFO: Pod var-expansion-cea260d2-7c9e-450b-adbe-45d093ed0f40 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:46:34.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8503" for this suite.
Nov 29 12:46:40.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:46:40.868: INFO: namespace var-expansion-8503 deletion completed in 6.128349905s
•SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:46:40.868: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-8715
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Nov 29 12:46:45.050: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8715 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 12:46:45.050: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Nov 29 12:46:45.647: INFO: Exec stderr: ""
Nov 29 12:46:45.647: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8715 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 12:46:45.647: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Nov 29 12:46:46.041: INFO: Exec stderr: ""
Nov 29 12:46:46.041: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8715 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 12:46:46.041: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Nov 29 12:46:46.430: INFO: Exec stderr: ""
Nov 29 12:46:46.430: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8715 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 12:46:46.430: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Nov 29 12:46:46.827: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Nov 29 12:46:46.827: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8715 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 12:46:46.827: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Nov 29 12:46:47.252: INFO: Exec stderr: ""
Nov 29 12:46:47.252: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8715 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 12:46:47.252: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Nov 29 12:46:47.651: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Nov 29 12:46:47.651: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8715 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 12:46:47.652: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Nov 29 12:46:48.012: INFO: Exec stderr: ""
Nov 29 12:46:48.012: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8715 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 12:46:48.012: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Nov 29 12:46:48.460: INFO: Exec stderr: ""
Nov 29 12:46:48.460: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-8715 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 12:46:48.460: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Nov 29 12:46:48.891: INFO: Exec stderr: ""
Nov 29 12:46:48.891: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-8715 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 12:46:48.891: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Nov 29 12:46:49.320: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:46:49.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-8715" for this suite.
Nov 29 12:47:49.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:47:49.502: INFO: namespace e2e-kubelet-etc-hosts-8715 deletion completed in 1m0.176442289s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:47:49.502: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1749
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Nov 29 12:47:49.653: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Nov 29 12:47:49.653: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-1749'
Nov 29 12:47:50.010: INFO: stderr: ""
Nov 29 12:47:50.010: INFO: stdout: "service/redis-slave created\n"
Nov 29 12:47:50.010: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Nov 29 12:47:50.010: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-1749'
Nov 29 12:47:50.222: INFO: stderr: ""
Nov 29 12:47:50.222: INFO: stdout: "service/redis-master created\n"
Nov 29 12:47:50.222: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Nov 29 12:47:50.222: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-1749'
Nov 29 12:47:50.455: INFO: stderr: ""
Nov 29 12:47:50.455: INFO: stdout: "service/frontend created\n"
Nov 29 12:47:50.455: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Nov 29 12:47:50.455: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-1749'
Nov 29 12:47:50.606: INFO: stderr: ""
Nov 29 12:47:50.606: INFO: stdout: "deployment.apps/frontend created\n"
Nov 29 12:47:50.606: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Nov 29 12:47:50.606: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-1749'
Nov 29 12:47:50.760: INFO: stderr: ""
Nov 29 12:47:50.760: INFO: stdout: "deployment.apps/redis-master created\n"
Nov 29 12:47:50.760: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Nov 29 12:47:50.760: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-1749'
Nov 29 12:47:50.975: INFO: stderr: ""
Nov 29 12:47:50.975: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Nov 29 12:47:50.975: INFO: Waiting for all frontend pods to be Running.
Nov 29 12:48:06.026: INFO: Waiting for frontend to serve content.
Nov 29 12:48:11.096: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Nov 29 12:48:16.180: INFO: Trying to add a new entry to the guestbook.
Nov 29 12:48:16.233: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Nov 29 12:48:16.246: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-1749'
Nov 29 12:48:16.327: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 12:48:16.327: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Nov 29 12:48:16.327: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-1749'
Nov 29 12:48:16.412: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 12:48:16.412: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Nov 29 12:48:16.412: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-1749'
Nov 29 12:48:16.497: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 12:48:16.497: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 29 12:48:16.497: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-1749'
Nov 29 12:48:16.567: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 12:48:16.568: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Nov 29 12:48:16.568: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-1749'
Nov 29 12:48:16.641: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 12:48:16.641: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Nov 29 12:48:16.642: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-1749'
Nov 29 12:48:16.714: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 12:48:16.714: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:48:16.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1749" for this suite.
Nov 29 12:49:02.730: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:49:02.853: INFO: namespace kubectl-1749 deletion completed in 46.134583327s
•
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:49:02.853: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8674
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-de22a8ba-96cb-49de-b474-465978f50399
STEP: Creating a pod to test consume secrets
Nov 29 12:49:03.010: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-19c71263-330a-43d7-bbea-09ddcce411f0" in namespace "projected-8674" to be "success or failure"
Nov 29 12:49:03.013: INFO: Pod "pod-projected-secrets-19c71263-330a-43d7-bbea-09ddcce411f0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.239415ms
Nov 29 12:49:05.018: INFO: Pod "pod-projected-secrets-19c71263-330a-43d7-bbea-09ddcce411f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008087131s
STEP: Saw pod success
Nov 29 12:49:05.018: INFO: Pod "pod-projected-secrets-19c71263-330a-43d7-bbea-09ddcce411f0" satisfied condition "success or failure"
Nov 29 12:49:05.023: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-projected-secrets-19c71263-330a-43d7-bbea-09ddcce411f0 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 29 12:49:05.062: INFO: Waiting for pod pod-projected-secrets-19c71263-330a-43d7-bbea-09ddcce411f0 to disappear
Nov 29 12:49:05.065: INFO: Pod pod-projected-secrets-19c71263-330a-43d7-bbea-09ddcce411f0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:49:05.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8674" for this suite.
Nov 29 12:49:11.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:49:11.222: INFO: namespace projected-8674 deletion completed in 6.152500965s
•SSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:49:11.222: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2962
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Nov 29 12:49:13.914: INFO: Successfully updated pod "annotationupdateca04483c-a4bd-4b50-bc56-eb893d9b0d0a"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:49:17.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2962" for this suite.
Nov 29 12:49:39.962: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:49:40.078: INFO: namespace downward-api-2962 deletion completed in 22.128737588s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:49:40.079: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8373
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Nov 29 12:49:40.231: INFO: Waiting up to 5m0s for pod "downward-api-33869c5f-d929-450f-afad-af07a79c6048" in namespace "downward-api-8373" to be "success or failure"
Nov 29 12:49:40.234: INFO: Pod "downward-api-33869c5f-d929-450f-afad-af07a79c6048": Phase="Pending", Reason="", readiness=false. Elapsed: 3.377794ms
Nov 29 12:49:42.238: INFO: Pod "downward-api-33869c5f-d929-450f-afad-af07a79c6048": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007239535s
STEP: Saw pod success
Nov 29 12:49:42.238: INFO: Pod "downward-api-33869c5f-d929-450f-afad-af07a79c6048" satisfied condition "success or failure"
Nov 29 12:49:42.242: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod downward-api-33869c5f-d929-450f-afad-af07a79c6048 container dapi-container: <nil>
STEP: delete the pod
Nov 29 12:49:42.260: INFO: Waiting for pod downward-api-33869c5f-d929-450f-afad-af07a79c6048 to disappear
Nov 29 12:49:42.263: INFO: Pod downward-api-33869c5f-d929-450f-afad-af07a79c6048 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:49:42.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8373" for this suite.
Nov 29 12:49:48.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:49:48.405: INFO: namespace downward-api-8373 deletion completed in 6.136814273s
•S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:49:48.405: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-238
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 29 12:49:48.554: INFO: Waiting up to 5m0s for pod "pod-6a46de30-35b7-413d-a2b6-1d58da2cc647" in namespace "emptydir-238" to be "success or failure"
Nov 29 12:49:48.559: INFO: Pod "pod-6a46de30-35b7-413d-a2b6-1d58da2cc647": Phase="Pending", Reason="", readiness=false. Elapsed: 4.33432ms
Nov 29 12:49:50.562: INFO: Pod "pod-6a46de30-35b7-413d-a2b6-1d58da2cc647": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008002311s
Nov 29 12:49:52.567: INFO: Pod "pod-6a46de30-35b7-413d-a2b6-1d58da2cc647": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013054218s
STEP: Saw pod success
Nov 29 12:49:52.568: INFO: Pod "pod-6a46de30-35b7-413d-a2b6-1d58da2cc647" satisfied condition "success or failure"
Nov 29 12:49:52.571: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-6a46de30-35b7-413d-a2b6-1d58da2cc647 container test-container: <nil>
STEP: delete the pod
Nov 29 12:49:52.591: INFO: Waiting for pod pod-6a46de30-35b7-413d-a2b6-1d58da2cc647 to disappear
Nov 29 12:49:52.594: INFO: Pod pod-6a46de30-35b7-413d-a2b6-1d58da2cc647 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:49:52.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-238" for this suite.
Nov 29 12:49:58.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:49:58.728: INFO: namespace emptydir-238 deletion completed in 6.128936799s
•SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:49:58.728: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8389
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 29 12:49:58.880: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f625d65-fb8e-4a5d-929c-65191cd127ff" in namespace "projected-8389" to be "success or failure"
Nov 29 12:49:58.885: INFO: Pod "downwardapi-volume-3f625d65-fb8e-4a5d-929c-65191cd127ff": Phase="Pending", Reason="", readiness=false. Elapsed: 4.579313ms
Nov 29 12:50:00.891: INFO: Pod "downwardapi-volume-3f625d65-fb8e-4a5d-929c-65191cd127ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010073942s
STEP: Saw pod success
Nov 29 12:50:00.891: INFO: Pod "downwardapi-volume-3f625d65-fb8e-4a5d-929c-65191cd127ff" satisfied condition "success or failure"
Nov 29 12:50:00.895: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod downwardapi-volume-3f625d65-fb8e-4a5d-929c-65191cd127ff container client-container: <nil>
STEP: delete the pod
Nov 29 12:50:00.914: INFO: Waiting for pod downwardapi-volume-3f625d65-fb8e-4a5d-929c-65191cd127ff to disappear
Nov 29 12:50:00.917: INFO: Pod downwardapi-volume-3f625d65-fb8e-4a5d-929c-65191cd127ff no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:50:00.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8389" for this suite.
Nov 29 12:50:06.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:50:07.060: INFO: namespace projected-8389 deletion completed in 6.138076779s
•S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:50:07.060: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6454
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Nov 29 12:50:11.245: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 12:50:11.248: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 29 12:50:13.248: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 12:50:13.253: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 29 12:50:15.248: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 12:50:15.252: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 29 12:50:17.248: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 12:50:17.252: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 29 12:50:19.248: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 12:50:19.252: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 29 12:50:21.248: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 12:50:21.252: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 29 12:50:23.248: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 12:50:23.253: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 29 12:50:25.248: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 12:50:25.253: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 29 12:50:27.248: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 12:50:27.260: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 29 12:50:29.248: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 12:50:29.253: INFO: Pod pod-with-prestop-exec-hook still exists
Nov 29 12:50:31.248: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Nov 29 12:50:31.252: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:50:31.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6454" for this suite.
Nov 29 12:50:53.286: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:50:53.420: INFO: namespace container-lifecycle-hook-6454 deletion completed in 22.147903444s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:50:53.420: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-2909
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2909.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-2909.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2909.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-2909.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-2909.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-2909.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2909.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 145.172.104.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.104.172.145_udp@PTR;check="$$(dig +tcp +noall +answer +search 145.172.104.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.104.172.145_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-2909.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-2909.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-2909.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-2909.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-2909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-2909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-2909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-2909.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-2909.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-2909.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2909.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 145.172.104.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.104.172.145_udp@PTR;check="$$(dig +tcp +noall +answer +search 145.172.104.100.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/100.104.172.145_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 12:51:03.696: INFO: Unable to read wheezy_udp@dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:03.737: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:03.744: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:03.749: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:04.237: INFO: Unable to read jessie_udp@dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:04.243: INFO: Unable to read jessie_tcp@dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:04.248: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:04.252: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:04.700: INFO: Lookups using dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809 failed for: [wheezy_udp@dns-test-service.dns-2909.svc.cluster.local wheezy_tcp@dns-test-service.dns-2909.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local jessie_udp@dns-test-service.dns-2909.svc.cluster.local jessie_tcp@dns-test-service.dns-2909.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local]

Nov 29 12:51:09.707: INFO: Unable to read wheezy_udp@dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:09.712: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:09.717: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:09.722: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:10.210: INFO: Unable to read jessie_udp@dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:10.215: INFO: Unable to read jessie_tcp@dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:10.220: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:10.225: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:10.676: INFO: Lookups using dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809 failed for: [wheezy_udp@dns-test-service.dns-2909.svc.cluster.local wheezy_tcp@dns-test-service.dns-2909.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local jessie_udp@dns-test-service.dns-2909.svc.cluster.local jessie_tcp@dns-test-service.dns-2909.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local]

Nov 29 12:51:14.708: INFO: Unable to read wheezy_udp@dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:14.714: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:14.718: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:14.723: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:15.209: INFO: Unable to read jessie_udp@dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:15.216: INFO: Unable to read jessie_tcp@dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:15.221: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:15.226: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:15.672: INFO: Lookups using dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809 failed for: [wheezy_udp@dns-test-service.dns-2909.svc.cluster.local wheezy_tcp@dns-test-service.dns-2909.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local jessie_udp@dns-test-service.dns-2909.svc.cluster.local jessie_tcp@dns-test-service.dns-2909.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local]

Nov 29 12:51:19.708: INFO: Unable to read wheezy_udp@dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:19.713: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:19.718: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:19.723: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:20.214: INFO: Unable to read jessie_udp@dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:20.220: INFO: Unable to read jessie_tcp@dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:20.226: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:20.231: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:20.676: INFO: Lookups using dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809 failed for: [wheezy_udp@dns-test-service.dns-2909.svc.cluster.local wheezy_tcp@dns-test-service.dns-2909.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local jessie_udp@dns-test-service.dns-2909.svc.cluster.local jessie_tcp@dns-test-service.dns-2909.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local]

Nov 29 12:51:24.707: INFO: Unable to read wheezy_udp@dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:24.712: INFO: Unable to read wheezy_tcp@dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:24.717: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:24.722: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:25.205: INFO: Unable to read jessie_udp@dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:25.212: INFO: Unable to read jessie_tcp@dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:25.218: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:25.223: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local from pod dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809: the server could not find the requested resource (get pods dns-test-dc3939fc-16bb-4cd0-856a-853153f40809)
Nov 29 12:51:25.668: INFO: Lookups using dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809 failed for: [wheezy_udp@dns-test-service.dns-2909.svc.cluster.local wheezy_tcp@dns-test-service.dns-2909.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local jessie_udp@dns-test-service.dns-2909.svc.cluster.local jessie_tcp@dns-test-service.dns-2909.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-2909.svc.cluster.local]

Nov 29 12:51:31.288: INFO: DNS probes using dns-2909/dns-test-dc3939fc-16bb-4cd0-856a-853153f40809 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:51:31.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2909" for this suite.
Nov 29 12:51:37.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:51:37.459: INFO: namespace dns-2909 deletion completed in 6.12958304s
•SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:51:37.459: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2856
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-fa171690-9e30-4ea2-b42c-ac4216c962c3
STEP: Creating secret with name secret-projected-all-test-volume-93af079c-278a-41a9-be8e-9cc49b07df84
STEP: Creating a pod to test Check all projections for projected volume plugin
Nov 29 12:51:37.636: INFO: Waiting up to 5m0s for pod "projected-volume-66629740-5219-4918-b229-4c1b3b9a787c" in namespace "projected-2856" to be "success or failure"
Nov 29 12:51:37.643: INFO: Pod "projected-volume-66629740-5219-4918-b229-4c1b3b9a787c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.12843ms
Nov 29 12:51:39.647: INFO: Pod "projected-volume-66629740-5219-4918-b229-4c1b3b9a787c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010784927s
STEP: Saw pod success
Nov 29 12:51:39.647: INFO: Pod "projected-volume-66629740-5219-4918-b229-4c1b3b9a787c" satisfied condition "success or failure"
Nov 29 12:51:39.651: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod projected-volume-66629740-5219-4918-b229-4c1b3b9a787c container projected-all-volume-test: <nil>
STEP: delete the pod
Nov 29 12:51:39.670: INFO: Waiting for pod projected-volume-66629740-5219-4918-b229-4c1b3b9a787c to disappear
Nov 29 12:51:39.672: INFO: Pod projected-volume-66629740-5219-4918-b229-4c1b3b9a787c no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:51:39.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2856" for this suite.
Nov 29 12:51:45.690: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:51:45.803: INFO: namespace projected-2856 deletion completed in 6.125798668s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:51:45.803: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5210
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:51:45.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5210" for this suite.
Nov 29 12:51:51.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:51:52.095: INFO: namespace services-5210 deletion completed in 6.135498248s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:51:52.095: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4848
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-415f7a6e-4f91-4379-85e5-2595cba20510
STEP: Creating a pod to test consume secrets
Nov 29 12:51:52.252: INFO: Waiting up to 5m0s for pod "pod-secrets-747b9102-f2df-4790-899e-c7cd4474e0df" in namespace "secrets-4848" to be "success or failure"
Nov 29 12:51:52.255: INFO: Pod "pod-secrets-747b9102-f2df-4790-899e-c7cd4474e0df": Phase="Pending", Reason="", readiness=false. Elapsed: 3.182697ms
Nov 29 12:51:54.259: INFO: Pod "pod-secrets-747b9102-f2df-4790-899e-c7cd4474e0df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007842935s
STEP: Saw pod success
Nov 29 12:51:54.260: INFO: Pod "pod-secrets-747b9102-f2df-4790-899e-c7cd4474e0df" satisfied condition "success or failure"
Nov 29 12:51:54.262: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-secrets-747b9102-f2df-4790-899e-c7cd4474e0df container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 12:51:54.290: INFO: Waiting for pod pod-secrets-747b9102-f2df-4790-899e-c7cd4474e0df to disappear
Nov 29 12:51:54.293: INFO: Pod pod-secrets-747b9102-f2df-4790-899e-c7cd4474e0df no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:51:54.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4848" for this suite.
Nov 29 12:52:00.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:52:00.444: INFO: namespace secrets-4848 deletion completed in 6.145591902s
•SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:52:00.444: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-598
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-4s5c
STEP: Creating a pod to test atomic-volume-subpath
Nov 29 12:52:00.605: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-4s5c" in namespace "subpath-598" to be "success or failure"
Nov 29 12:52:00.609: INFO: Pod "pod-subpath-test-downwardapi-4s5c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.65725ms
Nov 29 12:52:02.614: INFO: Pod "pod-subpath-test-downwardapi-4s5c": Phase="Running", Reason="", readiness=true. Elapsed: 2.008562747s
Nov 29 12:52:04.619: INFO: Pod "pod-subpath-test-downwardapi-4s5c": Phase="Running", Reason="", readiness=true. Elapsed: 4.013171828s
Nov 29 12:52:06.623: INFO: Pod "pod-subpath-test-downwardapi-4s5c": Phase="Running", Reason="", readiness=true. Elapsed: 6.017685659s
Nov 29 12:52:08.629: INFO: Pod "pod-subpath-test-downwardapi-4s5c": Phase="Running", Reason="", readiness=true. Elapsed: 8.024087312s
Nov 29 12:52:10.635: INFO: Pod "pod-subpath-test-downwardapi-4s5c": Phase="Running", Reason="", readiness=true. Elapsed: 10.029534291s
Nov 29 12:52:12.640: INFO: Pod "pod-subpath-test-downwardapi-4s5c": Phase="Running", Reason="", readiness=true. Elapsed: 12.034549856s
Nov 29 12:52:14.645: INFO: Pod "pod-subpath-test-downwardapi-4s5c": Phase="Running", Reason="", readiness=true. Elapsed: 14.039308424s
Nov 29 12:52:16.649: INFO: Pod "pod-subpath-test-downwardapi-4s5c": Phase="Running", Reason="", readiness=true. Elapsed: 16.043854677s
Nov 29 12:52:18.654: INFO: Pod "pod-subpath-test-downwardapi-4s5c": Phase="Running", Reason="", readiness=true. Elapsed: 18.048318158s
Nov 29 12:52:20.658: INFO: Pod "pod-subpath-test-downwardapi-4s5c": Phase="Running", Reason="", readiness=true. Elapsed: 20.052722325s
Nov 29 12:52:22.663: INFO: Pod "pod-subpath-test-downwardapi-4s5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.057343401s
STEP: Saw pod success
Nov 29 12:52:22.663: INFO: Pod "pod-subpath-test-downwardapi-4s5c" satisfied condition "success or failure"
Nov 29 12:52:22.666: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-subpath-test-downwardapi-4s5c container test-container-subpath-downwardapi-4s5c: <nil>
STEP: delete the pod
Nov 29 12:52:22.693: INFO: Waiting for pod pod-subpath-test-downwardapi-4s5c to disappear
Nov 29 12:52:22.695: INFO: Pod pod-subpath-test-downwardapi-4s5c no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-4s5c
Nov 29 12:52:22.695: INFO: Deleting pod "pod-subpath-test-downwardapi-4s5c" in namespace "subpath-598"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:52:22.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-598" for this suite.
Nov 29 12:52:28.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:52:28.876: INFO: namespace subpath-598 deletion completed in 6.174702626s
•SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:52:28.877: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4822
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Nov 29 12:52:29.026: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-4822'
Nov 29 12:52:29.311: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov 29 12:52:29.311: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Nov 29 12:52:29.318: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Nov 29 12:52:29.326: INFO: scanned /root for discovery docs: <nil>
Nov 29 12:52:29.326: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-4822'
Nov 29 12:52:45.082: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Nov 29 12:52:45.082: INFO: stdout: "Created e2e-test-nginx-rc-79dc80850b2d82ae1a5495419d9b9c71\nScaling up e2e-test-nginx-rc-79dc80850b2d82ae1a5495419d9b9c71 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-79dc80850b2d82ae1a5495419d9b9c71 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-79dc80850b2d82ae1a5495419d9b9c71 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Nov 29 12:52:45.082: INFO: stdout: "Created e2e-test-nginx-rc-79dc80850b2d82ae1a5495419d9b9c71\nScaling up e2e-test-nginx-rc-79dc80850b2d82ae1a5495419d9b9c71 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-79dc80850b2d82ae1a5495419d9b9c71 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-79dc80850b2d82ae1a5495419d9b9c71 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Nov 29 12:52:45.082: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-4822'
Nov 29 12:52:45.201: INFO: stderr: ""
Nov 29 12:52:45.201: INFO: stdout: "e2e-test-nginx-rc-79dc80850b2d82ae1a5495419d9b9c71-zqbrq "
Nov 29 12:52:45.201: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods e2e-test-nginx-rc-79dc80850b2d82ae1a5495419d9b9c71-zqbrq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4822'
Nov 29 12:52:45.274: INFO: stderr: ""
Nov 29 12:52:45.274: INFO: stdout: "true"
Nov 29 12:52:45.274: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods e2e-test-nginx-rc-79dc80850b2d82ae1a5495419d9b9c71-zqbrq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4822'
Nov 29 12:52:45.347: INFO: stderr: ""
Nov 29 12:52:45.347: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Nov 29 12:52:45.347: INFO: e2e-test-nginx-rc-79dc80850b2d82ae1a5495419d9b9c71-zqbrq is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1522
Nov 29 12:52:45.347: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete rc e2e-test-nginx-rc --namespace=kubectl-4822'
Nov 29 12:52:45.422: INFO: stderr: ""
Nov 29 12:52:45.422: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:52:45.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4822" for this suite.
Nov 29 12:52:51.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:52:51.553: INFO: namespace kubectl-4822 deletion completed in 6.125800023s
•
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:52:51.553: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9403
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Nov 29 12:52:51.709: INFO: Waiting up to 5m0s for pod "downward-api-37be51f6-e33b-4c41-8bf1-a20400de7928" in namespace "downward-api-9403" to be "success or failure"
Nov 29 12:52:51.714: INFO: Pod "downward-api-37be51f6-e33b-4c41-8bf1-a20400de7928": Phase="Pending", Reason="", readiness=false. Elapsed: 5.005813ms
Nov 29 12:52:53.718: INFO: Pod "downward-api-37be51f6-e33b-4c41-8bf1-a20400de7928": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009135609s
STEP: Saw pod success
Nov 29 12:52:53.718: INFO: Pod "downward-api-37be51f6-e33b-4c41-8bf1-a20400de7928" satisfied condition "success or failure"
Nov 29 12:52:53.721: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod downward-api-37be51f6-e33b-4c41-8bf1-a20400de7928 container dapi-container: <nil>
STEP: delete the pod
Nov 29 12:52:53.739: INFO: Waiting for pod downward-api-37be51f6-e33b-4c41-8bf1-a20400de7928 to disappear
Nov 29 12:52:53.741: INFO: Pod downward-api-37be51f6-e33b-4c41-8bf1-a20400de7928 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:52:53.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9403" for this suite.
Nov 29 12:52:59.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:52:59.877: INFO: namespace downward-api-9403 deletion completed in 6.130470889s
•S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:52:59.877: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8814
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 29 12:53:00.027: INFO: Waiting up to 5m0s for pod "downwardapi-volume-652d5c14-8f41-4473-95c8-6bd02fe4abb1" in namespace "projected-8814" to be "success or failure"
Nov 29 12:53:00.031: INFO: Pod "downwardapi-volume-652d5c14-8f41-4473-95c8-6bd02fe4abb1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.498594ms
Nov 29 12:53:02.036: INFO: Pod "downwardapi-volume-652d5c14-8f41-4473-95c8-6bd02fe4abb1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008262679s
STEP: Saw pod success
Nov 29 12:53:02.036: INFO: Pod "downwardapi-volume-652d5c14-8f41-4473-95c8-6bd02fe4abb1" satisfied condition "success or failure"
Nov 29 12:53:02.039: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod downwardapi-volume-652d5c14-8f41-4473-95c8-6bd02fe4abb1 container client-container: <nil>
STEP: delete the pod
Nov 29 12:53:02.056: INFO: Waiting for pod downwardapi-volume-652d5c14-8f41-4473-95c8-6bd02fe4abb1 to disappear
Nov 29 12:53:02.059: INFO: Pod downwardapi-volume-652d5c14-8f41-4473-95c8-6bd02fe4abb1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:53:02.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8814" for this suite.
Nov 29 12:53:08.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:53:08.194: INFO: namespace projected-8814 deletion completed in 6.130169386s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:53:08.195: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2906
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 29 12:53:08.339: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config version'
Nov 29 12:53:08.410: INFO: stderr: ""
Nov 29 12:53:08.410: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.6\", GitCommit:\"7015f71e75f670eb9e7ebd4b5749639d42e20079\", GitTreeState:\"clean\", BuildDate:\"2019-11-13T11:20:18Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.6\", GitCommit:\"7015f71e75f670eb9e7ebd4b5749639d42e20079\", GitTreeState:\"clean\", BuildDate:\"2019-11-13T11:11:50Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:53:08.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2906" for this suite.
Nov 29 12:53:14.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:53:14.548: INFO: namespace kubectl-2906 deletion completed in 6.13319521s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:53:14.549: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-8573
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Nov 29 12:53:14.692: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:53:18.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8573" for this suite.
Nov 29 12:53:24.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:53:24.506: INFO: namespace init-container-8573 deletion completed in 6.143244006s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:53:24.506: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9078
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Nov 29 12:53:24.652: INFO: namespace kubectl-9078
Nov 29 12:53:24.652: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-9078'
Nov 29 12:53:24.871: INFO: stderr: ""
Nov 29 12:53:24.871: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Nov 29 12:53:25.877: INFO: Selector matched 1 pods for map[app:redis]
Nov 29 12:53:25.877: INFO: Found 0 / 1
Nov 29 12:53:26.876: INFO: Selector matched 1 pods for map[app:redis]
Nov 29 12:53:26.876: INFO: Found 1 / 1
Nov 29 12:53:26.876: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Nov 29 12:53:26.880: INFO: Selector matched 1 pods for map[app:redis]
Nov 29 12:53:26.880: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 29 12:53:26.880: INFO: wait on redis-master startup in kubectl-9078 
Nov 29 12:53:26.880: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config logs redis-master-gv8vj redis-master --namespace=kubectl-9078'
Nov 29 12:53:26.976: INFO: stderr: ""
Nov 29 12:53:26.976: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 29 Nov 12:53:25.767 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 29 Nov 12:53:25.767 # Server started, Redis version 3.2.12\n1:M 29 Nov 12:53:25.767 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 29 Nov 12:53:25.767 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Nov 29 12:53:26.976: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-9078'
Nov 29 12:53:27.071: INFO: stderr: ""
Nov 29 12:53:27.071: INFO: stdout: "service/rm2 exposed\n"
Nov 29 12:53:27.075: INFO: Service rm2 in namespace kubectl-9078 found.
STEP: exposing service
Nov 29 12:53:29.083: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-9078'
Nov 29 12:53:29.185: INFO: stderr: ""
Nov 29 12:53:29.185: INFO: stdout: "service/rm3 exposed\n"
Nov 29 12:53:29.188: INFO: Service rm3 in namespace kubectl-9078 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:53:31.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9078" for this suite.
Nov 29 12:53:53.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:53:53.330: INFO: namespace kubectl-9078 deletion completed in 22.129055589s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:53:53.330: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4829
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-11007f72-baf5-431a-9ba4-1df3b4affb74
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:53:55.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4829" for this suite.
Nov 29 12:54:17.628: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:54:17.756: INFO: namespace configmap-4829 deletion completed in 22.139353317s
•SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:54:17.757: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-9296
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Nov 29 12:54:17.903: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 29 12:54:17.913: INFO: Waiting for terminating namespaces to be deleted...
Nov 29 12:54:17.916: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x before test
Nov 29 12:54:17.931: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-88d6cff74-67jk5 from kube-system started at 2019-11-29 12:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 12:54:17.931: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Nov 29 12:54:17.931: INFO: addons-kubernetes-dashboard-5c8d9945bc-cn9cj from kube-system started at 2019-11-29 12:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 12:54:17.931: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Nov 29 12:54:17.931: INFO: calico-typha-horizontal-autoscaler-554dfbfdd7-htv8l from kube-system started at 2019-11-29 12:14:17 +0000 UTC (1 container statuses recorded)
Nov 29 12:54:17.931: INFO: 	Container autoscaler ready: true, restart count 0
Nov 29 12:54:17.931: INFO: calico-node-nqnzx from kube-system started at 2019-11-29 12:14:04 +0000 UTC (1 container statuses recorded)
Nov 29 12:54:17.931: INFO: 	Container calico-node ready: true, restart count 0
Nov 29 12:54:17.931: INFO: calico-kube-controllers-5d785bc598-x2vsh from kube-system started at 2019-11-29 12:14:17 +0000 UTC (1 container statuses recorded)
Nov 29 12:54:17.931: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 29 12:54:17.931: INFO: node-problem-detector-v46z5 from kube-system started at 2019-11-29 12:14:04 +0000 UTC (1 container statuses recorded)
Nov 29 12:54:17.931: INFO: 	Container node-problem-detector ready: true, restart count 0
Nov 29 12:54:17.931: INFO: metrics-server-5d767db8b7-nk95s from kube-system started at 2019-11-29 12:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 12:54:17.931: INFO: 	Container metrics-server ready: true, restart count 0
Nov 29 12:54:17.931: INFO: addons-nginx-ingress-controller-8468678b64-6zjss from kube-system started at 2019-11-29 12:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 12:54:17.931: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Nov 29 12:54:17.931: INFO: calico-typha-vertical-autoscaler-656557779f-qhsh7 from kube-system started at 2019-11-29 12:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 12:54:17.931: INFO: 	Container autoscaler ready: true, restart count 4
Nov 29 12:54:17.931: INFO: coredns-858b686868-mnrpm from kube-system started at 2019-11-29 12:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 12:54:17.931: INFO: 	Container coredns ready: true, restart count 0
Nov 29 12:54:17.931: INFO: node-exporter-zhlwf from kube-system started at 2019-11-29 12:14:04 +0000 UTC (1 container statuses recorded)
Nov 29 12:54:17.931: INFO: 	Container node-exporter ready: true, restart count 0
Nov 29 12:54:17.931: INFO: kube-proxy-xhmk5 from kube-system started at 2019-11-29 12:14:04 +0000 UTC (1 container statuses recorded)
Nov 29 12:54:17.931: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 12:54:17.931: INFO: vpn-shoot-78776d6d84-gttn4 from kube-system started at 2019-11-29 12:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 12:54:17.931: INFO: 	Container vpn-shoot ready: true, restart count 0
Nov 29 12:54:17.931: INFO: coredns-858b686868-cd7fh from kube-system started at 2019-11-29 12:14:16 +0000 UTC (1 container statuses recorded)
Nov 29 12:54:17.931: INFO: 	Container coredns ready: true, restart count 0
Nov 29 12:54:17.931: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj before test
Nov 29 12:54:18.001: INFO: calico-node-pk2sw from kube-system started at 2019-11-29 12:13:49 +0000 UTC (1 container statuses recorded)
Nov 29 12:54:18.001: INFO: 	Container calico-node ready: true, restart count 0
Nov 29 12:54:18.001: INFO: blackbox-exporter-c87bdd467-lhxjt from kube-system started at 2019-11-29 12:13:49 +0000 UTC (1 container statuses recorded)
Nov 29 12:54:18.001: INFO: 	Container blackbox-exporter ready: true, restart count 0
Nov 29 12:54:18.001: INFO: node-exporter-rpgkl from kube-system started at 2019-11-29 12:13:49 +0000 UTC (1 container statuses recorded)
Nov 29 12:54:18.001: INFO: 	Container node-exporter ready: true, restart count 1
Nov 29 12:54:18.001: INFO: kube-proxy-n9fj2 from kube-system started at 2019-11-29 12:13:49 +0000 UTC (1 container statuses recorded)
Nov 29 12:54:18.001: INFO: 	Container kube-proxy ready: true, restart count 1
Nov 29 12:54:18.001: INFO: node-problem-detector-fb5tv from kube-system started at 2019-11-29 12:13:49 +0000 UTC (1 container statuses recorded)
Nov 29 12:54:18.001: INFO: 	Container node-problem-detector ready: true, restart count 0
Nov 29 12:54:18.001: INFO: calico-typha-deploy-5547c4cdc6-t5lfk from kube-system started at 2019-11-29 12:16:28 +0000 UTC (1 container statuses recorded)
Nov 29 12:54:18.001: INFO: 	Container calico-typha ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15dba34fd8b15f34], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:54:19.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9296" for this suite.
Nov 29 12:54:25.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:54:25.204: INFO: namespace sched-pred-9296 deletion completed in 6.173928004s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72
•S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:54:25.205: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5843
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1612
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Nov 29 12:54:25.365: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-5843'
Nov 29 12:54:25.465: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov 29 12:54:25.465: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1617
Nov 29 12:54:25.470: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete jobs e2e-test-nginx-job --namespace=kubectl-5843'
Nov 29 12:54:25.556: INFO: stderr: ""
Nov 29 12:54:25.556: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:54:25.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5843" for this suite.
Nov 29 12:54:47.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:54:47.693: INFO: namespace kubectl-5843 deletion completed in 22.130126757s
•SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:54:47.693: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8974
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-8974/configmap-test-3cdaf4ea-67d6-4c63-b451-ea34dfba3691
STEP: Creating a pod to test consume configMaps
Nov 29 12:54:47.847: INFO: Waiting up to 5m0s for pod "pod-configmaps-fcac7db8-804a-476f-827f-8d82f7f07175" in namespace "configmap-8974" to be "success or failure"
Nov 29 12:54:47.850: INFO: Pod "pod-configmaps-fcac7db8-804a-476f-827f-8d82f7f07175": Phase="Pending", Reason="", readiness=false. Elapsed: 2.910914ms
Nov 29 12:54:49.855: INFO: Pod "pod-configmaps-fcac7db8-804a-476f-827f-8d82f7f07175": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007605812s
STEP: Saw pod success
Nov 29 12:54:49.855: INFO: Pod "pod-configmaps-fcac7db8-804a-476f-827f-8d82f7f07175" satisfied condition "success or failure"
Nov 29 12:54:49.858: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-configmaps-fcac7db8-804a-476f-827f-8d82f7f07175 container env-test: <nil>
STEP: delete the pod
Nov 29 12:54:49.876: INFO: Waiting for pod pod-configmaps-fcac7db8-804a-476f-827f-8d82f7f07175 to disappear
Nov 29 12:54:49.879: INFO: Pod pod-configmaps-fcac7db8-804a-476f-827f-8d82f7f07175 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:54:49.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8974" for this suite.
Nov 29 12:54:55.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:54:56.015: INFO: namespace configmap-8974 deletion completed in 6.131245188s
•SSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:54:56.015: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-935
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 29 12:54:56.160: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Nov 29 12:54:56.167: INFO: Pod name sample-pod: Found 0 pods out of 1
Nov 29 12:55:01.173: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 29 12:55:01.173: INFO: Creating deployment "test-rolling-update-deployment"
Nov 29 12:55:01.178: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Nov 29 12:55:01.186: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Nov 29 12:55:03.195: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Nov 29 12:55:03.198: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Nov 29 12:55:03.208: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-935,SelfLink:/apis/apps/v1/namespaces/deployment-935/deployments/test-rolling-update-deployment,UID:f6063c29-3f61-4764-962e-937088e2060f,ResourceVersion:9657,Generation:1,CreationTimestamp:2019-11-29 12:55:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-11-29 12:55:01 +0000 UTC 2019-11-29 12:55:01 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-11-29 12:55:02 +0000 UTC 2019-11-29 12:55:01 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Nov 29 12:55:03.213: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-935,SelfLink:/apis/apps/v1/namespaces/deployment-935/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:7a0731bc-1980-42f3-ac29-0ab37c627ef0,ResourceVersion:9650,Generation:1,CreationTimestamp:2019-11-29 12:55:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment f6063c29-3f61-4764-962e-937088e2060f 0xc002412717 0xc002412718}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Nov 29 12:55:03.213: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Nov 29 12:55:03.213: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-935,SelfLink:/apis/apps/v1/namespaces/deployment-935/replicasets/test-rolling-update-controller,UID:f9e3564c-ca8d-475d-8493-ae8e07dcff33,ResourceVersion:9656,Generation:2,CreationTimestamp:2019-11-29 12:54:56 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment f6063c29-3f61-4764-962e-937088e2060f 0xc00241263f 0xc002412650}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Nov 29 12:55:03.217: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-tppqg" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-tppqg,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-935,SelfLink:/api/v1/namespaces/deployment-935/pods/test-rolling-update-deployment-79f6b9d75c-tppqg,UID:843941a9-8179-4734-817e-a28adda9f33c,ResourceVersion:9649,Generation:0,CreationTimestamp:2019-11-29 12:55:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.77/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 7a0731bc-1980-42f3-ac29-0ab37c627ef0 0xc002412fe7 0xc002412fe8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-rz5kt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-rz5kt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-rz5kt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002413050} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002413070}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 12:55:01 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 12:55:02 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 12:55:02 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 12:55:01 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:100.64.0.77,StartTime:2019-11-29 12:55:01 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-11-29 12:55:02 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://38311337ab81783afeb7d7de21a51e4276cda0fec18fb596a8b62e94aadfe7a4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:55:03.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-935" for this suite.
Nov 29 12:55:09.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:55:09.355: INFO: namespace deployment-935 deletion completed in 6.132783522s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:55:09.355: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7941
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Nov 29 12:55:12.046: INFO: Successfully updated pod "labelsupdateb464adb7-e005-44c5-9f8a-c59abb0c0b6e"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:55:14.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7941" for this suite.
Nov 29 12:55:36.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:55:36.198: INFO: namespace projected-7941 deletion completed in 22.127032557s
•SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:55:36.198: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3014
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Nov 29 12:55:36.346: INFO: Waiting up to 5m0s for pod "pod-bed19069-60be-4426-8845-ada379060970" in namespace "emptydir-3014" to be "success or failure"
Nov 29 12:55:36.350: INFO: Pod "pod-bed19069-60be-4426-8845-ada379060970": Phase="Pending", Reason="", readiness=false. Elapsed: 3.36883ms
Nov 29 12:55:38.354: INFO: Pod "pod-bed19069-60be-4426-8845-ada379060970": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00776699s
STEP: Saw pod success
Nov 29 12:55:38.354: INFO: Pod "pod-bed19069-60be-4426-8845-ada379060970" satisfied condition "success or failure"
Nov 29 12:55:38.357: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-bed19069-60be-4426-8845-ada379060970 container test-container: <nil>
STEP: delete the pod
Nov 29 12:55:38.377: INFO: Waiting for pod pod-bed19069-60be-4426-8845-ada379060970 to disappear
Nov 29 12:55:38.385: INFO: Pod pod-bed19069-60be-4426-8845-ada379060970 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:55:38.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3014" for this suite.
Nov 29 12:55:44.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:55:44.569: INFO: namespace emptydir-3014 deletion completed in 6.17865655s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:55:44.570: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1107
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Nov 29 12:55:44.716: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy --unix-socket=/tmp/kubectl-proxy-unix135845021/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:55:44.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1107" for this suite.
Nov 29 12:55:50.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:55:50.906: INFO: namespace kubectl-1107 deletion completed in 6.135973989s
•SS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:55:50.906: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1872
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Nov 29 12:55:53.075: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec pod-sharedvolume-221fc5a0-161e-498c-afaa-2bd1fadcfa61 -c busybox-main-container --namespace=emptydir-1872 -- cat /usr/share/volumeshare/shareddata.txt'
Nov 29 12:55:54.025: INFO: stderr: ""
Nov 29 12:55:54.025: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:55:54.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1872" for this suite.
Nov 29 12:56:00.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:56:00.199: INFO: namespace emptydir-1872 deletion completed in 6.167852641s
•SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:56:00.200: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9917
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-27651477-c79b-461a-a2d8-3b2084ec1273
STEP: Creating a pod to test consume configMaps
Nov 29 12:56:00.351: INFO: Waiting up to 5m0s for pod "pod-configmaps-6306e733-b944-4697-b0c2-92f463ccee10" in namespace "configmap-9917" to be "success or failure"
Nov 29 12:56:00.356: INFO: Pod "pod-configmaps-6306e733-b944-4697-b0c2-92f463ccee10": Phase="Pending", Reason="", readiness=false. Elapsed: 4.863998ms
Nov 29 12:56:02.361: INFO: Pod "pod-configmaps-6306e733-b944-4697-b0c2-92f463ccee10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009616332s
STEP: Saw pod success
Nov 29 12:56:02.361: INFO: Pod "pod-configmaps-6306e733-b944-4697-b0c2-92f463ccee10" satisfied condition "success or failure"
Nov 29 12:56:02.364: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-configmaps-6306e733-b944-4697-b0c2-92f463ccee10 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 12:56:02.387: INFO: Waiting for pod pod-configmaps-6306e733-b944-4697-b0c2-92f463ccee10 to disappear
Nov 29 12:56:02.390: INFO: Pod pod-configmaps-6306e733-b944-4697-b0c2-92f463ccee10 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:56:02.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9917" for this suite.
Nov 29 12:56:08.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:56:08.597: INFO: namespace configmap-9917 deletion completed in 6.201636504s
•SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:56:08.598: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2933
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 29 12:56:08.754: INFO: Waiting up to 5m0s for pod "pod-dfb837bc-2889-4a4a-aae9-f1608c015203" in namespace "emptydir-2933" to be "success or failure"
Nov 29 12:56:08.757: INFO: Pod "pod-dfb837bc-2889-4a4a-aae9-f1608c015203": Phase="Pending", Reason="", readiness=false. Elapsed: 2.848276ms
Nov 29 12:56:10.762: INFO: Pod "pod-dfb837bc-2889-4a4a-aae9-f1608c015203": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007935255s
STEP: Saw pod success
Nov 29 12:56:10.762: INFO: Pod "pod-dfb837bc-2889-4a4a-aae9-f1608c015203" satisfied condition "success or failure"
Nov 29 12:56:10.765: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-dfb837bc-2889-4a4a-aae9-f1608c015203 container test-container: <nil>
STEP: delete the pod
Nov 29 12:56:10.807: INFO: Waiting for pod pod-dfb837bc-2889-4a4a-aae9-f1608c015203 to disappear
Nov 29 12:56:10.810: INFO: Pod pod-dfb837bc-2889-4a4a-aae9-f1608c015203 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:56:10.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2933" for this suite.
Nov 29 12:56:16.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:56:16.950: INFO: namespace emptydir-2933 deletion completed in 6.133743053s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:56:16.950: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6503
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 29 12:56:17.105: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1cf364e7-cd69-4e09-9ac5-54726f7d279e" in namespace "downward-api-6503" to be "success or failure"
Nov 29 12:56:17.108: INFO: Pod "downwardapi-volume-1cf364e7-cd69-4e09-9ac5-54726f7d279e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.955374ms
Nov 29 12:56:19.113: INFO: Pod "downwardapi-volume-1cf364e7-cd69-4e09-9ac5-54726f7d279e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007814298s
STEP: Saw pod success
Nov 29 12:56:19.113: INFO: Pod "downwardapi-volume-1cf364e7-cd69-4e09-9ac5-54726f7d279e" satisfied condition "success or failure"
Nov 29 12:56:19.116: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod downwardapi-volume-1cf364e7-cd69-4e09-9ac5-54726f7d279e container client-container: <nil>
STEP: delete the pod
Nov 29 12:56:19.134: INFO: Waiting for pod downwardapi-volume-1cf364e7-cd69-4e09-9ac5-54726f7d279e to disappear
Nov 29 12:56:19.137: INFO: Pod downwardapi-volume-1cf364e7-cd69-4e09-9ac5-54726f7d279e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:56:19.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6503" for this suite.
Nov 29 12:56:25.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:56:25.297: INFO: namespace downward-api-6503 deletion completed in 6.155373871s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:56:25.298: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6978
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Nov 29 12:56:25.441: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config cluster-info'
Nov 29 12:56:25.560: INFO: stderr: ""
Nov 29 12:56:25.560: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\x1b[0;32mkubernetes-dashboard\x1b[0m is running at \x1b[0;33mhttps://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:56:25.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6978" for this suite.
Nov 29 12:56:31.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:56:31.695: INFO: namespace kubectl-6978 deletion completed in 6.127183538s
•SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:56:31.695: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4023
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-96174727-69b9-474e-bfd1-158c0955e399
STEP: Creating a pod to test consume configMaps
Nov 29 12:56:31.853: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-eb753c0e-4bbc-4fc7-aace-0b8105210212" in namespace "projected-4023" to be "success or failure"
Nov 29 12:56:31.858: INFO: Pod "pod-projected-configmaps-eb753c0e-4bbc-4fc7-aace-0b8105210212": Phase="Pending", Reason="", readiness=false. Elapsed: 4.99681ms
Nov 29 12:56:33.862: INFO: Pod "pod-projected-configmaps-eb753c0e-4bbc-4fc7-aace-0b8105210212": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00897135s
STEP: Saw pod success
Nov 29 12:56:33.862: INFO: Pod "pod-projected-configmaps-eb753c0e-4bbc-4fc7-aace-0b8105210212" satisfied condition "success or failure"
Nov 29 12:56:33.866: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-projected-configmaps-eb753c0e-4bbc-4fc7-aace-0b8105210212 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 12:56:33.885: INFO: Waiting for pod pod-projected-configmaps-eb753c0e-4bbc-4fc7-aace-0b8105210212 to disappear
Nov 29 12:56:33.888: INFO: Pod pod-projected-configmaps-eb753c0e-4bbc-4fc7-aace-0b8105210212 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:56:33.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4023" for this suite.
Nov 29 12:56:39.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:56:40.026: INFO: namespace projected-4023 deletion completed in 6.132748501s
•SSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:56:40.026: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-9187
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9187.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-9187.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9187.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-9187.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-9187.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-9187.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 12:56:42.864: INFO: DNS probes using dns-9187/dns-test-b9d64985-3a2b-4c8d-bd24-23cfad2e7453 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:56:42.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9187" for this suite.
Nov 29 12:56:48.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:56:49.010: INFO: namespace dns-9187 deletion completed in 6.130165398s
•SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:56:49.010: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4524
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4524
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Nov 29 12:56:49.173: INFO: Found 0 stateful pods, waiting for 3
Nov 29 12:56:59.179: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 12:56:59.179: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 12:56:59.179: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Nov 29 12:56:59.206: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Nov 29 12:57:09.239: INFO: Updating stateful set ss2
Nov 29 12:57:09.246: INFO: Waiting for Pod statefulset-4524/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Nov 29 12:57:19.278: INFO: Found 1 stateful pods, waiting for 3
Nov 29 12:57:29.283: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 12:57:29.283: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 12:57:29.283: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Nov 29 12:57:29.310: INFO: Updating stateful set ss2
Nov 29 12:57:29.319: INFO: Waiting for Pod statefulset-4524/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Nov 29 12:57:39.346: INFO: Updating stateful set ss2
Nov 29 12:57:39.355: INFO: Waiting for StatefulSet statefulset-4524/ss2 to complete update
Nov 29 12:57:39.355: INFO: Waiting for Pod statefulset-4524/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Nov 29 12:57:49.365: INFO: Deleting all statefulset in ns statefulset-4524
Nov 29 12:57:49.369: INFO: Scaling statefulset ss2 to 0
Nov 29 12:57:59.386: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 12:57:59.389: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:57:59.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4524" for this suite.
Nov 29 12:58:05.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:58:05.540: INFO: namespace statefulset-4524 deletion completed in 6.134251557s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:58:05.540: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-8647
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Nov 29 12:58:05.701: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8647,SelfLink:/api/v1/namespaces/watch-8647/configmaps/e2e-watch-test-configmap-a,UID:b32520d1-d0d4-4554-b009-c3de5d8cd23f,ResourceVersion:10417,Generation:0,CreationTimestamp:2019-11-29 12:58:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 29 12:58:05.701: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8647,SelfLink:/api/v1/namespaces/watch-8647/configmaps/e2e-watch-test-configmap-a,UID:b32520d1-d0d4-4554-b009-c3de5d8cd23f,ResourceVersion:10417,Generation:0,CreationTimestamp:2019-11-29 12:58:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Nov 29 12:58:15.710: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8647,SelfLink:/api/v1/namespaces/watch-8647/configmaps/e2e-watch-test-configmap-a,UID:b32520d1-d0d4-4554-b009-c3de5d8cd23f,ResourceVersion:10440,Generation:0,CreationTimestamp:2019-11-29 12:58:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Nov 29 12:58:15.710: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8647,SelfLink:/api/v1/namespaces/watch-8647/configmaps/e2e-watch-test-configmap-a,UID:b32520d1-d0d4-4554-b009-c3de5d8cd23f,ResourceVersion:10440,Generation:0,CreationTimestamp:2019-11-29 12:58:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Nov 29 12:58:25.718: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8647,SelfLink:/api/v1/namespaces/watch-8647/configmaps/e2e-watch-test-configmap-a,UID:b32520d1-d0d4-4554-b009-c3de5d8cd23f,ResourceVersion:10463,Generation:0,CreationTimestamp:2019-11-29 12:58:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 29 12:58:25.719: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8647,SelfLink:/api/v1/namespaces/watch-8647/configmaps/e2e-watch-test-configmap-a,UID:b32520d1-d0d4-4554-b009-c3de5d8cd23f,ResourceVersion:10463,Generation:0,CreationTimestamp:2019-11-29 12:58:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Nov 29 12:58:35.726: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8647,SelfLink:/api/v1/namespaces/watch-8647/configmaps/e2e-watch-test-configmap-a,UID:b32520d1-d0d4-4554-b009-c3de5d8cd23f,ResourceVersion:10487,Generation:0,CreationTimestamp:2019-11-29 12:58:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 29 12:58:35.726: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-8647,SelfLink:/api/v1/namespaces/watch-8647/configmaps/e2e-watch-test-configmap-a,UID:b32520d1-d0d4-4554-b009-c3de5d8cd23f,ResourceVersion:10487,Generation:0,CreationTimestamp:2019-11-29 12:58:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Nov 29 12:58:45.733: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8647,SelfLink:/api/v1/namespaces/watch-8647/configmaps/e2e-watch-test-configmap-b,UID:5e75928f-5c2e-4b44-9998-bf04ec55810e,ResourceVersion:10510,Generation:0,CreationTimestamp:2019-11-29 12:58:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 29 12:58:45.733: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8647,SelfLink:/api/v1/namespaces/watch-8647/configmaps/e2e-watch-test-configmap-b,UID:5e75928f-5c2e-4b44-9998-bf04ec55810e,ResourceVersion:10510,Generation:0,CreationTimestamp:2019-11-29 12:58:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Nov 29 12:58:55.739: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8647,SelfLink:/api/v1/namespaces/watch-8647/configmaps/e2e-watch-test-configmap-b,UID:5e75928f-5c2e-4b44-9998-bf04ec55810e,ResourceVersion:10533,Generation:0,CreationTimestamp:2019-11-29 12:58:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 29 12:58:55.739: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-8647,SelfLink:/api/v1/namespaces/watch-8647/configmaps/e2e-watch-test-configmap-b,UID:5e75928f-5c2e-4b44-9998-bf04ec55810e,ResourceVersion:10533,Generation:0,CreationTimestamp:2019-11-29 12:58:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:59:05.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8647" for this suite.
Nov 29 12:59:11.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:59:11.881: INFO: namespace watch-8647 deletion completed in 6.136643258s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:59:11.882: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6691
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 12:59:12.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6691" for this suite.
Nov 29 12:59:18.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 12:59:18.170: INFO: namespace kubelet-test-6691 deletion completed in 6.129606374s
•SS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 12:59:18.171: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-8040
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-23de7518-7c3c-4ff1-95f4-6b21e9d15c52 in namespace container-probe-8040
Nov 29 12:59:22.331: INFO: Started pod liveness-23de7518-7c3c-4ff1-95f4-6b21e9d15c52 in namespace container-probe-8040
STEP: checking the pod's current state and verifying that restartCount is present
Nov 29 12:59:22.334: INFO: Initial restart count of pod liveness-23de7518-7c3c-4ff1-95f4-6b21e9d15c52 is 0
Nov 29 12:59:32.362: INFO: Restart count of pod container-probe-8040/liveness-23de7518-7c3c-4ff1-95f4-6b21e9d15c52 is now 1 (10.027884276s elapsed)
Nov 29 12:59:52.410: INFO: Restart count of pod container-probe-8040/liveness-23de7518-7c3c-4ff1-95f4-6b21e9d15c52 is now 2 (30.075844513s elapsed)
Nov 29 13:00:12.495: INFO: Restart count of pod container-probe-8040/liveness-23de7518-7c3c-4ff1-95f4-6b21e9d15c52 is now 3 (50.160754081s elapsed)
Nov 29 13:00:32.546: INFO: Restart count of pod container-probe-8040/liveness-23de7518-7c3c-4ff1-95f4-6b21e9d15c52 is now 4 (1m10.212084991s elapsed)
Nov 29 13:00:52.592: INFO: Restart count of pod container-probe-8040/liveness-23de7518-7c3c-4ff1-95f4-6b21e9d15c52 is now 5 (1m30.257163718s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:00:52.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8040" for this suite.
Nov 29 13:00:58.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:00:58.735: INFO: namespace container-probe-8040 deletion completed in 6.129524001s
•SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:00:58.736: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-9508
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 29 13:01:00.902: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:01:00.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9508" for this suite.
Nov 29 13:01:06.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:01:07.068: INFO: namespace container-runtime-9508 deletion completed in 6.148603198s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:01:07.069: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-9937
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Nov 29 13:01:09.740: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-9937 pod-service-account-4d39e4ef-1212-483c-b41a-c08e6f0b950c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Nov 29 13:01:10.428: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-9937 pod-service-account-4d39e4ef-1212-483c-b41a-c08e6f0b950c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Nov 29 13:01:11.050: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl exec --namespace=svcaccounts-9937 pod-service-account-4d39e4ef-1212-483c-b41a-c08e6f0b950c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:01:11.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9937" for this suite.
Nov 29 13:01:17.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:01:17.773: INFO: namespace svcaccounts-9937 deletion completed in 6.137668442s
•SSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:01:17.773: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9931
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Nov 29 13:01:17.931: INFO: Waiting up to 5m0s for pod "downward-api-fdf18ff2-0b9e-417b-96a4-30d607cc058e" in namespace "downward-api-9931" to be "success or failure"
Nov 29 13:01:17.936: INFO: Pod "downward-api-fdf18ff2-0b9e-417b-96a4-30d607cc058e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.416928ms
Nov 29 13:01:19.941: INFO: Pod "downward-api-fdf18ff2-0b9e-417b-96a4-30d607cc058e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010018023s
STEP: Saw pod success
Nov 29 13:01:19.941: INFO: Pod "downward-api-fdf18ff2-0b9e-417b-96a4-30d607cc058e" satisfied condition "success or failure"
Nov 29 13:01:19.944: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod downward-api-fdf18ff2-0b9e-417b-96a4-30d607cc058e container dapi-container: <nil>
STEP: delete the pod
Nov 29 13:01:19.970: INFO: Waiting for pod downward-api-fdf18ff2-0b9e-417b-96a4-30d607cc058e to disappear
Nov 29 13:01:19.973: INFO: Pod downward-api-fdf18ff2-0b9e-417b-96a4-30d607cc058e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:01:19.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9931" for this suite.
Nov 29 13:01:25.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:01:26.116: INFO: namespace downward-api-9931 deletion completed in 6.137376857s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:01:26.116: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1554
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-kq97
STEP: Creating a pod to test atomic-volume-subpath
Nov 29 13:01:26.277: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-kq97" in namespace "subpath-1554" to be "success or failure"
Nov 29 13:01:26.282: INFO: Pod "pod-subpath-test-configmap-kq97": Phase="Pending", Reason="", readiness=false. Elapsed: 5.572954ms
Nov 29 13:01:28.287: INFO: Pod "pod-subpath-test-configmap-kq97": Phase="Running", Reason="", readiness=true. Elapsed: 2.010308083s
Nov 29 13:01:30.291: INFO: Pod "pod-subpath-test-configmap-kq97": Phase="Running", Reason="", readiness=true. Elapsed: 4.014455081s
Nov 29 13:01:32.296: INFO: Pod "pod-subpath-test-configmap-kq97": Phase="Running", Reason="", readiness=true. Elapsed: 6.019082063s
Nov 29 13:01:34.300: INFO: Pod "pod-subpath-test-configmap-kq97": Phase="Running", Reason="", readiness=true. Elapsed: 8.023387083s
Nov 29 13:01:36.305: INFO: Pod "pod-subpath-test-configmap-kq97": Phase="Running", Reason="", readiness=true. Elapsed: 10.027737715s
Nov 29 13:01:38.310: INFO: Pod "pod-subpath-test-configmap-kq97": Phase="Running", Reason="", readiness=true. Elapsed: 12.032904957s
Nov 29 13:01:40.315: INFO: Pod "pod-subpath-test-configmap-kq97": Phase="Running", Reason="", readiness=true. Elapsed: 14.03852249s
Nov 29 13:01:42.320: INFO: Pod "pod-subpath-test-configmap-kq97": Phase="Running", Reason="", readiness=true. Elapsed: 16.043032022s
Nov 29 13:01:44.325: INFO: Pod "pod-subpath-test-configmap-kq97": Phase="Running", Reason="", readiness=true. Elapsed: 18.047902986s
Nov 29 13:01:46.329: INFO: Pod "pod-subpath-test-configmap-kq97": Phase="Running", Reason="", readiness=true. Elapsed: 20.052447432s
Nov 29 13:01:48.334: INFO: Pod "pod-subpath-test-configmap-kq97": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.057010347s
STEP: Saw pod success
Nov 29 13:01:48.334: INFO: Pod "pod-subpath-test-configmap-kq97" satisfied condition "success or failure"
Nov 29 13:01:48.337: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-subpath-test-configmap-kq97 container test-container-subpath-configmap-kq97: <nil>
STEP: delete the pod
Nov 29 13:01:48.356: INFO: Waiting for pod pod-subpath-test-configmap-kq97 to disappear
Nov 29 13:01:48.361: INFO: Pod pod-subpath-test-configmap-kq97 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-kq97
Nov 29 13:01:48.361: INFO: Deleting pod "pod-subpath-test-configmap-kq97" in namespace "subpath-1554"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:01:48.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1554" for this suite.
Nov 29 13:01:54.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:01:54.502: INFO: namespace subpath-1554 deletion completed in 6.132667637s
•SSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:01:54.503: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-2065
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Nov 29 13:01:54.647: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 29 13:01:54.657: INFO: Waiting for terminating namespaces to be deleted...
Nov 29 13:01:54.660: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x before test
Nov 29 13:01:54.677: INFO: calico-node-nqnzx from kube-system started at 2019-11-29 12:14:04 +0000 UTC (1 container statuses recorded)
Nov 29 13:01:54.677: INFO: 	Container calico-node ready: true, restart count 0
Nov 29 13:01:54.677: INFO: calico-kube-controllers-5d785bc598-x2vsh from kube-system started at 2019-11-29 12:14:17 +0000 UTC (1 container statuses recorded)
Nov 29 13:01:54.677: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 29 13:01:54.677: INFO: coredns-858b686868-mnrpm from kube-system started at 2019-11-29 12:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 13:01:54.677: INFO: 	Container coredns ready: true, restart count 0
Nov 29 13:01:54.677: INFO: node-problem-detector-v46z5 from kube-system started at 2019-11-29 12:14:04 +0000 UTC (1 container statuses recorded)
Nov 29 13:01:54.677: INFO: 	Container node-problem-detector ready: true, restart count 0
Nov 29 13:01:54.677: INFO: metrics-server-5d767db8b7-nk95s from kube-system started at 2019-11-29 12:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 13:01:54.677: INFO: 	Container metrics-server ready: true, restart count 0
Nov 29 13:01:54.677: INFO: addons-nginx-ingress-controller-8468678b64-6zjss from kube-system started at 2019-11-29 12:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 13:01:54.677: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Nov 29 13:01:54.677: INFO: calico-typha-vertical-autoscaler-656557779f-qhsh7 from kube-system started at 2019-11-29 12:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 13:01:54.677: INFO: 	Container autoscaler ready: true, restart count 4
Nov 29 13:01:54.677: INFO: node-exporter-zhlwf from kube-system started at 2019-11-29 12:14:04 +0000 UTC (1 container statuses recorded)
Nov 29 13:01:54.677: INFO: 	Container node-exporter ready: true, restart count 0
Nov 29 13:01:54.677: INFO: kube-proxy-xhmk5 from kube-system started at 2019-11-29 12:14:04 +0000 UTC (1 container statuses recorded)
Nov 29 13:01:54.677: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 13:01:54.677: INFO: vpn-shoot-78776d6d84-gttn4 from kube-system started at 2019-11-29 12:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 13:01:54.677: INFO: 	Container vpn-shoot ready: true, restart count 0
Nov 29 13:01:54.677: INFO: coredns-858b686868-cd7fh from kube-system started at 2019-11-29 12:14:16 +0000 UTC (1 container statuses recorded)
Nov 29 13:01:54.677: INFO: 	Container coredns ready: true, restart count 0
Nov 29 13:01:54.677: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-88d6cff74-67jk5 from kube-system started at 2019-11-29 12:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 13:01:54.677: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Nov 29 13:01:54.677: INFO: addons-kubernetes-dashboard-5c8d9945bc-cn9cj from kube-system started at 2019-11-29 12:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 13:01:54.677: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Nov 29 13:01:54.677: INFO: calico-typha-horizontal-autoscaler-554dfbfdd7-htv8l from kube-system started at 2019-11-29 12:14:17 +0000 UTC (1 container statuses recorded)
Nov 29 13:01:54.677: INFO: 	Container autoscaler ready: true, restart count 0
Nov 29 13:01:54.677: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj before test
Nov 29 13:01:54.733: INFO: kube-proxy-n9fj2 from kube-system started at 2019-11-29 12:13:49 +0000 UTC (1 container statuses recorded)
Nov 29 13:01:54.733: INFO: 	Container kube-proxy ready: true, restart count 1
Nov 29 13:01:54.733: INFO: node-problem-detector-fb5tv from kube-system started at 2019-11-29 12:13:49 +0000 UTC (1 container statuses recorded)
Nov 29 13:01:54.733: INFO: 	Container node-problem-detector ready: true, restart count 0
Nov 29 13:01:54.733: INFO: calico-typha-deploy-5547c4cdc6-t5lfk from kube-system started at 2019-11-29 12:16:28 +0000 UTC (1 container statuses recorded)
Nov 29 13:01:54.733: INFO: 	Container calico-typha ready: true, restart count 0
Nov 29 13:01:54.733: INFO: calico-node-pk2sw from kube-system started at 2019-11-29 12:13:49 +0000 UTC (1 container statuses recorded)
Nov 29 13:01:54.733: INFO: 	Container calico-node ready: true, restart count 0
Nov 29 13:01:54.733: INFO: blackbox-exporter-c87bdd467-lhxjt from kube-system started at 2019-11-29 12:13:49 +0000 UTC (1 container statuses recorded)
Nov 29 13:01:54.733: INFO: 	Container blackbox-exporter ready: true, restart count 0
Nov 29 13:01:54.733: INFO: node-exporter-rpgkl from kube-system started at 2019-11-29 12:13:49 +0000 UTC (1 container statuses recorded)
Nov 29 13:01:54.733: INFO: 	Container node-exporter ready: true, restart count 1
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-13ff401a-3b59-4f42-a781-2e0ef9f69def 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-13ff401a-3b59-4f42-a781-2e0ef9f69def off the node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj
STEP: verifying the node doesn't have the label kubernetes.io/e2e-13ff401a-3b59-4f42-a781-2e0ef9f69def
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:01:58.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2065" for this suite.
Nov 29 13:02:10.818: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:02:10.978: INFO: namespace sched-pred-2065 deletion completed in 12.174533081s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72
•SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:02:10.978: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3061
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-52518b2f-7c66-48ad-ae80-812a68038e31
STEP: Creating a pod to test consume configMaps
Nov 29 13:02:11.133: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5d99623b-0c06-4c17-a660-831fd590d4bb" in namespace "projected-3061" to be "success or failure"
Nov 29 13:02:11.138: INFO: Pod "pod-projected-configmaps-5d99623b-0c06-4c17-a660-831fd590d4bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.179169ms
Nov 29 13:02:13.142: INFO: Pod "pod-projected-configmaps-5d99623b-0c06-4c17-a660-831fd590d4bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008377365s
STEP: Saw pod success
Nov 29 13:02:13.142: INFO: Pod "pod-projected-configmaps-5d99623b-0c06-4c17-a660-831fd590d4bb" satisfied condition "success or failure"
Nov 29 13:02:13.145: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-projected-configmaps-5d99623b-0c06-4c17-a660-831fd590d4bb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 13:02:13.165: INFO: Waiting for pod pod-projected-configmaps-5d99623b-0c06-4c17-a660-831fd590d4bb to disappear
Nov 29 13:02:13.168: INFO: Pod pod-projected-configmaps-5d99623b-0c06-4c17-a660-831fd590d4bb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:02:13.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3061" for this suite.
Nov 29 13:02:19.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:02:19.311: INFO: namespace projected-3061 deletion completed in 6.13812408s
•S
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:02:19.311: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-574
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 29 13:02:19.454: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:02:21.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-574" for this suite.
Nov 29 13:03:11.720: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:03:11.885: INFO: namespace pods-574 deletion completed in 50.177093288s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:03:11.886: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5845
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-b340ffe1-f00b-47bc-a301-e10bb05e7169
STEP: Creating configMap with name cm-test-opt-upd-27d7f41b-68f0-4dfb-989b-cfc3e310d83b
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-b340ffe1-f00b-47bc-a301-e10bb05e7169
STEP: Updating configmap cm-test-opt-upd-27d7f41b-68f0-4dfb-989b-cfc3e310d83b
STEP: Creating configMap with name cm-test-opt-create-ba479b45-9cdb-4574-940e-cdd0f07d6e3e
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:03:16.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5845" for this suite.
Nov 29 13:03:38.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:03:38.644: INFO: namespace configmap-5845 deletion completed in 22.124124264s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:03:38.644: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7978
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Nov 29 13:03:38.812: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7978,SelfLink:/api/v1/namespaces/watch-7978/configmaps/e2e-watch-test-resource-version,UID:88fd8791-6a5b-408c-ae47-4a2a148a5e9d,ResourceVersion:11408,Generation:0,CreationTimestamp:2019-11-29 13:03:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 29 13:03:38.812: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7978,SelfLink:/api/v1/namespaces/watch-7978/configmaps/e2e-watch-test-resource-version,UID:88fd8791-6a5b-408c-ae47-4a2a148a5e9d,ResourceVersion:11409,Generation:0,CreationTimestamp:2019-11-29 13:03:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:03:38.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7978" for this suite.
Nov 29 13:03:44.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:03:44.954: INFO: namespace watch-7978 deletion completed in 6.137906054s
•SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:03:44.955: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-106
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Nov 29 13:03:45.108: INFO: Waiting up to 5m0s for pod "pod-d5d0968e-9b44-4263-bc3a-a6c80a55f5b6" in namespace "emptydir-106" to be "success or failure"
Nov 29 13:03:45.113: INFO: Pod "pod-d5d0968e-9b44-4263-bc3a-a6c80a55f5b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.990282ms
Nov 29 13:03:47.117: INFO: Pod "pod-d5d0968e-9b44-4263-bc3a-a6c80a55f5b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008906333s
STEP: Saw pod success
Nov 29 13:03:47.117: INFO: Pod "pod-d5d0968e-9b44-4263-bc3a-a6c80a55f5b6" satisfied condition "success or failure"
Nov 29 13:03:47.122: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-d5d0968e-9b44-4263-bc3a-a6c80a55f5b6 container test-container: <nil>
STEP: delete the pod
Nov 29 13:03:47.143: INFO: Waiting for pod pod-d5d0968e-9b44-4263-bc3a-a6c80a55f5b6 to disappear
Nov 29 13:03:47.146: INFO: Pod pod-d5d0968e-9b44-4263-bc3a-a6c80a55f5b6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:03:47.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-106" for this suite.
Nov 29 13:03:53.162: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:03:53.286: INFO: namespace emptydir-106 deletion completed in 6.134792172s
•S
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:03:53.286: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8621
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 29 13:03:53.429: INFO: Creating deployment "test-recreate-deployment"
Nov 29 13:03:53.433: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Nov 29 13:03:53.440: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Nov 29 13:03:55.450: INFO: Waiting deployment "test-recreate-deployment" to complete
Nov 29 13:03:55.453: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Nov 29 13:03:55.460: INFO: Updating deployment test-recreate-deployment
Nov 29 13:03:55.460: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Nov 29 13:03:55.505: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-8621,SelfLink:/apis/apps/v1/namespaces/deployment-8621/deployments/test-recreate-deployment,UID:6478e976-2374-469d-8eee-3324f5153ee0,ResourceVersion:11504,Generation:2,CreationTimestamp:2019-11-29 13:03:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-11-29 13:03:55 +0000 UTC 2019-11-29 13:03:55 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-11-29 13:03:55 +0000 UTC 2019-11-29 13:03:53 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Nov 29 13:03:55.510: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-8621,SelfLink:/apis/apps/v1/namespaces/deployment-8621/replicasets/test-recreate-deployment-5c8c9cc69d,UID:9911b64e-0aa8-4c1f-9fae-b15254b49589,ResourceVersion:11503,Generation:1,CreationTimestamp:2019-11-29 13:03:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 6478e976-2374-469d-8eee-3324f5153ee0 0xc001c17577 0xc001c17578}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Nov 29 13:03:55.510: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Nov 29 13:03:55.510: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-8621,SelfLink:/apis/apps/v1/namespaces/deployment-8621/replicasets/test-recreate-deployment-6df85df6b9,UID:9ac368e5-d37e-4371-8fc6-ce5424683930,ResourceVersion:11496,Generation:2,CreationTimestamp:2019-11-29 13:03:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 6478e976-2374-469d-8eee-3324f5153ee0 0xc001c17647 0xc001c17648}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Nov 29 13:03:55.513: INFO: Pod "test-recreate-deployment-5c8c9cc69d-rbrht" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-rbrht,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-8621,SelfLink:/api/v1/namespaces/deployment-8621/pods/test-recreate-deployment-5c8c9cc69d-rbrht,UID:09fdb6da-b41f-4a22-bbb7-493758455966,ResourceVersion:11505,Generation:0,CreationTimestamp:2019-11-29 13:03:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 9911b64e-0aa8-4c1f-9fae-b15254b49589 0xc001c17f17 0xc001c17f18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lj2vv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lj2vv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-lj2vv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001c17f80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001c17fa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:03:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:03:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:03:55 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:03:55 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:,StartTime:2019-11-29 13:03:55 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:03:55.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8621" for this suite.
Nov 29 13:04:01.531: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:04:01.651: INFO: namespace deployment-8621 deletion completed in 6.13204747s
•SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:04:01.651: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6592
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 29 13:04:01.831: INFO: Number of nodes with available pods: 0
Nov 29 13:04:01.831: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:04:02.841: INFO: Number of nodes with available pods: 0
Nov 29 13:04:02.841: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:04:03.840: INFO: Number of nodes with available pods: 2
Nov 29 13:04:03.840: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Nov 29 13:04:03.860: INFO: Number of nodes with available pods: 1
Nov 29 13:04:03.860: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:04:04.870: INFO: Number of nodes with available pods: 1
Nov 29 13:04:04.870: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:04:05.870: INFO: Number of nodes with available pods: 1
Nov 29 13:04:05.870: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:04:06.870: INFO: Number of nodes with available pods: 1
Nov 29 13:04:06.870: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:04:07.870: INFO: Number of nodes with available pods: 1
Nov 29 13:04:07.870: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:04:08.870: INFO: Number of nodes with available pods: 1
Nov 29 13:04:08.870: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:04:09.871: INFO: Number of nodes with available pods: 1
Nov 29 13:04:09.871: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:04:10.870: INFO: Number of nodes with available pods: 1
Nov 29 13:04:10.870: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:04:11.870: INFO: Number of nodes with available pods: 1
Nov 29 13:04:11.870: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:04:12.870: INFO: Number of nodes with available pods: 1
Nov 29 13:04:12.870: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:04:13.869: INFO: Number of nodes with available pods: 1
Nov 29 13:04:13.869: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:04:14.869: INFO: Number of nodes with available pods: 1
Nov 29 13:04:14.869: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:04:15.869: INFO: Number of nodes with available pods: 1
Nov 29 13:04:15.869: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:04:16.869: INFO: Number of nodes with available pods: 1
Nov 29 13:04:16.869: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:04:17.869: INFO: Number of nodes with available pods: 1
Nov 29 13:04:17.869: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:04:18.870: INFO: Number of nodes with available pods: 1
Nov 29 13:04:18.870: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:04:19.870: INFO: Number of nodes with available pods: 2
Nov 29 13:04:19.870: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6592, will wait for the garbage collector to delete the pods
Nov 29 13:04:19.933: INFO: Deleting DaemonSet.extensions daemon-set took: 6.944456ms
Nov 29 13:04:20.333: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.215349ms
Nov 29 13:04:28.937: INFO: Number of nodes with available pods: 0
Nov 29 13:04:28.937: INFO: Number of running nodes: 0, number of available pods: 0
Nov 29 13:04:28.942: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6592/daemonsets","resourceVersion":"11643"},"items":null}

Nov 29 13:04:28.945: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6592/pods","resourceVersion":"11643"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:04:28.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6592" for this suite.
Nov 29 13:04:34.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:04:35.106: INFO: namespace daemonsets-6592 deletion completed in 6.131960131s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:04:35.106: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-7841
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8716
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2559
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:04:59.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7841" for this suite.
Nov 29 13:05:05.651: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:05:05.808: INFO: namespace namespaces-7841 deletion completed in 6.167475747s
STEP: Destroying namespace "nsdeletetest-8716" for this suite.
Nov 29 13:05:05.811: INFO: Namespace nsdeletetest-8716 was already deleted
STEP: Destroying namespace "nsdeletetest-2559" for this suite.
Nov 29 13:05:11.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:05:11.963: INFO: namespace nsdeletetest-2559 deletion completed in 6.152194282s
•SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:05:11.963: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-5670
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 29 13:05:14.139: INFO: Waiting up to 5m0s for pod "client-envvars-3f3e54bb-1f75-4bd5-bbc7-85765cc0e36c" in namespace "pods-5670" to be "success or failure"
Nov 29 13:05:14.143: INFO: Pod "client-envvars-3f3e54bb-1f75-4bd5-bbc7-85765cc0e36c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.583065ms
Nov 29 13:05:16.147: INFO: Pod "client-envvars-3f3e54bb-1f75-4bd5-bbc7-85765cc0e36c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007786626s
STEP: Saw pod success
Nov 29 13:05:16.147: INFO: Pod "client-envvars-3f3e54bb-1f75-4bd5-bbc7-85765cc0e36c" satisfied condition "success or failure"
Nov 29 13:05:16.150: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod client-envvars-3f3e54bb-1f75-4bd5-bbc7-85765cc0e36c container env3cont: <nil>
STEP: delete the pod
Nov 29 13:05:16.167: INFO: Waiting for pod client-envvars-3f3e54bb-1f75-4bd5-bbc7-85765cc0e36c to disappear
Nov 29 13:05:16.170: INFO: Pod client-envvars-3f3e54bb-1f75-4bd5-bbc7-85765cc0e36c no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:05:16.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5670" for this suite.
Nov 29 13:05:54.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:05:54.346: INFO: namespace pods-5670 deletion completed in 38.170417471s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:05:54.347: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4294
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 29 13:05:55.505: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:05:55.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4294" for this suite.
Nov 29 13:06:01.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:06:01.650: INFO: namespace container-runtime-4294 deletion completed in 6.129400826s
•SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:06:01.650: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3409
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-7f2938e5-735e-4eb5-912a-4ff949b67337
STEP: Creating a pod to test consume configMaps
Nov 29 13:06:01.810: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-192d118a-1db9-426f-955a-a35d8574e456" in namespace "projected-3409" to be "success or failure"
Nov 29 13:06:01.815: INFO: Pod "pod-projected-configmaps-192d118a-1db9-426f-955a-a35d8574e456": Phase="Pending", Reason="", readiness=false. Elapsed: 5.320683ms
Nov 29 13:06:03.820: INFO: Pod "pod-projected-configmaps-192d118a-1db9-426f-955a-a35d8574e456": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009530594s
STEP: Saw pod success
Nov 29 13:06:03.820: INFO: Pod "pod-projected-configmaps-192d118a-1db9-426f-955a-a35d8574e456" satisfied condition "success or failure"
Nov 29 13:06:03.823: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-projected-configmaps-192d118a-1db9-426f-955a-a35d8574e456 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 13:06:03.844: INFO: Waiting for pod pod-projected-configmaps-192d118a-1db9-426f-955a-a35d8574e456 to disappear
Nov 29 13:06:03.846: INFO: Pod pod-projected-configmaps-192d118a-1db9-426f-955a-a35d8574e456 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:06:03.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3409" for this suite.
Nov 29 13:06:09.862: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:06:09.980: INFO: namespace projected-3409 deletion completed in 6.129703075s
•SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:06:09.981: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-228
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Nov 29 13:06:10.159: INFO: Number of nodes with available pods: 0
Nov 29 13:06:10.159: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:06:11.168: INFO: Number of nodes with available pods: 0
Nov 29 13:06:11.168: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:06:12.168: INFO: Number of nodes with available pods: 2
Nov 29 13:06:12.168: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Nov 29 13:06:12.192: INFO: Number of nodes with available pods: 1
Nov 29 13:06:12.192: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:06:13.202: INFO: Number of nodes with available pods: 1
Nov 29 13:06:13.202: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:06:14.202: INFO: Number of nodes with available pods: 2
Nov 29 13:06:14.203: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-228, will wait for the garbage collector to delete the pods
Nov 29 13:06:14.269: INFO: Deleting DaemonSet.extensions daemon-set took: 6.127992ms
Nov 29 13:06:14.770: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.276692ms
Nov 29 13:06:18.974: INFO: Number of nodes with available pods: 0
Nov 29 13:06:18.974: INFO: Number of running nodes: 0, number of available pods: 0
Nov 29 13:06:18.977: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-228/daemonsets","resourceVersion":"12049"},"items":null}

Nov 29 13:06:18.980: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-228/pods","resourceVersion":"12049"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:06:18.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-228" for this suite.
Nov 29 13:06:25.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:06:25.167: INFO: namespace daemonsets-228 deletion completed in 6.170540098s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:06:25.167: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-4865
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:06:27.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4865" for this suite.
Nov 29 13:06:33.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:06:33.522: INFO: namespace emptydir-wrapper-4865 deletion completed in 6.125970487s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:06:33.523: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9460
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 29 13:06:33.684: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c15b11e3-94c7-4c5c-9b57-9e10f2bc64bd" in namespace "downward-api-9460" to be "success or failure"
Nov 29 13:06:33.689: INFO: Pod "downwardapi-volume-c15b11e3-94c7-4c5c-9b57-9e10f2bc64bd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.898966ms
Nov 29 13:06:35.693: INFO: Pod "downwardapi-volume-c15b11e3-94c7-4c5c-9b57-9e10f2bc64bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009398228s
STEP: Saw pod success
Nov 29 13:06:35.693: INFO: Pod "downwardapi-volume-c15b11e3-94c7-4c5c-9b57-9e10f2bc64bd" satisfied condition "success or failure"
Nov 29 13:06:35.697: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod downwardapi-volume-c15b11e3-94c7-4c5c-9b57-9e10f2bc64bd container client-container: <nil>
STEP: delete the pod
Nov 29 13:06:35.717: INFO: Waiting for pod downwardapi-volume-c15b11e3-94c7-4c5c-9b57-9e10f2bc64bd to disappear
Nov 29 13:06:35.720: INFO: Pod downwardapi-volume-c15b11e3-94c7-4c5c-9b57-9e10f2bc64bd no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:06:35.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9460" for this suite.
Nov 29 13:06:41.736: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:06:41.864: INFO: namespace downward-api-9460 deletion completed in 6.138868622s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:06:41.864: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7615
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 29 13:06:42.021: INFO: Waiting up to 5m0s for pod "downwardapi-volume-87c7ebab-4143-481f-a35b-74c3d96068a8" in namespace "downward-api-7615" to be "success or failure"
Nov 29 13:06:42.026: INFO: Pod "downwardapi-volume-87c7ebab-4143-481f-a35b-74c3d96068a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.658945ms
Nov 29 13:06:44.031: INFO: Pod "downwardapi-volume-87c7ebab-4143-481f-a35b-74c3d96068a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009189286s
STEP: Saw pod success
Nov 29 13:06:44.031: INFO: Pod "downwardapi-volume-87c7ebab-4143-481f-a35b-74c3d96068a8" satisfied condition "success or failure"
Nov 29 13:06:44.034: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod downwardapi-volume-87c7ebab-4143-481f-a35b-74c3d96068a8 container client-container: <nil>
STEP: delete the pod
Nov 29 13:06:44.052: INFO: Waiting for pod downwardapi-volume-87c7ebab-4143-481f-a35b-74c3d96068a8 to disappear
Nov 29 13:06:44.054: INFO: Pod downwardapi-volume-87c7ebab-4143-481f-a35b-74c3d96068a8 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:06:44.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7615" for this suite.
Nov 29 13:06:50.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:06:50.195: INFO: namespace downward-api-7615 deletion completed in 6.136419592s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:06:50.196: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2391
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Nov 29 13:06:50.338: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2391'
Nov 29 13:06:50.438: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov 29 13:06:50.438: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1426
Nov 29 13:06:52.448: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete deployment e2e-test-nginx-deployment --namespace=kubectl-2391'
Nov 29 13:06:52.527: INFO: stderr: ""
Nov 29 13:06:52.527: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:06:52.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2391" for this suite.
Nov 29 13:06:58.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:06:58.662: INFO: namespace kubectl-2391 deletion completed in 6.129544808s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:06:58.662: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1448
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-44e16e7a-fc3c-4e08-985d-27f3464cf6a3
STEP: Creating secret with name s-test-opt-upd-22e833a2-9137-4343-9092-0af6483dea7d
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-44e16e7a-fc3c-4e08-985d-27f3464cf6a3
STEP: Updating secret s-test-opt-upd-22e833a2-9137-4343-9092-0af6483dea7d
STEP: Creating secret with name s-test-opt-create-77b6b5d2-49fd-40e3-9496-0833039f077d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:08:17.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1448" for this suite.
Nov 29 13:08:39.761: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:08:39.877: INFO: namespace secrets-1448 deletion completed in 22.127244118s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:08:39.877: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2185
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 29 13:08:40.024: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Nov 29 13:08:41.058: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:08:41.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2185" for this suite.
Nov 29 13:08:47.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:08:47.207: INFO: namespace replication-controller-2185 deletion completed in 6.140262433s
•SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:08:47.207: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9160
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-c34eee49-cdfb-4aa3-8c50-f6f2e4bfb271
STEP: Creating a pod to test consume secrets
Nov 29 13:08:47.361: INFO: Waiting up to 5m0s for pod "pod-secrets-e74a24b0-3b0c-4f83-b22a-ae8daa5b2d1a" in namespace "secrets-9160" to be "success or failure"
Nov 29 13:08:47.364: INFO: Pod "pod-secrets-e74a24b0-3b0c-4f83-b22a-ae8daa5b2d1a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.083312ms
Nov 29 13:08:49.369: INFO: Pod "pod-secrets-e74a24b0-3b0c-4f83-b22a-ae8daa5b2d1a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007702577s
STEP: Saw pod success
Nov 29 13:08:49.369: INFO: Pod "pod-secrets-e74a24b0-3b0c-4f83-b22a-ae8daa5b2d1a" satisfied condition "success or failure"
Nov 29 13:08:49.372: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-secrets-e74a24b0-3b0c-4f83-b22a-ae8daa5b2d1a container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 13:08:49.394: INFO: Waiting for pod pod-secrets-e74a24b0-3b0c-4f83-b22a-ae8daa5b2d1a to disappear
Nov 29 13:08:49.397: INFO: Pod pod-secrets-e74a24b0-3b0c-4f83-b22a-ae8daa5b2d1a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:08:49.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9160" for this suite.
Nov 29 13:08:55.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:08:55.535: INFO: namespace secrets-9160 deletion completed in 6.13316069s
•SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:08:55.536: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-8974
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Nov 29 13:08:55.683: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-8974'
Nov 29 13:08:55.901: INFO: stderr: ""
Nov 29 13:08:55.901: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Nov 29 13:08:56.906: INFO: Selector matched 1 pods for map[app:redis]
Nov 29 13:08:56.906: INFO: Found 0 / 1
Nov 29 13:08:57.906: INFO: Selector matched 1 pods for map[app:redis]
Nov 29 13:08:57.906: INFO: Found 1 / 1
Nov 29 13:08:57.906: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Nov 29 13:08:57.909: INFO: Selector matched 1 pods for map[app:redis]
Nov 29 13:08:57.909: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Nov 29 13:08:57.909: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config patch pod redis-master-2c8n7 --namespace=kubectl-8974 -p {"metadata":{"annotations":{"x":"y"}}}'
Nov 29 13:08:58.176: INFO: stderr: ""
Nov 29 13:08:58.176: INFO: stdout: "pod/redis-master-2c8n7 patched\n"
STEP: checking annotations
Nov 29 13:08:58.180: INFO: Selector matched 1 pods for map[app:redis]
Nov 29 13:08:58.180: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:08:58.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8974" for this suite.
Nov 29 13:09:20.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:09:20.320: INFO: namespace kubectl-8974 deletion completed in 22.13541047s
•
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:09:20.320: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9757
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 29 13:09:20.474: INFO: Waiting up to 5m0s for pod "downwardapi-volume-628de39e-7556-45bf-9e5d-99763edeefa0" in namespace "downward-api-9757" to be "success or failure"
Nov 29 13:09:20.478: INFO: Pod "downwardapi-volume-628de39e-7556-45bf-9e5d-99763edeefa0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.607443ms
Nov 29 13:09:22.482: INFO: Pod "downwardapi-volume-628de39e-7556-45bf-9e5d-99763edeefa0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007599793s
STEP: Saw pod success
Nov 29 13:09:22.482: INFO: Pod "downwardapi-volume-628de39e-7556-45bf-9e5d-99763edeefa0" satisfied condition "success or failure"
Nov 29 13:09:22.485: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod downwardapi-volume-628de39e-7556-45bf-9e5d-99763edeefa0 container client-container: <nil>
STEP: delete the pod
Nov 29 13:09:22.505: INFO: Waiting for pod downwardapi-volume-628de39e-7556-45bf-9e5d-99763edeefa0 to disappear
Nov 29 13:09:22.508: INFO: Pod downwardapi-volume-628de39e-7556-45bf-9e5d-99763edeefa0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:09:22.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9757" for this suite.
Nov 29 13:09:28.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:09:28.650: INFO: namespace downward-api-9757 deletion completed in 6.137830145s
•
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:09:28.651: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2778
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-2123
STEP: Creating secret with name secret-test-67b10c91-b483-448f-bf0b-2858a29e8bd7
STEP: Creating a pod to test consume secrets
Nov 29 13:09:28.945: INFO: Waiting up to 5m0s for pod "pod-secrets-47f3e50f-6033-4e79-bf02-0d05dda5e8c0" in namespace "secrets-2778" to be "success or failure"
Nov 29 13:09:28.949: INFO: Pod "pod-secrets-47f3e50f-6033-4e79-bf02-0d05dda5e8c0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.788135ms
Nov 29 13:09:30.954: INFO: Pod "pod-secrets-47f3e50f-6033-4e79-bf02-0d05dda5e8c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009376235s
STEP: Saw pod success
Nov 29 13:09:30.955: INFO: Pod "pod-secrets-47f3e50f-6033-4e79-bf02-0d05dda5e8c0" satisfied condition "success or failure"
Nov 29 13:09:30.958: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-secrets-47f3e50f-6033-4e79-bf02-0d05dda5e8c0 container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 13:09:30.977: INFO: Waiting for pod pod-secrets-47f3e50f-6033-4e79-bf02-0d05dda5e8c0 to disappear
Nov 29 13:09:30.980: INFO: Pod pod-secrets-47f3e50f-6033-4e79-bf02-0d05dda5e8c0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:09:30.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2778" for this suite.
Nov 29 13:09:36.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:09:37.117: INFO: namespace secrets-2778 deletion completed in 6.133167659s
STEP: Destroying namespace "secret-namespace-2123" for this suite.
Nov 29 13:09:43.130: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:09:43.255: INFO: namespace secret-namespace-2123 deletion completed in 6.137581965s
•SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:09:43.255: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1055
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-7745451c-0f24-49b5-ad62-e97f2ecff3e8
STEP: Creating a pod to test consume configMaps
Nov 29 13:09:43.411: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cbe1710f-8562-428a-b9be-d26beb73abbb" in namespace "projected-1055" to be "success or failure"
Nov 29 13:09:43.416: INFO: Pod "pod-projected-configmaps-cbe1710f-8562-428a-b9be-d26beb73abbb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.629852ms
Nov 29 13:09:45.420: INFO: Pod "pod-projected-configmaps-cbe1710f-8562-428a-b9be-d26beb73abbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009315382s
STEP: Saw pod success
Nov 29 13:09:45.420: INFO: Pod "pod-projected-configmaps-cbe1710f-8562-428a-b9be-d26beb73abbb" satisfied condition "success or failure"
Nov 29 13:09:45.424: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-projected-configmaps-cbe1710f-8562-428a-b9be-d26beb73abbb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 13:09:45.443: INFO: Waiting for pod pod-projected-configmaps-cbe1710f-8562-428a-b9be-d26beb73abbb to disappear
Nov 29 13:09:45.446: INFO: Pod pod-projected-configmaps-cbe1710f-8562-428a-b9be-d26beb73abbb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:09:45.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1055" for this suite.
Nov 29 13:09:51.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:09:51.579: INFO: namespace projected-1055 deletion completed in 6.127113038s
•SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:09:51.579: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1774
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1774.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1774.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1774.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1774.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 13:09:55.908: INFO: DNS probes using dns-test-fe4d55b1-76aa-4b67-b153-b8dd48e4ec5a succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1774.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1774.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1774.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1774.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 13:10:00.033: INFO: File wheezy_udp@dns-test-service-3.dns-1774.svc.cluster.local from pod  dns-1774/dns-test-d76314f5-f4fb-4370-be1f-b16cddeddb9f contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 13:10:00.116: INFO: File jessie_udp@dns-test-service-3.dns-1774.svc.cluster.local from pod  dns-1774/dns-test-d76314f5-f4fb-4370-be1f-b16cddeddb9f contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 13:10:00.116: INFO: Lookups using dns-1774/dns-test-d76314f5-f4fb-4370-be1f-b16cddeddb9f failed for: [wheezy_udp@dns-test-service-3.dns-1774.svc.cluster.local jessie_udp@dns-test-service-3.dns-1774.svc.cluster.local]

Nov 29 13:10:05.123: INFO: File wheezy_udp@dns-test-service-3.dns-1774.svc.cluster.local from pod  dns-1774/dns-test-d76314f5-f4fb-4370-be1f-b16cddeddb9f contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 13:10:05.200: INFO: File jessie_udp@dns-test-service-3.dns-1774.svc.cluster.local from pod  dns-1774/dns-test-d76314f5-f4fb-4370-be1f-b16cddeddb9f contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 13:10:05.200: INFO: Lookups using dns-1774/dns-test-d76314f5-f4fb-4370-be1f-b16cddeddb9f failed for: [wheezy_udp@dns-test-service-3.dns-1774.svc.cluster.local jessie_udp@dns-test-service-3.dns-1774.svc.cluster.local]

Nov 29 13:10:10.123: INFO: File wheezy_udp@dns-test-service-3.dns-1774.svc.cluster.local from pod  dns-1774/dns-test-d76314f5-f4fb-4370-be1f-b16cddeddb9f contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 13:10:10.200: INFO: File jessie_udp@dns-test-service-3.dns-1774.svc.cluster.local from pod  dns-1774/dns-test-d76314f5-f4fb-4370-be1f-b16cddeddb9f contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 13:10:10.201: INFO: Lookups using dns-1774/dns-test-d76314f5-f4fb-4370-be1f-b16cddeddb9f failed for: [wheezy_udp@dns-test-service-3.dns-1774.svc.cluster.local jessie_udp@dns-test-service-3.dns-1774.svc.cluster.local]

Nov 29 13:10:15.123: INFO: File wheezy_udp@dns-test-service-3.dns-1774.svc.cluster.local from pod  dns-1774/dns-test-d76314f5-f4fb-4370-be1f-b16cddeddb9f contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 13:10:15.200: INFO: File jessie_udp@dns-test-service-3.dns-1774.svc.cluster.local from pod  dns-1774/dns-test-d76314f5-f4fb-4370-be1f-b16cddeddb9f contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 13:10:15.200: INFO: Lookups using dns-1774/dns-test-d76314f5-f4fb-4370-be1f-b16cddeddb9f failed for: [wheezy_udp@dns-test-service-3.dns-1774.svc.cluster.local jessie_udp@dns-test-service-3.dns-1774.svc.cluster.local]

Nov 29 13:10:20.122: INFO: File wheezy_udp@dns-test-service-3.dns-1774.svc.cluster.local from pod  dns-1774/dns-test-d76314f5-f4fb-4370-be1f-b16cddeddb9f contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 13:10:20.200: INFO: File jessie_udp@dns-test-service-3.dns-1774.svc.cluster.local from pod  dns-1774/dns-test-d76314f5-f4fb-4370-be1f-b16cddeddb9f contains 'foo.example.com.
' instead of 'bar.example.com.'
Nov 29 13:10:20.200: INFO: Lookups using dns-1774/dns-test-d76314f5-f4fb-4370-be1f-b16cddeddb9f failed for: [wheezy_udp@dns-test-service-3.dns-1774.svc.cluster.local jessie_udp@dns-test-service-3.dns-1774.svc.cluster.local]

Nov 29 13:10:25.241: INFO: DNS probes using dns-test-d76314f5-f4fb-4370-be1f-b16cddeddb9f succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1774.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1774.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1774.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1774.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 13:10:29.468: INFO: DNS probes using dns-test-c5277c6b-a2c9-4e36-839f-fe4c6c082cb8 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:10:29.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1774" for this suite.
Nov 29 13:10:35.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:10:35.621: INFO: namespace dns-1774 deletion completed in 6.125654626s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:10:35.622: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1402
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 29 13:10:35.775: INFO: Waiting up to 5m0s for pod "downwardapi-volume-96fcbddd-2f55-4fef-b40a-70055edfa7b2" in namespace "downward-api-1402" to be "success or failure"
Nov 29 13:10:35.779: INFO: Pod "downwardapi-volume-96fcbddd-2f55-4fef-b40a-70055edfa7b2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.836474ms
Nov 29 13:10:37.783: INFO: Pod "downwardapi-volume-96fcbddd-2f55-4fef-b40a-70055edfa7b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008000081s
STEP: Saw pod success
Nov 29 13:10:37.783: INFO: Pod "downwardapi-volume-96fcbddd-2f55-4fef-b40a-70055edfa7b2" satisfied condition "success or failure"
Nov 29 13:10:37.786: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod downwardapi-volume-96fcbddd-2f55-4fef-b40a-70055edfa7b2 container client-container: <nil>
STEP: delete the pod
Nov 29 13:10:37.806: INFO: Waiting for pod downwardapi-volume-96fcbddd-2f55-4fef-b40a-70055edfa7b2 to disappear
Nov 29 13:10:37.809: INFO: Pod downwardapi-volume-96fcbddd-2f55-4fef-b40a-70055edfa7b2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:10:37.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1402" for this suite.
Nov 29 13:10:43.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:10:43.940: INFO: namespace downward-api-1402 deletion completed in 6.127167519s
•SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:10:43.940: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8787
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 29 13:10:44.084: INFO: Creating deployment "nginx-deployment"
Nov 29 13:10:44.088: INFO: Waiting for observed generation 1
Nov 29 13:10:46.096: INFO: Waiting for all required pods to come up
Nov 29 13:10:46.101: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Nov 29 13:10:48.112: INFO: Waiting for deployment "nginx-deployment" to complete
Nov 29 13:10:48.118: INFO: Updating deployment "nginx-deployment" with a non-existent image
Nov 29 13:10:48.126: INFO: Updating deployment nginx-deployment
Nov 29 13:10:48.126: INFO: Waiting for observed generation 2
Nov 29 13:10:50.135: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Nov 29 13:10:50.138: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Nov 29 13:10:50.141: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Nov 29 13:10:50.152: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Nov 29 13:10:50.152: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Nov 29 13:10:50.155: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Nov 29 13:10:50.161: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Nov 29 13:10:50.161: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Nov 29 13:10:50.169: INFO: Updating deployment nginx-deployment
Nov 29 13:10:50.169: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Nov 29 13:10:50.179: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Nov 29 13:10:52.188: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Nov 29 13:10:52.195: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-8787,SelfLink:/apis/apps/v1/namespaces/deployment-8787/deployments/nginx-deployment,UID:65b70d0c-0782-4151-85f9-b58d8fe3d737,ResourceVersion:13170,Generation:3,CreationTimestamp:2019-11-29 13:10:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-11-29 13:10:50 +0000 UTC 2019-11-29 13:10:50 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-11-29 13:10:50 +0000 UTC 2019-11-29 13:10:44 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Nov 29 13:10:52.198: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-8787,SelfLink:/apis/apps/v1/namespaces/deployment-8787/replicasets/nginx-deployment-55fb7cb77f,UID:5c2b653e-ab77-4b65-9f51-c0652576268e,ResourceVersion:13169,Generation:3,CreationTimestamp:2019-11-29 13:10:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 65b70d0c-0782-4151-85f9-b58d8fe3d737 0xc002d9cc67 0xc002d9cc68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Nov 29 13:10:52.198: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Nov 29 13:10:52.198: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-8787,SelfLink:/apis/apps/v1/namespaces/deployment-8787/replicasets/nginx-deployment-7b8c6f4498,UID:c78a8ed2-625f-471d-99c4-33242710e2f5,ResourceVersion:13160,Generation:3,CreationTimestamp:2019-11-29 13:10:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 65b70d0c-0782-4151-85f9-b58d8fe3d737 0xc002d9cd37 0xc002d9cd38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Nov 29 13:10:52.206: INFO: Pod "nginx-deployment-55fb7cb77f-2qcc6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-2qcc6,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-55fb7cb77f-2qcc6,UID:b0790da2-c809-457e-8ac1-fd9bb84065e7,ResourceVersion:13113,Generation:0,CreationTimestamp:2019-11-29 13:10:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.133/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5c2b653e-ab77-4b65-9f51-c0652576268e 0xc0033109a7 0xc0033109a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003310a10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003310a30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:48 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:,StartTime:2019-11-29 13:10:48 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.206: INFO: Pod "nginx-deployment-55fb7cb77f-5rszw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-5rszw,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-55fb7cb77f-5rszw,UID:64da2537-5d75-4734-941d-a71f434c7022,ResourceVersion:13115,Generation:0,CreationTimestamp:2019-11-29 13:10:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.35/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5c2b653e-ab77-4b65-9f51-c0652576268e 0xc003310b10 0xc003310b11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003310b80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003310ba0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:48 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-11-29 13:10:48 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.206: INFO: Pod "nginx-deployment-55fb7cb77f-cqd7r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-cqd7r,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-55fb7cb77f-cqd7r,UID:ee6d8067-6615-441f-92da-8134068baeee,ResourceVersion:13186,Generation:0,CreationTimestamp:2019-11-29 13:10:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5c2b653e-ab77-4b65-9f51-c0652576268e 0xc003310c70 0xc003310c71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003310ce0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003310d00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-11-29 13:10:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.206: INFO: Pod "nginx-deployment-55fb7cb77f-h2ssn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-h2ssn,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-55fb7cb77f-h2ssn,UID:4f09c2cf-6b4f-40c8-8851-21791076499f,ResourceVersion:13194,Generation:0,CreationTimestamp:2019-11-29 13:10:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.132/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5c2b653e-ab77-4b65-9f51-c0652576268e 0xc003310de0 0xc003310de1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003310e50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003310e70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:48 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:100.64.0.132,StartTime:2019-11-29 13:10:48 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.207: INFO: Pod "nginx-deployment-55fb7cb77f-j5mbg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-j5mbg,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-55fb7cb77f-j5mbg,UID:5ff0f5d5-3110-4a3d-91d1-c57f21e11686,ResourceVersion:13196,Generation:0,CreationTimestamp:2019-11-29 13:10:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.136/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5c2b653e-ab77-4b65-9f51-c0652576268e 0xc003310f70 0xc003310f71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003310fe0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003311000}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:,StartTime:2019-11-29 13:10:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.207: INFO: Pod "nginx-deployment-55fb7cb77f-krrrs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-krrrs,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-55fb7cb77f-krrrs,UID:d061c816-e460-4c41-81fb-8567a6205acb,ResourceVersion:13198,Generation:0,CreationTimestamp:2019-11-29 13:10:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.137/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5c2b653e-ab77-4b65-9f51-c0652576268e 0xc0033110e0 0xc0033110e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003311150} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003311170}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:,StartTime:2019-11-29 13:10:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.207: INFO: Pod "nginx-deployment-55fb7cb77f-lxk67" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-lxk67,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-55fb7cb77f-lxk67,UID:45635a6a-14bb-473f-bf44-06c210aace55,ResourceVersion:13182,Generation:0,CreationTimestamp:2019-11-29 13:10:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5c2b653e-ab77-4b65-9f51-c0652576268e 0xc003311240 0xc003311241}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0033112b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0033112d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-11-29 13:10:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.207: INFO: Pod "nginx-deployment-55fb7cb77f-pbhj9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-pbhj9,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-55fb7cb77f-pbhj9,UID:45f2f81d-8d68-4102-9767-d403787aee83,ResourceVersion:13112,Generation:0,CreationTimestamp:2019-11-29 13:10:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.33/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5c2b653e-ab77-4b65-9f51-c0652576268e 0xc0033113b0 0xc0033113b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003311420} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003311440}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:48 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-11-29 13:10:48 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.208: INFO: Pod "nginx-deployment-55fb7cb77f-rphdf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-rphdf,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-55fb7cb77f-rphdf,UID:325e9da6-23ef-40fe-bc64-3f14657b7f19,ResourceVersion:13178,Generation:0,CreationTimestamp:2019-11-29 13:10:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5c2b653e-ab77-4b65-9f51-c0652576268e 0xc003311510 0xc003311511}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003311580} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0033115a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:,StartTime:2019-11-29 13:10:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.208: INFO: Pod "nginx-deployment-55fb7cb77f-vpnsd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-vpnsd,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-55fb7cb77f-vpnsd,UID:86115278-aff8-4cc6-881d-bd9c80faf264,ResourceVersion:13114,Generation:0,CreationTimestamp:2019-11-29 13:10:48 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.34/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5c2b653e-ab77-4b65-9f51-c0652576268e 0xc003311680 0xc003311681}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0033116f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003311710}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:48 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:48 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-11-29 13:10:48 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.208: INFO: Pod "nginx-deployment-55fb7cb77f-wqxdx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-wqxdx,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-55fb7cb77f-wqxdx,UID:c7b4d0cc-b2a2-48a7-b5d2-ed54909c337a,ResourceVersion:13185,Generation:0,CreationTimestamp:2019-11-29 13:10:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5c2b653e-ab77-4b65-9f51-c0652576268e 0xc0033117e0 0xc0033117e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003311850} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003311870}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-11-29 13:10:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.208: INFO: Pod "nginx-deployment-55fb7cb77f-xlxmq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-xlxmq,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-55fb7cb77f-xlxmq,UID:d2d46e54-7167-4eee-9a68-adc3be8b83a4,ResourceVersion:13187,Generation:0,CreationTimestamp:2019-11-29 13:10:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5c2b653e-ab77-4b65-9f51-c0652576268e 0xc003311940 0xc003311941}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0033119b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0033119d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-11-29 13:10:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.209: INFO: Pod "nginx-deployment-55fb7cb77f-zrfw4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-zrfw4,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-55fb7cb77f-zrfw4,UID:62ee55a2-d3c7-45a3-8e33-f9fb4695ee3e,ResourceVersion:13188,Generation:0,CreationTimestamp:2019-11-29 13:10:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5c2b653e-ab77-4b65-9f51-c0652576268e 0xc003311aa0 0xc003311aa1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003311b10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003311b30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-11-29 13:10:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.209: INFO: Pod "nginx-deployment-7b8c6f4498-27crf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-27crf,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-7b8c6f4498-27crf,UID:62752767-c840-4747-ae42-de1737773d52,ResourceVersion:13184,Generation:0,CreationTimestamp:2019-11-29 13:10:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c78a8ed2-625f-471d-99c4-33242710e2f5 0xc003311c00 0xc003311c01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003311c60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003311c80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-11-29 13:10:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.209: INFO: Pod "nginx-deployment-7b8c6f4498-2dsd6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-2dsd6,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-7b8c6f4498-2dsd6,UID:0c633b73-06a9-497a-9b80-a3ddc1d0abe1,ResourceVersion:13068,Generation:0,CreationTimestamp:2019-11-29 13:10:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.30/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c78a8ed2-625f-471d-99c4-33242710e2f5 0xc003311d50 0xc003311d51}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003311db0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003311dd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:100.64.1.30,StartTime:2019-11-29 13:10:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-29 13:10:46 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://fbbad2ddb14cc72cd8116cc6fce391785a9c000b5c697747dd040e8f09cec8bd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.209: INFO: Pod "nginx-deployment-7b8c6f4498-2qc8b" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-2qc8b,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-7b8c6f4498-2qc8b,UID:b666a99b-b7f9-4a42-bf06-c522a19320bc,ResourceVersion:13049,Generation:0,CreationTimestamp:2019-11-29 13:10:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.125/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c78a8ed2-625f-471d-99c4-33242710e2f5 0xc003311ec0 0xc003311ec1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003311f20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003311f40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:100.64.0.125,StartTime:2019-11-29 13:10:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-29 13:10:45 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://29a4b4b6a03d871c0c4270db684ea7c429cf3fbe69dd9257e28d4367707cbb76}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.210: INFO: Pod "nginx-deployment-7b8c6f4498-856wp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-856wp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-7b8c6f4498-856wp,UID:772875bf-0b10-4425-86b0-d3125286126f,ResourceVersion:13058,Generation:0,CreationTimestamp:2019-11-29 13:10:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.128/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c78a8ed2-625f-471d-99c4-33242710e2f5 0xc003796020 0xc003796021}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003796080} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037960a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:100.64.0.128,StartTime:2019-11-29 13:10:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-29 13:10:46 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c89fc41cfeed2f81035220ffee122d7c8ca2172021ff6a49286129d33167f7d2}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.210: INFO: Pod "nginx-deployment-7b8c6f4498-chdnp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-chdnp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-7b8c6f4498-chdnp,UID:2335cd84-e01c-4473-a90c-1e6cd8520bbd,ResourceVersion:13191,Generation:0,CreationTimestamp:2019-11-29 13:10:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.134/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c78a8ed2-625f-471d-99c4-33242710e2f5 0xc003796180 0xc003796181}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037961e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003796200}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:,StartTime:2019-11-29 13:10:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.210: INFO: Pod "nginx-deployment-7b8c6f4498-cw7wv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-cw7wv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-7b8c6f4498-cw7wv,UID:76b83f07-1ac2-4235-9f0a-381b440793c4,ResourceVersion:13179,Generation:0,CreationTimestamp:2019-11-29 13:10:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c78a8ed2-625f-471d-99c4-33242710e2f5 0xc0037962c0 0xc0037962c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003796320} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003796340}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-11-29 13:10:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.210: INFO: Pod "nginx-deployment-7b8c6f4498-dmn66" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-dmn66,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-7b8c6f4498-dmn66,UID:aeb184e5-84d8-4ca3-ab4f-7896ea14c957,ResourceVersion:13052,Generation:0,CreationTimestamp:2019-11-29 13:10:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.126/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c78a8ed2-625f-471d-99c4-33242710e2f5 0xc003796410 0xc003796411}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003796470} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003796490}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:100.64.0.126,StartTime:2019-11-29 13:10:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-29 13:10:46 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://9d36c97e370287964f406d2ca22577ad80b57d5b74b7df888732de3868e8d0b9}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.211: INFO: Pod "nginx-deployment-7b8c6f4498-gskzs" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-gskzs,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-7b8c6f4498-gskzs,UID:b9c7a1e7-2391-4d4a-93ed-269718a8a58e,ResourceVersion:13071,Generation:0,CreationTimestamp:2019-11-29 13:10:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.32/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c78a8ed2-625f-471d-99c4-33242710e2f5 0xc003796570 0xc003796571}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037965d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037965f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:100.64.1.32,StartTime:2019-11-29 13:10:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-29 13:10:46 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://1aab97e5f25f9a714d09c4f4bb63f7b137d24e7640a08655637aa4481863097e}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.211: INFO: Pod "nginx-deployment-7b8c6f4498-hjkwg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-hjkwg,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-7b8c6f4498-hjkwg,UID:dc7d829c-9d05-4e1d-a8a9-d7b0cf7899ce,ResourceVersion:13197,Generation:0,CreationTimestamp:2019-11-29 13:10:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.1.36/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c78a8ed2-625f-471d-99c4-33242710e2f5 0xc0037966d0 0xc0037966d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003796730} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003796750}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-11-29 13:10:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.211: INFO: Pod "nginx-deployment-7b8c6f4498-hrtrp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-hrtrp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-7b8c6f4498-hrtrp,UID:b8baeb1b-2e05-4710-ab3a-7c9f488aa827,ResourceVersion:13136,Generation:0,CreationTimestamp:2019-11-29 13:10:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c78a8ed2-625f-471d-99c4-33242710e2f5 0xc003796810 0xc003796811}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003796870} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003796890}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-11-29 13:10:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.211: INFO: Pod "nginx-deployment-7b8c6f4498-klv9c" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-klv9c,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-7b8c6f4498-klv9c,UID:8d97d740-606b-40c9-9b9e-ab5878dfae58,ResourceVersion:13181,Generation:0,CreationTimestamp:2019-11-29 13:10:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c78a8ed2-625f-471d-99c4-33242710e2f5 0xc003796950 0xc003796951}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037969b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037969d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-11-29 13:10:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.211: INFO: Pod "nginx-deployment-7b8c6f4498-n2bsq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-n2bsq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-7b8c6f4498-n2bsq,UID:8bfed7ba-0aef-47f6-9b1a-4a2497f5b1ef,ResourceVersion:13201,Generation:0,CreationTimestamp:2019-11-29 13:10:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.139/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c78a8ed2-625f-471d-99c4-33242710e2f5 0xc003796aa0 0xc003796aa1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003796b00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003796b20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:,StartTime:2019-11-29 13:10:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.211: INFO: Pod "nginx-deployment-7b8c6f4498-s726l" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-s726l,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-7b8c6f4498-s726l,UID:c78e58cf-63dd-42a2-bb20-0dc3a2e3d751,ResourceVersion:13200,Generation:0,CreationTimestamp:2019-11-29 13:10:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.138/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c78a8ed2-625f-471d-99c4-33242710e2f5 0xc003796bf0 0xc003796bf1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003796c50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003796c70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:,StartTime:2019-11-29 13:10:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.212: INFO: Pod "nginx-deployment-7b8c6f4498-sjtz2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-sjtz2,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-7b8c6f4498-sjtz2,UID:d2c6d3bc-26de-44b2-9871-94dd86246d24,ResourceVersion:13061,Generation:0,CreationTimestamp:2019-11-29 13:10:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.130/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c78a8ed2-625f-471d-99c4-33242710e2f5 0xc003796d40 0xc003796d41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003796da0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003796dc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:100.64.0.130,StartTime:2019-11-29 13:10:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-29 13:10:46 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://b5628cd4216287d4841bf72d29144b67cb80ea1a4376f6584932f840843d2c3f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.212: INFO: Pod "nginx-deployment-7b8c6f4498-svddg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-svddg,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-7b8c6f4498-svddg,UID:fc52b5e4-c9e7-46ed-b102-8513cc43b594,ResourceVersion:13180,Generation:0,CreationTimestamp:2019-11-29 13:10:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c78a8ed2-625f-471d-99c4-33242710e2f5 0xc003796e90 0xc003796e91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003796ef0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003796f10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-11-29 13:10:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.212: INFO: Pod "nginx-deployment-7b8c6f4498-tdfsz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-tdfsz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-7b8c6f4498-tdfsz,UID:ef77158a-6f01-4bc5-a6c8-59f720112948,ResourceVersion:13171,Generation:0,CreationTimestamp:2019-11-29 13:10:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c78a8ed2-625f-471d-99c4-33242710e2f5 0xc003796fd0 0xc003796fd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003797030} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003797050}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-11-29 13:10:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.212: INFO: Pod "nginx-deployment-7b8c6f4498-tzpcg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-tzpcg,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-7b8c6f4498-tzpcg,UID:e150ffd7-db2d-4a89-9daf-46490ee62ec2,ResourceVersion:13183,Generation:0,CreationTimestamp:2019-11-29 13:10:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c78a8ed2-625f-471d-99c4-33242710e2f5 0xc003797110 0xc003797111}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003797170} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003797190}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.11,PodIP:,StartTime:2019-11-29 13:10:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.212: INFO: Pod "nginx-deployment-7b8c6f4498-wwdhn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-wwdhn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-7b8c6f4498-wwdhn,UID:39dbad8b-b5d4-4ab3-93ae-5d1766c1347e,ResourceVersion:13043,Generation:0,CreationTimestamp:2019-11-29 13:10:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.129/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c78a8ed2-625f-471d-99c4-33242710e2f5 0xc003797260 0xc003797261}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0037972c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0037972e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:100.64.0.129,StartTime:2019-11-29 13:10:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-29 13:10:46 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://f2e59e376cdb64c4cf5d53fb6ff63aa11caee13bce954b0ad3440936151c7da3}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.212: INFO: Pod "nginx-deployment-7b8c6f4498-x987d" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-x987d,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-7b8c6f4498-x987d,UID:25498fb8-08e7-4239-91ee-95f3654ddaf4,ResourceVersion:13192,Generation:0,CreationTimestamp:2019-11-29 13:10:50 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.135/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c78a8ed2-625f-471d-99c4-33242710e2f5 0xc0037973c0 0xc0037973c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003797420} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003797440}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:50 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:,StartTime:2019-11-29 13:10:50 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Nov 29 13:10:52.212: INFO: Pod "nginx-deployment-7b8c6f4498-xc2bn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xc2bn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8787,SelfLink:/api/v1/namespaces/deployment-8787/pods/nginx-deployment-7b8c6f4498-xc2bn,UID:e6e02014-166d-4ab3-91ed-bb5b9379b511,ResourceVersion:13055,Generation:0,CreationTimestamp:2019-11-29 13:10:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.127/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 c78a8ed2-625f-471d-99c4-33242710e2f5 0xc003797510 0xc003797511}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wnplb {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wnplb,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-wnplb true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003797570} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003797590}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:44 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:10:44 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:100.64.0.127,StartTime:2019-11-29 13:10:44 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-11-29 13:10:46 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://193321effb62fc0175cb51d2bd0983b870e2687dffdc24898501822e33a3d61c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:10:52.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8787" for this suite.
Nov 29 13:10:58.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:10:58.383: INFO: namespace deployment-8787 deletion completed in 6.166853371s
•SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:10:58.384: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-7141
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-7141
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7141 to expose endpoints map[]
Nov 29 13:10:58.539: INFO: Get endpoints failed (2.884094ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Nov 29 13:10:59.543: INFO: successfully validated that service multi-endpoint-test in namespace services-7141 exposes endpoints map[] (1.006602667s elapsed)
STEP: Creating pod pod1 in namespace services-7141
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7141 to expose endpoints map[pod1:[100]]
Nov 29 13:11:02.578: INFO: successfully validated that service multi-endpoint-test in namespace services-7141 exposes endpoints map[pod1:[100]] (3.029322955s elapsed)
STEP: Creating pod pod2 in namespace services-7141
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7141 to expose endpoints map[pod1:[100] pod2:[101]]
Nov 29 13:11:04.611: INFO: successfully validated that service multi-endpoint-test in namespace services-7141 exposes endpoints map[pod1:[100] pod2:[101]] (2.029261317s elapsed)
STEP: Deleting pod pod1 in namespace services-7141
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7141 to expose endpoints map[pod2:[101]]
Nov 29 13:11:04.623: INFO: successfully validated that service multi-endpoint-test in namespace services-7141 exposes endpoints map[pod2:[101]] (7.573294ms elapsed)
STEP: Deleting pod pod2 in namespace services-7141
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7141 to expose endpoints map[]
Nov 29 13:11:04.632: INFO: successfully validated that service multi-endpoint-test in namespace services-7141 exposes endpoints map[] (3.345833ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:11:04.646: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7141" for this suite.
Nov 29 13:11:26.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:11:26.780: INFO: namespace services-7141 deletion completed in 22.128368077s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:11:26.780: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5001
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:11:30.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5001" for this suite.
Nov 29 13:11:36.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:11:37.087: INFO: namespace kubelet-test-5001 deletion completed in 6.136039616s
•SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:11:37.087: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-3528
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 29 13:11:39.251: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:11:39.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3528" for this suite.
Nov 29 13:11:45.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:11:45.422: INFO: namespace container-runtime-3528 deletion completed in 6.154283761s
•SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:11:45.422: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-148
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Nov 29 13:11:45.570: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-148'
Nov 29 13:11:45.751: INFO: stderr: ""
Nov 29 13:11:45.751: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Nov 29 13:11:45.754: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pods e2e-test-nginx-pod --namespace=kubectl-148'
Nov 29 13:11:58.906: INFO: stderr: ""
Nov 29 13:11:58.906: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:11:58.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-148" for this suite.
Nov 29 13:12:04.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:12:05.055: INFO: namespace kubectl-148 deletion completed in 6.143568983s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:12:05.055: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7187
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 29 13:12:23.217: INFO: Container started at 2019-11-29 13:12:06 +0000 UTC, pod became ready at 2019-11-29 13:12:21 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:12:23.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7187" for this suite.
Nov 29 13:12:45.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:12:45.365: INFO: namespace container-probe-7187 deletion completed in 22.142892748s
•SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:12:45.365: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3502
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 29 13:12:45.513: INFO: Waiting up to 5m0s for pod "downwardapi-volume-723500d0-4a33-43ae-ba51-e69efdc979c3" in namespace "projected-3502" to be "success or failure"
Nov 29 13:12:45.516: INFO: Pod "downwardapi-volume-723500d0-4a33-43ae-ba51-e69efdc979c3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.763752ms
Nov 29 13:12:47.521: INFO: Pod "downwardapi-volume-723500d0-4a33-43ae-ba51-e69efdc979c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007634395s
STEP: Saw pod success
Nov 29 13:12:47.521: INFO: Pod "downwardapi-volume-723500d0-4a33-43ae-ba51-e69efdc979c3" satisfied condition "success or failure"
Nov 29 13:12:47.525: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod downwardapi-volume-723500d0-4a33-43ae-ba51-e69efdc979c3 container client-container: <nil>
STEP: delete the pod
Nov 29 13:12:47.549: INFO: Waiting for pod downwardapi-volume-723500d0-4a33-43ae-ba51-e69efdc979c3 to disappear
Nov 29 13:12:47.552: INFO: Pod downwardapi-volume-723500d0-4a33-43ae-ba51-e69efdc979c3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:12:47.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3502" for this suite.
Nov 29 13:12:53.569: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:12:53.691: INFO: namespace projected-3502 deletion completed in 6.133972197s
•SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:12:53.691: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8116
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-b58b7825-cfa5-48fd-9ad1-64ace8bcc44a
STEP: Creating a pod to test consume secrets
Nov 29 13:12:53.853: INFO: Waiting up to 5m0s for pod "pod-secrets-88b02ec4-50e5-4f49-979d-da5197844b8b" in namespace "secrets-8116" to be "success or failure"
Nov 29 13:12:53.857: INFO: Pod "pod-secrets-88b02ec4-50e5-4f49-979d-da5197844b8b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.04264ms
Nov 29 13:12:55.862: INFO: Pod "pod-secrets-88b02ec4-50e5-4f49-979d-da5197844b8b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009058253s
Nov 29 13:12:57.868: INFO: Pod "pod-secrets-88b02ec4-50e5-4f49-979d-da5197844b8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015181242s
STEP: Saw pod success
Nov 29 13:12:57.868: INFO: Pod "pod-secrets-88b02ec4-50e5-4f49-979d-da5197844b8b" satisfied condition "success or failure"
Nov 29 13:12:57.871: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-secrets-88b02ec4-50e5-4f49-979d-da5197844b8b container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 13:12:57.890: INFO: Waiting for pod pod-secrets-88b02ec4-50e5-4f49-979d-da5197844b8b to disappear
Nov 29 13:12:57.894: INFO: Pod pod-secrets-88b02ec4-50e5-4f49-979d-da5197844b8b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:12:57.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8116" for this suite.
Nov 29 13:13:03.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:13:04.041: INFO: namespace secrets-8116 deletion completed in 6.141605179s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:13:04.041: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8509
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Nov 29 13:13:04.732: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1129 13:13:04.732270    4828 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:13:04.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8509" for this suite.
Nov 29 13:13:10.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:13:10.865: INFO: namespace gc-8509 deletion completed in 6.129508701s
•SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:13:10.866: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4864
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4864
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-4864
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4864
Nov 29 13:13:11.043: INFO: Found 0 stateful pods, waiting for 1
Nov 29 13:13:21.048: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Nov 29 13:13:21.051: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 29 13:13:22.054: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov 29 13:13:22.054: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 29 13:13:22.054: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 29 13:13:22.059: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Nov 29 13:13:32.064: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 13:13:32.064: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 13:13:32.078: INFO: POD   NODE                                                      PHASE    GRACE  CONDITIONS
Nov 29 13:13:32.078: INFO: ss-0  shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:22 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:11 +0000 UTC  }]
Nov 29 13:13:32.078: INFO: 
Nov 29 13:13:32.078: INFO: StatefulSet ss has not reached scale 3, at 1
Nov 29 13:13:33.084: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996503314s
Nov 29 13:13:34.090: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.990826473s
Nov 29 13:13:35.095: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.984743771s
Nov 29 13:13:36.100: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979549197s
Nov 29 13:13:37.105: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.974669825s
Nov 29 13:13:38.109: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.969821221s
Nov 29 13:13:39.115: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.965020461s
Nov 29 13:13:40.120: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.959618842s
Nov 29 13:13:41.125: INFO: Verifying statefulset ss doesn't scale past 3 for another 954.332819ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4864
Nov 29 13:13:42.130: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:13:42.613: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Nov 29 13:13:42.614: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 29 13:13:42.614: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov 29 13:13:42.614: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:13:43.095: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 29 13:13:43.095: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 29 13:13:43.095: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov 29 13:13:43.095: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:13:43.564: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Nov 29 13:13:43.564: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 29 13:13:43.564: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov 29 13:13:43.569: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 13:13:43.569: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 13:13:43.569: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Nov 29 13:13:43.573: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 29 13:13:44.046: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov 29 13:13:44.046: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 29 13:13:44.046: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 29 13:13:44.046: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 29 13:13:44.519: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov 29 13:13:44.519: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 29 13:13:44.519: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 29 13:13:44.520: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 29 13:13:45.006: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov 29 13:13:45.007: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 29 13:13:45.007: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 29 13:13:45.007: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 13:13:45.013: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Nov 29 13:13:55.021: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 13:13:55.021: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 13:13:55.021: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Nov 29 13:13:55.033: INFO: POD   NODE                                                      PHASE    GRACE  CONDITIONS
Nov 29 13:13:55.033: INFO: ss-0  shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:11 +0000 UTC  }]
Nov 29 13:13:55.033: INFO: ss-1  shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  }]
Nov 29 13:13:55.033: INFO: ss-2  shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  }]
Nov 29 13:13:55.033: INFO: 
Nov 29 13:13:55.033: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 29 13:13:56.039: INFO: POD   NODE                                                      PHASE    GRACE  CONDITIONS
Nov 29 13:13:56.039: INFO: ss-0  shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:11 +0000 UTC  }]
Nov 29 13:13:56.039: INFO: ss-1  shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  }]
Nov 29 13:13:56.039: INFO: ss-2  shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  }]
Nov 29 13:13:56.039: INFO: 
Nov 29 13:13:56.039: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 29 13:13:57.044: INFO: POD   NODE                                                      PHASE    GRACE  CONDITIONS
Nov 29 13:13:57.045: INFO: ss-0  shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:11 +0000 UTC  }]
Nov 29 13:13:57.045: INFO: ss-1  shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  }]
Nov 29 13:13:57.045: INFO: ss-2  shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  }]
Nov 29 13:13:57.045: INFO: 
Nov 29 13:13:57.045: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 29 13:13:58.050: INFO: POD   NODE                                                      PHASE    GRACE  CONDITIONS
Nov 29 13:13:58.050: INFO: ss-0  shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:11 +0000 UTC  }]
Nov 29 13:13:58.050: INFO: ss-1  shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  }]
Nov 29 13:13:58.050: INFO: ss-2  shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  }]
Nov 29 13:13:58.050: INFO: 
Nov 29 13:13:58.050: INFO: StatefulSet ss has not reached scale 0, at 3
Nov 29 13:13:59.056: INFO: POD   NODE                                                      PHASE    GRACE  CONDITIONS
Nov 29 13:13:59.056: INFO: ss-2  shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  }]
Nov 29 13:13:59.056: INFO: 
Nov 29 13:13:59.056: INFO: StatefulSet ss has not reached scale 0, at 1
Nov 29 13:14:00.061: INFO: POD   NODE                                                      PHASE    GRACE  CONDITIONS
Nov 29 13:14:00.061: INFO: ss-2  shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  }]
Nov 29 13:14:00.061: INFO: 
Nov 29 13:14:00.061: INFO: StatefulSet ss has not reached scale 0, at 1
Nov 29 13:14:01.067: INFO: POD   NODE                                                      PHASE    GRACE  CONDITIONS
Nov 29 13:14:01.067: INFO: ss-2  shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  }]
Nov 29 13:14:01.067: INFO: 
Nov 29 13:14:01.067: INFO: StatefulSet ss has not reached scale 0, at 1
Nov 29 13:14:02.072: INFO: POD   NODE                                                      PHASE    GRACE  CONDITIONS
Nov 29 13:14:02.072: INFO: ss-2  shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  }]
Nov 29 13:14:02.072: INFO: 
Nov 29 13:14:02.072: INFO: StatefulSet ss has not reached scale 0, at 1
Nov 29 13:14:03.077: INFO: POD   NODE                                                      PHASE    GRACE  CONDITIONS
Nov 29 13:14:03.077: INFO: ss-2  shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  }]
Nov 29 13:14:03.077: INFO: 
Nov 29 13:14:03.077: INFO: StatefulSet ss has not reached scale 0, at 1
Nov 29 13:14:04.082: INFO: POD   NODE                                                      PHASE    GRACE  CONDITIONS
Nov 29 13:14:04.082: INFO: ss-2  shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:45 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:13:32 +0000 UTC  }]
Nov 29 13:14:04.082: INFO: 
Nov 29 13:14:04.082: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4864
Nov 29 13:14:05.088: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:14:05.316: INFO: rc: 1
Nov 29 13:14:05.316: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc002af3440 exit status 1 <nil> <nil> true [0xc00233f238 0xc00233f260 0xc00233f2b0] [0xc00233f238 0xc00233f260 0xc00233f2b0] [0xc00233f250 0xc00233f298] [0xba6c10 0xba6c10] 0xc001ce7da0 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Nov 29 13:14:15.316: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:14:15.395: INFO: rc: 1
Nov 29 13:14:15.395: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002c7e2d0 exit status 1 <nil> <nil> true [0xc0010831a8 0xc0010831e8 0xc001083218] [0xc0010831a8 0xc0010831e8 0xc001083218] [0xc0010831e0 0xc0010831f8] [0xba6c10 0xba6c10] 0xc0023914a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:14:25.395: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:14:25.497: INFO: rc: 1
Nov 29 13:14:25.497: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0011ada40 exit status 1 <nil> <nil> true [0xc00289aa68 0xc00289aa80 0xc00289aaa0] [0xc00289aa68 0xc00289aa80 0xc00289aaa0] [0xc00289aa78 0xc00289aa90] [0xba6c10 0xba6c10] 0xc002236f60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:14:35.498: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:14:35.759: INFO: rc: 1
Nov 29 13:14:35.759: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002c62d80 exit status 1 <nil> <nil> true [0xc002b54da8 0xc002b54dc0 0xc002b54dd8] [0xc002b54da8 0xc002b54dc0 0xc002b54dd8] [0xc002b54db8 0xc002b54dd0] [0xba6c10 0xba6c10] 0xc001fcc7e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:14:45.760: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:14:45.835: INFO: rc: 1
Nov 29 13:14:45.835: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002c7e8d0 exit status 1 <nil> <nil> true [0xc001083230 0xc001083248 0xc001083290] [0xc001083230 0xc001083248 0xc001083290] [0xc001083240 0xc001083278] [0xba6c10 0xba6c10] 0xc001f0e6c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:14:55.835: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:14:55.917: INFO: rc: 1
Nov 29 13:14:55.917: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002c63500 exit status 1 <nil> <nil> true [0xc002b54de0 0xc002b54df8 0xc002b54e10] [0xc002b54de0 0xc002b54df8 0xc002b54e10] [0xc002b54df0 0xc002b54e08] [0xba6c10 0xba6c10] 0xc00213ea80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:15:05.917: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:15:06.197: INFO: rc: 1
Nov 29 13:15:06.197: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002396630 exit status 1 <nil> <nil> true [0xc000213128 0xc000213210 0xc000213370] [0xc000213128 0xc000213210 0xc000213370] [0xc0002131e0 0xc000213338] [0xba6c10 0xba6c10] 0xc001fcc900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:15:16.197: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:15:16.269: INFO: rc: 1
Nov 29 13:15:16.269: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0038005d0 exit status 1 <nil> <nil> true [0xc0003a22f0 0xc0003a27a0 0xc0003a2f58] [0xc0003a22f0 0xc0003a27a0 0xc0003a2f58] [0xc0003a2680 0xc0003a28e0] [0xba6c10 0xba6c10] 0xc002174de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:15:26.269: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:15:26.345: INFO: rc: 1
Nov 29 13:15:26.345: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c2c6c0 exit status 1 <nil> <nil> true [0xc0013ec020 0xc0013ec120 0xc0013ec218] [0xc0013ec020 0xc0013ec120 0xc0013ec218] [0xc0013ec0c8 0xc0013ec150] [0xba6c10 0xba6c10] 0xc001ef7680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:15:36.345: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:15:36.601: INFO: rc: 1
Nov 29 13:15:36.601: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0028be630 exit status 1 <nil> <nil> true [0xc000010990 0xc000010ef0 0xc0000116a8] [0xc000010990 0xc000010ef0 0xc0000116a8] [0xc000010e68 0xc0000112e8] [0xba6c10 0xba6c10] 0xc001a25560 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:15:46.601: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:15:46.731: INFO: rc: 1
Nov 29 13:15:46.732: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002396c30 exit status 1 <nil> <nil> true [0xc000213398 0xc0002134b8 0xc000213748] [0xc000213398 0xc0002134b8 0xc000213748] [0xc000213430 0xc0002136f0] [0xba6c10 0xba6c10] 0xc002391320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:15:56.732: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:15:56.997: INFO: rc: 1
Nov 29 13:15:56.997: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c2cd20 exit status 1 <nil> <nil> true [0xc0013ec350 0xc0013ec650 0xc0013ec7f8] [0xc0013ec350 0xc0013ec650 0xc0013ec7f8] [0xc0013ec548 0xc0013ec758] [0xba6c10 0xba6c10] 0xc0003544e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:16:06.998: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:16:07.073: INFO: rc: 1
Nov 29 13:16:07.073: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0028bec60 exit status 1 <nil> <nil> true [0xc000011848 0xc00289a000 0xc00289a030] [0xc000011848 0xc00289a000 0xc00289a030] [0xc000011fe0 0xc00289a020] [0xba6c10 0xba6c10] 0xc001e86fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:16:17.074: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:16:17.146: INFO: rc: 1
Nov 29 13:16:17.147: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0028bf290 exit status 1 <nil> <nil> true [0xc00289a040 0xc00289a088 0xc00289a0e0] [0xc00289a040 0xc00289a088 0xc00289a0e0] [0xc00289a080 0xc00289a0c0] [0xba6c10 0xba6c10] 0xc002196660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:16:27.147: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:16:27.227: INFO: rc: 1
Nov 29 13:16:27.227: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023972c0 exit status 1 <nil> <nil> true [0xc000213820 0xc000213af8 0xc000213d38] [0xc000213820 0xc000213af8 0xc000213d38] [0xc000213ab0 0xc000213ce0] [0xba6c10 0xba6c10] 0xc0022dd380 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:16:37.227: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:16:37.506: INFO: rc: 1
Nov 29 13:16:37.507: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0028bf8c0 exit status 1 <nil> <nil> true [0xc00289a0f0 0xc00289a120 0xc00289a138] [0xc00289a0f0 0xc00289a120 0xc00289a138] [0xc00289a110 0xc00289a130] [0xba6c10 0xba6c10] 0xc001df0de0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:16:47.507: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:16:47.604: INFO: rc: 1
Nov 29 13:16:47.604: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c2d3b0 exit status 1 <nil> <nil> true [0xc0013ec948 0xc0013ecbd8 0xc0013ecd48] [0xc0013ec948 0xc0013ecbd8 0xc0013ecd48] [0xc0013ecaf8 0xc0013ecc38] [0xba6c10 0xba6c10] 0xc001afd020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:16:57.604: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:16:57.716: INFO: rc: 1
Nov 29 13:16:57.717: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc003800cc0 exit status 1 <nil> <nil> true [0xc0003a35f8 0xc0003a3ba8 0xc0003a3ec8] [0xc0003a35f8 0xc0003a3ba8 0xc0003a3ec8] [0xc0003a3b10 0xc0003a3bc8] [0xba6c10 0xba6c10] 0xc00218f980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:17:07.717: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:17:07.790: INFO: rc: 1
Nov 29 13:17:07.790: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0028be690 exit status 1 <nil> <nil> true [0xc000010990 0xc000010ef0 0xc0000116a8] [0xc000010990 0xc000010ef0 0xc0000116a8] [0xc000010e68 0xc0000112e8] [0xba6c10 0xba6c10] 0xc002197800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:17:17.791: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:17:17.918: INFO: rc: 1
Nov 29 13:17:17.918: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002396660 exit status 1 <nil> <nil> true [0xc00289a000 0xc00289a030 0xc00289a080] [0xc00289a000 0xc00289a030 0xc00289a080] [0xc00289a020 0xc00289a060] [0xba6c10 0xba6c10] 0xc001e87ce0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:17:27.919: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:17:27.998: INFO: rc: 1
Nov 29 13:17:27.998: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002396c90 exit status 1 <nil> <nil> true [0xc00289a088 0xc00289a0e0 0xc00289a110] [0xc00289a088 0xc00289a0e0 0xc00289a110] [0xc00289a0c0 0xc00289a100] [0xba6c10 0xba6c10] 0xc000355620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:17:37.998: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:17:38.071: INFO: rc: 1
Nov 29 13:17:38.071: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc002397350 exit status 1 <nil> <nil> true [0xc00289a120 0xc00289a138 0xc00289a150] [0xc00289a120 0xc00289a138 0xc00289a150] [0xc00289a130 0xc00289a148] [0xba6c10 0xba6c10] 0xc001a24060 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:17:48.072: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:17:48.203: INFO: rc: 1
Nov 29 13:17:48.203: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c2c6f0 exit status 1 <nil> <nil> true [0xc000213128 0xc000213210 0xc000213370] [0xc000213128 0xc000213210 0xc000213370] [0xc0002131e0 0xc000213338] [0xba6c10 0xba6c10] 0xc001ef6840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:17:58.203: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:17:58.284: INFO: rc: 1
Nov 29 13:17:58.284: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0023979b0 exit status 1 <nil> <nil> true [0xc00289a168 0xc00289a190 0xc00289a1a8] [0xc00289a168 0xc00289a190 0xc00289a1a8] [0xc00289a188 0xc00289a1a0] [0xba6c10 0xba6c10] 0xc002174b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:18:08.284: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:18:08.357: INFO: rc: 1
Nov 29 13:18:08.357: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0028becf0 exit status 1 <nil> <nil> true [0xc000011848 0xc0003a22f0 0xc0003a27a0] [0xc000011848 0xc0003a22f0 0xc0003a27a0] [0xc000011fe0 0xc0003a2680] [0xba6c10 0xba6c10] 0xc002390840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:18:18.358: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:18:18.494: INFO: rc: 1
Nov 29 13:18:18.494: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c2cd80 exit status 1 <nil> <nil> true [0xc000213398 0xc0002134b8 0xc000213748] [0xc000213398 0xc0002134b8 0xc000213748] [0xc000213430 0xc0002136f0] [0xba6c10 0xba6c10] 0xc001fcc7e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:18:28.495: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:18:28.569: INFO: rc: 1
Nov 29 13:18:28.569: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c2d3e0 exit status 1 <nil> <nil> true [0xc000213820 0xc000213af8 0xc000213d38] [0xc000213820 0xc000213af8 0xc000213d38] [0xc000213ab0 0xc000213ce0] [0xba6c10 0xba6c10] 0xc001df0540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:18:38.569: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:18:38.671: INFO: rc: 1
Nov 29 13:18:38.671: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0028bf320 exit status 1 <nil> <nil> true [0xc0003a2830 0xc0013ec020 0xc0013ec120] [0xc0003a2830 0xc0013ec020 0xc0013ec120] [0xc0003a2f58 0xc0013ec0c8] [0xba6c10 0xba6c10] 0xc002391ec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:18:48.672: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:18:48.968: INFO: rc: 1
Nov 29 13:18:48.968: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc000c2da10 exit status 1 <nil> <nil> true [0xc000213e00 0xc000213e68 0xc000213f78] [0xc000213e00 0xc000213e68 0xc000213f78] [0xc000213e40 0xc000213f20] [0xba6c10 0xba6c10] 0xc00218fc80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:18:58.968: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:18:59.045: INFO: rc: 1
Nov 29 13:18:59.045: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl [kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-2" not found
 [] <nil> 0xc0028be630 exit status 1 <nil> <nil> true [0xc0003a2460 0xc0003a2830 0xc0003a3540] [0xc0003a2460 0xc0003a2830 0xc0003a3540] [0xc0003a27a0 0xc0003a2f58] [0xba6c10 0xba6c10] 0xc001df1080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-2" not found

error:
exit status 1
Nov 29 13:19:09.045: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4864 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:19:09.119: INFO: rc: 1
Nov 29 13:19:09.119: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: 
Nov 29 13:19:09.119: INFO: Scaling statefulset ss to 0
Nov 29 13:19:09.141: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Nov 29 13:19:09.145: INFO: Deleting all statefulset in ns statefulset-4864
Nov 29 13:19:09.149: INFO: Scaling statefulset ss to 0
Nov 29 13:19:09.161: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 13:19:09.164: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:19:09.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4864" for this suite.
Nov 29 13:19:15.194: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:19:15.314: INFO: namespace statefulset-4864 deletion completed in 6.131708407s

• [SLOW TEST:364.448 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:19:15.314: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-1148
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-56942040-156c-4b02-b505-a6dfd79a1916
STEP: Creating a pod to test consume secrets
Nov 29 13:19:15.474: INFO: Waiting up to 5m0s for pod "pod-secrets-038c3a62-64e7-4157-b5f6-4ba9a8386255" in namespace "secrets-1148" to be "success or failure"
Nov 29 13:19:15.479: INFO: Pod "pod-secrets-038c3a62-64e7-4157-b5f6-4ba9a8386255": Phase="Pending", Reason="", readiness=false. Elapsed: 4.456032ms
Nov 29 13:19:17.484: INFO: Pod "pod-secrets-038c3a62-64e7-4157-b5f6-4ba9a8386255": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009648976s
STEP: Saw pod success
Nov 29 13:19:17.484: INFO: Pod "pod-secrets-038c3a62-64e7-4157-b5f6-4ba9a8386255" satisfied condition "success or failure"
Nov 29 13:19:17.489: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-secrets-038c3a62-64e7-4157-b5f6-4ba9a8386255 container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 13:19:17.513: INFO: Waiting for pod pod-secrets-038c3a62-64e7-4157-b5f6-4ba9a8386255 to disappear
Nov 29 13:19:17.516: INFO: Pod pod-secrets-038c3a62-64e7-4157-b5f6-4ba9a8386255 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:19:17.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1148" for this suite.
Nov 29 13:19:23.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:19:23.694: INFO: namespace secrets-1148 deletion completed in 6.173246039s
•
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:19:23.694: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-5523
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Nov 29 13:19:24.363: INFO: created pod pod-service-account-defaultsa
Nov 29 13:19:24.363: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Nov 29 13:19:24.370: INFO: created pod pod-service-account-mountsa
Nov 29 13:19:24.370: INFO: pod pod-service-account-mountsa service account token volume mount: true
Nov 29 13:19:24.377: INFO: created pod pod-service-account-nomountsa
Nov 29 13:19:24.377: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Nov 29 13:19:24.385: INFO: created pod pod-service-account-defaultsa-mountspec
Nov 29 13:19:24.385: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Nov 29 13:19:24.390: INFO: created pod pod-service-account-mountsa-mountspec
Nov 29 13:19:24.390: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Nov 29 13:19:24.405: INFO: created pod pod-service-account-nomountsa-mountspec
Nov 29 13:19:24.405: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Nov 29 13:19:24.411: INFO: created pod pod-service-account-defaultsa-nomountspec
Nov 29 13:19:24.411: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Nov 29 13:19:24.415: INFO: created pod pod-service-account-mountsa-nomountspec
Nov 29 13:19:24.415: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Nov 29 13:19:24.420: INFO: created pod pod-service-account-nomountsa-nomountspec
Nov 29 13:19:24.420: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:19:24.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5523" for this suite.
Nov 29 13:19:30.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:19:30.560: INFO: namespace svcaccounts-5523 deletion completed in 6.135960624s
•SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:19:30.561: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1089
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 29 13:19:30.712: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f7c7a074-5452-4dee-a919-0f51a482f080" in namespace "projected-1089" to be "success or failure"
Nov 29 13:19:30.715: INFO: Pod "downwardapi-volume-f7c7a074-5452-4dee-a919-0f51a482f080": Phase="Pending", Reason="", readiness=false. Elapsed: 3.077592ms
Nov 29 13:19:32.720: INFO: Pod "downwardapi-volume-f7c7a074-5452-4dee-a919-0f51a482f080": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007727037s
STEP: Saw pod success
Nov 29 13:19:32.720: INFO: Pod "downwardapi-volume-f7c7a074-5452-4dee-a919-0f51a482f080" satisfied condition "success or failure"
Nov 29 13:19:32.723: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod downwardapi-volume-f7c7a074-5452-4dee-a919-0f51a482f080 container client-container: <nil>
STEP: delete the pod
Nov 29 13:19:32.745: INFO: Waiting for pod downwardapi-volume-f7c7a074-5452-4dee-a919-0f51a482f080 to disappear
Nov 29 13:19:32.750: INFO: Pod downwardapi-volume-f7c7a074-5452-4dee-a919-0f51a482f080 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:19:32.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1089" for this suite.
Nov 29 13:19:38.766: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:19:38.932: INFO: namespace projected-1089 deletion completed in 6.177784956s
•SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:19:38.933: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7163
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Nov 29 13:19:41.149: INFO: Asynchronously running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Nov 29 13:19:51.432: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:19:51.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7163" for this suite.
Nov 29 13:19:57.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:19:57.574: INFO: namespace pods-7163 deletion completed in 6.134834851s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:19:57.575: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-5028
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-0ad00b4c-7deb-4fa4-b279-924d395c2fd5
STEP: Creating a pod to test consume configMaps
Nov 29 13:19:57.730: INFO: Waiting up to 5m0s for pod "pod-configmaps-5ded2302-6a56-4532-8188-b54021743597" in namespace "configmap-5028" to be "success or failure"
Nov 29 13:19:57.733: INFO: Pod "pod-configmaps-5ded2302-6a56-4532-8188-b54021743597": Phase="Pending", Reason="", readiness=false. Elapsed: 3.297994ms
Nov 29 13:19:59.738: INFO: Pod "pod-configmaps-5ded2302-6a56-4532-8188-b54021743597": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007976001s
STEP: Saw pod success
Nov 29 13:19:59.738: INFO: Pod "pod-configmaps-5ded2302-6a56-4532-8188-b54021743597" satisfied condition "success or failure"
Nov 29 13:19:59.741: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-configmaps-5ded2302-6a56-4532-8188-b54021743597 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 13:19:59.760: INFO: Waiting for pod pod-configmaps-5ded2302-6a56-4532-8188-b54021743597 to disappear
Nov 29 13:19:59.763: INFO: Pod pod-configmaps-5ded2302-6a56-4532-8188-b54021743597 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:19:59.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5028" for this suite.
Nov 29 13:20:05.780: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:20:05.939: INFO: namespace configmap-5028 deletion completed in 6.171487621s
•SSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:20:05.939: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-4215
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Nov 29 13:20:06.091: INFO: PodSpec: initContainers in spec.initContainers
Nov 29 13:20:50.502: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-77df030e-6030-40bc-99a2-85d9304bd44a", GenerateName:"", Namespace:"init-container-4215", SelfLink:"/api/v1/namespaces/init-container-4215/pods/pod-init-77df030e-6030-40bc-99a2-85d9304bd44a", UID:"db41d813-236c-42a2-8514-085164e9ff72", ResourceVersion:"15143", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63710630406, loc:(*time.Location)(0x7ed1a20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"91048182"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"100.64.0.163/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-c24s8", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0033c0a80), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-c24s8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-c24s8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-c24s8", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0029fdd38), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000a053e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0029fdfe0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002b66040)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002b66048), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002b6604c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630406, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630406, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630406, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630406, loc:(*time.Location)(0x7ed1a20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.250.0.10", PodIP:"100.64.0.163", StartTime:(*v1.Time)(0xc002ab4820), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000efda40)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000efdab0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://27d504cbb910daf055d04fdf95fb933826b77eb808dfffa2e9d5cacdddcef5d0"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002ab4860), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002ab4840), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:20:50.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4215" for this suite.
Nov 29 13:21:12.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:21:12.643: INFO: namespace init-container-4215 deletion completed in 22.135502132s
•SSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:21:12.643: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4639
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 29 13:21:15.321: INFO: Successfully updated pod "pod-update-c2ae08a2-5d85-463b-a3ac-03a19d48ba33"
STEP: verifying the updated pod is in kubernetes
Nov 29 13:21:15.327: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:21:15.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4639" for this suite.
Nov 29 13:21:37.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:21:37.463: INFO: namespace pods-4639 deletion completed in 22.130761367s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:21:37.463: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5601
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-5988d4d5-d8a2-4dbe-94b6-4beeae5429b0
STEP: Creating a pod to test consume secrets
Nov 29 13:21:37.619: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ce02a35e-6ab8-476f-83ab-7fc3af5a6abe" in namespace "projected-5601" to be "success or failure"
Nov 29 13:21:37.625: INFO: Pod "pod-projected-secrets-ce02a35e-6ab8-476f-83ab-7fc3af5a6abe": Phase="Pending", Reason="", readiness=false. Elapsed: 5.183181ms
Nov 29 13:21:39.630: INFO: Pod "pod-projected-secrets-ce02a35e-6ab8-476f-83ab-7fc3af5a6abe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010130791s
STEP: Saw pod success
Nov 29 13:21:39.630: INFO: Pod "pod-projected-secrets-ce02a35e-6ab8-476f-83ab-7fc3af5a6abe" satisfied condition "success or failure"
Nov 29 13:21:39.633: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-projected-secrets-ce02a35e-6ab8-476f-83ab-7fc3af5a6abe container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 13:21:39.652: INFO: Waiting for pod pod-projected-secrets-ce02a35e-6ab8-476f-83ab-7fc3af5a6abe to disappear
Nov 29 13:21:39.655: INFO: Pod pod-projected-secrets-ce02a35e-6ab8-476f-83ab-7fc3af5a6abe no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:21:39.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5601" for this suite.
Nov 29 13:21:45.671: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:21:45.789: INFO: namespace projected-5601 deletion completed in 6.129264175s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:21:45.789: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4390
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 29 13:21:45.953: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Nov 29 13:21:45.967: INFO: Number of nodes with available pods: 0
Nov 29 13:21:45.967: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:21:46.976: INFO: Number of nodes with available pods: 0
Nov 29 13:21:46.976: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:21:47.976: INFO: Number of nodes with available pods: 2
Nov 29 13:21:47.976: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Nov 29 13:21:48.008: INFO: Wrong image for pod: daemon-set-sc66l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:21:48.008: INFO: Wrong image for pod: daemon-set-tk86j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:21:49.016: INFO: Wrong image for pod: daemon-set-sc66l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:21:49.016: INFO: Wrong image for pod: daemon-set-tk86j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:21:50.016: INFO: Wrong image for pod: daemon-set-sc66l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:21:50.017: INFO: Wrong image for pod: daemon-set-tk86j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:21:51.017: INFO: Wrong image for pod: daemon-set-sc66l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:21:51.017: INFO: Pod daemon-set-sc66l is not available
Nov 29 13:21:51.017: INFO: Wrong image for pod: daemon-set-tk86j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:21:52.016: INFO: Wrong image for pod: daemon-set-sc66l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:21:52.017: INFO: Pod daemon-set-sc66l is not available
Nov 29 13:21:52.017: INFO: Wrong image for pod: daemon-set-tk86j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:21:53.016: INFO: Wrong image for pod: daemon-set-sc66l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:21:53.016: INFO: Pod daemon-set-sc66l is not available
Nov 29 13:21:53.016: INFO: Wrong image for pod: daemon-set-tk86j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:21:54.016: INFO: Wrong image for pod: daemon-set-sc66l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:21:54.016: INFO: Pod daemon-set-sc66l is not available
Nov 29 13:21:54.016: INFO: Wrong image for pod: daemon-set-tk86j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:21:55.016: INFO: Wrong image for pod: daemon-set-sc66l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:21:55.016: INFO: Pod daemon-set-sc66l is not available
Nov 29 13:21:55.016: INFO: Wrong image for pod: daemon-set-tk86j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:21:56.017: INFO: Wrong image for pod: daemon-set-sc66l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:21:56.017: INFO: Pod daemon-set-sc66l is not available
Nov 29 13:21:56.017: INFO: Wrong image for pod: daemon-set-tk86j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:21:57.016: INFO: Wrong image for pod: daemon-set-sc66l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:21:57.016: INFO: Pod daemon-set-sc66l is not available
Nov 29 13:21:57.016: INFO: Wrong image for pod: daemon-set-tk86j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:21:58.016: INFO: Wrong image for pod: daemon-set-sc66l. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:21:58.016: INFO: Pod daemon-set-sc66l is not available
Nov 29 13:21:58.016: INFO: Wrong image for pod: daemon-set-tk86j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:21:59.016: INFO: Pod daemon-set-pfkqn is not available
Nov 29 13:21:59.017: INFO: Wrong image for pod: daemon-set-tk86j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:22:00.017: INFO: Pod daemon-set-pfkqn is not available
Nov 29 13:22:00.017: INFO: Wrong image for pod: daemon-set-tk86j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:22:01.015: INFO: Wrong image for pod: daemon-set-tk86j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:22:02.017: INFO: Wrong image for pod: daemon-set-tk86j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:22:02.017: INFO: Pod daemon-set-tk86j is not available
Nov 29 13:22:03.016: INFO: Wrong image for pod: daemon-set-tk86j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:22:03.016: INFO: Pod daemon-set-tk86j is not available
Nov 29 13:22:04.016: INFO: Wrong image for pod: daemon-set-tk86j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:22:04.016: INFO: Pod daemon-set-tk86j is not available
Nov 29 13:22:05.016: INFO: Wrong image for pod: daemon-set-tk86j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:22:05.016: INFO: Pod daemon-set-tk86j is not available
Nov 29 13:22:06.016: INFO: Wrong image for pod: daemon-set-tk86j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:22:06.016: INFO: Pod daemon-set-tk86j is not available
Nov 29 13:22:07.017: INFO: Wrong image for pod: daemon-set-tk86j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:22:07.017: INFO: Pod daemon-set-tk86j is not available
Nov 29 13:22:08.016: INFO: Wrong image for pod: daemon-set-tk86j. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Nov 29 13:22:08.016: INFO: Pod daemon-set-tk86j is not available
Nov 29 13:22:09.016: INFO: Pod daemon-set-tqsl6 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Nov 29 13:22:09.028: INFO: Number of nodes with available pods: 1
Nov 29 13:22:09.028: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj is running more than one daemon pod
Nov 29 13:22:10.037: INFO: Number of nodes with available pods: 1
Nov 29 13:22:10.037: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj is running more than one daemon pod
Nov 29 13:22:11.038: INFO: Number of nodes with available pods: 2
Nov 29 13:22:11.038: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4390, will wait for the garbage collector to delete the pods
Nov 29 13:22:11.116: INFO: Deleting DaemonSet.extensions daemon-set took: 6.128231ms
Nov 29 13:22:13.317: INFO: Terminating DaemonSet.extensions daemon-set pods took: 2.200278159s
Nov 29 13:22:18.120: INFO: Number of nodes with available pods: 0
Nov 29 13:22:18.120: INFO: Number of running nodes: 0, number of available pods: 0
Nov 29 13:22:18.124: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4390/daemonsets","resourceVersion":"15455"},"items":null}

Nov 29 13:22:18.127: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4390/pods","resourceVersion":"15455"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:22:18.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4390" for this suite.
Nov 29 13:22:24.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:22:24.282: INFO: namespace daemonsets-4390 deletion completed in 6.139049513s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:22:24.283: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-9722
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 29 13:22:24.437: INFO: Pod name rollover-pod: Found 0 pods out of 1
Nov 29 13:22:29.442: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 29 13:22:29.443: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Nov 29 13:22:31.448: INFO: Creating deployment "test-rollover-deployment"
Nov 29 13:22:31.457: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Nov 29 13:22:33.464: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Nov 29 13:22:33.471: INFO: Ensure that both replica sets have 1 created replica
Nov 29 13:22:33.477: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Nov 29 13:22:33.486: INFO: Updating deployment test-rollover-deployment
Nov 29 13:22:33.486: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Nov 29 13:22:35.494: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Nov 29 13:22:35.501: INFO: Make sure deployment "test-rollover-deployment" is complete
Nov 29 13:22:35.509: INFO: all replica sets need to contain the pod-template-hash label
Nov 29 13:22:35.509: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630551, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630551, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630554, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630551, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 13:22:37.521: INFO: all replica sets need to contain the pod-template-hash label
Nov 29 13:22:37.521: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630551, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630551, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630554, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630551, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 13:22:39.517: INFO: all replica sets need to contain the pod-template-hash label
Nov 29 13:22:39.517: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630551, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630551, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630554, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630551, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 13:22:41.519: INFO: all replica sets need to contain the pod-template-hash label
Nov 29 13:22:41.519: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630551, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630551, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630554, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630551, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 13:22:43.518: INFO: all replica sets need to contain the pod-template-hash label
Nov 29 13:22:43.518: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630551, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630551, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630554, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710630551, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 13:22:45.517: INFO: 
Nov 29 13:22:45.517: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Nov 29 13:22:45.527: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-9722,SelfLink:/apis/apps/v1/namespaces/deployment-9722/deployments/test-rollover-deployment,UID:b9adbf79-0b98-4fba-80c4-fa0945fc8b58,ResourceVersion:15579,Generation:2,CreationTimestamp:2019-11-29 13:22:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-11-29 13:22:31 +0000 UTC 2019-11-29 13:22:31 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-11-29 13:22:44 +0000 UTC 2019-11-29 13:22:31 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Nov 29 13:22:45.531: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-9722,SelfLink:/apis/apps/v1/namespaces/deployment-9722/replicasets/test-rollover-deployment-854595fc44,UID:58d38d3b-448d-4dbe-a661-2b26e210b794,ResourceVersion:15572,Generation:2,CreationTimestamp:2019-11-29 13:22:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment b9adbf79-0b98-4fba-80c4-fa0945fc8b58 0xc0029ee007 0xc0029ee008}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Nov 29 13:22:45.531: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Nov 29 13:22:45.531: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-9722,SelfLink:/apis/apps/v1/namespaces/deployment-9722/replicasets/test-rollover-controller,UID:13baaf75-0a31-4579-9c68-d33033dfaa10,ResourceVersion:15578,Generation:2,CreationTimestamp:2019-11-29 13:22:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment b9adbf79-0b98-4fba-80c4-fa0945fc8b58 0xc001dc5f27 0xc001dc5f28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Nov 29 13:22:45.531: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-9722,SelfLink:/apis/apps/v1/namespaces/deployment-9722/replicasets/test-rollover-deployment-9b8b997cf,UID:5c78a25c-1f99-424c-be8f-70bc69b10fdf,ResourceVersion:15537,Generation:2,CreationTimestamp:2019-11-29 13:22:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment b9adbf79-0b98-4fba-80c4-fa0945fc8b58 0xc0029ee0d0 0xc0029ee0d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Nov 29 13:22:45.535: INFO: Pod "test-rollover-deployment-854595fc44-5rjcm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-5rjcm,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-9722,SelfLink:/api/v1/namespaces/deployment-9722/pods/test-rollover-deployment-854595fc44-5rjcm,UID:8c97d61f-0564-460e-a0b7-67d79c7f9d55,ResourceVersion:15546,Generation:0,CreationTimestamp:2019-11-29 13:22:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.170/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 58d38d3b-448d-4dbe-a661-2b26e210b794 0xc003859837 0xc003859838}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-pwqvf {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-pwqvf,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-pwqvf true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0038598a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0038598c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:22:33 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:22:34 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:22:34 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:22:33 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:100.64.0.170,StartTime:2019-11-29 13:22:33 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-11-29 13:22:34 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://767d071c043aea111c5902bc88e0b76a5d8ca3affbbe8ad12c367f29942608f5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:22:45.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9722" for this suite.
Nov 29 13:22:51.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:22:51.669: INFO: namespace deployment-9722 deletion completed in 6.129449783s
•SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:22:51.671: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-3537
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-3537
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-3537
STEP: Deleting pre-stop pod
Nov 29 13:23:02.941: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:23:02.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-3537" for this suite.
Nov 29 13:23:40.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:23:41.148: INFO: namespace prestop-3537 deletion completed in 38.195430688s
•SSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:23:41.148: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5423
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-d07c0291-08ac-4170-a710-d49a059a78c2 in namespace container-probe-5423
Nov 29 13:23:43.330: INFO: Started pod busybox-d07c0291-08ac-4170-a710-d49a059a78c2 in namespace container-probe-5423
STEP: checking the pod's current state and verifying that restartCount is present
Nov 29 13:23:43.334: INFO: Initial restart count of pod busybox-d07c0291-08ac-4170-a710-d49a059a78c2 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:27:43.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5423" for this suite.
Nov 29 13:27:49.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:27:50.061: INFO: namespace container-probe-5423 deletion completed in 6.139459203s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:27:50.062: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1417
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-c4a90019-4606-471e-9911-0706f33ace32
STEP: Creating a pod to test consume secrets
Nov 29 13:27:50.216: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-851b1dbb-e7b0-4d48-b684-3a3def33cda7" in namespace "projected-1417" to be "success or failure"
Nov 29 13:27:50.219: INFO: Pod "pod-projected-secrets-851b1dbb-e7b0-4d48-b684-3a3def33cda7": Phase="Pending", Reason="", readiness=false. Elapsed: 3.166338ms
Nov 29 13:27:52.223: INFO: Pod "pod-projected-secrets-851b1dbb-e7b0-4d48-b684-3a3def33cda7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0068729s
STEP: Saw pod success
Nov 29 13:27:52.223: INFO: Pod "pod-projected-secrets-851b1dbb-e7b0-4d48-b684-3a3def33cda7" satisfied condition "success or failure"
Nov 29 13:27:52.227: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-projected-secrets-851b1dbb-e7b0-4d48-b684-3a3def33cda7 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 29 13:27:52.251: INFO: Waiting for pod pod-projected-secrets-851b1dbb-e7b0-4d48-b684-3a3def33cda7 to disappear
Nov 29 13:27:52.254: INFO: Pod pod-projected-secrets-851b1dbb-e7b0-4d48-b684-3a3def33cda7 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:27:52.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1417" for this suite.
Nov 29 13:27:58.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:27:58.394: INFO: namespace projected-1417 deletion completed in 6.135433996s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:27:58.394: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8817
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 29 13:27:58.552: INFO: Waiting up to 5m0s for pod "pod-219f8fe8-286a-484e-992d-297e1fa8c779" in namespace "emptydir-8817" to be "success or failure"
Nov 29 13:27:58.557: INFO: Pod "pod-219f8fe8-286a-484e-992d-297e1fa8c779": Phase="Pending", Reason="", readiness=false. Elapsed: 5.258703ms
Nov 29 13:28:00.562: INFO: Pod "pod-219f8fe8-286a-484e-992d-297e1fa8c779": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01014501s
STEP: Saw pod success
Nov 29 13:28:00.562: INFO: Pod "pod-219f8fe8-286a-484e-992d-297e1fa8c779" satisfied condition "success or failure"
Nov 29 13:28:00.566: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-219f8fe8-286a-484e-992d-297e1fa8c779 container test-container: <nil>
STEP: delete the pod
Nov 29 13:28:00.588: INFO: Waiting for pod pod-219f8fe8-286a-484e-992d-297e1fa8c779 to disappear
Nov 29 13:28:00.591: INFO: Pod pod-219f8fe8-286a-484e-992d-297e1fa8c779 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:28:00.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8817" for this suite.
Nov 29 13:28:06.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:28:06.734: INFO: namespace emptydir-8817 deletion completed in 6.138730702s
•SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:28:06.735: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6921
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-4065ce8f-f7bb-4f8e-83a4-e8e9a9ca3564
STEP: Creating a pod to test consume configMaps
Nov 29 13:28:06.897: INFO: Waiting up to 5m0s for pod "pod-configmaps-413c6255-2128-470d-8c77-22d6eb882539" in namespace "configmap-6921" to be "success or failure"
Nov 29 13:28:06.901: INFO: Pod "pod-configmaps-413c6255-2128-470d-8c77-22d6eb882539": Phase="Pending", Reason="", readiness=false. Elapsed: 3.891094ms
Nov 29 13:28:08.905: INFO: Pod "pod-configmaps-413c6255-2128-470d-8c77-22d6eb882539": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008448051s
STEP: Saw pod success
Nov 29 13:28:08.905: INFO: Pod "pod-configmaps-413c6255-2128-470d-8c77-22d6eb882539" satisfied condition "success or failure"
Nov 29 13:28:08.908: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-configmaps-413c6255-2128-470d-8c77-22d6eb882539 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 13:28:08.928: INFO: Waiting for pod pod-configmaps-413c6255-2128-470d-8c77-22d6eb882539 to disappear
Nov 29 13:28:08.931: INFO: Pod pod-configmaps-413c6255-2128-470d-8c77-22d6eb882539 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:28:08.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6921" for this suite.
Nov 29 13:28:14.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:28:15.071: INFO: namespace configmap-6921 deletion completed in 6.134755943s
•SSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:28:15.071: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-1985
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Nov 29 13:28:21.243: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1129 13:28:21.243446    4828 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:28:21.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1985" for this suite.
Nov 29 13:28:27.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:28:27.376: INFO: namespace gc-1985 deletion completed in 6.129567602s
•SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:28:27.377: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-3450
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Nov 29 13:28:27.522: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config --namespace=kubectl-3450 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Nov 29 13:28:29.541: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Nov 29 13:28:29.541: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:28:31.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3450" for this suite.
Nov 29 13:28:37.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:28:37.727: INFO: namespace kubectl-3450 deletion completed in 6.169314871s
•SS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:28:37.727: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-652
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:28:43.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-652" for this suite.
Nov 29 13:28:49.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:28:49.556: INFO: namespace watch-652 deletion completed in 6.221795926s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:28:49.556: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-9771
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Nov 29 13:28:49.702: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:28:52.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9771" for this suite.
Nov 29 13:28:58.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:28:58.670: INFO: namespace init-container-9771 deletion completed in 6.169154542s
•SSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:28:58.670: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8642
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Nov 29 13:28:58.824: INFO: Waiting up to 5m0s for pod "client-containers-126879af-f105-4bed-a58c-ac12a94003f5" in namespace "containers-8642" to be "success or failure"
Nov 29 13:28:58.828: INFO: Pod "client-containers-126879af-f105-4bed-a58c-ac12a94003f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.194492ms
Nov 29 13:29:00.832: INFO: Pod "client-containers-126879af-f105-4bed-a58c-ac12a94003f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008383724s
Nov 29 13:29:02.855: INFO: Pod "client-containers-126879af-f105-4bed-a58c-ac12a94003f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.031400173s
STEP: Saw pod success
Nov 29 13:29:02.855: INFO: Pod "client-containers-126879af-f105-4bed-a58c-ac12a94003f5" satisfied condition "success or failure"
Nov 29 13:29:02.859: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod client-containers-126879af-f105-4bed-a58c-ac12a94003f5 container test-container: <nil>
STEP: delete the pod
Nov 29 13:29:02.879: INFO: Waiting for pod client-containers-126879af-f105-4bed-a58c-ac12a94003f5 to disappear
Nov 29 13:29:02.885: INFO: Pod client-containers-126879af-f105-4bed-a58c-ac12a94003f5 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:29:02.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8642" for this suite.
Nov 29 13:29:08.902: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:29:09.027: INFO: namespace containers-8642 deletion completed in 6.136641533s
•SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:29:09.027: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9554
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1557
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Nov 29 13:29:09.198: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-9554'
Nov 29 13:29:09.464: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Nov 29 13:29:09.464: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Nov 29 13:29:11.473: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete deployment e2e-test-nginx-deployment --namespace=kubectl-9554'
Nov 29 13:29:11.551: INFO: stderr: ""
Nov 29 13:29:11.551: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:29:11.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9554" for this suite.
Nov 29 13:29:17.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:29:17.690: INFO: namespace kubectl-9554 deletion completed in 6.133268953s
•SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:29:17.690: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8437
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 29 13:29:17.842: INFO: Waiting up to 5m0s for pod "downwardapi-volume-08053921-2256-4517-afd6-279a42befda6" in namespace "projected-8437" to be "success or failure"
Nov 29 13:29:17.847: INFO: Pod "downwardapi-volume-08053921-2256-4517-afd6-279a42befda6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.141832ms
Nov 29 13:29:19.852: INFO: Pod "downwardapi-volume-08053921-2256-4517-afd6-279a42befda6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010075322s
STEP: Saw pod success
Nov 29 13:29:19.852: INFO: Pod "downwardapi-volume-08053921-2256-4517-afd6-279a42befda6" satisfied condition "success or failure"
Nov 29 13:29:19.855: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod downwardapi-volume-08053921-2256-4517-afd6-279a42befda6 container client-container: <nil>
STEP: delete the pod
Nov 29 13:29:19.873: INFO: Waiting for pod downwardapi-volume-08053921-2256-4517-afd6-279a42befda6 to disappear
Nov 29 13:29:19.877: INFO: Pod downwardapi-volume-08053921-2256-4517-afd6-279a42befda6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:29:19.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8437" for this suite.
Nov 29 13:29:25.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:29:26.020: INFO: namespace projected-8437 deletion completed in 6.138505466s
•SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:29:26.020: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9890
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Nov 29 13:29:26.170: INFO: Waiting up to 5m0s for pod "var-expansion-e513163e-c65c-4e4f-9d0c-07869255ab56" in namespace "var-expansion-9890" to be "success or failure"
Nov 29 13:29:26.174: INFO: Pod "var-expansion-e513163e-c65c-4e4f-9d0c-07869255ab56": Phase="Pending", Reason="", readiness=false. Elapsed: 3.94744ms
Nov 29 13:29:28.181: INFO: Pod "var-expansion-e513163e-c65c-4e4f-9d0c-07869255ab56": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01111434s
STEP: Saw pod success
Nov 29 13:29:28.181: INFO: Pod "var-expansion-e513163e-c65c-4e4f-9d0c-07869255ab56" satisfied condition "success or failure"
Nov 29 13:29:28.184: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod var-expansion-e513163e-c65c-4e4f-9d0c-07869255ab56 container dapi-container: <nil>
STEP: delete the pod
Nov 29 13:29:28.204: INFO: Waiting for pod var-expansion-e513163e-c65c-4e4f-9d0c-07869255ab56 to disappear
Nov 29 13:29:28.207: INFO: Pod var-expansion-e513163e-c65c-4e4f-9d0c-07869255ab56 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:29:28.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9890" for this suite.
Nov 29 13:29:34.224: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:29:34.388: INFO: namespace var-expansion-9890 deletion completed in 6.17510841s
•SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:29:34.388: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-246
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Nov 29 13:29:38.571: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 29 13:29:38.574: INFO: Pod pod-with-prestop-http-hook still exists
Nov 29 13:29:40.575: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 29 13:29:40.580: INFO: Pod pod-with-prestop-http-hook still exists
Nov 29 13:29:42.574: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 29 13:29:42.579: INFO: Pod pod-with-prestop-http-hook still exists
Nov 29 13:29:44.574: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 29 13:29:44.579: INFO: Pod pod-with-prestop-http-hook still exists
Nov 29 13:29:46.574: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 29 13:29:46.579: INFO: Pod pod-with-prestop-http-hook still exists
Nov 29 13:29:48.575: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 29 13:29:48.580: INFO: Pod pod-with-prestop-http-hook still exists
Nov 29 13:29:50.574: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Nov 29 13:29:50.580: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:29:50.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-246" for this suite.
Nov 29 13:30:12.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:30:12.746: INFO: namespace container-lifecycle-hook-246 deletion completed in 22.143435802s
•
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:30:12.746: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-7957
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-0fe9c72c-42b6-46ef-b36d-4df1e5fb4620
STEP: Creating a pod to test consume secrets
Nov 29 13:30:12.901: INFO: Waiting up to 5m0s for pod "pod-secrets-3af19614-d1dc-4d5e-bfc0-fff55b750adf" in namespace "secrets-7957" to be "success or failure"
Nov 29 13:30:12.905: INFO: Pod "pod-secrets-3af19614-d1dc-4d5e-bfc0-fff55b750adf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.093621ms
Nov 29 13:30:14.910: INFO: Pod "pod-secrets-3af19614-d1dc-4d5e-bfc0-fff55b750adf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008828329s
STEP: Saw pod success
Nov 29 13:30:14.910: INFO: Pod "pod-secrets-3af19614-d1dc-4d5e-bfc0-fff55b750adf" satisfied condition "success or failure"
Nov 29 13:30:14.913: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-secrets-3af19614-d1dc-4d5e-bfc0-fff55b750adf container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 13:30:14.932: INFO: Waiting for pod pod-secrets-3af19614-d1dc-4d5e-bfc0-fff55b750adf to disappear
Nov 29 13:30:14.935: INFO: Pod pod-secrets-3af19614-d1dc-4d5e-bfc0-fff55b750adf no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:30:14.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7957" for this suite.
Nov 29 13:30:20.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:30:21.079: INFO: namespace secrets-7957 deletion completed in 6.138780445s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:30:21.079: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-6458
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Nov 29 13:30:23.254: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-0cf13404-464a-45f6-8cc2-0a154fe18be7,GenerateName:,Namespace:events-6458,SelfLink:/api/v1/namespaces/events-6458/pods/send-events-0cf13404-464a-45f6-8cc2-0a154fe18be7,UID:1e33b02a-e163-4eb9-bcf7-d97e62bea30e,ResourceVersion:17138,Generation:0,CreationTimestamp:2019-11-29 13:30:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 233800969,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.193/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-nwx82 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-nwx82,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-nwx82 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0019c6190} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0019c61b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:30:21 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:30:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:30:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:30:21 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:100.64.0.193,StartTime:2019-11-29 13:30:21 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-11-29 13:30:22 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://c2e605e3f34f5d05ff97994b6bd4b5749182e4a915a235fce150a37bcfdb83c7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Nov 29 13:30:25.259: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Nov 29 13:30:27.265: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:30:27.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6458" for this suite.
Nov 29 13:31:05.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:31:05.406: INFO: namespace events-6458 deletion completed in 38.128556959s
•SSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:31:05.406: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5764
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-5764/secret-test-6fd70206-5fc5-4b24-be03-a00f2f8cdb39
STEP: Creating a pod to test consume secrets
Nov 29 13:31:05.563: INFO: Waiting up to 5m0s for pod "pod-configmaps-c75d08a8-ba78-4744-a7ec-8fb0a836b1f4" in namespace "secrets-5764" to be "success or failure"
Nov 29 13:31:05.569: INFO: Pod "pod-configmaps-c75d08a8-ba78-4744-a7ec-8fb0a836b1f4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.790794ms
Nov 29 13:31:07.574: INFO: Pod "pod-configmaps-c75d08a8-ba78-4744-a7ec-8fb0a836b1f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010766356s
STEP: Saw pod success
Nov 29 13:31:07.574: INFO: Pod "pod-configmaps-c75d08a8-ba78-4744-a7ec-8fb0a836b1f4" satisfied condition "success or failure"
Nov 29 13:31:07.578: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-configmaps-c75d08a8-ba78-4744-a7ec-8fb0a836b1f4 container env-test: <nil>
STEP: delete the pod
Nov 29 13:31:07.599: INFO: Waiting for pod pod-configmaps-c75d08a8-ba78-4744-a7ec-8fb0a836b1f4 to disappear
Nov 29 13:31:07.602: INFO: Pod pod-configmaps-c75d08a8-ba78-4744-a7ec-8fb0a836b1f4 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:31:07.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5764" for this suite.
Nov 29 13:31:13.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:31:13.787: INFO: namespace secrets-5764 deletion completed in 6.180072466s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:31:13.787: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4216
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Nov 29 13:31:13.940: INFO: Waiting up to 5m0s for pod "pod-6f06761d-5b3d-4d18-920c-1ccaa7375a8b" in namespace "emptydir-4216" to be "success or failure"
Nov 29 13:31:13.944: INFO: Pod "pod-6f06761d-5b3d-4d18-920c-1ccaa7375a8b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.370641ms
Nov 29 13:31:15.949: INFO: Pod "pod-6f06761d-5b3d-4d18-920c-1ccaa7375a8b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009543956s
STEP: Saw pod success
Nov 29 13:31:15.949: INFO: Pod "pod-6f06761d-5b3d-4d18-920c-1ccaa7375a8b" satisfied condition "success or failure"
Nov 29 13:31:15.953: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-6f06761d-5b3d-4d18-920c-1ccaa7375a8b container test-container: <nil>
STEP: delete the pod
Nov 29 13:31:15.970: INFO: Waiting for pod pod-6f06761d-5b3d-4d18-920c-1ccaa7375a8b to disappear
Nov 29 13:31:15.973: INFO: Pod pod-6f06761d-5b3d-4d18-920c-1ccaa7375a8b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:31:15.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4216" for this suite.
Nov 29 13:31:21.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:31:22.107: INFO: namespace emptydir-4216 deletion completed in 6.130059953s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:31:22.108: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8691
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 29 13:31:22.260: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5968ebdc-3240-497d-8847-af460b5c5e86" in namespace "downward-api-8691" to be "success or failure"
Nov 29 13:31:22.263: INFO: Pod "downwardapi-volume-5968ebdc-3240-497d-8847-af460b5c5e86": Phase="Pending", Reason="", readiness=false. Elapsed: 3.114543ms
Nov 29 13:31:24.267: INFO: Pod "downwardapi-volume-5968ebdc-3240-497d-8847-af460b5c5e86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007763107s
STEP: Saw pod success
Nov 29 13:31:24.268: INFO: Pod "downwardapi-volume-5968ebdc-3240-497d-8847-af460b5c5e86" satisfied condition "success or failure"
Nov 29 13:31:24.272: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod downwardapi-volume-5968ebdc-3240-497d-8847-af460b5c5e86 container client-container: <nil>
STEP: delete the pod
Nov 29 13:31:24.292: INFO: Waiting for pod downwardapi-volume-5968ebdc-3240-497d-8847-af460b5c5e86 to disappear
Nov 29 13:31:24.295: INFO: Pod downwardapi-volume-5968ebdc-3240-497d-8847-af460b5c5e86 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:31:24.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8691" for this suite.
Nov 29 13:31:30.311: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:31:30.472: INFO: namespace downward-api-8691 deletion completed in 6.17241968s
•S
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:31:30.472: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8576
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-5acf6edb-368d-4ba5-9e69-f68c002578ba
STEP: Creating secret with name s-test-opt-upd-2d3fcb20-6799-4fc0-bf2d-787ade60c341
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-5acf6edb-368d-4ba5-9e69-f68c002578ba
STEP: Updating secret s-test-opt-upd-2d3fcb20-6799-4fc0-bf2d-787ade60c341
STEP: Creating secret with name s-test-opt-create-0737ac72-1c4a-4978-8041-450f5067f4b5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:32:37.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8576" for this suite.
Nov 29 13:32:59.595: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:32:59.724: INFO: namespace projected-8576 deletion completed in 22.139932282s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:32:59.724: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1849
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-1a77fe5f-1402-4696-ab11-5ced9432a082
STEP: Creating a pod to test consume configMaps
Nov 29 13:32:59.876: INFO: Waiting up to 5m0s for pod "pod-configmaps-24f0deeb-4606-4873-812c-808e9dffa6c2" in namespace "configmap-1849" to be "success or failure"
Nov 29 13:32:59.879: INFO: Pod "pod-configmaps-24f0deeb-4606-4873-812c-808e9dffa6c2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.711731ms
Nov 29 13:33:01.884: INFO: Pod "pod-configmaps-24f0deeb-4606-4873-812c-808e9dffa6c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007499291s
STEP: Saw pod success
Nov 29 13:33:01.884: INFO: Pod "pod-configmaps-24f0deeb-4606-4873-812c-808e9dffa6c2" satisfied condition "success or failure"
Nov 29 13:33:01.887: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-configmaps-24f0deeb-4606-4873-812c-808e9dffa6c2 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 13:33:01.907: INFO: Waiting for pod pod-configmaps-24f0deeb-4606-4873-812c-808e9dffa6c2 to disappear
Nov 29 13:33:01.910: INFO: Pod pod-configmaps-24f0deeb-4606-4873-812c-808e9dffa6c2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:33:01.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1849" for this suite.
Nov 29 13:33:07.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:33:08.093: INFO: namespace configmap-1849 deletion completed in 6.17926481s
•SSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:33:08.094: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-4132
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Nov 29 13:33:08.249: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4132,SelfLink:/api/v1/namespaces/watch-4132/configmaps/e2e-watch-test-watch-closed,UID:693abbfa-5dfd-429c-891e-e81689213f8c,ResourceVersion:17629,Generation:0,CreationTimestamp:2019-11-29 13:33:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 29 13:33:08.249: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4132,SelfLink:/api/v1/namespaces/watch-4132/configmaps/e2e-watch-test-watch-closed,UID:693abbfa-5dfd-429c-891e-e81689213f8c,ResourceVersion:17630,Generation:0,CreationTimestamp:2019-11-29 13:33:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Nov 29 13:33:08.262: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4132,SelfLink:/api/v1/namespaces/watch-4132/configmaps/e2e-watch-test-watch-closed,UID:693abbfa-5dfd-429c-891e-e81689213f8c,ResourceVersion:17631,Generation:0,CreationTimestamp:2019-11-29 13:33:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 29 13:33:08.263: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4132,SelfLink:/api/v1/namespaces/watch-4132/configmaps/e2e-watch-test-watch-closed,UID:693abbfa-5dfd-429c-891e-e81689213f8c,ResourceVersion:17632,Generation:0,CreationTimestamp:2019-11-29 13:33:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:33:08.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4132" for this suite.
Nov 29 13:33:14.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:33:14.401: INFO: namespace watch-4132 deletion completed in 6.134363919s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:33:14.401: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3561
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-9d52ef8d-4dfb-4383-b8d7-f46f9c53c4c2
STEP: Creating a pod to test consume configMaps
Nov 29 13:33:14.555: INFO: Waiting up to 5m0s for pod "pod-configmaps-a8c8c243-3d7f-49b0-8211-f40723be9d84" in namespace "configmap-3561" to be "success or failure"
Nov 29 13:33:14.559: INFO: Pod "pod-configmaps-a8c8c243-3d7f-49b0-8211-f40723be9d84": Phase="Pending", Reason="", readiness=false. Elapsed: 3.884896ms
Nov 29 13:33:16.563: INFO: Pod "pod-configmaps-a8c8c243-3d7f-49b0-8211-f40723be9d84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008384068s
STEP: Saw pod success
Nov 29 13:33:16.564: INFO: Pod "pod-configmaps-a8c8c243-3d7f-49b0-8211-f40723be9d84" satisfied condition "success or failure"
Nov 29 13:33:16.566: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-configmaps-a8c8c243-3d7f-49b0-8211-f40723be9d84 container configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 13:33:16.586: INFO: Waiting for pod pod-configmaps-a8c8c243-3d7f-49b0-8211-f40723be9d84 to disappear
Nov 29 13:33:16.591: INFO: Pod pod-configmaps-a8c8c243-3d7f-49b0-8211-f40723be9d84 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:33:16.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3561" for this suite.
Nov 29 13:33:22.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:33:22.737: INFO: namespace configmap-3561 deletion completed in 6.141224955s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:33:22.737: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-5089
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Nov 29 13:33:22.885: INFO: Waiting up to 5m0s for pod "client-containers-763a9c74-a798-4a41-b587-4d865e785e10" in namespace "containers-5089" to be "success or failure"
Nov 29 13:33:22.888: INFO: Pod "client-containers-763a9c74-a798-4a41-b587-4d865e785e10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.952539ms
Nov 29 13:33:24.892: INFO: Pod "client-containers-763a9c74-a798-4a41-b587-4d865e785e10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007036422s
STEP: Saw pod success
Nov 29 13:33:24.892: INFO: Pod "client-containers-763a9c74-a798-4a41-b587-4d865e785e10" satisfied condition "success or failure"
Nov 29 13:33:24.895: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod client-containers-763a9c74-a798-4a41-b587-4d865e785e10 container test-container: <nil>
STEP: delete the pod
Nov 29 13:33:24.920: INFO: Waiting for pod client-containers-763a9c74-a798-4a41-b587-4d865e785e10 to disappear
Nov 29 13:33:24.924: INFO: Pod client-containers-763a9c74-a798-4a41-b587-4d865e785e10 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:33:24.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5089" for this suite.
Nov 29 13:33:30.941: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:33:31.067: INFO: namespace containers-5089 deletion completed in 6.138516727s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:33:31.068: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3783
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-3783
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 29 13:33:31.215: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Nov 29 13:33:49.289: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.1.58 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3783 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 13:33:49.289: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Nov 29 13:33:50.737: INFO: Found all expected endpoints: [netserver-0]
Nov 29 13:33:50.741: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 100.64.0.201 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3783 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 13:33:50.741: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Nov 29 13:33:52.189: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:33:52.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3783" for this suite.
Nov 29 13:34:14.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:34:14.330: INFO: namespace pod-network-test-3783 deletion completed in 22.134136758s
•
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:34:14.330: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9575
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-129439d3-020f-4bc6-8b86-7c2411ea4300
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:34:14.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9575" for this suite.
Nov 29 13:34:20.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:34:20.627: INFO: namespace configmap-9575 deletion completed in 6.147215134s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:34:20.627: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9780
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-9780
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9780 to expose endpoints map[]
Nov 29 13:34:20.797: INFO: successfully validated that service endpoint-test2 in namespace services-9780 exposes endpoints map[] (14.489224ms elapsed)
STEP: Creating pod pod1 in namespace services-9780
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9780 to expose endpoints map[pod1:[80]]
Nov 29 13:34:22.844: INFO: successfully validated that service endpoint-test2 in namespace services-9780 exposes endpoints map[pod1:[80]] (2.041173743s elapsed)
STEP: Creating pod pod2 in namespace services-9780
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9780 to expose endpoints map[pod1:[80] pod2:[80]]
Nov 29 13:34:23.876: INFO: successfully validated that service endpoint-test2 in namespace services-9780 exposes endpoints map[pod1:[80] pod2:[80]] (1.026048106s elapsed)
STEP: Deleting pod pod1 in namespace services-9780
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9780 to expose endpoints map[pod2:[80]]
Nov 29 13:34:23.887: INFO: successfully validated that service endpoint-test2 in namespace services-9780 exposes endpoints map[pod2:[80]] (6.394708ms elapsed)
STEP: Deleting pod pod2 in namespace services-9780
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-9780 to expose endpoints map[]
Nov 29 13:34:23.898: INFO: successfully validated that service endpoint-test2 in namespace services-9780 exposes endpoints map[] (5.711933ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:34:23.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9780" for this suite.
Nov 29 13:34:45.926: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:34:46.040: INFO: namespace services-9780 deletion completed in 22.124786803s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92
•SS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:34:46.041: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9690
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Nov 29 13:34:46.193: INFO: Waiting up to 5m0s for pod "pod-1ccb7d7d-53c0-48d4-a573-67117d5913b5" in namespace "emptydir-9690" to be "success or failure"
Nov 29 13:34:46.197: INFO: Pod "pod-1ccb7d7d-53c0-48d4-a573-67117d5913b5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.765919ms
Nov 29 13:34:48.202: INFO: Pod "pod-1ccb7d7d-53c0-48d4-a573-67117d5913b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008431119s
STEP: Saw pod success
Nov 29 13:34:48.202: INFO: Pod "pod-1ccb7d7d-53c0-48d4-a573-67117d5913b5" satisfied condition "success or failure"
Nov 29 13:34:48.205: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-1ccb7d7d-53c0-48d4-a573-67117d5913b5 container test-container: <nil>
STEP: delete the pod
Nov 29 13:34:48.230: INFO: Waiting for pod pod-1ccb7d7d-53c0-48d4-a573-67117d5913b5 to disappear
Nov 29 13:34:48.233: INFO: Pod pod-1ccb7d7d-53c0-48d4-a573-67117d5913b5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:34:48.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9690" for this suite.
Nov 29 13:34:54.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:34:54.375: INFO: namespace emptydir-9690 deletion completed in 6.137277646s
•SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:34:54.375: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-5140
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 29 13:34:54.529: INFO: Creating ReplicaSet my-hostname-basic-d5e6ea10-05cb-4c7a-a750-53727c657f03
Nov 29 13:34:54.537: INFO: Pod name my-hostname-basic-d5e6ea10-05cb-4c7a-a750-53727c657f03: Found 0 pods out of 1
Nov 29 13:34:59.542: INFO: Pod name my-hostname-basic-d5e6ea10-05cb-4c7a-a750-53727c657f03: Found 1 pods out of 1
Nov 29 13:34:59.542: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-d5e6ea10-05cb-4c7a-a750-53727c657f03" is running
Nov 29 13:34:59.549: INFO: Pod "my-hostname-basic-d5e6ea10-05cb-4c7a-a750-53727c657f03-kjrqz" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-29 13:34:54 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-29 13:34:56 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-29 13:34:56 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-11-29 13:34:54 +0000 UTC Reason: Message:}])
Nov 29 13:34:59.549: INFO: Trying to dial the pod
Nov 29 13:35:04.651: INFO: Controller my-hostname-basic-d5e6ea10-05cb-4c7a-a750-53727c657f03: Got expected result from replica 1 [my-hostname-basic-d5e6ea10-05cb-4c7a-a750-53727c657f03-kjrqz]: "my-hostname-basic-d5e6ea10-05cb-4c7a-a750-53727c657f03-kjrqz", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:35:04.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5140" for this suite.
Nov 29 13:35:10.669: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:35:10.787: INFO: namespace replicaset-5140 deletion completed in 6.129979817s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:35:10.788: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2429
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Nov 29 13:35:10.942: INFO: Waiting up to 5m0s for pod "pod-b4d05284-e604-4324-92fb-4bf29c6e4a85" in namespace "emptydir-2429" to be "success or failure"
Nov 29 13:35:10.946: INFO: Pod "pod-b4d05284-e604-4324-92fb-4bf29c6e4a85": Phase="Pending", Reason="", readiness=false. Elapsed: 3.959537ms
Nov 29 13:35:12.951: INFO: Pod "pod-b4d05284-e604-4324-92fb-4bf29c6e4a85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008662115s
STEP: Saw pod success
Nov 29 13:35:12.951: INFO: Pod "pod-b4d05284-e604-4324-92fb-4bf29c6e4a85" satisfied condition "success or failure"
Nov 29 13:35:12.954: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-b4d05284-e604-4324-92fb-4bf29c6e4a85 container test-container: <nil>
STEP: delete the pod
Nov 29 13:35:12.977: INFO: Waiting for pod pod-b4d05284-e604-4324-92fb-4bf29c6e4a85 to disappear
Nov 29 13:35:12.980: INFO: Pod pod-b4d05284-e604-4324-92fb-4bf29c6e4a85 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:35:12.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2429" for this suite.
Nov 29 13:35:18.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:35:19.130: INFO: namespace emptydir-2429 deletion completed in 6.144407587s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:35:19.130: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-656
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Nov 29 13:35:21.806: INFO: Successfully updated pod "pod-update-activedeadlineseconds-243b8ea6-66df-42e4-b35d-1fd598cf64ff"
Nov 29 13:35:21.806: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-243b8ea6-66df-42e4-b35d-1fd598cf64ff" in namespace "pods-656" to be "terminated due to deadline exceeded"
Nov 29 13:35:21.809: INFO: Pod "pod-update-activedeadlineseconds-243b8ea6-66df-42e4-b35d-1fd598cf64ff": Phase="Running", Reason="", readiness=true. Elapsed: 3.019441ms
Nov 29 13:35:23.814: INFO: Pod "pod-update-activedeadlineseconds-243b8ea6-66df-42e4-b35d-1fd598cf64ff": Phase="Running", Reason="", readiness=true. Elapsed: 2.008397136s
Nov 29 13:35:25.819: INFO: Pod "pod-update-activedeadlineseconds-243b8ea6-66df-42e4-b35d-1fd598cf64ff": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.012999736s
Nov 29 13:35:25.819: INFO: Pod "pod-update-activedeadlineseconds-243b8ea6-66df-42e4-b35d-1fd598cf64ff" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:35:25.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-656" for this suite.
Nov 29 13:35:31.837: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:35:31.959: INFO: namespace pods-656 deletion completed in 6.134735865s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:35:31.960: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8898
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-c25ce971-ce69-434e-ae28-984ca4e35a23
STEP: Creating a pod to test consume secrets
Nov 29 13:35:32.117: INFO: Waiting up to 5m0s for pod "pod-secrets-74b3eff2-629b-4f6d-b4d0-536f32917e5c" in namespace "secrets-8898" to be "success or failure"
Nov 29 13:35:32.122: INFO: Pod "pod-secrets-74b3eff2-629b-4f6d-b4d0-536f32917e5c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.094292ms
Nov 29 13:35:34.126: INFO: Pod "pod-secrets-74b3eff2-629b-4f6d-b4d0-536f32917e5c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009260226s
STEP: Saw pod success
Nov 29 13:35:34.126: INFO: Pod "pod-secrets-74b3eff2-629b-4f6d-b4d0-536f32917e5c" satisfied condition "success or failure"
Nov 29 13:35:34.129: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-secrets-74b3eff2-629b-4f6d-b4d0-536f32917e5c container secret-env-test: <nil>
STEP: delete the pod
Nov 29 13:35:34.160: INFO: Waiting for pod pod-secrets-74b3eff2-629b-4f6d-b4d0-536f32917e5c to disappear
Nov 29 13:35:34.163: INFO: Pod pod-secrets-74b3eff2-629b-4f6d-b4d0-536f32917e5c no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:35:34.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8898" for this suite.
Nov 29 13:35:40.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:35:40.335: INFO: namespace secrets-8898 deletion completed in 6.167520591s
•SSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:35:40.335: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-5438
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Nov 29 13:35:40.489: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-5438" to be "success or failure"
Nov 29 13:35:40.493: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 4.505145ms
Nov 29 13:35:42.498: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009804822s
STEP: Saw pod success
Nov 29 13:35:42.499: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Nov 29 13:35:42.502: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Nov 29 13:35:42.527: INFO: Waiting for pod pod-host-path-test to disappear
Nov 29 13:35:42.530: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:35:42.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-5438" for this suite.
Nov 29 13:35:48.547: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:35:48.702: INFO: namespace hostpath-5438 deletion completed in 6.16726563s
•SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:35:48.702: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3924
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1129 13:35:58.874599    4828 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 29 13:35:58.874: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:35:58.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3924" for this suite.
Nov 29 13:36:04.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:36:05.009: INFO: namespace gc-3924 deletion completed in 6.129863454s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:36:05.009: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4259
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Nov 29 13:36:09.216: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 29 13:36:09.219: INFO: Pod pod-with-poststart-http-hook still exists
Nov 29 13:36:11.219: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 29 13:36:11.224: INFO: Pod pod-with-poststart-http-hook still exists
Nov 29 13:36:13.219: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 29 13:36:13.223: INFO: Pod pod-with-poststart-http-hook still exists
Nov 29 13:36:15.219: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 29 13:36:15.224: INFO: Pod pod-with-poststart-http-hook still exists
Nov 29 13:36:17.219: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 29 13:36:17.224: INFO: Pod pod-with-poststart-http-hook still exists
Nov 29 13:36:19.219: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Nov 29 13:36:19.223: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:36:19.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4259" for this suite.
Nov 29 13:36:41.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:36:41.377: INFO: namespace container-lifecycle-hook-4259 deletion completed in 22.148643251s
•SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:36:41.377: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4434
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Nov 29 13:36:41.528: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:36:48.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4434" for this suite.
Nov 29 13:36:54.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:36:55.045: INFO: namespace pods-4434 deletion completed in 6.128480091s
•S
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:36:55.045: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7495
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Nov 29 13:36:57.209: INFO: Pod pod-hostip-88cac257-e40b-4920-87fd-f345bdc1c8ec has hostIP: 10.250.0.10
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:36:57.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7495" for this suite.
Nov 29 13:37:19.227: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:37:19.351: INFO: namespace pods-7495 deletion completed in 22.136694261s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:37:19.352: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6576
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Nov 29 13:37:19.501: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-6576'
Nov 29 13:37:20.015: INFO: stderr: ""
Nov 29 13:37:20.015: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 29 13:37:20.015: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6576'
Nov 29 13:37:20.093: INFO: stderr: ""
Nov 29 13:37:20.093: INFO: stdout: "update-demo-nautilus-597mt update-demo-nautilus-msb4k "
Nov 29 13:37:20.093: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-597mt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6576'
Nov 29 13:37:20.180: INFO: stderr: ""
Nov 29 13:37:20.180: INFO: stdout: ""
Nov 29 13:37:20.180: INFO: update-demo-nautilus-597mt is created but not running
Nov 29 13:37:25.180: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6576'
Nov 29 13:37:25.256: INFO: stderr: ""
Nov 29 13:37:25.256: INFO: stdout: "update-demo-nautilus-597mt update-demo-nautilus-msb4k "
Nov 29 13:37:25.256: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-597mt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6576'
Nov 29 13:37:25.329: INFO: stderr: ""
Nov 29 13:37:25.329: INFO: stdout: "true"
Nov 29 13:37:25.329: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-597mt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6576'
Nov 29 13:37:25.410: INFO: stderr: ""
Nov 29 13:37:25.410: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 29 13:37:25.410: INFO: validating pod update-demo-nautilus-597mt
Nov 29 13:37:25.497: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 29 13:37:25.497: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 29 13:37:25.497: INFO: update-demo-nautilus-597mt is verified up and running
Nov 29 13:37:25.497: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-msb4k -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6576'
Nov 29 13:37:25.568: INFO: stderr: ""
Nov 29 13:37:25.568: INFO: stdout: "true"
Nov 29 13:37:25.568: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-msb4k -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6576'
Nov 29 13:37:25.643: INFO: stderr: ""
Nov 29 13:37:25.643: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 29 13:37:25.643: INFO: validating pod update-demo-nautilus-msb4k
Nov 29 13:37:25.729: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 29 13:37:25.729: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 29 13:37:25.729: INFO: update-demo-nautilus-msb4k is verified up and running
STEP: rolling-update to new replication controller
Nov 29 13:37:25.731: INFO: scanned /root for discovery docs: <nil>
Nov 29 13:37:25.731: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-6576'
Nov 29 13:37:48.109: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Nov 29 13:37:48.109: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 29 13:37:48.109: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6576'
Nov 29 13:37:48.189: INFO: stderr: ""
Nov 29 13:37:48.189: INFO: stdout: "update-demo-kitten-7tlxw update-demo-kitten-nnddj update-demo-nautilus-msb4k "
STEP: Replicas for name=update-demo: expected=2 actual=3
Nov 29 13:37:53.189: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6576'
Nov 29 13:37:53.320: INFO: stderr: ""
Nov 29 13:37:53.320: INFO: stdout: "update-demo-kitten-7tlxw update-demo-kitten-nnddj "
Nov 29 13:37:53.320: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-7tlxw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6576'
Nov 29 13:37:53.393: INFO: stderr: ""
Nov 29 13:37:53.393: INFO: stdout: "true"
Nov 29 13:37:53.393: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-7tlxw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6576'
Nov 29 13:37:53.476: INFO: stderr: ""
Nov 29 13:37:53.476: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Nov 29 13:37:53.476: INFO: validating pod update-demo-kitten-7tlxw
Nov 29 13:37:53.561: INFO: got data: {
  "image": "kitten.jpg"
}

Nov 29 13:37:53.561: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Nov 29 13:37:53.561: INFO: update-demo-kitten-7tlxw is verified up and running
Nov 29 13:37:53.561: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-nnddj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6576'
Nov 29 13:37:53.697: INFO: stderr: ""
Nov 29 13:37:53.697: INFO: stdout: "true"
Nov 29 13:37:53.697: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-kitten-nnddj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6576'
Nov 29 13:37:53.767: INFO: stderr: ""
Nov 29 13:37:53.767: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Nov 29 13:37:53.767: INFO: validating pod update-demo-kitten-nnddj
Nov 29 13:37:53.853: INFO: got data: {
  "image": "kitten.jpg"
}

Nov 29 13:37:53.853: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Nov 29 13:37:53.853: INFO: update-demo-kitten-nnddj is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:37:53.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6576" for this suite.
Nov 29 13:38:15.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:38:15.983: INFO: namespace kubectl-6576 deletion completed in 22.125114742s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:38:15.984: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3187
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 29 13:38:16.154: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"cbdf0c53-bc89-44e9-af12-9c66b9f9684e", Controller:(*bool)(0xc00277f69a), BlockOwnerDeletion:(*bool)(0xc00277f69b)}}
Nov 29 13:38:16.159: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"ff0ecdd0-252b-4eab-89e3-63f4eae314bf", Controller:(*bool)(0xc00277f856), BlockOwnerDeletion:(*bool)(0xc00277f857)}}
Nov 29 13:38:16.165: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"ffa7ec62-e6e2-49ec-8b2c-0c7135991a64", Controller:(*bool)(0xc000e14e66), BlockOwnerDeletion:(*bool)(0xc000e14e67)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:38:21.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3187" for this suite.
Nov 29 13:38:27.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:38:27.310: INFO: namespace gc-3187 deletion completed in 6.131060319s
•SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:38:27.310: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-5635
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Nov 29 13:38:27.455: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Nov 29 13:38:27.933: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Nov 29 13:38:29.971: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710631507, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710631507, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710631507, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710631507, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 13:38:31.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710631507, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710631507, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710631507, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710631507, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 13:38:33.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710631507, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710631507, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710631507, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710631507, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 13:38:35.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710631507, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710631507, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710631507, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710631507, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 13:38:37.976: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710631507, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710631507, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63710631507, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63710631507, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Nov 29 13:38:41.027: INFO: Waited 1.045549637s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:38:41.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-5635" for this suite.
Nov 29 13:38:47.718: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:38:47.880: INFO: namespace aggregator-5635 deletion completed in 6.18489834s
•SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:38:47.880: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9567
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 29 13:38:48.031: INFO: Waiting up to 5m0s for pod "downwardapi-volume-87d2985b-6ddc-43cb-b589-5d2f40bbd1b3" in namespace "projected-9567" to be "success or failure"
Nov 29 13:38:48.034: INFO: Pod "downwardapi-volume-87d2985b-6ddc-43cb-b589-5d2f40bbd1b3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.038706ms
Nov 29 13:38:50.039: INFO: Pod "downwardapi-volume-87d2985b-6ddc-43cb-b589-5d2f40bbd1b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007842327s
STEP: Saw pod success
Nov 29 13:38:50.039: INFO: Pod "downwardapi-volume-87d2985b-6ddc-43cb-b589-5d2f40bbd1b3" satisfied condition "success or failure"
Nov 29 13:38:50.042: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod downwardapi-volume-87d2985b-6ddc-43cb-b589-5d2f40bbd1b3 container client-container: <nil>
STEP: delete the pod
Nov 29 13:38:50.063: INFO: Waiting for pod downwardapi-volume-87d2985b-6ddc-43cb-b589-5d2f40bbd1b3 to disappear
Nov 29 13:38:50.067: INFO: Pod downwardapi-volume-87d2985b-6ddc-43cb-b589-5d2f40bbd1b3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:38:50.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9567" for this suite.
Nov 29 13:38:56.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:38:56.198: INFO: namespace projected-9567 deletion completed in 6.126349119s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:38:56.198: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4001
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
STEP: creating the pod
Nov 29 13:38:56.340: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-4001'
Nov 29 13:38:56.639: INFO: stderr: ""
Nov 29 13:38:56.639: INFO: stdout: "pod/pause created\n"
Nov 29 13:38:56.639: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Nov 29 13:38:56.639: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4001" to be "running and ready"
Nov 29 13:38:56.642: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 3.435598ms
Nov 29 13:38:58.648: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.008994606s
Nov 29 13:38:58.648: INFO: Pod "pause" satisfied condition "running and ready"
Nov 29 13:38:58.648: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Nov 29 13:38:58.648: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config label pods pause testing-label=testing-label-value --namespace=kubectl-4001'
Nov 29 13:38:58.773: INFO: stderr: ""
Nov 29 13:38:58.773: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Nov 29 13:38:58.773: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-4001'
Nov 29 13:38:58.853: INFO: stderr: ""
Nov 29 13:38:58.853: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Nov 29 13:38:58.853: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config label pods pause testing-label- --namespace=kubectl-4001'
Nov 29 13:38:58.953: INFO: stderr: ""
Nov 29 13:38:58.953: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Nov 29 13:38:58.953: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod pause -L testing-label --namespace=kubectl-4001'
Nov 29 13:38:59.034: INFO: stderr: ""
Nov 29 13:38:59.034: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1217
STEP: using delete to clean up resources
Nov 29 13:38:59.034: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-4001'
Nov 29 13:38:59.112: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 13:38:59.112: INFO: stdout: "pod \"pause\" force deleted\n"
Nov 29 13:38:59.112: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=pause --no-headers --namespace=kubectl-4001'
Nov 29 13:38:59.195: INFO: stderr: "No resources found.\n"
Nov 29 13:38:59.195: INFO: stdout: ""
Nov 29 13:38:59.195: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=pause --namespace=kubectl-4001 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 29 13:38:59.268: INFO: stderr: ""
Nov 29 13:38:59.268: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:38:59.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4001" for this suite.
Nov 29 13:39:05.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:39:05.400: INFO: namespace kubectl-4001 deletion completed in 6.127299301s
•SS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:39:05.401: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3148
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-2484b353-3562-45bb-8b66-beaf564cf917
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-2484b353-3562-45bb-8b66-beaf564cf917
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:39:09.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3148" for this suite.
Nov 29 13:39:31.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:39:31.805: INFO: namespace configmap-3148 deletion completed in 22.127085375s
•SS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:39:31.806: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-5252
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-5252
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-5252
STEP: Creating statefulset with conflicting port in namespace statefulset-5252
STEP: Waiting until pod test-pod will start running in namespace statefulset-5252
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-5252
Nov 29 13:39:34.020: INFO: Observed stateful pod in namespace: statefulset-5252, name: ss-0, uid: f5cbb9d2-f931-460f-8bd7-205e458e23f3, status phase: Pending. Waiting for statefulset controller to delete.
Nov 29 13:39:34.024: INFO: Observed stateful pod in namespace: statefulset-5252, name: ss-0, uid: f5cbb9d2-f931-460f-8bd7-205e458e23f3, status phase: Failed. Waiting for statefulset controller to delete.
Nov 29 13:39:34.065: INFO: Observed stateful pod in namespace: statefulset-5252, name: ss-0, uid: f5cbb9d2-f931-460f-8bd7-205e458e23f3, status phase: Failed. Waiting for statefulset controller to delete.
Nov 29 13:39:34.066: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-5252
STEP: Removing pod with conflicting port in namespace statefulset-5252
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-5252 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Nov 29 13:39:36.083: INFO: Deleting all statefulset in ns statefulset-5252
Nov 29 13:39:36.086: INFO: Scaling statefulset ss to 0
Nov 29 13:39:56.103: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 13:39:56.106: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:39:56.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5252" for this suite.
Nov 29 13:40:02.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:40:02.263: INFO: namespace statefulset-5252 deletion completed in 6.139565811s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:40:02.263: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1710
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 29 13:40:02.414: INFO: Waiting up to 5m0s for pod "downwardapi-volume-13ccd823-821b-4ceb-a774-cb28e7002a24" in namespace "projected-1710" to be "success or failure"
Nov 29 13:40:02.417: INFO: Pod "downwardapi-volume-13ccd823-821b-4ceb-a774-cb28e7002a24": Phase="Pending", Reason="", readiness=false. Elapsed: 2.799644ms
Nov 29 13:40:04.421: INFO: Pod "downwardapi-volume-13ccd823-821b-4ceb-a774-cb28e7002a24": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007201184s
STEP: Saw pod success
Nov 29 13:40:04.421: INFO: Pod "downwardapi-volume-13ccd823-821b-4ceb-a774-cb28e7002a24" satisfied condition "success or failure"
Nov 29 13:40:04.425: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod downwardapi-volume-13ccd823-821b-4ceb-a774-cb28e7002a24 container client-container: <nil>
STEP: delete the pod
Nov 29 13:40:04.442: INFO: Waiting for pod downwardapi-volume-13ccd823-821b-4ceb-a774-cb28e7002a24 to disappear
Nov 29 13:40:04.445: INFO: Pod downwardapi-volume-13ccd823-821b-4ceb-a774-cb28e7002a24 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:40:04.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1710" for this suite.
Nov 29 13:40:10.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:40:10.625: INFO: namespace projected-1710 deletion completed in 6.175554743s
•SSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:40:10.625: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-9405
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:40:13.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-9405" for this suite.
Nov 29 13:40:35.838: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:40:35.956: INFO: namespace replication-controller-9405 deletion completed in 22.129632688s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:40:35.957: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3805
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-d4522811-e091-41d3-9aaa-058fa469348e
STEP: Creating configMap with name cm-test-opt-upd-33515d9d-6451-4624-bfef-6b3f0cb3946d
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-d4522811-e091-41d3-9aaa-058fa469348e
STEP: Updating configmap cm-test-opt-upd-33515d9d-6451-4624-bfef-6b3f0cb3946d
STEP: Creating configMap with name cm-test-opt-create-4cf08914-244d-40dc-89ec-fd39c4239b49
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:40:40.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3805" for this suite.
Nov 29 13:41:02.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:41:02.700: INFO: namespace projected-3805 deletion completed in 22.137131452s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:41:02.700: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-841
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-841
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 29 13:41:02.843: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Nov 29 13:41:18.921: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.0.228:8080/dial?request=hostName&protocol=udp&host=100.64.1.62&port=8081&tries=1'] Namespace:pod-network-test-841 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 13:41:18.921: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Nov 29 13:41:19.329: INFO: Waiting for endpoints: map[]
Nov 29 13:41:19.333: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://100.64.0.228:8080/dial?request=hostName&protocol=udp&host=100.64.0.227&port=8081&tries=1'] Namespace:pod-network-test-841 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 13:41:19.333: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Nov 29 13:41:19.788: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:41:19.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-841" for this suite.
Nov 29 13:41:41.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:41:41.985: INFO: namespace pod-network-test-841 deletion completed in 22.188566573s
•
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:41:41.986: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-2527
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Nov 29 13:41:42.140: INFO: Waiting up to 5m0s for pod "client-containers-e9e85c09-c0ec-4dfd-8f6d-6ac4f5cee8b6" in namespace "containers-2527" to be "success or failure"
Nov 29 13:41:42.145: INFO: Pod "client-containers-e9e85c09-c0ec-4dfd-8f6d-6ac4f5cee8b6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.160455ms
Nov 29 13:41:44.149: INFO: Pod "client-containers-e9e85c09-c0ec-4dfd-8f6d-6ac4f5cee8b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009683338s
STEP: Saw pod success
Nov 29 13:41:44.150: INFO: Pod "client-containers-e9e85c09-c0ec-4dfd-8f6d-6ac4f5cee8b6" satisfied condition "success or failure"
Nov 29 13:41:44.153: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod client-containers-e9e85c09-c0ec-4dfd-8f6d-6ac4f5cee8b6 container test-container: <nil>
STEP: delete the pod
Nov 29 13:41:44.217: INFO: Waiting for pod client-containers-e9e85c09-c0ec-4dfd-8f6d-6ac4f5cee8b6 to disappear
Nov 29 13:41:44.220: INFO: Pod client-containers-e9e85c09-c0ec-4dfd-8f6d-6ac4f5cee8b6 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:41:44.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2527" for this suite.
Nov 29 13:41:50.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:41:50.373: INFO: namespace containers-2527 deletion completed in 6.148202384s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:41:50.374: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8257
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-c3ed2cfb-4a99-4d3e-816d-ccaceaccfeb8
STEP: Creating a pod to test consume secrets
Nov 29 13:41:50.545: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6f2cb8ea-0117-40dc-b7e4-6cb1cdc4c8c6" in namespace "projected-8257" to be "success or failure"
Nov 29 13:41:50.550: INFO: Pod "pod-projected-secrets-6f2cb8ea-0117-40dc-b7e4-6cb1cdc4c8c6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.568752ms
Nov 29 13:41:52.557: INFO: Pod "pod-projected-secrets-6f2cb8ea-0117-40dc-b7e4-6cb1cdc4c8c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011603519s
STEP: Saw pod success
Nov 29 13:41:52.557: INFO: Pod "pod-projected-secrets-6f2cb8ea-0117-40dc-b7e4-6cb1cdc4c8c6" satisfied condition "success or failure"
Nov 29 13:41:52.562: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-projected-secrets-6f2cb8ea-0117-40dc-b7e4-6cb1cdc4c8c6 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 29 13:41:52.589: INFO: Waiting for pod pod-projected-secrets-6f2cb8ea-0117-40dc-b7e4-6cb1cdc4c8c6 to disappear
Nov 29 13:41:52.594: INFO: Pod pod-projected-secrets-6f2cb8ea-0117-40dc-b7e4-6cb1cdc4c8c6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:41:52.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8257" for this suite.
Nov 29 13:41:58.620: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:41:58.824: INFO: namespace projected-8257 deletion completed in 6.220218388s
•SS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:41:58.824: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-3059
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Nov 29 13:41:59.258: INFO: Pod name wrapped-volume-race-911ed3aa-26dc-4884-b0e9-07d5d595753e: Found 0 pods out of 5
Nov 29 13:42:04.274: INFO: Pod name wrapped-volume-race-911ed3aa-26dc-4884-b0e9-07d5d595753e: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-911ed3aa-26dc-4884-b0e9-07d5d595753e in namespace emptydir-wrapper-3059, will wait for the garbage collector to delete the pods
Nov 29 13:42:12.371: INFO: Deleting ReplicationController wrapped-volume-race-911ed3aa-26dc-4884-b0e9-07d5d595753e took: 8.227132ms
Nov 29 13:42:12.772: INFO: Terminating ReplicationController wrapped-volume-race-911ed3aa-26dc-4884-b0e9-07d5d595753e pods took: 400.258053ms
STEP: Creating RC which spawns configmap-volume pods
Nov 29 13:42:48.295: INFO: Pod name wrapped-volume-race-8b706630-10bd-422f-b1a8-f80633f811c0: Found 0 pods out of 5
Nov 29 13:42:53.311: INFO: Pod name wrapped-volume-race-8b706630-10bd-422f-b1a8-f80633f811c0: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8b706630-10bd-422f-b1a8-f80633f811c0 in namespace emptydir-wrapper-3059, will wait for the garbage collector to delete the pods
Nov 29 13:42:53.408: INFO: Deleting ReplicationController wrapped-volume-race-8b706630-10bd-422f-b1a8-f80633f811c0 took: 10.093848ms
Nov 29 13:42:53.810: INFO: Terminating ReplicationController wrapped-volume-race-8b706630-10bd-422f-b1a8-f80633f811c0 pods took: 401.589234ms
STEP: Creating RC which spawns configmap-volume pods
Nov 29 13:43:38.226: INFO: Pod name wrapped-volume-race-899daecc-72b5-4993-99f1-c8d3a1ba8744: Found 0 pods out of 5
Nov 29 13:43:43.235: INFO: Pod name wrapped-volume-race-899daecc-72b5-4993-99f1-c8d3a1ba8744: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-899daecc-72b5-4993-99f1-c8d3a1ba8744 in namespace emptydir-wrapper-3059, will wait for the garbage collector to delete the pods
Nov 29 13:43:43.312: INFO: Deleting ReplicationController wrapped-volume-race-899daecc-72b5-4993-99f1-c8d3a1ba8744 took: 6.906221ms
Nov 29 13:43:43.813: INFO: Terminating ReplicationController wrapped-volume-race-899daecc-72b5-4993-99f1-c8d3a1ba8744 pods took: 500.375514ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:44:18.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3059" for this suite.
Nov 29 13:44:24.384: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:44:24.510: INFO: namespace emptydir-wrapper-3059 deletion completed in 6.137989922s
•SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:44:24.510: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3768
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-0934ab05-6cff-4106-a718-c76f1e19b924 in namespace container-probe-3768
Nov 29 13:44:26.675: INFO: Started pod liveness-0934ab05-6cff-4106-a718-c76f1e19b924 in namespace container-probe-3768
STEP: checking the pod's current state and verifying that restartCount is present
Nov 29 13:44:26.678: INFO: Initial restart count of pod liveness-0934ab05-6cff-4106-a718-c76f1e19b924 is 0
Nov 29 13:44:50.738: INFO: Restart count of pod container-probe-3768/liveness-0934ab05-6cff-4106-a718-c76f1e19b924 is now 1 (24.059772211s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:44:50.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3768" for this suite.
Nov 29 13:44:56.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:44:56.886: INFO: namespace container-probe-3768 deletion completed in 6.135009499s
•SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:44:56.886: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-9752
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 29 13:44:57.029: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:44:58.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9752" for this suite.
Nov 29 13:45:04.096: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:45:04.219: INFO: namespace custom-resource-definition-9752 deletion completed in 6.134976003s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:45:04.219: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2570
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 29 13:45:04.376: INFO: Waiting up to 5m0s for pod "pod-ddb1416f-638c-4b08-9858-a71148e56e35" in namespace "emptydir-2570" to be "success or failure"
Nov 29 13:45:04.382: INFO: Pod "pod-ddb1416f-638c-4b08-9858-a71148e56e35": Phase="Pending", Reason="", readiness=false. Elapsed: 5.646313ms
Nov 29 13:45:06.390: INFO: Pod "pod-ddb1416f-638c-4b08-9858-a71148e56e35": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013899944s
STEP: Saw pod success
Nov 29 13:45:06.390: INFO: Pod "pod-ddb1416f-638c-4b08-9858-a71148e56e35" satisfied condition "success or failure"
Nov 29 13:45:06.395: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-ddb1416f-638c-4b08-9858-a71148e56e35 container test-container: <nil>
STEP: delete the pod
Nov 29 13:45:06.417: INFO: Waiting for pod pod-ddb1416f-638c-4b08-9858-a71148e56e35 to disappear
Nov 29 13:45:06.421: INFO: Pod pod-ddb1416f-638c-4b08-9858-a71148e56e35 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:45:06.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2570" for this suite.
Nov 29 13:45:12.438: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:45:12.565: INFO: namespace emptydir-2570 deletion completed in 6.138924911s
•
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:45:12.565: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7151
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 29 13:45:12.719: INFO: Waiting up to 5m0s for pod "pod-5c00750d-42c5-4a27-98ae-8f866c1fb897" in namespace "emptydir-7151" to be "success or failure"
Nov 29 13:45:12.722: INFO: Pod "pod-5c00750d-42c5-4a27-98ae-8f866c1fb897": Phase="Pending", Reason="", readiness=false. Elapsed: 3.487896ms
Nov 29 13:45:14.729: INFO: Pod "pod-5c00750d-42c5-4a27-98ae-8f866c1fb897": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010330038s
STEP: Saw pod success
Nov 29 13:45:14.729: INFO: Pod "pod-5c00750d-42c5-4a27-98ae-8f866c1fb897" satisfied condition "success or failure"
Nov 29 13:45:14.733: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-5c00750d-42c5-4a27-98ae-8f866c1fb897 container test-container: <nil>
STEP: delete the pod
Nov 29 13:45:14.751: INFO: Waiting for pod pod-5c00750d-42c5-4a27-98ae-8f866c1fb897 to disappear
Nov 29 13:45:14.754: INFO: Pod pod-5c00750d-42c5-4a27-98ae-8f866c1fb897 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:45:14.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7151" for this suite.
Nov 29 13:45:20.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:45:20.916: INFO: namespace emptydir-7151 deletion completed in 6.156669843s
•SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:45:20.916: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-4334
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Nov 29 13:45:24.097: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:45:25.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-4334" for this suite.
Nov 29 13:45:47.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:45:47.264: INFO: namespace replicaset-4334 deletion completed in 22.143660237s
•SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:45:47.264: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4520
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4520
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Nov 29 13:45:47.423: INFO: Found 0 stateful pods, waiting for 3
Nov 29 13:45:57.428: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 13:45:57.428: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 13:45:57.428: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Nov 29 13:45:57.439: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4520 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 29 13:45:58.140: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov 29 13:45:58.140: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 29 13:45:58.140: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Nov 29 13:46:08.174: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Nov 29 13:46:18.194: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4520 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:46:18.678: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Nov 29 13:46:18.678: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 29 13:46:18.678: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov 29 13:46:38.702: INFO: Waiting for StatefulSet statefulset-4520/ss2 to complete update
Nov 29 13:46:38.702: INFO: Waiting for Pod statefulset-4520/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Nov 29 13:46:48.710: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4520 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Nov 29 13:46:49.210: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Nov 29 13:46:49.210: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Nov 29 13:46:49.210: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Nov 29 13:46:59.245: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Nov 29 13:47:09.267: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config exec --namespace=statefulset-4520 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Nov 29 13:47:09.822: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Nov 29 13:47:09.822: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Nov 29 13:47:09.822: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Nov 29 13:47:19.844: INFO: Waiting for StatefulSet statefulset-4520/ss2 to complete update
Nov 29 13:47:19.845: INFO: Waiting for Pod statefulset-4520/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Nov 29 13:47:29.854: INFO: Deleting all statefulset in ns statefulset-4520
Nov 29 13:47:29.857: INFO: Scaling statefulset ss2 to 0
Nov 29 13:47:59.873: INFO: Waiting for statefulset status.replicas updated to 0
Nov 29 13:47:59.877: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:47:59.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4520" for this suite.
Nov 29 13:48:05.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:48:06.073: INFO: namespace statefulset-4520 deletion completed in 6.177372742s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:48:06.074: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-5426
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Nov 29 13:48:06.240: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5426,SelfLink:/api/v1/namespaces/watch-5426/configmaps/e2e-watch-test-label-changed,UID:f3d67ba8-b619-44c2-b75d-131172d506f5,ResourceVersion:21096,Generation:0,CreationTimestamp:2019-11-29 13:48:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Nov 29 13:48:06.240: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5426,SelfLink:/api/v1/namespaces/watch-5426/configmaps/e2e-watch-test-label-changed,UID:f3d67ba8-b619-44c2-b75d-131172d506f5,ResourceVersion:21097,Generation:0,CreationTimestamp:2019-11-29 13:48:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Nov 29 13:48:06.240: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5426,SelfLink:/api/v1/namespaces/watch-5426/configmaps/e2e-watch-test-label-changed,UID:f3d67ba8-b619-44c2-b75d-131172d506f5,ResourceVersion:21098,Generation:0,CreationTimestamp:2019-11-29 13:48:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Nov 29 13:48:16.277: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5426,SelfLink:/api/v1/namespaces/watch-5426/configmaps/e2e-watch-test-label-changed,UID:f3d67ba8-b619-44c2-b75d-131172d506f5,ResourceVersion:21123,Generation:0,CreationTimestamp:2019-11-29 13:48:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Nov 29 13:48:16.277: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5426,SelfLink:/api/v1/namespaces/watch-5426/configmaps/e2e-watch-test-label-changed,UID:f3d67ba8-b619-44c2-b75d-131172d506f5,ResourceVersion:21124,Generation:0,CreationTimestamp:2019-11-29 13:48:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Nov 29 13:48:16.277: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-5426,SelfLink:/api/v1/namespaces/watch-5426/configmaps/e2e-watch-test-label-changed,UID:f3d67ba8-b619-44c2-b75d-131172d506f5,ResourceVersion:21125,Generation:0,CreationTimestamp:2019-11-29 13:48:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:48:16.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5426" for this suite.
Nov 29 13:48:22.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:48:22.432: INFO: namespace watch-5426 deletion completed in 6.146062752s
•SSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:48:22.432: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-652
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 29 13:48:22.591: INFO: (0) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 9.338479ms)
Nov 29 13:48:22.634: INFO: (1) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 43.203303ms)
Nov 29 13:48:22.640: INFO: (2) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.0459ms)
Nov 29 13:48:22.646: INFO: (3) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.455452ms)
Nov 29 13:48:22.653: INFO: (4) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.225126ms)
Nov 29 13:48:22.659: INFO: (5) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.27759ms)
Nov 29 13:48:22.664: INFO: (6) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.140027ms)
Nov 29 13:48:22.669: INFO: (7) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.075748ms)
Nov 29 13:48:22.677: INFO: (8) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.0254ms)
Nov 29 13:48:22.683: INFO: (9) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.412778ms)
Nov 29 13:48:22.688: INFO: (10) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.956664ms)
Nov 29 13:48:22.693: INFO: (11) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.695027ms)
Nov 29 13:48:22.699: INFO: (12) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.323415ms)
Nov 29 13:48:22.705: INFO: (13) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.779552ms)
Nov 29 13:48:22.710: INFO: (14) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.725975ms)
Nov 29 13:48:22.719: INFO: (15) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.223345ms)
Nov 29 13:48:22.726: INFO: (16) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.535896ms)
Nov 29 13:48:22.731: INFO: (17) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.026647ms)
Nov 29 13:48:22.737: INFO: (18) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.535025ms)
Nov 29 13:48:22.742: INFO: (19) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.130074ms)
[AfterEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:48:22.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-652" for this suite.
Nov 29 13:48:28.759: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:48:28.883: INFO: namespace proxy-652 deletion completed in 6.136693806s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:48:28.884: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2892
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:48:31.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2892" for this suite.
Nov 29 13:49:21.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:49:21.211: INFO: namespace kubelet-test-2892 deletion completed in 50.142488447s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:49:21.211: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6325
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Nov 29 13:49:21.360: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config create -f - --namespace=kubectl-6325'
Nov 29 13:49:21.648: INFO: stderr: ""
Nov 29 13:49:21.648: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 29 13:49:21.648: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6325'
Nov 29 13:49:21.941: INFO: stderr: ""
Nov 29 13:49:21.941: INFO: stdout: "update-demo-nautilus-4wtkw update-demo-nautilus-5xp5v "
Nov 29 13:49:21.941: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-4wtkw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6325'
Nov 29 13:49:22.013: INFO: stderr: ""
Nov 29 13:49:22.013: INFO: stdout: ""
Nov 29 13:49:22.013: INFO: update-demo-nautilus-4wtkw is created but not running
Nov 29 13:49:27.014: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6325'
Nov 29 13:49:27.091: INFO: stderr: ""
Nov 29 13:49:27.091: INFO: stdout: "update-demo-nautilus-4wtkw update-demo-nautilus-5xp5v "
Nov 29 13:49:27.091: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-4wtkw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6325'
Nov 29 13:49:27.183: INFO: stderr: ""
Nov 29 13:49:27.184: INFO: stdout: "true"
Nov 29 13:49:27.184: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-4wtkw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6325'
Nov 29 13:49:27.278: INFO: stderr: ""
Nov 29 13:49:27.278: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 29 13:49:27.278: INFO: validating pod update-demo-nautilus-4wtkw
Nov 29 13:49:27.365: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 29 13:49:27.365: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 29 13:49:27.365: INFO: update-demo-nautilus-4wtkw is verified up and running
Nov 29 13:49:27.365: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-5xp5v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6325'
Nov 29 13:49:27.440: INFO: stderr: ""
Nov 29 13:49:27.440: INFO: stdout: "true"
Nov 29 13:49:27.440: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-5xp5v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6325'
Nov 29 13:49:27.523: INFO: stderr: ""
Nov 29 13:49:27.523: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 29 13:49:27.523: INFO: validating pod update-demo-nautilus-5xp5v
Nov 29 13:49:27.570: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 29 13:49:27.570: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 29 13:49:27.570: INFO: update-demo-nautilus-5xp5v is verified up and running
STEP: scaling down the replication controller
Nov 29 13:49:27.572: INFO: scanned /root for discovery docs: <nil>
Nov 29 13:49:27.573: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-6325'
Nov 29 13:49:28.682: INFO: stderr: ""
Nov 29 13:49:28.682: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 29 13:49:28.682: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6325'
Nov 29 13:49:28.765: INFO: stderr: ""
Nov 29 13:49:28.765: INFO: stdout: "update-demo-nautilus-4wtkw update-demo-nautilus-5xp5v "
STEP: Replicas for name=update-demo: expected=1 actual=2
Nov 29 13:49:33.766: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6325'
Nov 29 13:49:33.850: INFO: stderr: ""
Nov 29 13:49:33.850: INFO: stdout: "update-demo-nautilus-4wtkw update-demo-nautilus-5xp5v "
STEP: Replicas for name=update-demo: expected=1 actual=2
Nov 29 13:49:38.850: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6325'
Nov 29 13:49:39.112: INFO: stderr: ""
Nov 29 13:49:39.112: INFO: stdout: "update-demo-nautilus-4wtkw "
Nov 29 13:49:39.112: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-4wtkw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6325'
Nov 29 13:49:39.187: INFO: stderr: ""
Nov 29 13:49:39.187: INFO: stdout: "true"
Nov 29 13:49:39.187: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-4wtkw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6325'
Nov 29 13:49:39.263: INFO: stderr: ""
Nov 29 13:49:39.263: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 29 13:49:39.263: INFO: validating pod update-demo-nautilus-4wtkw
Nov 29 13:49:39.270: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 29 13:49:39.270: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 29 13:49:39.270: INFO: update-demo-nautilus-4wtkw is verified up and running
STEP: scaling up the replication controller
Nov 29 13:49:39.272: INFO: scanned /root for discovery docs: <nil>
Nov 29 13:49:39.272: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-6325'
Nov 29 13:49:40.381: INFO: stderr: ""
Nov 29 13:49:40.382: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Nov 29 13:49:40.382: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6325'
Nov 29 13:49:40.473: INFO: stderr: ""
Nov 29 13:49:40.473: INFO: stdout: "update-demo-nautilus-27kmq update-demo-nautilus-4wtkw "
Nov 29 13:49:40.473: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-27kmq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6325'
Nov 29 13:49:40.549: INFO: stderr: ""
Nov 29 13:49:40.549: INFO: stdout: "true"
Nov 29 13:49:40.549: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-27kmq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6325'
Nov 29 13:49:40.622: INFO: stderr: ""
Nov 29 13:49:40.622: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 29 13:49:40.622: INFO: validating pod update-demo-nautilus-27kmq
Nov 29 13:49:40.713: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 29 13:49:40.714: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 29 13:49:40.714: INFO: update-demo-nautilus-27kmq is verified up and running
Nov 29 13:49:40.714: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-4wtkw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6325'
Nov 29 13:49:40.792: INFO: stderr: ""
Nov 29 13:49:40.792: INFO: stdout: "true"
Nov 29 13:49:40.792: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods update-demo-nautilus-4wtkw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6325'
Nov 29 13:49:40.865: INFO: stderr: ""
Nov 29 13:49:40.865: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Nov 29 13:49:40.865: INFO: validating pod update-demo-nautilus-4wtkw
Nov 29 13:49:40.872: INFO: got data: {
  "image": "nautilus.jpg"
}

Nov 29 13:49:40.872: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Nov 29 13:49:40.872: INFO: update-demo-nautilus-4wtkw is verified up and running
STEP: using delete to clean up resources
Nov 29 13:49:40.872: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete --grace-period=0 --force -f - --namespace=kubectl-6325'
Nov 29 13:49:40.962: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Nov 29 13:49:40.963: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Nov 29 13:49:40.963: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6325'
Nov 29 13:49:41.078: INFO: stderr: "No resources found.\n"
Nov 29 13:49:41.078: INFO: stdout: ""
Nov 29 13:49:41.078: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pods -l name=update-demo --namespace=kubectl-6325 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Nov 29 13:49:41.159: INFO: stderr: ""
Nov 29 13:49:41.159: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:49:41.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6325" for this suite.
Nov 29 13:50:03.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:50:03.312: INFO: namespace kubectl-6325 deletion completed in 22.142001957s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:50:03.312: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8651
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 29 13:50:03.465: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1ba42596-b9d9-4e84-b397-50ae86af32e6" in namespace "downward-api-8651" to be "success or failure"
Nov 29 13:50:03.471: INFO: Pod "downwardapi-volume-1ba42596-b9d9-4e84-b397-50ae86af32e6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.350143ms
Nov 29 13:50:05.475: INFO: Pod "downwardapi-volume-1ba42596-b9d9-4e84-b397-50ae86af32e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010070967s
STEP: Saw pod success
Nov 29 13:50:05.476: INFO: Pod "downwardapi-volume-1ba42596-b9d9-4e84-b397-50ae86af32e6" satisfied condition "success or failure"
Nov 29 13:50:05.478: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod downwardapi-volume-1ba42596-b9d9-4e84-b397-50ae86af32e6 container client-container: <nil>
STEP: delete the pod
Nov 29 13:50:05.499: INFO: Waiting for pod downwardapi-volume-1ba42596-b9d9-4e84-b397-50ae86af32e6 to disappear
Nov 29 13:50:05.503: INFO: Pod downwardapi-volume-1ba42596-b9d9-4e84-b397-50ae86af32e6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:50:05.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8651" for this suite.
Nov 29 13:50:11.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:50:11.644: INFO: namespace downward-api-8651 deletion completed in 6.136101543s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:50:11.644: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-6429
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-6429
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Nov 29 13:50:11.796: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Nov 29 13:50:31.868: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.1.82:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6429 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 13:50:31.868: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Nov 29 13:50:32.238: INFO: Found all expected endpoints: [netserver-0]
Nov 29 13:50:32.243: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://100.64.0.246:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-6429 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Nov 29 13:50:32.243: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
Nov 29 13:50:32.662: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:50:32.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6429" for this suite.
Nov 29 13:50:54.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:50:54.818: INFO: namespace pod-network-test-6429 deletion completed in 22.150357657s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:50:54.819: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9631
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Nov 29 13:50:54.970: INFO: Waiting up to 5m0s for pod "var-expansion-7bf33f2b-41dd-4316-a14f-3548ec5fbfc1" in namespace "var-expansion-9631" to be "success or failure"
Nov 29 13:50:54.973: INFO: Pod "var-expansion-7bf33f2b-41dd-4316-a14f-3548ec5fbfc1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.2656ms
Nov 29 13:50:56.978: INFO: Pod "var-expansion-7bf33f2b-41dd-4316-a14f-3548ec5fbfc1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00801958s
STEP: Saw pod success
Nov 29 13:50:56.978: INFO: Pod "var-expansion-7bf33f2b-41dd-4316-a14f-3548ec5fbfc1" satisfied condition "success or failure"
Nov 29 13:50:56.981: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod var-expansion-7bf33f2b-41dd-4316-a14f-3548ec5fbfc1 container dapi-container: <nil>
STEP: delete the pod
Nov 29 13:50:57.002: INFO: Waiting for pod var-expansion-7bf33f2b-41dd-4316-a14f-3548ec5fbfc1 to disappear
Nov 29 13:50:57.005: INFO: Pod var-expansion-7bf33f2b-41dd-4316-a14f-3548ec5fbfc1 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:50:57.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9631" for this suite.
Nov 29 13:51:03.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:51:03.146: INFO: namespace var-expansion-9631 deletion completed in 6.136478183s
•SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:51:03.146: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-6316
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-hxnj
STEP: Creating a pod to test atomic-volume-subpath
Nov 29 13:51:03.305: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-hxnj" in namespace "subpath-6316" to be "success or failure"
Nov 29 13:51:03.310: INFO: Pod "pod-subpath-test-configmap-hxnj": Phase="Pending", Reason="", readiness=false. Elapsed: 4.375468ms
Nov 29 13:51:05.315: INFO: Pod "pod-subpath-test-configmap-hxnj": Phase="Running", Reason="", readiness=true. Elapsed: 2.009452466s
Nov 29 13:51:07.330: INFO: Pod "pod-subpath-test-configmap-hxnj": Phase="Running", Reason="", readiness=true. Elapsed: 4.02398893s
Nov 29 13:51:09.342: INFO: Pod "pod-subpath-test-configmap-hxnj": Phase="Running", Reason="", readiness=true. Elapsed: 6.036490055s
Nov 29 13:51:11.356: INFO: Pod "pod-subpath-test-configmap-hxnj": Phase="Running", Reason="", readiness=true. Elapsed: 8.050507529s
Nov 29 13:51:13.361: INFO: Pod "pod-subpath-test-configmap-hxnj": Phase="Running", Reason="", readiness=true. Elapsed: 10.055471008s
Nov 29 13:51:15.366: INFO: Pod "pod-subpath-test-configmap-hxnj": Phase="Running", Reason="", readiness=true. Elapsed: 12.060077191s
Nov 29 13:51:17.370: INFO: Pod "pod-subpath-test-configmap-hxnj": Phase="Running", Reason="", readiness=true. Elapsed: 14.06421987s
Nov 29 13:51:19.375: INFO: Pod "pod-subpath-test-configmap-hxnj": Phase="Running", Reason="", readiness=true. Elapsed: 16.069193082s
Nov 29 13:51:21.380: INFO: Pod "pod-subpath-test-configmap-hxnj": Phase="Running", Reason="", readiness=true. Elapsed: 18.0743487s
Nov 29 13:51:23.385: INFO: Pod "pod-subpath-test-configmap-hxnj": Phase="Running", Reason="", readiness=true. Elapsed: 20.079032624s
Nov 29 13:51:25.390: INFO: Pod "pod-subpath-test-configmap-hxnj": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.084207734s
STEP: Saw pod success
Nov 29 13:51:25.390: INFO: Pod "pod-subpath-test-configmap-hxnj" satisfied condition "success or failure"
Nov 29 13:51:25.394: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-subpath-test-configmap-hxnj container test-container-subpath-configmap-hxnj: <nil>
STEP: delete the pod
Nov 29 13:51:25.414: INFO: Waiting for pod pod-subpath-test-configmap-hxnj to disappear
Nov 29 13:51:25.417: INFO: Pod pod-subpath-test-configmap-hxnj no longer exists
STEP: Deleting pod pod-subpath-test-configmap-hxnj
Nov 29 13:51:25.417: INFO: Deleting pod "pod-subpath-test-configmap-hxnj" in namespace "subpath-6316"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:51:25.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6316" for this suite.
Nov 29 13:51:31.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:51:31.593: INFO: namespace subpath-6316 deletion completed in 6.167417623s
•SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:51:31.593: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4313
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 29 13:51:31.763: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Nov 29 13:51:31.772: INFO: Number of nodes with available pods: 0
Nov 29 13:51:31.772: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Nov 29 13:51:31.789: INFO: Number of nodes with available pods: 0
Nov 29 13:51:31.789: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:51:32.794: INFO: Number of nodes with available pods: 0
Nov 29 13:51:32.794: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:51:33.794: INFO: Number of nodes with available pods: 1
Nov 29 13:51:33.794: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Nov 29 13:51:33.809: INFO: Number of nodes with available pods: 1
Nov 29 13:51:33.809: INFO: Number of running nodes: 0, number of available pods: 1
Nov 29 13:51:34.814: INFO: Number of nodes with available pods: 0
Nov 29 13:51:34.815: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Nov 29 13:51:34.824: INFO: Number of nodes with available pods: 0
Nov 29 13:51:34.825: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:51:35.830: INFO: Number of nodes with available pods: 0
Nov 29 13:51:35.830: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:51:36.829: INFO: Number of nodes with available pods: 0
Nov 29 13:51:36.829: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:51:37.829: INFO: Number of nodes with available pods: 0
Nov 29 13:51:37.829: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:51:38.829: INFO: Number of nodes with available pods: 0
Nov 29 13:51:38.829: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:51:39.829: INFO: Number of nodes with available pods: 0
Nov 29 13:51:39.829: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:51:40.829: INFO: Number of nodes with available pods: 0
Nov 29 13:51:40.830: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:51:41.837: INFO: Number of nodes with available pods: 0
Nov 29 13:51:41.837: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:51:42.829: INFO: Number of nodes with available pods: 0
Nov 29 13:51:42.829: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:51:43.830: INFO: Number of nodes with available pods: 0
Nov 29 13:51:43.830: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:51:44.829: INFO: Number of nodes with available pods: 0
Nov 29 13:51:44.829: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:51:45.830: INFO: Number of nodes with available pods: 0
Nov 29 13:51:45.830: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:51:46.830: INFO: Number of nodes with available pods: 0
Nov 29 13:51:46.834: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:51:47.829: INFO: Number of nodes with available pods: 0
Nov 29 13:51:47.829: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:51:48.829: INFO: Number of nodes with available pods: 0
Nov 29 13:51:48.829: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:51:49.829: INFO: Number of nodes with available pods: 1
Nov 29 13:51:49.829: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4313, will wait for the garbage collector to delete the pods
Nov 29 13:51:49.895: INFO: Deleting DaemonSet.extensions daemon-set took: 6.346322ms
Nov 29 13:51:50.296: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.328373ms
Nov 29 13:51:58.103: INFO: Number of nodes with available pods: 0
Nov 29 13:51:58.104: INFO: Number of running nodes: 0, number of available pods: 0
Nov 29 13:51:58.108: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4313/daemonsets","resourceVersion":"21857"},"items":null}

Nov 29 13:51:58.111: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4313/pods","resourceVersion":"21857"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:51:58.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4313" for this suite.
Nov 29 13:52:04.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:52:04.297: INFO: namespace daemonsets-4313 deletion completed in 6.165220726s
•SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:52:04.298: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6421
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1721
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Nov 29 13:52:04.443: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-6421'
Nov 29 13:52:04.563: INFO: stderr: ""
Nov 29 13:52:04.563: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Nov 29 13:52:09.616: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config get pod e2e-test-nginx-pod --namespace=kubectl-6421 -o json'
Nov 29 13:52:09.752: INFO: stderr: ""
Nov 29 13:52:09.752: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"100.64.0.250/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-11-29T13:52:04Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-6421\",\n        \"resourceVersion\": \"21892\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6421/pods/e2e-test-nginx-pod\",\n        \"uid\": \"7b635534-e32a-47e3-a0c6-3dbcb22626c7\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-9dkmt\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-9dkmt\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-9dkmt\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-11-29T13:52:04Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-11-29T13:52:06Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-11-29T13:52:06Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-11-29T13:52:04Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://e038d6a1f1900960790cd397f6122b217802d891781bc28f79978087ac429333\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-11-29T13:52:05Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.250.0.10\",\n        \"phase\": \"Running\",\n        \"podIP\": \"100.64.0.250\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-11-29T13:52:04Z\"\n    }\n}\n"
STEP: replace the image in the pod
Nov 29 13:52:09.752: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config replace -f - --namespace=kubectl-6421'
Nov 29 13:52:10.004: INFO: stderr: ""
Nov 29 13:52:10.004: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1726
Nov 29 13:52:10.008: INFO: Running '/go/src/k8s.io/kubernetes/platforms/linux/amd64/kubectl --server=https://api.tm0z6-8fb.it.internal.staging.k8s.ondemand.com --kubeconfig=/tmp/tm/kubeconfig/shoot.config delete pods e2e-test-nginx-pod --namespace=kubectl-6421'
Nov 29 13:52:18.906: INFO: stderr: ""
Nov 29 13:52:18.906: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:52:18.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6421" for this suite.
Nov 29 13:52:24.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:52:25.056: INFO: namespace kubectl-6421 deletion completed in 6.144379457s
•
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:52:25.056: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9279
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-85d950e7-fcce-431a-a2f7-c18676385ed6
STEP: Creating a pod to test consume secrets
Nov 29 13:52:25.211: INFO: Waiting up to 5m0s for pod "pod-secrets-158cfaf2-e2c8-4053-9b46-57940b9aee10" in namespace "secrets-9279" to be "success or failure"
Nov 29 13:52:25.215: INFO: Pod "pod-secrets-158cfaf2-e2c8-4053-9b46-57940b9aee10": Phase="Pending", Reason="", readiness=false. Elapsed: 4.062161ms
Nov 29 13:52:27.220: INFO: Pod "pod-secrets-158cfaf2-e2c8-4053-9b46-57940b9aee10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00909689s
STEP: Saw pod success
Nov 29 13:52:27.220: INFO: Pod "pod-secrets-158cfaf2-e2c8-4053-9b46-57940b9aee10" satisfied condition "success or failure"
Nov 29 13:52:27.224: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-secrets-158cfaf2-e2c8-4053-9b46-57940b9aee10 container secret-volume-test: <nil>
STEP: delete the pod
Nov 29 13:52:27.245: INFO: Waiting for pod pod-secrets-158cfaf2-e2c8-4053-9b46-57940b9aee10 to disappear
Nov 29 13:52:27.249: INFO: Pod pod-secrets-158cfaf2-e2c8-4053-9b46-57940b9aee10 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:52:27.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9279" for this suite.
Nov 29 13:52:33.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:52:33.408: INFO: namespace secrets-9279 deletion completed in 6.153107909s
•S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:52:33.408: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7877
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 29 13:52:33.558: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ec0b3a26-86a9-418f-83f0-c7a99b35b5e7" in namespace "projected-7877" to be "success or failure"
Nov 29 13:52:33.562: INFO: Pod "downwardapi-volume-ec0b3a26-86a9-418f-83f0-c7a99b35b5e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.288266ms
Nov 29 13:52:35.567: INFO: Pod "downwardapi-volume-ec0b3a26-86a9-418f-83f0-c7a99b35b5e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008878845s
STEP: Saw pod success
Nov 29 13:52:35.567: INFO: Pod "downwardapi-volume-ec0b3a26-86a9-418f-83f0-c7a99b35b5e7" satisfied condition "success or failure"
Nov 29 13:52:35.570: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod downwardapi-volume-ec0b3a26-86a9-418f-83f0-c7a99b35b5e7 container client-container: <nil>
STEP: delete the pod
Nov 29 13:52:35.594: INFO: Waiting for pod downwardapi-volume-ec0b3a26-86a9-418f-83f0-c7a99b35b5e7 to disappear
Nov 29 13:52:35.597: INFO: Pod downwardapi-volume-ec0b3a26-86a9-418f-83f0-c7a99b35b5e7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:52:35.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7877" for this suite.
Nov 29 13:52:41.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:52:41.769: INFO: namespace projected-7877 deletion completed in 6.16691097s
•SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:52:41.770: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4142
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4142.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4142.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Nov 29 13:52:44.590: INFO: DNS probes using dns-4142/dns-test-97f88ce6-0ba2-415f-991d-6b6c162f11f4 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:52:44.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4142" for this suite.
Nov 29 13:52:50.647: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:52:50.786: INFO: namespace dns-4142 deletion completed in 6.162962009s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:52:50.786: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7652
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-5e4436ba-3f3c-4a2a-b623-2d64576ea1b0
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-5e4436ba-3f3c-4a2a-b623-2d64576ea1b0
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:52:55.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7652" for this suite.
Nov 29 13:53:17.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:53:17.258: INFO: namespace projected-7652 deletion completed in 22.156788107s
•SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:53:17.258: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-7374
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:53:19.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7374" for this suite.
Nov 29 13:53:59.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:53:59.593: INFO: namespace kubelet-test-7374 deletion completed in 40.141923827s
•SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:53:59.593: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-5118
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:54:01.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5118" for this suite.
Nov 29 13:54:39.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:54:39.911: INFO: namespace kubelet-test-5118 deletion completed in 38.135309906s
•SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:54:39.911: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2211
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Nov 29 13:54:40.063: INFO: Waiting up to 5m0s for pod "pod-ea87dcee-ad9d-4d9f-8a46-a79060321ee7" in namespace "emptydir-2211" to be "success or failure"
Nov 29 13:54:40.069: INFO: Pod "pod-ea87dcee-ad9d-4d9f-8a46-a79060321ee7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.706333ms
Nov 29 13:54:42.073: INFO: Pod "pod-ea87dcee-ad9d-4d9f-8a46-a79060321ee7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010613581s
STEP: Saw pod success
Nov 29 13:54:42.074: INFO: Pod "pod-ea87dcee-ad9d-4d9f-8a46-a79060321ee7" satisfied condition "success or failure"
Nov 29 13:54:42.077: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-ea87dcee-ad9d-4d9f-8a46-a79060321ee7 container test-container: <nil>
STEP: delete the pod
Nov 29 13:54:42.097: INFO: Waiting for pod pod-ea87dcee-ad9d-4d9f-8a46-a79060321ee7 to disappear
Nov 29 13:54:42.100: INFO: Pod pod-ea87dcee-ad9d-4d9f-8a46-a79060321ee7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:54:42.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2211" for this suite.
Nov 29 13:54:48.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:54:48.288: INFO: namespace emptydir-2211 deletion completed in 6.181254393s
•SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:54:48.288: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5926
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Nov 29 13:54:48.436: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Nov 29 13:54:48.445: INFO: Waiting for terminating namespaces to be deleted...
Nov 29 13:54:48.448: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x before test
Nov 29 13:54:48.466: INFO: node-exporter-zhlwf from kube-system started at 2019-11-29 12:14:04 +0000 UTC (1 container statuses recorded)
Nov 29 13:54:48.466: INFO: 	Container node-exporter ready: true, restart count 0
Nov 29 13:54:48.466: INFO: vpn-shoot-78776d6d84-gttn4 from kube-system started at 2019-11-29 12:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 13:54:48.466: INFO: 	Container vpn-shoot ready: true, restart count 0
Nov 29 13:54:48.466: INFO: coredns-858b686868-cd7fh from kube-system started at 2019-11-29 12:14:16 +0000 UTC (1 container statuses recorded)
Nov 29 13:54:48.466: INFO: 	Container coredns ready: true, restart count 0
Nov 29 13:54:48.466: INFO: calico-typha-horizontal-autoscaler-554dfbfdd7-htv8l from kube-system started at 2019-11-29 12:14:17 +0000 UTC (1 container statuses recorded)
Nov 29 13:54:48.466: INFO: 	Container autoscaler ready: true, restart count 0
Nov 29 13:54:48.466: INFO: metrics-server-5d767db8b7-nk95s from kube-system started at 2019-11-29 12:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 13:54:48.466: INFO: 	Container metrics-server ready: true, restart count 0
Nov 29 13:54:48.466: INFO: addons-nginx-ingress-controller-8468678b64-6zjss from kube-system started at 2019-11-29 12:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 13:54:48.466: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Nov 29 13:54:48.466: INFO: kube-proxy-xhmk5 from kube-system started at 2019-11-29 12:14:04 +0000 UTC (1 container statuses recorded)
Nov 29 13:54:48.466: INFO: 	Container kube-proxy ready: true, restart count 0
Nov 29 13:54:48.466: INFO: addons-nginx-ingress-nginx-ingress-k8s-backend-88d6cff74-67jk5 from kube-system started at 2019-11-29 12:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 13:54:48.466: INFO: 	Container nginx-ingress-nginx-ingress-k8s-backend ready: true, restart count 0
Nov 29 13:54:48.466: INFO: addons-kubernetes-dashboard-5c8d9945bc-cn9cj from kube-system started at 2019-11-29 12:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 13:54:48.466: INFO: 	Container kubernetes-dashboard ready: true, restart count 0
Nov 29 13:54:48.466: INFO: calico-node-nqnzx from kube-system started at 2019-11-29 12:14:04 +0000 UTC (1 container statuses recorded)
Nov 29 13:54:48.466: INFO: 	Container calico-node ready: true, restart count 0
Nov 29 13:54:48.466: INFO: calico-kube-controllers-5d785bc598-x2vsh from kube-system started at 2019-11-29 12:14:17 +0000 UTC (1 container statuses recorded)
Nov 29 13:54:48.466: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Nov 29 13:54:48.466: INFO: node-problem-detector-v46z5 from kube-system started at 2019-11-29 12:14:04 +0000 UTC (1 container statuses recorded)
Nov 29 13:54:48.466: INFO: 	Container node-problem-detector ready: true, restart count 0
Nov 29 13:54:48.466: INFO: calico-typha-vertical-autoscaler-656557779f-qhsh7 from kube-system started at 2019-11-29 12:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 13:54:48.466: INFO: 	Container autoscaler ready: true, restart count 4
Nov 29 13:54:48.466: INFO: coredns-858b686868-mnrpm from kube-system started at 2019-11-29 12:14:14 +0000 UTC (1 container statuses recorded)
Nov 29 13:54:48.466: INFO: 	Container coredns ready: true, restart count 0
Nov 29 13:54:48.466: INFO: 
Logging pods the kubelet thinks is on node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj before test
Nov 29 13:54:48.478: INFO: calico-node-pk2sw from kube-system started at 2019-11-29 12:13:49 +0000 UTC (1 container statuses recorded)
Nov 29 13:54:48.478: INFO: 	Container calico-node ready: true, restart count 0
Nov 29 13:54:48.478: INFO: node-problem-detector-fb5tv from kube-system started at 2019-11-29 12:13:49 +0000 UTC (1 container statuses recorded)
Nov 29 13:54:48.478: INFO: 	Container node-problem-detector ready: true, restart count 0
Nov 29 13:54:48.478: INFO: blackbox-exporter-c87bdd467-lhxjt from kube-system started at 2019-11-29 12:13:49 +0000 UTC (1 container statuses recorded)
Nov 29 13:54:48.478: INFO: 	Container blackbox-exporter ready: true, restart count 0
Nov 29 13:54:48.478: INFO: node-exporter-rpgkl from kube-system started at 2019-11-29 12:13:49 +0000 UTC (1 container statuses recorded)
Nov 29 13:54:48.478: INFO: 	Container node-exporter ready: true, restart count 1
Nov 29 13:54:48.478: INFO: kube-proxy-n9fj2 from kube-system started at 2019-11-29 12:13:49 +0000 UTC (1 container statuses recorded)
Nov 29 13:54:48.478: INFO: 	Container kube-proxy ready: true, restart count 1
Nov 29 13:54:48.478: INFO: calico-typha-deploy-5547c4cdc6-t5lfk from kube-system started at 2019-11-29 12:16:28 +0000 UTC (1 container statuses recorded)
Nov 29 13:54:48.478: INFO: 	Container calico-typha ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x
STEP: verifying the node has the label node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj
Nov 29 13:54:48.516: INFO: Pod addons-kubernetes-dashboard-5c8d9945bc-cn9cj requesting resource cpu=50m on Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x
Nov 29 13:54:48.516: INFO: Pod addons-nginx-ingress-controller-8468678b64-6zjss requesting resource cpu=100m on Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x
Nov 29 13:54:48.516: INFO: Pod addons-nginx-ingress-nginx-ingress-k8s-backend-88d6cff74-67jk5 requesting resource cpu=0m on Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x
Nov 29 13:54:48.516: INFO: Pod blackbox-exporter-c87bdd467-lhxjt requesting resource cpu=5m on Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj
Nov 29 13:54:48.516: INFO: Pod calico-kube-controllers-5d785bc598-x2vsh requesting resource cpu=0m on Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x
Nov 29 13:54:48.516: INFO: Pod calico-node-nqnzx requesting resource cpu=100m on Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x
Nov 29 13:54:48.516: INFO: Pod calico-node-pk2sw requesting resource cpu=100m on Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj
Nov 29 13:54:48.516: INFO: Pod calico-typha-deploy-5547c4cdc6-t5lfk requesting resource cpu=0m on Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj
Nov 29 13:54:48.516: INFO: Pod calico-typha-horizontal-autoscaler-554dfbfdd7-htv8l requesting resource cpu=10m on Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x
Nov 29 13:54:48.516: INFO: Pod calico-typha-vertical-autoscaler-656557779f-qhsh7 requesting resource cpu=0m on Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x
Nov 29 13:54:48.516: INFO: Pod coredns-858b686868-cd7fh requesting resource cpu=50m on Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x
Nov 29 13:54:48.516: INFO: Pod coredns-858b686868-mnrpm requesting resource cpu=50m on Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x
Nov 29 13:54:48.516: INFO: Pod kube-proxy-n9fj2 requesting resource cpu=20m on Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj
Nov 29 13:54:48.516: INFO: Pod kube-proxy-xhmk5 requesting resource cpu=20m on Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x
Nov 29 13:54:48.516: INFO: Pod metrics-server-5d767db8b7-nk95s requesting resource cpu=20m on Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x
Nov 29 13:54:48.516: INFO: Pod node-exporter-rpgkl requesting resource cpu=5m on Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj
Nov 29 13:54:48.516: INFO: Pod node-exporter-zhlwf requesting resource cpu=5m on Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x
Nov 29 13:54:48.516: INFO: Pod node-problem-detector-fb5tv requesting resource cpu=20m on Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj
Nov 29 13:54:48.516: INFO: Pod node-problem-detector-v46z5 requesting resource cpu=20m on Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x
Nov 29 13:54:48.516: INFO: Pod vpn-shoot-78776d6d84-gttn4 requesting resource cpu=100m on Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-32615dde-e597-412d-a029-b0c0ceb2f03d.15dba69d2367b0f9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5926/filler-pod-32615dde-e597-412d-a029-b0c0ceb2f03d to shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-32615dde-e597-412d-a029-b0c0ceb2f03d.15dba69d4ceb202d], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-32615dde-e597-412d-a029-b0c0ceb2f03d.15dba69d78c3ae0f], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-32615dde-e597-412d-a029-b0c0ceb2f03d.15dba69d7bfba1ce], Reason = [Created], Message = [Created container filler-pod-32615dde-e597-412d-a029-b0c0ceb2f03d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-32615dde-e597-412d-a029-b0c0ceb2f03d.15dba69d8545d165], Reason = [Started], Message = [Started container filler-pod-32615dde-e597-412d-a029-b0c0ceb2f03d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7829c707-4ab0-46d9-b742-7c615e4b96e3.15dba69d2310d326], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5926/filler-pod-7829c707-4ab0-46d9-b742-7c615e4b96e3 to shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7829c707-4ab0-46d9-b742-7c615e4b96e3.15dba69d4dbbb70d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7829c707-4ab0-46d9-b742-7c615e4b96e3.15dba69d50c57deb], Reason = [Created], Message = [Created container filler-pod-7829c707-4ab0-46d9-b742-7c615e4b96e3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-7829c707-4ab0-46d9-b742-7c615e4b96e3.15dba69d573feed9], Reason = [Started], Message = [Started container filler-pod-7829c707-4ab0-46d9-b742-7c615e4b96e3]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15dba69e133d909e], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu.]
STEP: removing the label node off the node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:54:53.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5926" for this suite.
Nov 29 13:54:59.607: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:54:59.733: INFO: namespace sched-pred-5926 deletion completed in 6.138812268s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:54:59.734: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-538
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-538/configmap-test-2c9a7f8b-21ee-4c7d-86d5-8d870890eca7
STEP: Creating a pod to test consume configMaps
Nov 29 13:54:59.888: INFO: Waiting up to 5m0s for pod "pod-configmaps-bf2c7f38-c0af-4f9e-81be-7d940c015082" in namespace "configmap-538" to be "success or failure"
Nov 29 13:54:59.892: INFO: Pod "pod-configmaps-bf2c7f38-c0af-4f9e-81be-7d940c015082": Phase="Pending", Reason="", readiness=false. Elapsed: 3.293709ms
Nov 29 13:55:01.897: INFO: Pod "pod-configmaps-bf2c7f38-c0af-4f9e-81be-7d940c015082": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008641401s
STEP: Saw pod success
Nov 29 13:55:01.897: INFO: Pod "pod-configmaps-bf2c7f38-c0af-4f9e-81be-7d940c015082" satisfied condition "success or failure"
Nov 29 13:55:01.901: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-configmaps-bf2c7f38-c0af-4f9e-81be-7d940c015082 container env-test: <nil>
STEP: delete the pod
Nov 29 13:55:01.921: INFO: Waiting for pod pod-configmaps-bf2c7f38-c0af-4f9e-81be-7d940c015082 to disappear
Nov 29 13:55:01.924: INFO: Pod pod-configmaps-bf2c7f38-c0af-4f9e-81be-7d940c015082 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:55:01.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-538" for this suite.
Nov 29 13:55:07.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:55:08.107: INFO: namespace configmap-538 deletion completed in 6.177149686s
•SS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:55:08.107: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-1637
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Nov 29 13:55:10.272: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:55:10.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1637" for this suite.
Nov 29 13:55:16.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:55:16.432: INFO: namespace container-runtime-1637 deletion completed in 6.140079457s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:55:16.433: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-8814
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1129 13:55:26.657254    4828 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Nov 29 13:55:26.657: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:55:26.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8814" for this suite.
Nov 29 13:55:32.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:55:32.811: INFO: namespace gc-8814 deletion completed in 6.149136566s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:55:32.814: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-6517
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 29 13:55:32.966: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Nov 29 13:55:37.971: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Nov 29 13:55:37.972: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Nov 29 13:55:40.004: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-6517,SelfLink:/apis/apps/v1/namespaces/deployment-6517/deployments/test-cleanup-deployment,UID:06ebe455-ce68-46c0-b927-f6413d1d0823,ResourceVersion:22738,Generation:1,CreationTimestamp:2019-11-29 13:55:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-11-29 13:55:38 +0000 UTC 2019-11-29 13:55:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-11-29 13:55:39 +0000 UTC 2019-11-29 13:55:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Nov 29 13:55:40.009: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-6517,SelfLink:/apis/apps/v1/namespaces/deployment-6517/replicasets/test-cleanup-deployment-55bbcbc84c,UID:feae4efb-bf15-45a8-b2c0-fb3a0653f8ef,ResourceVersion:22731,Generation:1,CreationTimestamp:2019-11-29 13:55:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 06ebe455-ce68-46c0-b927-f6413d1d0823 0xc003859df7 0xc003859df8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Nov 29 13:55:40.013: INFO: Pod "test-cleanup-deployment-55bbcbc84c-77ws7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-77ws7,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-6517,SelfLink:/api/v1/namespaces/deployment-6517/pods/test-cleanup-deployment-55bbcbc84c-77ws7,UID:fc4fa21a-4f8c-459c-bf92-e9b996bf5647,ResourceVersion:22730,Generation:0,CreationTimestamp:2019-11-29 13:55:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 100.64.0.18/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c feae4efb-bf15-45a8-b2c0-fb3a0653f8ef 0xc002fbceb7 0xc002fbceb8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9vxt4 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9vxt4,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-9vxt4 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002fbcf20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002fbcf40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:55:38 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:55:39 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:55:39 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-11-29 13:55:38 +0000 UTC  }],Message:,Reason:,HostIP:10.250.0.10,PodIP:100.64.0.18,StartTime:2019-11-29 13:55:38 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-11-29 13:55:38 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://946a06140bfaa7fa58789e88874f597f2ec076b7fb0b9718f370b7dceb6d2b39}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:55:40.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6517" for this suite.
Nov 29 13:55:46.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:55:46.230: INFO: namespace deployment-6517 deletion completed in 6.211163781s
•SSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:55:46.230: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-6953
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:55:46.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6953" for this suite.
Nov 29 13:56:08.409: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:56:08.535: INFO: namespace pods-6953 deletion completed in 22.140324168s
•SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:56:08.535: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2833
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-77630075-af90-49b6-b09b-f0781c265fc3
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:56:08.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2833" for this suite.
Nov 29 13:56:14.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:56:14.839: INFO: namespace secrets-2833 deletion completed in 6.148607382s
•SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:56:14.839: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3694
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Nov 29 13:56:17.535: INFO: Successfully updated pod "annotationupdate218aba9f-f13c-431b-b8ae-8d79aece0f83"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:56:19.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3694" for this suite.
Nov 29 13:56:41.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:56:41.696: INFO: namespace projected-3694 deletion completed in 22.134474632s
•SSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:56:41.696: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-9021
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 29 13:56:41.863: INFO: (0) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.098046ms)
Nov 29 13:56:41.906: INFO: (1) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 42.767872ms)
Nov 29 13:56:41.913: INFO: (2) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.584363ms)
Nov 29 13:56:41.919: INFO: (3) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.131578ms)
Nov 29 13:56:41.925: INFO: (4) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.518675ms)
Nov 29 13:56:41.930: INFO: (5) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.31637ms)
Nov 29 13:56:41.936: INFO: (6) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.537787ms)
Nov 29 13:56:41.943: INFO: (7) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.897907ms)
Nov 29 13:56:41.953: INFO: (8) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 9.496131ms)
Nov 29 13:56:41.958: INFO: (9) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.148809ms)
Nov 29 13:56:41.964: INFO: (10) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.614937ms)
Nov 29 13:56:41.971: INFO: (11) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.143636ms)
Nov 29 13:56:41.977: INFO: (12) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.710237ms)
Nov 29 13:56:41.982: INFO: (13) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.900935ms)
Nov 29 13:56:41.987: INFO: (14) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.247404ms)
Nov 29 13:56:41.992: INFO: (15) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.943726ms)
Nov 29 13:56:41.997: INFO: (16) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.892746ms)
Nov 29 13:56:42.002: INFO: (17) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.121998ms)
Nov 29 13:56:42.008: INFO: (18) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.621703ms)
Nov 29 13:56:42.013: INFO: (19) /api/v1/nodes/shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.284634ms)
[AfterEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:56:42.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9021" for this suite.
Nov 29 13:56:48.032: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:56:48.155: INFO: namespace proxy-9021 deletion completed in 6.136693041s
•SS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:56:48.155: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-313
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 29 13:56:48.304: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:56:50.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-313" for this suite.
Nov 29 13:57:28.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:57:28.544: INFO: namespace pods-313 deletion completed in 38.133947271s
•SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:57:28.544: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8012
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Nov 29 13:57:28.698: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c7c4e328-5b9e-474c-9332-740c749730fd" in namespace "projected-8012" to be "success or failure"
Nov 29 13:57:28.703: INFO: Pod "downwardapi-volume-c7c4e328-5b9e-474c-9332-740c749730fd": Phase="Pending", Reason="", readiness=false. Elapsed: 4.473464ms
Nov 29 13:57:30.707: INFO: Pod "downwardapi-volume-c7c4e328-5b9e-474c-9332-740c749730fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008610436s
STEP: Saw pod success
Nov 29 13:57:30.707: INFO: Pod "downwardapi-volume-c7c4e328-5b9e-474c-9332-740c749730fd" satisfied condition "success or failure"
Nov 29 13:57:30.710: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod downwardapi-volume-c7c4e328-5b9e-474c-9332-740c749730fd container client-container: <nil>
STEP: delete the pod
Nov 29 13:57:30.729: INFO: Waiting for pod downwardapi-volume-c7c4e328-5b9e-474c-9332-740c749730fd to disappear
Nov 29 13:57:30.732: INFO: Pod downwardapi-volume-c7c4e328-5b9e-474c-9332-740c749730fd no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:57:30.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8012" for this suite.
Nov 29 13:57:36.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:57:36.877: INFO: namespace projected-8012 deletion completed in 6.139295288s
•SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:57:36.877: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2426
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Nov 29 13:57:37.026: INFO: Waiting up to 5m0s for pod "pod-24fb5840-634f-4928-abfd-695bb85f9a8f" in namespace "emptydir-2426" to be "success or failure"
Nov 29 13:57:37.029: INFO: Pod "pod-24fb5840-634f-4928-abfd-695bb85f9a8f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.889191ms
Nov 29 13:57:39.034: INFO: Pod "pod-24fb5840-634f-4928-abfd-695bb85f9a8f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007542172s
STEP: Saw pod success
Nov 29 13:57:39.034: INFO: Pod "pod-24fb5840-634f-4928-abfd-695bb85f9a8f" satisfied condition "success or failure"
Nov 29 13:57:39.037: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-24fb5840-634f-4928-abfd-695bb85f9a8f container test-container: <nil>
STEP: delete the pod
Nov 29 13:57:39.055: INFO: Waiting for pod pod-24fb5840-634f-4928-abfd-695bb85f9a8f to disappear
Nov 29 13:57:39.059: INFO: Pod pod-24fb5840-634f-4928-abfd-695bb85f9a8f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:57:39.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2426" for this suite.
Nov 29 13:57:45.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:57:45.198: INFO: namespace emptydir-2426 deletion completed in 6.134437022s
•SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:57:45.198: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9207
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-46a0350a-0449-4190-a5d9-1c7619333f58
STEP: Creating a pod to test consume secrets
Nov 29 13:57:45.354: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-38ebb6a1-874c-4b2c-893c-8ad8b3083d64" in namespace "projected-9207" to be "success or failure"
Nov 29 13:57:45.357: INFO: Pod "pod-projected-secrets-38ebb6a1-874c-4b2c-893c-8ad8b3083d64": Phase="Pending", Reason="", readiness=false. Elapsed: 3.462979ms
Nov 29 13:57:47.364: INFO: Pod "pod-projected-secrets-38ebb6a1-874c-4b2c-893c-8ad8b3083d64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010224732s
STEP: Saw pod success
Nov 29 13:57:47.364: INFO: Pod "pod-projected-secrets-38ebb6a1-874c-4b2c-893c-8ad8b3083d64" satisfied condition "success or failure"
Nov 29 13:57:47.368: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-projected-secrets-38ebb6a1-874c-4b2c-893c-8ad8b3083d64 container projected-secret-volume-test: <nil>
STEP: delete the pod
Nov 29 13:57:47.388: INFO: Waiting for pod pod-projected-secrets-38ebb6a1-874c-4b2c-893c-8ad8b3083d64 to disappear
Nov 29 13:57:47.391: INFO: Pod pod-projected-secrets-38ebb6a1-874c-4b2c-893c-8ad8b3083d64 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:57:47.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9207" for this suite.
Nov 29 13:57:53.408: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:57:53.532: INFO: namespace projected-9207 deletion completed in 6.13548845s
•SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:57:53.533: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-3535
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Nov 29 13:57:53.692: INFO: Waiting up to 5m0s for pod "client-containers-122dcce5-ab35-4475-aa0f-501ab4f178a9" in namespace "containers-3535" to be "success or failure"
Nov 29 13:57:53.695: INFO: Pod "client-containers-122dcce5-ab35-4475-aa0f-501ab4f178a9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.108135ms
Nov 29 13:57:55.700: INFO: Pod "client-containers-122dcce5-ab35-4475-aa0f-501ab4f178a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007692424s
STEP: Saw pod success
Nov 29 13:57:55.700: INFO: Pod "client-containers-122dcce5-ab35-4475-aa0f-501ab4f178a9" satisfied condition "success or failure"
Nov 29 13:57:55.703: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod client-containers-122dcce5-ab35-4475-aa0f-501ab4f178a9 container test-container: <nil>
STEP: delete the pod
Nov 29 13:57:55.723: INFO: Waiting for pod client-containers-122dcce5-ab35-4475-aa0f-501ab4f178a9 to disappear
Nov 29 13:57:55.726: INFO: Pod client-containers-122dcce5-ab35-4475-aa0f-501ab4f178a9 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:57:55.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3535" for this suite.
Nov 29 13:58:01.741: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:58:01.908: INFO: namespace containers-3535 deletion completed in 6.17811618s
•SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:58:01.908: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3521
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-9b5897e6-5ca5-47e0-8ade-0da70a662ac2
STEP: Creating a pod to test consume configMaps
Nov 29 13:58:02.063: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d2a0d6f5-6925-44b2-917b-b47326387776" in namespace "projected-3521" to be "success or failure"
Nov 29 13:58:02.067: INFO: Pod "pod-projected-configmaps-d2a0d6f5-6925-44b2-917b-b47326387776": Phase="Pending", Reason="", readiness=false. Elapsed: 3.924452ms
Nov 29 13:58:04.071: INFO: Pod "pod-projected-configmaps-d2a0d6f5-6925-44b2-917b-b47326387776": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008410367s
STEP: Saw pod success
Nov 29 13:58:04.071: INFO: Pod "pod-projected-configmaps-d2a0d6f5-6925-44b2-917b-b47326387776" satisfied condition "success or failure"
Nov 29 13:58:04.075: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-projected-configmaps-d2a0d6f5-6925-44b2-917b-b47326387776 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Nov 29 13:58:04.108: INFO: Waiting for pod pod-projected-configmaps-d2a0d6f5-6925-44b2-917b-b47326387776 to disappear
Nov 29 13:58:04.110: INFO: Pod pod-projected-configmaps-d2a0d6f5-6925-44b2-917b-b47326387776 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:58:04.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3521" for this suite.
Nov 29 13:58:10.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:58:10.245: INFO: namespace projected-3521 deletion completed in 6.130003806s
•SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:58:10.245: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6560
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Nov 29 13:58:10.414: INFO: Create a RollingUpdate DaemonSet
Nov 29 13:58:10.418: INFO: Check that daemon pods launch on every node of the cluster
Nov 29 13:58:10.429: INFO: Number of nodes with available pods: 0
Nov 29 13:58:10.429: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:58:11.439: INFO: Number of nodes with available pods: 0
Nov 29 13:58:11.439: INFO: Node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-5rv5x is running more than one daemon pod
Nov 29 13:58:12.438: INFO: Number of nodes with available pods: 2
Nov 29 13:58:12.439: INFO: Number of running nodes: 2, number of available pods: 2
Nov 29 13:58:12.439: INFO: Update the DaemonSet to trigger a rollout
Nov 29 13:58:12.446: INFO: Updating DaemonSet daemon-set
Nov 29 13:58:19.464: INFO: Roll back the DaemonSet before rollout is complete
Nov 29 13:58:19.471: INFO: Updating DaemonSet daemon-set
Nov 29 13:58:19.471: INFO: Make sure DaemonSet rollback is complete
Nov 29 13:58:19.475: INFO: Wrong image for pod: daemon-set-ftjbx. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Nov 29 13:58:19.475: INFO: Pod daemon-set-ftjbx is not available
Nov 29 13:58:20.484: INFO: Wrong image for pod: daemon-set-ftjbx. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Nov 29 13:58:20.484: INFO: Pod daemon-set-ftjbx is not available
Nov 29 13:58:21.483: INFO: Pod daemon-set-6sprq is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6560, will wait for the garbage collector to delete the pods
Nov 29 13:58:21.554: INFO: Deleting DaemonSet.extensions daemon-set took: 6.126698ms
Nov 29 13:58:21.955: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.244135ms
Nov 29 13:58:28.159: INFO: Number of nodes with available pods: 0
Nov 29 13:58:28.159: INFO: Number of running nodes: 0, number of available pods: 0
Nov 29 13:58:28.164: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6560/daemonsets","resourceVersion":"23366"},"items":null}

Nov 29 13:58:28.168: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6560/pods","resourceVersion":"23366"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:58:28.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6560" for this suite.
Nov 29 13:58:34.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:58:34.325: INFO: namespace daemonsets-6560 deletion completed in 6.137567916s
•SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Nov 29 13:58:34.325: INFO: >>> kubeConfig: /tmp/tm/kubeconfig/shoot.config
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4269
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Nov 29 13:58:34.482: INFO: Waiting up to 5m0s for pod "pod-8f0f3609-feb7-42d0-a3ed-2ba3c849be26" in namespace "emptydir-4269" to be "success or failure"
Nov 29 13:58:34.485: INFO: Pod "pod-8f0f3609-feb7-42d0-a3ed-2ba3c849be26": Phase="Pending", Reason="", readiness=false. Elapsed: 3.224537ms
Nov 29 13:58:36.490: INFO: Pod "pod-8f0f3609-feb7-42d0-a3ed-2ba3c849be26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00759038s
STEP: Saw pod success
Nov 29 13:58:36.490: INFO: Pod "pod-8f0f3609-feb7-42d0-a3ed-2ba3c849be26" satisfied condition "success or failure"
Nov 29 13:58:36.494: INFO: Trying to get logs from node shoot--it--tm0z6-8fb-coreos-xhmxyljw-z1-7446cd8b54-dtkrj pod pod-8f0f3609-feb7-42d0-a3ed-2ba3c849be26 container test-container: <nil>
STEP: delete the pod
Nov 29 13:58:36.515: INFO: Waiting for pod pod-8f0f3609-feb7-42d0-a3ed-2ba3c849be26 to disappear
Nov 29 13:58:36.518: INFO: Pod pod-8f0f3609-feb7-42d0-a3ed-2ba3c849be26 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Nov 29 13:58:36.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4269" for this suite.
Nov 29 13:58:42.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Nov 29 13:58:42.690: INFO: namespace emptydir-4269 deletion completed in 6.166857121s
•SSSSSSSSSSSSSSSSSSSSSSSSSSNov 29 13:58:42.690: INFO: Running AfterSuite actions on all nodes
Nov 29 13:58:42.690: INFO: Running AfterSuite actions on node 1
Nov 29 13:58:42.690: INFO: Skipping dumping logs from cluster

Ran 215 of 4413 Specs in 5748.490 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Flaked | 0 Pending | 4198 Skipped
PASS

Ginkgo ran 1 suite in 1h36m36.970357253s
Test Suite Passed
