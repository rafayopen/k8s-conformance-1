I0626 14:05:10.545068      18 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-254532258
I0626 14:05:10.545191      18 e2e.go:241] Starting e2e run "634cb3d6-6010-4236-bfd0-1b2039a055b2" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1561557908 - Will randomize all specs
Will run 215 of 4411 specs

Jun 26 14:05:10.769: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
Jun 26 14:05:10.774: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun 26 14:05:10.790: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun 26 14:05:10.877: INFO: 18 / 18 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun 26 14:05:10.877: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Jun 26 14:05:10.877: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun 26 14:05:10.912: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jun 26 14:05:10.912: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jun 26 14:05:10.913: INFO: e2e test version: v1.15.0
Jun 26 14:05:10.916: INFO: kube-apiserver version: v1.15.0
SSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:05:10.918: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename downward-api
Jun 26 14:05:11.022: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 26 14:05:11.046: INFO: Waiting up to 5m0s for pod "downward-api-d2cc42ab-1945-448d-8b20-0faadcd9a5d4" in namespace "downward-api-3471" to be "success or failure"
Jun 26 14:05:11.054: INFO: Pod "downward-api-d2cc42ab-1945-448d-8b20-0faadcd9a5d4": Phase="Pending", Reason="", readiness=false. Elapsed: 7.992011ms
Jun 26 14:05:13.063: INFO: Pod "downward-api-d2cc42ab-1945-448d-8b20-0faadcd9a5d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016376937s
Jun 26 14:05:15.067: INFO: Pod "downward-api-d2cc42ab-1945-448d-8b20-0faadcd9a5d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020326184s
STEP: Saw pod success
Jun 26 14:05:15.067: INFO: Pod "downward-api-d2cc42ab-1945-448d-8b20-0faadcd9a5d4" satisfied condition "success or failure"
Jun 26 14:05:15.069: INFO: Trying to get logs from node k8s-worker-1-dev pod downward-api-d2cc42ab-1945-448d-8b20-0faadcd9a5d4 container dapi-container: <nil>
STEP: delete the pod
Jun 26 14:05:15.094: INFO: Waiting for pod downward-api-d2cc42ab-1945-448d-8b20-0faadcd9a5d4 to disappear
Jun 26 14:05:15.097: INFO: Pod downward-api-d2cc42ab-1945-448d-8b20-0faadcd9a5d4 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:05:15.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3471" for this suite.
Jun 26 14:05:21.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:05:21.205: INFO: namespace downward-api-3471 deletion completed in 6.103866067s

• [SLOW TEST:10.287 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:05:21.206: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 26 14:05:21.237: INFO: Waiting up to 5m0s for pod "pod-4c4dec17-faac-4af7-a714-47f82af3cca3" in namespace "emptydir-3050" to be "success or failure"
Jun 26 14:05:21.241: INFO: Pod "pod-4c4dec17-faac-4af7-a714-47f82af3cca3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.850076ms
Jun 26 14:05:23.248: INFO: Pod "pod-4c4dec17-faac-4af7-a714-47f82af3cca3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010543393s
Jun 26 14:05:25.254: INFO: Pod "pod-4c4dec17-faac-4af7-a714-47f82af3cca3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01705412s
STEP: Saw pod success
Jun 26 14:05:25.254: INFO: Pod "pod-4c4dec17-faac-4af7-a714-47f82af3cca3" satisfied condition "success or failure"
Jun 26 14:05:25.260: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-4c4dec17-faac-4af7-a714-47f82af3cca3 container test-container: <nil>
STEP: delete the pod
Jun 26 14:05:25.305: INFO: Waiting for pod pod-4c4dec17-faac-4af7-a714-47f82af3cca3 to disappear
Jun 26 14:05:25.311: INFO: Pod pod-4c4dec17-faac-4af7-a714-47f82af3cca3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:05:25.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3050" for this suite.
Jun 26 14:05:31.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:05:31.564: INFO: namespace emptydir-3050 deletion completed in 6.24447218s

• [SLOW TEST:10.358 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:05:31.565: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 26 14:05:31.658: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4bbd972a-1c64-4a66-8ab8-334b3e97e982" in namespace "projected-8297" to be "success or failure"
Jun 26 14:05:31.667: INFO: Pod "downwardapi-volume-4bbd972a-1c64-4a66-8ab8-334b3e97e982": Phase="Pending", Reason="", readiness=false. Elapsed: 8.644948ms
Jun 26 14:05:33.676: INFO: Pod "downwardapi-volume-4bbd972a-1c64-4a66-8ab8-334b3e97e982": Phase="Pending", Reason="", readiness=false. Elapsed: 2.017010739s
Jun 26 14:05:35.684: INFO: Pod "downwardapi-volume-4bbd972a-1c64-4a66-8ab8-334b3e97e982": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025324232s
Jun 26 14:05:37.694: INFO: Pod "downwardapi-volume-4bbd972a-1c64-4a66-8ab8-334b3e97e982": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035628976s
STEP: Saw pod success
Jun 26 14:05:37.695: INFO: Pod "downwardapi-volume-4bbd972a-1c64-4a66-8ab8-334b3e97e982" satisfied condition "success or failure"
Jun 26 14:05:37.702: INFO: Trying to get logs from node k8s-worker-1-dev pod downwardapi-volume-4bbd972a-1c64-4a66-8ab8-334b3e97e982 container client-container: <nil>
STEP: delete the pod
Jun 26 14:05:37.752: INFO: Waiting for pod downwardapi-volume-4bbd972a-1c64-4a66-8ab8-334b3e97e982 to disappear
Jun 26 14:05:37.760: INFO: Pod downwardapi-volume-4bbd972a-1c64-4a66-8ab8-334b3e97e982 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:05:37.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8297" for this suite.
Jun 26 14:05:43.796: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:05:43.875: INFO: namespace projected-8297 deletion completed in 6.105064224s

• [SLOW TEST:12.310 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:05:43.876: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-5f4b9560-70c8-4a48-983d-3668b7fb71c9
STEP: Creating a pod to test consume secrets
Jun 26 14:05:43.914: INFO: Waiting up to 5m0s for pod "pod-secrets-a6f6b4d3-4e03-4e78-8800-a3f54695cea2" in namespace "secrets-1972" to be "success or failure"
Jun 26 14:05:43.918: INFO: Pod "pod-secrets-a6f6b4d3-4e03-4e78-8800-a3f54695cea2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.382097ms
Jun 26 14:05:45.923: INFO: Pod "pod-secrets-a6f6b4d3-4e03-4e78-8800-a3f54695cea2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009467198s
Jun 26 14:05:47.927: INFO: Pod "pod-secrets-a6f6b4d3-4e03-4e78-8800-a3f54695cea2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01312904s
STEP: Saw pod success
Jun 26 14:05:47.927: INFO: Pod "pod-secrets-a6f6b4d3-4e03-4e78-8800-a3f54695cea2" satisfied condition "success or failure"
Jun 26 14:05:47.929: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-secrets-a6f6b4d3-4e03-4e78-8800-a3f54695cea2 container secret-volume-test: <nil>
STEP: delete the pod
Jun 26 14:05:47.944: INFO: Waiting for pod pod-secrets-a6f6b4d3-4e03-4e78-8800-a3f54695cea2 to disappear
Jun 26 14:05:47.946: INFO: Pod pod-secrets-a6f6b4d3-4e03-4e78-8800-a3f54695cea2 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:05:47.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1972" for this suite.
Jun 26 14:05:53.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:05:54.151: INFO: namespace secrets-1972 deletion completed in 6.200683532s

• [SLOW TEST:10.275 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:05:54.151: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 26 14:05:54.234: INFO: Waiting up to 5m0s for pod "downwardapi-volume-096b759d-31ed-455a-a395-6db90be6e918" in namespace "projected-6257" to be "success or failure"
Jun 26 14:05:54.247: INFO: Pod "downwardapi-volume-096b759d-31ed-455a-a395-6db90be6e918": Phase="Pending", Reason="", readiness=false. Elapsed: 12.510122ms
Jun 26 14:05:56.252: INFO: Pod "downwardapi-volume-096b759d-31ed-455a-a395-6db90be6e918": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018281787s
STEP: Saw pod success
Jun 26 14:05:56.253: INFO: Pod "downwardapi-volume-096b759d-31ed-455a-a395-6db90be6e918" satisfied condition "success or failure"
Jun 26 14:05:56.256: INFO: Trying to get logs from node k8s-worker-1-dev pod downwardapi-volume-096b759d-31ed-455a-a395-6db90be6e918 container client-container: <nil>
STEP: delete the pod
Jun 26 14:05:56.277: INFO: Waiting for pod downwardapi-volume-096b759d-31ed-455a-a395-6db90be6e918 to disappear
Jun 26 14:05:56.281: INFO: Pod downwardapi-volume-096b759d-31ed-455a-a395-6db90be6e918 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:05:56.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6257" for this suite.
Jun 26 14:06:02.301: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:06:02.494: INFO: namespace projected-6257 deletion completed in 6.206732052s

• [SLOW TEST:8.342 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:06:02.495: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 26 14:06:02.567: INFO: Waiting up to 5m0s for pod "downward-api-4f38a1bc-c707-42d4-9f36-fb9b9f00bc30" in namespace "downward-api-8037" to be "success or failure"
Jun 26 14:06:02.572: INFO: Pod "downward-api-4f38a1bc-c707-42d4-9f36-fb9b9f00bc30": Phase="Pending", Reason="", readiness=false. Elapsed: 4.654348ms
Jun 26 14:06:04.578: INFO: Pod "downward-api-4f38a1bc-c707-42d4-9f36-fb9b9f00bc30": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011299699s
Jun 26 14:06:06.585: INFO: Pod "downward-api-4f38a1bc-c707-42d4-9f36-fb9b9f00bc30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018017392s
STEP: Saw pod success
Jun 26 14:06:06.585: INFO: Pod "downward-api-4f38a1bc-c707-42d4-9f36-fb9b9f00bc30" satisfied condition "success or failure"
Jun 26 14:06:06.591: INFO: Trying to get logs from node k8s-worker-2-dev pod downward-api-4f38a1bc-c707-42d4-9f36-fb9b9f00bc30 container dapi-container: <nil>
STEP: delete the pod
Jun 26 14:06:06.620: INFO: Waiting for pod downward-api-4f38a1bc-c707-42d4-9f36-fb9b9f00bc30 to disappear
Jun 26 14:06:06.631: INFO: Pod downward-api-4f38a1bc-c707-42d4-9f36-fb9b9f00bc30 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:06:06.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8037" for this suite.
Jun 26 14:06:12.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:06:12.733: INFO: namespace downward-api-8037 deletion completed in 6.091915749s

• [SLOW TEST:10.238 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:06:12.734: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-0ece6981-1406-4e92-92bd-ac8f932e9277 in namespace container-probe-7846
Jun 26 14:06:16.781: INFO: Started pod busybox-0ece6981-1406-4e92-92bd-ac8f932e9277 in namespace container-probe-7846
STEP: checking the pod's current state and verifying that restartCount is present
Jun 26 14:06:16.787: INFO: Initial restart count of pod busybox-0ece6981-1406-4e92-92bd-ac8f932e9277 is 0
Jun 26 14:07:08.929: INFO: Restart count of pod container-probe-7846/busybox-0ece6981-1406-4e92-92bd-ac8f932e9277 is now 1 (52.142598285s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:07:08.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7846" for this suite.
Jun 26 14:07:14.959: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:07:15.164: INFO: namespace container-probe-7846 deletion completed in 6.221847736s

• [SLOW TEST:62.431 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:07:15.167: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:07:17.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5424" for this suite.
Jun 26 14:08:05.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:08:05.391: INFO: namespace kubelet-test-5424 deletion completed in 48.099911626s

• [SLOW TEST:50.225 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:08:05.392: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-7928be66-34a7-42c9-bdc7-ea7c44433da7
STEP: Creating a pod to test consume secrets
Jun 26 14:08:05.430: INFO: Waiting up to 5m0s for pod "pod-secrets-b45e5988-e8bd-4fc5-9b0c-c2bb00df83bd" in namespace "secrets-3530" to be "success or failure"
Jun 26 14:08:05.434: INFO: Pod "pod-secrets-b45e5988-e8bd-4fc5-9b0c-c2bb00df83bd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.560423ms
Jun 26 14:08:07.442: INFO: Pod "pod-secrets-b45e5988-e8bd-4fc5-9b0c-c2bb00df83bd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011316458s
Jun 26 14:08:09.450: INFO: Pod "pod-secrets-b45e5988-e8bd-4fc5-9b0c-c2bb00df83bd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01941638s
STEP: Saw pod success
Jun 26 14:08:09.450: INFO: Pod "pod-secrets-b45e5988-e8bd-4fc5-9b0c-c2bb00df83bd" satisfied condition "success or failure"
Jun 26 14:08:09.457: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-secrets-b45e5988-e8bd-4fc5-9b0c-c2bb00df83bd container secret-env-test: <nil>
STEP: delete the pod
Jun 26 14:08:09.497: INFO: Waiting for pod pod-secrets-b45e5988-e8bd-4fc5-9b0c-c2bb00df83bd to disappear
Jun 26 14:08:09.503: INFO: Pod pod-secrets-b45e5988-e8bd-4fc5-9b0c-c2bb00df83bd no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:08:09.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3530" for this suite.
Jun 26 14:08:15.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:08:15.760: INFO: namespace secrets-3530 deletion completed in 6.249936824s

• [SLOW TEST:10.368 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:08:15.761: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 26 14:08:15.848: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5e64ea9c-b18e-469e-b895-5a8688c99e90" in namespace "downward-api-5860" to be "success or failure"
Jun 26 14:08:15.854: INFO: Pod "downwardapi-volume-5e64ea9c-b18e-469e-b895-5a8688c99e90": Phase="Pending", Reason="", readiness=false. Elapsed: 6.611003ms
Jun 26 14:08:17.858: INFO: Pod "downwardapi-volume-5e64ea9c-b18e-469e-b895-5a8688c99e90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009805367s
STEP: Saw pod success
Jun 26 14:08:17.858: INFO: Pod "downwardapi-volume-5e64ea9c-b18e-469e-b895-5a8688c99e90" satisfied condition "success or failure"
Jun 26 14:08:17.861: INFO: Trying to get logs from node k8s-worker-2-dev pod downwardapi-volume-5e64ea9c-b18e-469e-b895-5a8688c99e90 container client-container: <nil>
STEP: delete the pod
Jun 26 14:08:17.877: INFO: Waiting for pod downwardapi-volume-5e64ea9c-b18e-469e-b895-5a8688c99e90 to disappear
Jun 26 14:08:17.879: INFO: Pod downwardapi-volume-5e64ea9c-b18e-469e-b895-5a8688c99e90 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:08:17.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5860" for this suite.
Jun 26 14:08:23.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:08:24.098: INFO: namespace downward-api-5860 deletion completed in 6.216052626s

• [SLOW TEST:8.337 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:08:24.099: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 26 14:08:24.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-3018'
Jun 26 14:08:24.657: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 26 14:08:24.657: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Jun 26 14:08:26.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 delete deployment e2e-test-nginx-deployment --namespace=kubectl-3018'
Jun 26 14:08:26.906: INFO: stderr: ""
Jun 26 14:08:26.906: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:08:26.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3018" for this suite.
Jun 26 14:08:32.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:08:33.022: INFO: namespace kubectl-3018 deletion completed in 6.102941099s

• [SLOW TEST:8.923 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:08:33.022: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Jun 26 14:08:33.049: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 create -f - --namespace=kubectl-9572'
Jun 26 14:08:33.528: INFO: stderr: ""
Jun 26 14:08:33.528: INFO: stdout: "pod/pause created\n"
Jun 26 14:08:33.528: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun 26 14:08:33.528: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9572" to be "running and ready"
Jun 26 14:08:33.531: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.951031ms
Jun 26 14:08:35.542: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.01391419s
Jun 26 14:08:35.542: INFO: Pod "pause" satisfied condition "running and ready"
Jun 26 14:08:35.542: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Jun 26 14:08:35.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 label pods pause testing-label=testing-label-value --namespace=kubectl-9572'
Jun 26 14:08:35.768: INFO: stderr: ""
Jun 26 14:08:35.768: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun 26 14:08:35.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pod pause -L testing-label --namespace=kubectl-9572'
Jun 26 14:08:35.970: INFO: stderr: ""
Jun 26 14:08:35.970: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun 26 14:08:35.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 label pods pause testing-label- --namespace=kubectl-9572'
Jun 26 14:08:36.180: INFO: stderr: ""
Jun 26 14:08:36.180: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun 26 14:08:36.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pod pause -L testing-label --namespace=kubectl-9572'
Jun 26 14:08:36.369: INFO: stderr: ""
Jun 26 14:08:36.369: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Jun 26 14:08:36.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 delete --grace-period=0 --force -f - --namespace=kubectl-9572'
Jun 26 14:08:36.582: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 26 14:08:36.582: INFO: stdout: "pod \"pause\" force deleted\n"
Jun 26 14:08:36.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get rc,svc -l name=pause --no-headers --namespace=kubectl-9572'
Jun 26 14:08:36.818: INFO: stderr: "No resources found.\n"
Jun 26 14:08:36.818: INFO: stdout: ""
Jun 26 14:08:36.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods -l name=pause --namespace=kubectl-9572 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 26 14:08:37.014: INFO: stderr: ""
Jun 26 14:08:37.014: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:08:37.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9572" for this suite.
Jun 26 14:08:43.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:08:43.259: INFO: namespace kubectl-9572 deletion completed in 6.229661453s

• [SLOW TEST:10.236 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:08:43.260: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-hgmk
STEP: Creating a pod to test atomic-volume-subpath
Jun 26 14:08:43.368: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-hgmk" in namespace "subpath-3940" to be "success or failure"
Jun 26 14:08:43.393: INFO: Pod "pod-subpath-test-secret-hgmk": Phase="Pending", Reason="", readiness=false. Elapsed: 24.795209ms
Jun 26 14:08:45.397: INFO: Pod "pod-subpath-test-secret-hgmk": Phase="Running", Reason="", readiness=true. Elapsed: 2.028595877s
Jun 26 14:08:47.400: INFO: Pod "pod-subpath-test-secret-hgmk": Phase="Running", Reason="", readiness=true. Elapsed: 4.032106277s
Jun 26 14:08:49.408: INFO: Pod "pod-subpath-test-secret-hgmk": Phase="Running", Reason="", readiness=true. Elapsed: 6.039471549s
Jun 26 14:08:51.416: INFO: Pod "pod-subpath-test-secret-hgmk": Phase="Running", Reason="", readiness=true. Elapsed: 8.047885401s
Jun 26 14:08:53.422: INFO: Pod "pod-subpath-test-secret-hgmk": Phase="Running", Reason="", readiness=true. Elapsed: 10.054423161s
Jun 26 14:08:55.431: INFO: Pod "pod-subpath-test-secret-hgmk": Phase="Running", Reason="", readiness=true. Elapsed: 12.062925986s
Jun 26 14:08:57.439: INFO: Pod "pod-subpath-test-secret-hgmk": Phase="Running", Reason="", readiness=true. Elapsed: 14.071379193s
Jun 26 14:08:59.443: INFO: Pod "pod-subpath-test-secret-hgmk": Phase="Running", Reason="", readiness=true. Elapsed: 16.075389082s
Jun 26 14:09:01.448: INFO: Pod "pod-subpath-test-secret-hgmk": Phase="Running", Reason="", readiness=true. Elapsed: 18.079805999s
Jun 26 14:09:03.455: INFO: Pod "pod-subpath-test-secret-hgmk": Phase="Running", Reason="", readiness=true. Elapsed: 20.086966855s
Jun 26 14:09:05.464: INFO: Pod "pod-subpath-test-secret-hgmk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.096082471s
STEP: Saw pod success
Jun 26 14:09:05.464: INFO: Pod "pod-subpath-test-secret-hgmk" satisfied condition "success or failure"
Jun 26 14:09:05.469: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-subpath-test-secret-hgmk container test-container-subpath-secret-hgmk: <nil>
STEP: delete the pod
Jun 26 14:09:05.502: INFO: Waiting for pod pod-subpath-test-secret-hgmk to disappear
Jun 26 14:09:05.507: INFO: Pod pod-subpath-test-secret-hgmk no longer exists
STEP: Deleting pod pod-subpath-test-secret-hgmk
Jun 26 14:09:05.507: INFO: Deleting pod "pod-subpath-test-secret-hgmk" in namespace "subpath-3940"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:09:05.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3940" for this suite.
Jun 26 14:09:11.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:09:11.658: INFO: namespace subpath-3940 deletion completed in 6.135198313s

• [SLOW TEST:28.398 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:09:11.658: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-vv5q
STEP: Creating a pod to test atomic-volume-subpath
Jun 26 14:09:11.719: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-vv5q" in namespace "subpath-4209" to be "success or failure"
Jun 26 14:09:11.725: INFO: Pod "pod-subpath-test-configmap-vv5q": Phase="Pending", Reason="", readiness=false. Elapsed: 5.225066ms
Jun 26 14:09:13.729: INFO: Pod "pod-subpath-test-configmap-vv5q": Phase="Running", Reason="", readiness=true. Elapsed: 2.008974131s
Jun 26 14:09:15.732: INFO: Pod "pod-subpath-test-configmap-vv5q": Phase="Running", Reason="", readiness=true. Elapsed: 4.012055959s
Jun 26 14:09:17.739: INFO: Pod "pod-subpath-test-configmap-vv5q": Phase="Running", Reason="", readiness=true. Elapsed: 6.019267459s
Jun 26 14:09:19.747: INFO: Pod "pod-subpath-test-configmap-vv5q": Phase="Running", Reason="", readiness=true. Elapsed: 8.027064159s
Jun 26 14:09:21.753: INFO: Pod "pod-subpath-test-configmap-vv5q": Phase="Running", Reason="", readiness=true. Elapsed: 10.033350096s
Jun 26 14:09:23.761: INFO: Pod "pod-subpath-test-configmap-vv5q": Phase="Running", Reason="", readiness=true. Elapsed: 12.041072228s
Jun 26 14:09:25.769: INFO: Pod "pod-subpath-test-configmap-vv5q": Phase="Running", Reason="", readiness=true. Elapsed: 14.048902462s
Jun 26 14:09:27.776: INFO: Pod "pod-subpath-test-configmap-vv5q": Phase="Running", Reason="", readiness=true. Elapsed: 16.05610874s
Jun 26 14:09:29.779: INFO: Pod "pod-subpath-test-configmap-vv5q": Phase="Running", Reason="", readiness=true. Elapsed: 18.059876613s
Jun 26 14:09:31.787: INFO: Pod "pod-subpath-test-configmap-vv5q": Phase="Running", Reason="", readiness=true. Elapsed: 20.067771328s
Jun 26 14:09:33.795: INFO: Pod "pod-subpath-test-configmap-vv5q": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.074930434s
STEP: Saw pod success
Jun 26 14:09:33.795: INFO: Pod "pod-subpath-test-configmap-vv5q" satisfied condition "success or failure"
Jun 26 14:09:33.803: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-subpath-test-configmap-vv5q container test-container-subpath-configmap-vv5q: <nil>
STEP: delete the pod
Jun 26 14:09:33.838: INFO: Waiting for pod pod-subpath-test-configmap-vv5q to disappear
Jun 26 14:09:33.844: INFO: Pod pod-subpath-test-configmap-vv5q no longer exists
STEP: Deleting pod pod-subpath-test-configmap-vv5q
Jun 26 14:09:33.844: INFO: Deleting pod "pod-subpath-test-configmap-vv5q" in namespace "subpath-4209"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:09:33.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4209" for this suite.
Jun 26 14:09:39.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:09:40.067: INFO: namespace subpath-4209 deletion completed in 6.210890437s

• [SLOW TEST:28.409 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:09:40.070: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-7ps8
STEP: Creating a pod to test atomic-volume-subpath
Jun 26 14:09:40.159: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-7ps8" in namespace "subpath-4166" to be "success or failure"
Jun 26 14:09:40.174: INFO: Pod "pod-subpath-test-projected-7ps8": Phase="Pending", Reason="", readiness=false. Elapsed: 14.618707ms
Jun 26 14:09:42.178: INFO: Pod "pod-subpath-test-projected-7ps8": Phase="Running", Reason="", readiness=true. Elapsed: 2.018646835s
Jun 26 14:09:44.182: INFO: Pod "pod-subpath-test-projected-7ps8": Phase="Running", Reason="", readiness=true. Elapsed: 4.022674577s
Jun 26 14:09:46.190: INFO: Pod "pod-subpath-test-projected-7ps8": Phase="Running", Reason="", readiness=true. Elapsed: 6.030421497s
Jun 26 14:09:48.194: INFO: Pod "pod-subpath-test-projected-7ps8": Phase="Running", Reason="", readiness=true. Elapsed: 8.034268467s
Jun 26 14:09:50.197: INFO: Pod "pod-subpath-test-projected-7ps8": Phase="Running", Reason="", readiness=true. Elapsed: 10.037623904s
Jun 26 14:09:52.204: INFO: Pod "pod-subpath-test-projected-7ps8": Phase="Running", Reason="", readiness=true. Elapsed: 12.044597844s
Jun 26 14:09:54.213: INFO: Pod "pod-subpath-test-projected-7ps8": Phase="Running", Reason="", readiness=true. Elapsed: 14.053009154s
Jun 26 14:09:56.216: INFO: Pod "pod-subpath-test-projected-7ps8": Phase="Running", Reason="", readiness=true. Elapsed: 16.056272575s
Jun 26 14:09:58.220: INFO: Pod "pod-subpath-test-projected-7ps8": Phase="Running", Reason="", readiness=true. Elapsed: 18.060049675s
Jun 26 14:10:00.227: INFO: Pod "pod-subpath-test-projected-7ps8": Phase="Running", Reason="", readiness=true. Elapsed: 20.067548635s
Jun 26 14:10:02.231: INFO: Pod "pod-subpath-test-projected-7ps8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.071837088s
STEP: Saw pod success
Jun 26 14:10:02.232: INFO: Pod "pod-subpath-test-projected-7ps8" satisfied condition "success or failure"
Jun 26 14:10:02.234: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-subpath-test-projected-7ps8 container test-container-subpath-projected-7ps8: <nil>
STEP: delete the pod
Jun 26 14:10:02.254: INFO: Waiting for pod pod-subpath-test-projected-7ps8 to disappear
Jun 26 14:10:02.256: INFO: Pod pod-subpath-test-projected-7ps8 no longer exists
STEP: Deleting pod pod-subpath-test-projected-7ps8
Jun 26 14:10:02.256: INFO: Deleting pod "pod-subpath-test-projected-7ps8" in namespace "subpath-4166"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:10:02.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4166" for this suite.
Jun 26 14:10:08.276: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:10:08.363: INFO: namespace subpath-4166 deletion completed in 6.100671981s

• [SLOW TEST:28.293 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:10:08.363: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:10:12.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2888" for this suite.
Jun 26 14:10:18.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:10:18.634: INFO: namespace kubelet-test-2888 deletion completed in 6.214688955s

• [SLOW TEST:10.271 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:10:18.636: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 26 14:10:18.715: INFO: Waiting up to 5m0s for pod "pod-a3e3310d-ac16-4b76-a19b-90a21255d873" in namespace "emptydir-5976" to be "success or failure"
Jun 26 14:10:18.720: INFO: Pod "pod-a3e3310d-ac16-4b76-a19b-90a21255d873": Phase="Pending", Reason="", readiness=false. Elapsed: 5.368783ms
Jun 26 14:10:20.728: INFO: Pod "pod-a3e3310d-ac16-4b76-a19b-90a21255d873": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013031852s
Jun 26 14:10:22.732: INFO: Pod "pod-a3e3310d-ac16-4b76-a19b-90a21255d873": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.016821892s
STEP: Saw pod success
Jun 26 14:10:22.732: INFO: Pod "pod-a3e3310d-ac16-4b76-a19b-90a21255d873" satisfied condition "success or failure"
Jun 26 14:10:22.734: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-a3e3310d-ac16-4b76-a19b-90a21255d873 container test-container: <nil>
STEP: delete the pod
Jun 26 14:10:22.751: INFO: Waiting for pod pod-a3e3310d-ac16-4b76-a19b-90a21255d873 to disappear
Jun 26 14:10:22.753: INFO: Pod pod-a3e3310d-ac16-4b76-a19b-90a21255d873 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:10:22.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5976" for this suite.
Jun 26 14:10:28.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:10:28.928: INFO: namespace emptydir-5976 deletion completed in 6.171936068s

• [SLOW TEST:10.293 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:10:28.929: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 26 14:10:35.059: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 26 14:10:35.064: INFO: Pod pod-with-poststart-http-hook still exists
Jun 26 14:10:37.064: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 26 14:10:37.068: INFO: Pod pod-with-poststart-http-hook still exists
Jun 26 14:10:39.064: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 26 14:10:39.068: INFO: Pod pod-with-poststart-http-hook still exists
Jun 26 14:10:41.064: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 26 14:10:41.068: INFO: Pod pod-with-poststart-http-hook still exists
Jun 26 14:10:43.064: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 26 14:10:43.068: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:10:43.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-7393" for this suite.
Jun 26 14:11:05.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:11:05.158: INFO: namespace container-lifecycle-hook-7393 deletion completed in 22.086651616s

• [SLOW TEST:36.229 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:11:05.159: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-9aa51f77-e711-4566-a4d9-73b879f1f941
STEP: Creating secret with name s-test-opt-upd-020b5388-9782-4937-a5ab-ab86e8da088c
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-9aa51f77-e711-4566-a4d9-73b879f1f941
STEP: Updating secret s-test-opt-upd-020b5388-9782-4937-a5ab-ab86e8da088c
STEP: Creating secret with name s-test-opt-create-6b0f9a14-8123-44a6-b24e-8511e4b6b313
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:11:09.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1926" for this suite.
Jun 26 14:11:31.345: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:11:31.419: INFO: namespace projected-1926 deletion completed in 22.083438485s

• [SLOW TEST:26.261 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:11:31.420: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 26 14:11:31.456: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun 26 14:11:36.464: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 26 14:11:36.464: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun 26 14:11:38.468: INFO: Creating deployment "test-rollover-deployment"
Jun 26 14:11:38.477: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun 26 14:11:40.484: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun 26 14:11:40.492: INFO: Ensure that both replica sets have 1 created replica
Jun 26 14:11:40.499: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun 26 14:11:40.508: INFO: Updating deployment test-rollover-deployment
Jun 26 14:11:40.508: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun 26 14:11:42.520: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun 26 14:11:42.532: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun 26 14:11:42.543: INFO: all replica sets need to contain the pod-template-hash label
Jun 26 14:11:42.544: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155098, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155098, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155100, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155098, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 26 14:11:44.558: INFO: all replica sets need to contain the pod-template-hash label
Jun 26 14:11:44.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155098, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155098, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155103, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155098, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 26 14:11:46.557: INFO: all replica sets need to contain the pod-template-hash label
Jun 26 14:11:46.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155098, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155098, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155103, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155098, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 26 14:11:48.550: INFO: all replica sets need to contain the pod-template-hash label
Jun 26 14:11:48.550: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155098, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155098, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155103, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155098, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 26 14:11:50.559: INFO: all replica sets need to contain the pod-template-hash label
Jun 26 14:11:50.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155098, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155098, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155103, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155098, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 26 14:11:52.558: INFO: all replica sets need to contain the pod-template-hash label
Jun 26 14:11:52.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155098, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155098, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155103, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697155098, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 26 14:11:54.550: INFO: 
Jun 26 14:11:54.550: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 26 14:11:54.559: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-4689,SelfLink:/apis/apps/v1/namespaces/deployment-4689/deployments/test-rollover-deployment,UID:607196b2-53da-4fb6-a2c5-ecf27cc336d1,ResourceVersion:12729,Generation:2,CreationTimestamp:2019-06-26 14:11:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-06-26 14:11:38 +0000 UTC 2019-06-26 14:11:38 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-06-26 14:11:53 +0000 UTC 2019-06-26 14:11:38 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jun 26 14:11:54.562: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-4689,SelfLink:/apis/apps/v1/namespaces/deployment-4689/replicasets/test-rollover-deployment-854595fc44,UID:ecfe0a20-1e10-4367-bf1c-c93e0738b576,ResourceVersion:12718,Generation:2,CreationTimestamp:2019-06-26 14:11:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 607196b2-53da-4fb6-a2c5-ecf27cc336d1 0xc002a91797 0xc002a91798}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 26 14:11:54.562: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun 26 14:11:54.562: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-4689,SelfLink:/apis/apps/v1/namespaces/deployment-4689/replicasets/test-rollover-controller,UID:e1539758-4f6b-4942-a8b6-1e2af71e124d,ResourceVersion:12727,Generation:2,CreationTimestamp:2019-06-26 14:11:31 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 607196b2-53da-4fb6-a2c5-ecf27cc336d1 0xc002a916c7 0xc002a916c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 26 14:11:54.562: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-4689,SelfLink:/apis/apps/v1/namespaces/deployment-4689/replicasets/test-rollover-deployment-9b8b997cf,UID:33f72935-5542-4fb5-a41d-080bc006d1e8,ResourceVersion:12680,Generation:2,CreationTimestamp:2019-06-26 14:11:38 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 607196b2-53da-4fb6-a2c5-ecf27cc336d1 0xc002a91860 0xc002a91861}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 26 14:11:54.565: INFO: Pod "test-rollover-deployment-854595fc44-r4s2d" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-r4s2d,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-4689,SelfLink:/api/v1/namespaces/deployment-4689/pods/test-rollover-deployment-854595fc44-r4s2d,UID:0d243d60-d833-4182-b251-3d1379423718,ResourceVersion:12699,Generation:0,CreationTimestamp:2019-06-26 14:11:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.249.13/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 ecfe0a20-1e10-4367-bf1c-c93e0738b576 0xc0027a6b67 0xc0027a6b68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-7bdrq {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-7bdrq,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-7bdrq true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1-dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0027a6be0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0027a6c00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 14:11:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 14:11:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 14:11:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 14:11:40 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.245,PodIP:172.16.249.13,StartTime:2019-06-26 14:11:40 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-06-26 14:11:43 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://8379380a2067ab4465e9506fc3c5d0f815612d37c365abb28370278d45bdbc75}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:11:54.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4689" for this suite.
Jun 26 14:12:00.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:12:00.758: INFO: namespace deployment-4689 deletion completed in 6.189019245s

• [SLOW TEST:29.338 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:12:00.760: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:12:02.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5279" for this suite.
Jun 26 14:12:40.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:12:41.027: INFO: namespace kubelet-test-5279 deletion completed in 38.191520235s

• [SLOW TEST:40.267 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:12:41.028: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-3296/configmap-test-d613c408-f238-42f4-8ca0-4228ca40eaed
STEP: Creating a pod to test consume configMaps
Jun 26 14:12:41.098: INFO: Waiting up to 5m0s for pod "pod-configmaps-88d6f38e-2c72-4436-858f-c0157a9c5098" in namespace "configmap-3296" to be "success or failure"
Jun 26 14:12:41.105: INFO: Pod "pod-configmaps-88d6f38e-2c72-4436-858f-c0157a9c5098": Phase="Pending", Reason="", readiness=false. Elapsed: 6.654205ms
Jun 26 14:12:43.108: INFO: Pod "pod-configmaps-88d6f38e-2c72-4436-858f-c0157a9c5098": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010104382s
STEP: Saw pod success
Jun 26 14:12:43.109: INFO: Pod "pod-configmaps-88d6f38e-2c72-4436-858f-c0157a9c5098" satisfied condition "success or failure"
Jun 26 14:12:43.111: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-configmaps-88d6f38e-2c72-4436-858f-c0157a9c5098 container env-test: <nil>
STEP: delete the pod
Jun 26 14:12:43.128: INFO: Waiting for pod pod-configmaps-88d6f38e-2c72-4436-858f-c0157a9c5098 to disappear
Jun 26 14:12:43.130: INFO: Pod pod-configmaps-88d6f38e-2c72-4436-858f-c0157a9c5098 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:12:43.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3296" for this suite.
Jun 26 14:12:49.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:12:49.217: INFO: namespace configmap-3296 deletion completed in 6.083207133s

• [SLOW TEST:8.189 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:12:49.217: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 26 14:12:49.246: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9552c5ef-0bab-4a93-9550-54c9e4619627" in namespace "projected-7764" to be "success or failure"
Jun 26 14:12:49.251: INFO: Pod "downwardapi-volume-9552c5ef-0bab-4a93-9550-54c9e4619627": Phase="Pending", Reason="", readiness=false. Elapsed: 4.154524ms
Jun 26 14:12:51.258: INFO: Pod "downwardapi-volume-9552c5ef-0bab-4a93-9550-54c9e4619627": Phase="Running", Reason="", readiness=true. Elapsed: 2.011559971s
Jun 26 14:12:53.265: INFO: Pod "downwardapi-volume-9552c5ef-0bab-4a93-9550-54c9e4619627": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018650319s
STEP: Saw pod success
Jun 26 14:12:53.265: INFO: Pod "downwardapi-volume-9552c5ef-0bab-4a93-9550-54c9e4619627" satisfied condition "success or failure"
Jun 26 14:12:53.270: INFO: Trying to get logs from node k8s-worker-2-dev pod downwardapi-volume-9552c5ef-0bab-4a93-9550-54c9e4619627 container client-container: <nil>
STEP: delete the pod
Jun 26 14:12:53.304: INFO: Waiting for pod downwardapi-volume-9552c5ef-0bab-4a93-9550-54c9e4619627 to disappear
Jun 26 14:12:53.309: INFO: Pod downwardapi-volume-9552c5ef-0bab-4a93-9550-54c9e4619627 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:12:53.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7764" for this suite.
Jun 26 14:12:59.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:12:59.416: INFO: namespace projected-7764 deletion completed in 6.099282983s

• [SLOW TEST:10.199 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:12:59.416: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:13:07.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8768" for this suite.
Jun 26 14:13:14.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:13:14.112: INFO: namespace watch-8768 deletion completed in 6.17940408s

• [SLOW TEST:14.696 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:13:14.113: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 26 14:13:14.157: INFO: Waiting up to 5m0s for pod "downwardapi-volume-12262502-2101-4399-9e58-def7c75520ee" in namespace "downward-api-3740" to be "success or failure"
Jun 26 14:13:14.161: INFO: Pod "downwardapi-volume-12262502-2101-4399-9e58-def7c75520ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.093889ms
Jun 26 14:13:16.177: INFO: Pod "downwardapi-volume-12262502-2101-4399-9e58-def7c75520ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019645267s
STEP: Saw pod success
Jun 26 14:13:16.177: INFO: Pod "downwardapi-volume-12262502-2101-4399-9e58-def7c75520ee" satisfied condition "success or failure"
Jun 26 14:13:16.183: INFO: Trying to get logs from node k8s-worker-1-dev pod downwardapi-volume-12262502-2101-4399-9e58-def7c75520ee container client-container: <nil>
STEP: delete the pod
Jun 26 14:13:16.244: INFO: Waiting for pod downwardapi-volume-12262502-2101-4399-9e58-def7c75520ee to disappear
Jun 26 14:13:16.251: INFO: Pod downwardapi-volume-12262502-2101-4399-9e58-def7c75520ee no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:13:16.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3740" for this suite.
Jun 26 14:13:22.285: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:13:22.473: INFO: namespace downward-api-3740 deletion completed in 6.211576665s

• [SLOW TEST:8.360 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:13:22.474: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-454b437c-7491-4221-a138-79ea1afb3109
STEP: Creating configMap with name cm-test-opt-upd-1de3487c-c3a6-40f4-8415-818de66085f5
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-454b437c-7491-4221-a138-79ea1afb3109
STEP: Updating configmap cm-test-opt-upd-1de3487c-c3a6-40f4-8415-818de66085f5
STEP: Creating configMap with name cm-test-opt-create-93cb36b6-9246-48a0-a7a7-ef5c38fcaee6
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:13:30.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9008" for this suite.
Jun 26 14:13:52.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:13:52.845: INFO: namespace projected-9008 deletion completed in 22.192354145s

• [SLOW TEST:30.372 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:13:52.847: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun 26 14:13:52.925: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9891,SelfLink:/api/v1/namespaces/watch-9891/configmaps/e2e-watch-test-label-changed,UID:08fc59fd-6f00-4250-9d67-5e67ec9c0721,ResourceVersion:13242,Generation:0,CreationTimestamp:2019-06-26 14:13:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 26 14:13:52.925: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9891,SelfLink:/api/v1/namespaces/watch-9891/configmaps/e2e-watch-test-label-changed,UID:08fc59fd-6f00-4250-9d67-5e67ec9c0721,ResourceVersion:13243,Generation:0,CreationTimestamp:2019-06-26 14:13:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 26 14:13:52.926: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9891,SelfLink:/api/v1/namespaces/watch-9891/configmaps/e2e-watch-test-label-changed,UID:08fc59fd-6f00-4250-9d67-5e67ec9c0721,ResourceVersion:13244,Generation:0,CreationTimestamp:2019-06-26 14:13:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun 26 14:14:02.978: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9891,SelfLink:/api/v1/namespaces/watch-9891/configmaps/e2e-watch-test-label-changed,UID:08fc59fd-6f00-4250-9d67-5e67ec9c0721,ResourceVersion:13262,Generation:0,CreationTimestamp:2019-06-26 14:13:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 26 14:14:02.979: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9891,SelfLink:/api/v1/namespaces/watch-9891/configmaps/e2e-watch-test-label-changed,UID:08fc59fd-6f00-4250-9d67-5e67ec9c0721,ResourceVersion:13263,Generation:0,CreationTimestamp:2019-06-26 14:13:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jun 26 14:14:02.979: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9891,SelfLink:/api/v1/namespaces/watch-9891/configmaps/e2e-watch-test-label-changed,UID:08fc59fd-6f00-4250-9d67-5e67ec9c0721,ResourceVersion:13264,Generation:0,CreationTimestamp:2019-06-26 14:13:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:14:02.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9891" for this suite.
Jun 26 14:14:09.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:14:09.092: INFO: namespace watch-9891 deletion completed in 6.103263814s

• [SLOW TEST:16.244 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:14:09.092: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-4945f27c-89de-4eb6-a7b7-0ac38a4e7bf8
STEP: Creating secret with name s-test-opt-upd-1a7b2638-1345-412e-9683-d07abd4adc11
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-4945f27c-89de-4eb6-a7b7-0ac38a4e7bf8
STEP: Updating secret s-test-opt-upd-1a7b2638-1345-412e-9683-d07abd4adc11
STEP: Creating secret with name s-test-opt-create-ed12cd2f-4468-4f5b-80dc-fac7e408a75f
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:15:43.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9465" for this suite.
Jun 26 14:16:05.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:16:06.111: INFO: namespace secrets-9465 deletion completed in 22.217746351s

• [SLOW TEST:117.019 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:16:06.111: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jun 26 14:16:08.771: INFO: Successfully updated pod "annotationupdate9a764729-bc1b-47a4-a4ab-44d41157a98f"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:16:12.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3282" for this suite.
Jun 26 14:16:34.849: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:16:35.054: INFO: namespace downward-api-3282 deletion completed in 22.227889355s

• [SLOW TEST:28.943 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:16:35.056: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-91e6eaf4-24c1-4335-8b3e-62d94fdf2526
STEP: Creating a pod to test consume secrets
Jun 26 14:16:35.150: INFO: Waiting up to 5m0s for pod "pod-secrets-87e421e3-1bc5-49c5-bf0a-70036bfaa8a9" in namespace "secrets-6191" to be "success or failure"
Jun 26 14:16:35.159: INFO: Pod "pod-secrets-87e421e3-1bc5-49c5-bf0a-70036bfaa8a9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.875288ms
Jun 26 14:16:37.167: INFO: Pod "pod-secrets-87e421e3-1bc5-49c5-bf0a-70036bfaa8a9": Phase="Running", Reason="", readiness=true. Elapsed: 2.017556123s
Jun 26 14:16:39.176: INFO: Pod "pod-secrets-87e421e3-1bc5-49c5-bf0a-70036bfaa8a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02599991s
STEP: Saw pod success
Jun 26 14:16:39.176: INFO: Pod "pod-secrets-87e421e3-1bc5-49c5-bf0a-70036bfaa8a9" satisfied condition "success or failure"
Jun 26 14:16:39.183: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-secrets-87e421e3-1bc5-49c5-bf0a-70036bfaa8a9 container secret-volume-test: <nil>
STEP: delete the pod
Jun 26 14:16:39.219: INFO: Waiting for pod pod-secrets-87e421e3-1bc5-49c5-bf0a-70036bfaa8a9 to disappear
Jun 26 14:16:39.226: INFO: Pod pod-secrets-87e421e3-1bc5-49c5-bf0a-70036bfaa8a9 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:16:39.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6191" for this suite.
Jun 26 14:16:45.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:16:45.345: INFO: namespace secrets-6191 deletion completed in 6.108577013s

• [SLOW TEST:10.289 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:16:45.349: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:17:45.393: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-698" for this suite.
Jun 26 14:18:07.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:18:07.626: INFO: namespace container-probe-698 deletion completed in 22.224861245s

• [SLOW TEST:82.278 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:18:07.628: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun 26 14:18:07.727: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8332,SelfLink:/api/v1/namespaces/watch-8332/configmaps/e2e-watch-test-watch-closed,UID:c3250f9e-0769-4cdd-b9e9-d1a131acb45f,ResourceVersion:13810,Generation:0,CreationTimestamp:2019-06-26 14:18:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 26 14:18:07.728: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8332,SelfLink:/api/v1/namespaces/watch-8332/configmaps/e2e-watch-test-watch-closed,UID:c3250f9e-0769-4cdd-b9e9-d1a131acb45f,ResourceVersion:13811,Generation:0,CreationTimestamp:2019-06-26 14:18:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun 26 14:18:07.761: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8332,SelfLink:/api/v1/namespaces/watch-8332/configmaps/e2e-watch-test-watch-closed,UID:c3250f9e-0769-4cdd-b9e9-d1a131acb45f,ResourceVersion:13813,Generation:0,CreationTimestamp:2019-06-26 14:18:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 26 14:18:07.761: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-8332,SelfLink:/api/v1/namespaces/watch-8332/configmaps/e2e-watch-test-watch-closed,UID:c3250f9e-0769-4cdd-b9e9-d1a131acb45f,ResourceVersion:13815,Generation:0,CreationTimestamp:2019-06-26 14:18:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:18:07.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8332" for this suite.
Jun 26 14:18:13.790: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:18:13.996: INFO: namespace watch-8332 deletion completed in 6.22657883s

• [SLOW TEST:6.368 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:18:13.998: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-4059
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-4059
STEP: Deleting pre-stop pod
Jun 26 14:18:27.146: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:18:27.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-4059" for this suite.
Jun 26 14:19:05.198: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:19:05.280: INFO: namespace prestop-4059 deletion completed in 38.10307504s

• [SLOW TEST:51.282 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:19:05.281: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-4415, will wait for the garbage collector to delete the pods
Jun 26 14:19:09.380: INFO: Deleting Job.batch foo took: 6.098785ms
Jun 26 14:19:09.680: INFO: Terminating Job.batch foo pods took: 300.394243ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:19:52.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4415" for this suite.
Jun 26 14:19:58.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:19:58.355: INFO: namespace job-4415 deletion completed in 6.157887211s

• [SLOW TEST:53.074 seconds]
[sig-apps] Job
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:19:58.356: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-537f12a9-cccd-41d3-92d7-783482c77ad4
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-537f12a9-cccd-41d3-92d7-783482c77ad4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:21:27.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5822" for this suite.
Jun 26 14:21:49.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:21:49.237: INFO: namespace projected-5822 deletion completed in 22.210251825s

• [SLOW TEST:110.881 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:21:49.239: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-226ab413-4e58-4a53-a99e-839379cec007
STEP: Creating a pod to test consume configMaps
Jun 26 14:21:49.301: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-708e73b3-8334-4035-a589-dbb2272a1d0f" in namespace "projected-4622" to be "success or failure"
Jun 26 14:21:49.304: INFO: Pod "pod-projected-configmaps-708e73b3-8334-4035-a589-dbb2272a1d0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.880185ms
Jun 26 14:21:51.319: INFO: Pod "pod-projected-configmaps-708e73b3-8334-4035-a589-dbb2272a1d0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017563632s
STEP: Saw pod success
Jun 26 14:21:51.319: INFO: Pod "pod-projected-configmaps-708e73b3-8334-4035-a589-dbb2272a1d0f" satisfied condition "success or failure"
Jun 26 14:21:51.328: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-projected-configmaps-708e73b3-8334-4035-a589-dbb2272a1d0f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 26 14:21:51.413: INFO: Waiting for pod pod-projected-configmaps-708e73b3-8334-4035-a589-dbb2272a1d0f to disappear
Jun 26 14:21:51.419: INFO: Pod pod-projected-configmaps-708e73b3-8334-4035-a589-dbb2272a1d0f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:21:51.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4622" for this suite.
Jun 26 14:21:57.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:21:57.528: INFO: namespace projected-4622 deletion completed in 6.100819556s

• [SLOW TEST:8.289 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:21:57.528: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-aec52431-b4e5-496d-906b-c95ddbf799f2
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:21:57.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7752" for this suite.
Jun 26 14:22:03.580: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:22:03.822: INFO: namespace secrets-7752 deletion completed in 6.256834501s

• [SLOW TEST:6.293 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:22:03.823: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-1a415473-3b0f-446b-9b5a-743eaa62cdc2
STEP: Creating a pod to test consume configMaps
Jun 26 14:22:03.919: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-956bb43a-2606-4953-a17b-819bc41a70e6" in namespace "projected-4758" to be "success or failure"
Jun 26 14:22:03.925: INFO: Pod "pod-projected-configmaps-956bb43a-2606-4953-a17b-819bc41a70e6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.795173ms
Jun 26 14:22:05.933: INFO: Pod "pod-projected-configmaps-956bb43a-2606-4953-a17b-819bc41a70e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013998661s
STEP: Saw pod success
Jun 26 14:22:05.933: INFO: Pod "pod-projected-configmaps-956bb43a-2606-4953-a17b-819bc41a70e6" satisfied condition "success or failure"
Jun 26 14:22:05.939: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-projected-configmaps-956bb43a-2606-4953-a17b-819bc41a70e6 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 26 14:22:05.978: INFO: Waiting for pod pod-projected-configmaps-956bb43a-2606-4953-a17b-819bc41a70e6 to disappear
Jun 26 14:22:05.983: INFO: Pod pod-projected-configmaps-956bb43a-2606-4953-a17b-819bc41a70e6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:22:05.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4758" for this suite.
Jun 26 14:22:12.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:22:12.099: INFO: namespace projected-4758 deletion completed in 6.105040549s

• [SLOW TEST:8.277 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:22:12.101: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 26 14:22:18.203: INFO: Waiting up to 5m0s for pod "client-envvars-624f039f-3b64-41ac-bfab-f6347644ee89" in namespace "pods-6490" to be "success or failure"
Jun 26 14:22:18.213: INFO: Pod "client-envvars-624f039f-3b64-41ac-bfab-f6347644ee89": Phase="Pending", Reason="", readiness=false. Elapsed: 9.826455ms
Jun 26 14:22:20.219: INFO: Pod "client-envvars-624f039f-3b64-41ac-bfab-f6347644ee89": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015703524s
STEP: Saw pod success
Jun 26 14:22:20.219: INFO: Pod "client-envvars-624f039f-3b64-41ac-bfab-f6347644ee89" satisfied condition "success or failure"
Jun 26 14:22:20.224: INFO: Trying to get logs from node k8s-worker-1-dev pod client-envvars-624f039f-3b64-41ac-bfab-f6347644ee89 container env3cont: <nil>
STEP: delete the pod
Jun 26 14:22:20.255: INFO: Waiting for pod client-envvars-624f039f-3b64-41ac-bfab-f6347644ee89 to disappear
Jun 26 14:22:20.260: INFO: Pod client-envvars-624f039f-3b64-41ac-bfab-f6347644ee89 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:22:20.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6490" for this suite.
Jun 26 14:23:04.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:23:04.492: INFO: namespace pods-6490 deletion completed in 44.22520396s

• [SLOW TEST:52.392 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:23:04.494: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 26 14:23:07.797: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun 26 14:23:12.804: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 26 14:23:12.804: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 26 14:23:12.853: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-3245,SelfLink:/apis/apps/v1/namespaces/deployment-3245/deployments/test-cleanup-deployment,UID:3cbe4ba1-1619-45d7-a5a0-7da615e8b3a3,ResourceVersion:14599,Generation:1,CreationTimestamp:2019-06-26 14:23:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Jun 26 14:23:12.873: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-3245,SelfLink:/apis/apps/v1/namespaces/deployment-3245/replicasets/test-cleanup-deployment-55bbcbc84c,UID:bff766e0-7974-4474-b847-609fd91e0ebe,ResourceVersion:14601,Generation:1,CreationTimestamp:2019-06-26 14:23:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 3cbe4ba1-1619-45d7-a5a0-7da615e8b3a3 0xc0028cd6a7 0xc0028cd6a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 26 14:23:12.873: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jun 26 14:23:12.874: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-3245,SelfLink:/apis/apps/v1/namespaces/deployment-3245/replicasets/test-cleanup-controller,UID:80ec8625-b212-4478-8d29-d706abad5965,ResourceVersion:14600,Generation:1,CreationTimestamp:2019-06-26 14:23:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 3cbe4ba1-1619-45d7-a5a0-7da615e8b3a3 0xc0028cd5d7 0xc0028cd5d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 26 14:23:12.889: INFO: Pod "test-cleanup-controller-2vrsb" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-2vrsb,GenerateName:test-cleanup-controller-,Namespace:deployment-3245,SelfLink:/api/v1/namespaces/deployment-3245/pods/test-cleanup-controller-2vrsb,UID:69f43c47-e31a-4d5f-aec1-e8fdaf70cb51,ResourceVersion:14597,Generation:0,CreationTimestamp:2019-06-26 14:23:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.146.214/32,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller 80ec8625-b212-4478-8d29-d706abad5965 0xc0028cdf77 0xc0028cdf78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-phs4q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-phs4q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-phs4q true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2-dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0028cdff0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0017e0010}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 14:23:08 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 14:23:12 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 14:23:12 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 14:23:07 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.247,PodIP:172.16.146.214,StartTime:2019-06-26 14:23:08 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-26 14:23:11 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://b2e58784a63a1b9094ee2a8176375491f938db80e6aa9beb473b370ad7405aad}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 26 14:23:12.890: INFO: Pod "test-cleanup-deployment-55bbcbc84c-k7xq2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-k7xq2,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-3245,SelfLink:/api/v1/namespaces/deployment-3245/pods/test-cleanup-deployment-55bbcbc84c-k7xq2,UID:fc6cd0cb-4224-48e8-94da-9cfc1eea04e3,ResourceVersion:14606,Generation:0,CreationTimestamp:2019-06-26 14:23:12 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c bff766e0-7974-4474-b847-609fd91e0ebe 0xc0017e00f7 0xc0017e00f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-phs4q {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-phs4q,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-phs4q true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1-dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0017e0170} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0017e0190}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 14:23:12 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:23:12.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3245" for this suite.
Jun 26 14:23:18.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:23:19.161: INFO: namespace deployment-3245 deletion completed in 6.256801118s

• [SLOW TEST:14.667 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:23:19.162: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jun 26 14:23:19.228: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 26 14:23:19.243: INFO: Waiting for terminating namespaces to be deleted...
Jun 26 14:23:19.249: INFO: 
Logging pods the kubelet thinks is on node k8s-worker-1-dev before test
Jun 26 14:23:19.264: INFO: nginx-7bb7cd8db5-kt54d from default started at 2019-06-26 12:40:54 +0000 UTC (1 container statuses recorded)
Jun 26 14:23:19.264: INFO: 	Container nginx ready: true, restart count 0
Jun 26 14:23:19.264: INFO: sonobuoy-systemd-logs-daemon-set-750e9b7608a045f9-dm828 from heptio-sonobuoy started at 2019-06-26 14:04:39 +0000 UTC (2 container statuses recorded)
Jun 26 14:23:19.264: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 26 14:23:19.264: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 26 14:23:19.264: INFO: kube-proxy-kt9gc from kube-system started at 2019-06-26 12:36:48 +0000 UTC (1 container statuses recorded)
Jun 26 14:23:19.264: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 26 14:23:19.264: INFO: nginx-proxy-k8s-worker-1-dev from kube-system started at 2019-06-26 12:36:55 +0000 UTC (1 container statuses recorded)
Jun 26 14:23:19.264: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 26 14:23:19.264: INFO: calico-node-qkw4j from kube-system started at 2019-06-26 12:38:32 +0000 UTC (1 container statuses recorded)
Jun 26 14:23:19.264: INFO: 	Container calico-node ready: true, restart count 0
Jun 26 14:23:19.264: INFO: coredns-5c98db65d4-kvsnz from kube-system started at 2019-06-26 12:38:56 +0000 UTC (1 container statuses recorded)
Jun 26 14:23:19.264: INFO: 	Container coredns ready: true, restart count 0
Jun 26 14:23:19.264: INFO: 
Logging pods the kubelet thinks is on node k8s-worker-2-dev before test
Jun 26 14:23:19.279: INFO: nginx-proxy-k8s-worker-2-dev from kube-system started at 2019-06-26 12:37:22 +0000 UTC (1 container statuses recorded)
Jun 26 14:23:19.279: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 26 14:23:19.279: INFO: kube-proxy-rgh7t from kube-system started at 2019-06-26 12:36:49 +0000 UTC (1 container statuses recorded)
Jun 26 14:23:19.279: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 26 14:23:19.279: INFO: calico-node-glqjd from kube-system started at 2019-06-26 12:38:32 +0000 UTC (1 container statuses recorded)
Jun 26 14:23:19.279: INFO: 	Container calico-node ready: true, restart count 0
Jun 26 14:23:19.279: INFO: calico-kube-controllers-6fb584dd97-wfj5r from kube-system started at 2019-06-26 12:38:56 +0000 UTC (1 container statuses recorded)
Jun 26 14:23:19.279: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 26 14:23:19.279: INFO: sonobuoy-systemd-logs-daemon-set-750e9b7608a045f9-tm998 from heptio-sonobuoy started at 2019-06-26 14:04:39 +0000 UTC (2 container statuses recorded)
Jun 26 14:23:19.279: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 26 14:23:19.279: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 26 14:23:19.279: INFO: 
Logging pods the kubelet thinks is on node k8s-worker-3-dev before test
Jun 26 14:23:19.297: INFO: sonobuoy-systemd-logs-daemon-set-750e9b7608a045f9-kgkw4 from heptio-sonobuoy started at 2019-06-26 14:04:39 +0000 UTC (2 container statuses recorded)
Jun 26 14:23:19.297: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 26 14:23:19.297: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 26 14:23:19.297: INFO: kube-proxy-ngw7p from kube-system started at 2019-06-26 12:36:48 +0000 UTC (1 container statuses recorded)
Jun 26 14:23:19.297: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 26 14:23:19.297: INFO: nginx-proxy-k8s-worker-3-dev from kube-system started at 2019-06-26 12:36:56 +0000 UTC (1 container statuses recorded)
Jun 26 14:23:19.297: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 26 14:23:19.297: INFO: calico-node-c28zf from kube-system started at 2019-06-26 12:38:32 +0000 UTC (1 container statuses recorded)
Jun 26 14:23:19.297: INFO: 	Container calico-node ready: true, restart count 0
Jun 26 14:23:19.297: INFO: coredns-5c98db65d4-s8zsb from kube-system started at 2019-06-26 12:38:56 +0000 UTC (1 container statuses recorded)
Jun 26 14:23:19.298: INFO: 	Container coredns ready: true, restart count 0
Jun 26 14:23:19.298: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-26 14:04:36 +0000 UTC (1 container statuses recorded)
Jun 26 14:23:19.298: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 26 14:23:19.298: INFO: sonobuoy-e2e-job-75c21c1d55e6417f from heptio-sonobuoy started at 2019-06-26 14:04:39 +0000 UTC (2 container statuses recorded)
Jun 26 14:23:19.298: INFO: 	Container e2e ready: true, restart count 0
Jun 26 14:23:19.298: INFO: 	Container sonobuoy-worker ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node k8s-worker-1-dev
STEP: verifying the node has the label node k8s-worker-2-dev
STEP: verifying the node has the label node k8s-worker-3-dev
Jun 26 14:23:19.345: INFO: Pod nginx-7bb7cd8db5-kt54d requesting resource cpu=0m on Node k8s-worker-1-dev
Jun 26 14:23:19.345: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-worker-3-dev
Jun 26 14:23:19.345: INFO: Pod sonobuoy-e2e-job-75c21c1d55e6417f requesting resource cpu=0m on Node k8s-worker-3-dev
Jun 26 14:23:19.345: INFO: Pod sonobuoy-systemd-logs-daemon-set-750e9b7608a045f9-dm828 requesting resource cpu=0m on Node k8s-worker-1-dev
Jun 26 14:23:19.345: INFO: Pod sonobuoy-systemd-logs-daemon-set-750e9b7608a045f9-kgkw4 requesting resource cpu=0m on Node k8s-worker-3-dev
Jun 26 14:23:19.345: INFO: Pod sonobuoy-systemd-logs-daemon-set-750e9b7608a045f9-tm998 requesting resource cpu=0m on Node k8s-worker-2-dev
Jun 26 14:23:19.345: INFO: Pod calico-kube-controllers-6fb584dd97-wfj5r requesting resource cpu=0m on Node k8s-worker-2-dev
Jun 26 14:23:19.345: INFO: Pod calico-node-c28zf requesting resource cpu=250m on Node k8s-worker-3-dev
Jun 26 14:23:19.345: INFO: Pod calico-node-glqjd requesting resource cpu=250m on Node k8s-worker-2-dev
Jun 26 14:23:19.345: INFO: Pod calico-node-qkw4j requesting resource cpu=250m on Node k8s-worker-1-dev
Jun 26 14:23:19.345: INFO: Pod coredns-5c98db65d4-kvsnz requesting resource cpu=100m on Node k8s-worker-1-dev
Jun 26 14:23:19.345: INFO: Pod coredns-5c98db65d4-s8zsb requesting resource cpu=100m on Node k8s-worker-3-dev
Jun 26 14:23:19.345: INFO: Pod kube-proxy-kt9gc requesting resource cpu=0m on Node k8s-worker-1-dev
Jun 26 14:23:19.345: INFO: Pod kube-proxy-ngw7p requesting resource cpu=0m on Node k8s-worker-3-dev
Jun 26 14:23:19.345: INFO: Pod kube-proxy-rgh7t requesting resource cpu=0m on Node k8s-worker-2-dev
Jun 26 14:23:19.345: INFO: Pod nginx-proxy-k8s-worker-1-dev requesting resource cpu=25m on Node k8s-worker-1-dev
Jun 26 14:23:19.345: INFO: Pod nginx-proxy-k8s-worker-2-dev requesting resource cpu=25m on Node k8s-worker-2-dev
Jun 26 14:23:19.345: INFO: Pod nginx-proxy-k8s-worker-3-dev requesting resource cpu=25m on Node k8s-worker-3-dev
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-23a3a506-7ae5-450e-bf56-13c2090d77f2.15abc5a2ed02dbf6], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7151/filler-pod-23a3a506-7ae5-450e-bf56-13c2090d77f2 to k8s-worker-3-dev]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-23a3a506-7ae5-450e-bf56-13c2090d77f2.15abc5a3288fb516], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-23a3a506-7ae5-450e-bf56-13c2090d77f2.15abc5a32e0f524c], Reason = [Created], Message = [Created container filler-pod-23a3a506-7ae5-450e-bf56-13c2090d77f2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-23a3a506-7ae5-450e-bf56-13c2090d77f2.15abc5a3403c4f14], Reason = [Started], Message = [Started container filler-pod-23a3a506-7ae5-450e-bf56-13c2090d77f2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-922c2f4a-8c11-48fc-963b-f4068e96501a.15abc5a2eccbe014], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7151/filler-pod-922c2f4a-8c11-48fc-963b-f4068e96501a to k8s-worker-2-dev]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-922c2f4a-8c11-48fc-963b-f4068e96501a.15abc5a3235b8fe3], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-922c2f4a-8c11-48fc-963b-f4068e96501a.15abc5a329384c87], Reason = [Created], Message = [Created container filler-pod-922c2f4a-8c11-48fc-963b-f4068e96501a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-922c2f4a-8c11-48fc-963b-f4068e96501a.15abc5a3393a0441], Reason = [Started], Message = [Started container filler-pod-922c2f4a-8c11-48fc-963b-f4068e96501a]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cc5e1384-6873-4818-b02c-74f98f77cfe6.15abc5a2ecacee77], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7151/filler-pod-cc5e1384-6873-4818-b02c-74f98f77cfe6 to k8s-worker-1-dev]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cc5e1384-6873-4818-b02c-74f98f77cfe6.15abc5a3252fd19e], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cc5e1384-6873-4818-b02c-74f98f77cfe6.15abc5a32b2fac41], Reason = [Created], Message = [Created container filler-pod-cc5e1384-6873-4818-b02c-74f98f77cfe6]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cc5e1384-6873-4818-b02c-74f98f77cfe6.15abc5a33bf19e7e], Reason = [Started], Message = [Started container filler-pod-cc5e1384-6873-4818-b02c-74f98f77cfe6]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15abc5a3dd418efb], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 3 Insufficient cpu.]
STEP: removing the label node off the node k8s-worker-1-dev
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-worker-2-dev
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-worker-3-dev
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:23:24.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7151" for this suite.
Jun 26 14:23:30.532: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:23:30.611: INFO: namespace sched-pred-7151 deletion completed in 6.099272429s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:11.449 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:23:30.612: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-cbea9f6e-c79c-43fa-a7f7-67f736b869e4
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-cbea9f6e-c79c-43fa-a7f7-67f736b869e4
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:23:34.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4826" for this suite.
Jun 26 14:23:56.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:23:56.964: INFO: namespace configmap-4826 deletion completed in 22.220859144s

• [SLOW TEST:26.352 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:23:56.964: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-16220084-e0d9-4437-8e63-9c67da719e32 in namespace container-probe-1148
Jun 26 14:23:59.062: INFO: Started pod busybox-16220084-e0d9-4437-8e63-9c67da719e32 in namespace container-probe-1148
STEP: checking the pod's current state and verifying that restartCount is present
Jun 26 14:23:59.069: INFO: Initial restart count of pod busybox-16220084-e0d9-4437-8e63-9c67da719e32 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:27:59.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1148" for this suite.
Jun 26 14:28:05.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:28:05.904: INFO: namespace container-probe-1148 deletion completed in 6.08977234s

• [SLOW TEST:248.940 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:28:05.905: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 26 14:28:05.939: INFO: Waiting up to 5m0s for pod "downward-api-13e77e10-dde3-4af4-ad8a-8e2e8bf0e4d0" in namespace "downward-api-9211" to be "success or failure"
Jun 26 14:28:05.943: INFO: Pod "downward-api-13e77e10-dde3-4af4-ad8a-8e2e8bf0e4d0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.813741ms
Jun 26 14:28:07.947: INFO: Pod "downward-api-13e77e10-dde3-4af4-ad8a-8e2e8bf0e4d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008067401s
STEP: Saw pod success
Jun 26 14:28:07.947: INFO: Pod "downward-api-13e77e10-dde3-4af4-ad8a-8e2e8bf0e4d0" satisfied condition "success or failure"
Jun 26 14:28:07.950: INFO: Trying to get logs from node k8s-worker-2-dev pod downward-api-13e77e10-dde3-4af4-ad8a-8e2e8bf0e4d0 container dapi-container: <nil>
STEP: delete the pod
Jun 26 14:28:07.968: INFO: Waiting for pod downward-api-13e77e10-dde3-4af4-ad8a-8e2e8bf0e4d0 to disappear
Jun 26 14:28:07.971: INFO: Pod downward-api-13e77e10-dde3-4af4-ad8a-8e2e8bf0e4d0 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:28:07.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9211" for this suite.
Jun 26 14:28:13.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:28:14.064: INFO: namespace downward-api-9211 deletion completed in 6.089142827s

• [SLOW TEST:8.159 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:28:14.065: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-f398e53b-7cdd-41b0-a562-ad60584579fb
STEP: Creating configMap with name cm-test-opt-upd-1e812782-edd0-4051-9e62-28e8724a90df
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-f398e53b-7cdd-41b0-a562-ad60584579fb
STEP: Updating configmap cm-test-opt-upd-1e812782-edd0-4051-9e62-28e8724a90df
STEP: Creating configMap with name cm-test-opt-create-cc169191-3fc4-4a94-9411-f3c4c2abe224
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:29:52.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6935" for this suite.
Jun 26 14:30:14.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:30:14.817: INFO: namespace configmap-6935 deletion completed in 22.213919343s

• [SLOW TEST:120.752 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:30:14.819: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Jun 26 14:30:14.903: INFO: Waiting up to 5m0s for pod "client-containers-327af565-e74c-4b1d-aede-1270fb3b5fec" in namespace "containers-9430" to be "success or failure"
Jun 26 14:30:14.914: INFO: Pod "client-containers-327af565-e74c-4b1d-aede-1270fb3b5fec": Phase="Pending", Reason="", readiness=false. Elapsed: 11.355733ms
Jun 26 14:30:16.922: INFO: Pod "client-containers-327af565-e74c-4b1d-aede-1270fb3b5fec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019355564s
Jun 26 14:30:18.932: INFO: Pod "client-containers-327af565-e74c-4b1d-aede-1270fb3b5fec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029050787s
STEP: Saw pod success
Jun 26 14:30:18.932: INFO: Pod "client-containers-327af565-e74c-4b1d-aede-1270fb3b5fec" satisfied condition "success or failure"
Jun 26 14:30:18.937: INFO: Trying to get logs from node k8s-worker-2-dev pod client-containers-327af565-e74c-4b1d-aede-1270fb3b5fec container test-container: <nil>
STEP: delete the pod
Jun 26 14:30:18.980: INFO: Waiting for pod client-containers-327af565-e74c-4b1d-aede-1270fb3b5fec to disappear
Jun 26 14:30:18.985: INFO: Pod client-containers-327af565-e74c-4b1d-aede-1270fb3b5fec no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:30:18.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9430" for this suite.
Jun 26 14:30:25.014: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:30:25.189: INFO: namespace containers-9430 deletion completed in 6.19700866s

• [SLOW TEST:10.371 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:30:25.189: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 26 14:30:25.261: INFO: Waiting up to 5m0s for pod "pod-669ffd93-2e4f-4dad-934f-f8bca76d05b8" in namespace "emptydir-3480" to be "success or failure"
Jun 26 14:30:25.265: INFO: Pod "pod-669ffd93-2e4f-4dad-934f-f8bca76d05b8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.493931ms
Jun 26 14:30:27.272: INFO: Pod "pod-669ffd93-2e4f-4dad-934f-f8bca76d05b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011210595s
Jun 26 14:30:29.279: INFO: Pod "pod-669ffd93-2e4f-4dad-934f-f8bca76d05b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017698653s
STEP: Saw pod success
Jun 26 14:30:29.279: INFO: Pod "pod-669ffd93-2e4f-4dad-934f-f8bca76d05b8" satisfied condition "success or failure"
Jun 26 14:30:29.283: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-669ffd93-2e4f-4dad-934f-f8bca76d05b8 container test-container: <nil>
STEP: delete the pod
Jun 26 14:30:29.318: INFO: Waiting for pod pod-669ffd93-2e4f-4dad-934f-f8bca76d05b8 to disappear
Jun 26 14:30:29.322: INFO: Pod pod-669ffd93-2e4f-4dad-934f-f8bca76d05b8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:30:29.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3480" for this suite.
Jun 26 14:30:35.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:30:35.434: INFO: namespace emptydir-3480 deletion completed in 6.105436031s

• [SLOW TEST:10.245 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:30:35.436: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 26 14:30:35.468: INFO: Creating deployment "test-recreate-deployment"
Jun 26 14:30:35.473: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun 26 14:30:35.484: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jun 26 14:30:37.490: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun 26 14:30:37.493: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697156235, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697156235, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697156235, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697156235, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 26 14:30:39.497: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697156235, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697156235, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697156235, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697156235, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-6df85df6b9\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 26 14:30:41.500: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun 26 14:30:41.515: INFO: Updating deployment test-recreate-deployment
Jun 26 14:30:41.515: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 26 14:30:41.673: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-4974,SelfLink:/apis/apps/v1/namespaces/deployment-4974/deployments/test-recreate-deployment,UID:68ff81b9-3f20-4787-8fb3-f801663687b4,ResourceVersion:15687,Generation:2,CreationTimestamp:2019-06-26 14:30:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-06-26 14:30:41 +0000 UTC 2019-06-26 14:30:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-06-26 14:30:41 +0000 UTC 2019-06-26 14:30:35 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jun 26 14:30:41.682: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-4974,SelfLink:/apis/apps/v1/namespaces/deployment-4974/replicasets/test-recreate-deployment-5c8c9cc69d,UID:6d002094-b9a5-4d97-894c-8f85da8cf919,ResourceVersion:15685,Generation:1,CreationTimestamp:2019-06-26 14:30:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 68ff81b9-3f20-4787-8fb3-f801663687b4 0xc0034ea0d7 0xc0034ea0d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 26 14:30:41.682: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun 26 14:30:41.684: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-4974,SelfLink:/apis/apps/v1/namespaces/deployment-4974/replicasets/test-recreate-deployment-6df85df6b9,UID:f6f15410-c858-427d-a7ac-1783a5d73e28,ResourceVersion:15675,Generation:2,CreationTimestamp:2019-06-26 14:30:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 68ff81b9-3f20-4787-8fb3-f801663687b4 0xc0034ea1a7 0xc0034ea1a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 26 14:30:41.692: INFO: Pod "test-recreate-deployment-5c8c9cc69d-5gh76" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-5gh76,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-4974,SelfLink:/api/v1/namespaces/deployment-4974/pods/test-recreate-deployment-5c8c9cc69d-5gh76,UID:dfc5c232-f4e6-4833-b8c0-15382b250d7d,ResourceVersion:15686,Generation:0,CreationTimestamp:2019-06-26 14:30:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 6d002094-b9a5-4d97-894c-8f85da8cf919 0xc002c14057 0xc002c14058}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95qtl {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95qtl,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-95qtl true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1-dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c14130} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c14170}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 14:30:41 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 14:30:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 14:30:41 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 14:30:41 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.245,PodIP:,StartTime:2019-06-26 14:30:41 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:30:41.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4974" for this suite.
Jun 26 14:30:47.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:30:47.823: INFO: namespace deployment-4974 deletion completed in 6.120996709s

• [SLOW TEST:12.387 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:30:47.825: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-8d26a754-dcba-43b2-955f-fe86590209e5 in namespace container-probe-9784
Jun 26 14:30:49.872: INFO: Started pod test-webserver-8d26a754-dcba-43b2-955f-fe86590209e5 in namespace container-probe-9784
STEP: checking the pod's current state and verifying that restartCount is present
Jun 26 14:30:49.874: INFO: Initial restart count of pod test-webserver-8d26a754-dcba-43b2-955f-fe86590209e5 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:34:50.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9784" for this suite.
Jun 26 14:34:56.492: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:34:56.689: INFO: namespace container-probe-9784 deletion completed in 6.220993975s

• [SLOW TEST:248.864 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:34:56.690: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jun 26 14:34:56.759: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 26 14:34:56.778: INFO: Waiting for terminating namespaces to be deleted...
Jun 26 14:34:56.783: INFO: 
Logging pods the kubelet thinks is on node k8s-worker-1-dev before test
Jun 26 14:34:56.805: INFO: kube-proxy-kt9gc from kube-system started at 2019-06-26 12:36:48 +0000 UTC (1 container statuses recorded)
Jun 26 14:34:56.805: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 26 14:34:56.805: INFO: nginx-proxy-k8s-worker-1-dev from kube-system started at 2019-06-26 12:36:55 +0000 UTC (1 container statuses recorded)
Jun 26 14:34:56.805: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 26 14:34:56.805: INFO: calico-node-qkw4j from kube-system started at 2019-06-26 12:38:32 +0000 UTC (1 container statuses recorded)
Jun 26 14:34:56.806: INFO: 	Container calico-node ready: true, restart count 0
Jun 26 14:34:56.806: INFO: coredns-5c98db65d4-kvsnz from kube-system started at 2019-06-26 12:38:56 +0000 UTC (1 container statuses recorded)
Jun 26 14:34:56.806: INFO: 	Container coredns ready: true, restart count 0
Jun 26 14:34:56.806: INFO: nginx-7bb7cd8db5-kt54d from default started at 2019-06-26 12:40:54 +0000 UTC (1 container statuses recorded)
Jun 26 14:34:56.806: INFO: 	Container nginx ready: true, restart count 0
Jun 26 14:34:56.806: INFO: sonobuoy-systemd-logs-daemon-set-750e9b7608a045f9-dm828 from heptio-sonobuoy started at 2019-06-26 14:04:39 +0000 UTC (2 container statuses recorded)
Jun 26 14:34:56.806: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 26 14:34:56.806: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 26 14:34:56.806: INFO: 
Logging pods the kubelet thinks is on node k8s-worker-2-dev before test
Jun 26 14:34:56.823: INFO: nginx-proxy-k8s-worker-2-dev from kube-system started at 2019-06-26 12:37:22 +0000 UTC (1 container statuses recorded)
Jun 26 14:34:56.823: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 26 14:34:56.823: INFO: kube-proxy-rgh7t from kube-system started at 2019-06-26 12:36:49 +0000 UTC (1 container statuses recorded)
Jun 26 14:34:56.823: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 26 14:34:56.823: INFO: calico-node-glqjd from kube-system started at 2019-06-26 12:38:32 +0000 UTC (1 container statuses recorded)
Jun 26 14:34:56.823: INFO: 	Container calico-node ready: true, restart count 0
Jun 26 14:34:56.823: INFO: calico-kube-controllers-6fb584dd97-wfj5r from kube-system started at 2019-06-26 12:38:56 +0000 UTC (1 container statuses recorded)
Jun 26 14:34:56.823: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 26 14:34:56.823: INFO: sonobuoy-systemd-logs-daemon-set-750e9b7608a045f9-tm998 from heptio-sonobuoy started at 2019-06-26 14:04:39 +0000 UTC (2 container statuses recorded)
Jun 26 14:34:56.823: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 26 14:34:56.823: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 26 14:34:56.824: INFO: 
Logging pods the kubelet thinks is on node k8s-worker-3-dev before test
Jun 26 14:34:56.846: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-26 14:04:36 +0000 UTC (1 container statuses recorded)
Jun 26 14:34:56.846: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 26 14:34:56.846: INFO: sonobuoy-e2e-job-75c21c1d55e6417f from heptio-sonobuoy started at 2019-06-26 14:04:39 +0000 UTC (2 container statuses recorded)
Jun 26 14:34:56.846: INFO: 	Container e2e ready: true, restart count 0
Jun 26 14:34:56.846: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 26 14:34:56.846: INFO: sonobuoy-systemd-logs-daemon-set-750e9b7608a045f9-kgkw4 from heptio-sonobuoy started at 2019-06-26 14:04:39 +0000 UTC (2 container statuses recorded)
Jun 26 14:34:56.846: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 26 14:34:56.846: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 26 14:34:56.846: INFO: kube-proxy-ngw7p from kube-system started at 2019-06-26 12:36:48 +0000 UTC (1 container statuses recorded)
Jun 26 14:34:56.846: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 26 14:34:56.846: INFO: nginx-proxy-k8s-worker-3-dev from kube-system started at 2019-06-26 12:36:56 +0000 UTC (1 container statuses recorded)
Jun 26 14:34:56.846: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 26 14:34:56.846: INFO: calico-node-c28zf from kube-system started at 2019-06-26 12:38:32 +0000 UTC (1 container statuses recorded)
Jun 26 14:34:56.846: INFO: 	Container calico-node ready: true, restart count 0
Jun 26 14:34:56.846: INFO: coredns-5c98db65d4-s8zsb from kube-system started at 2019-06-26 12:38:56 +0000 UTC (1 container statuses recorded)
Jun 26 14:34:56.846: INFO: 	Container coredns ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-c733969a-e1dc-4247-9062-002f94b2b19e 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-c733969a-e1dc-4247-9062-002f94b2b19e off the node k8s-worker-1-dev
STEP: verifying the node doesn't have the label kubernetes.io/e2e-c733969a-e1dc-4247-9062-002f94b2b19e
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:35:04.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-688" for this suite.
Jun 26 14:35:13.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:35:13.223: INFO: namespace sched-pred-688 deletion completed in 8.221766165s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:16.533 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:35:13.225: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Jun 26 14:35:13.303: INFO: Waiting up to 5m0s for pod "client-containers-31e6d5b5-e3dc-4136-a640-d2d93dd555ae" in namespace "containers-6021" to be "success or failure"
Jun 26 14:35:13.311: INFO: Pod "client-containers-31e6d5b5-e3dc-4136-a640-d2d93dd555ae": Phase="Pending", Reason="", readiness=false. Elapsed: 7.564394ms
Jun 26 14:35:15.315: INFO: Pod "client-containers-31e6d5b5-e3dc-4136-a640-d2d93dd555ae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011365804s
STEP: Saw pod success
Jun 26 14:35:15.315: INFO: Pod "client-containers-31e6d5b5-e3dc-4136-a640-d2d93dd555ae" satisfied condition "success or failure"
Jun 26 14:35:15.317: INFO: Trying to get logs from node k8s-worker-2-dev pod client-containers-31e6d5b5-e3dc-4136-a640-d2d93dd555ae container test-container: <nil>
STEP: delete the pod
Jun 26 14:35:15.329: INFO: Waiting for pod client-containers-31e6d5b5-e3dc-4136-a640-d2d93dd555ae to disappear
Jun 26 14:35:15.332: INFO: Pod client-containers-31e6d5b5-e3dc-4136-a640-d2d93dd555ae no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:35:15.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6021" for this suite.
Jun 26 14:35:21.347: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:35:21.562: INFO: namespace containers-6021 deletion completed in 6.227612643s

• [SLOW TEST:8.337 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:35:21.562: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 26 14:35:21.636: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:35:24.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3105" for this suite.
Jun 26 14:36:14.247: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:36:14.323: INFO: namespace pods-3105 deletion completed in 50.096054303s

• [SLOW TEST:52.761 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:36:14.325: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 26 14:36:14.359: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9691b41c-b9cf-4394-8c20-9bd6eac35746" in namespace "projected-4186" to be "success or failure"
Jun 26 14:36:14.366: INFO: Pod "downwardapi-volume-9691b41c-b9cf-4394-8c20-9bd6eac35746": Phase="Pending", Reason="", readiness=false. Elapsed: 6.281586ms
Jun 26 14:36:16.372: INFO: Pod "downwardapi-volume-9691b41c-b9cf-4394-8c20-9bd6eac35746": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012379547s
STEP: Saw pod success
Jun 26 14:36:16.372: INFO: Pod "downwardapi-volume-9691b41c-b9cf-4394-8c20-9bd6eac35746" satisfied condition "success or failure"
Jun 26 14:36:16.377: INFO: Trying to get logs from node k8s-worker-2-dev pod downwardapi-volume-9691b41c-b9cf-4394-8c20-9bd6eac35746 container client-container: <nil>
STEP: delete the pod
Jun 26 14:36:16.417: INFO: Waiting for pod downwardapi-volume-9691b41c-b9cf-4394-8c20-9bd6eac35746 to disappear
Jun 26 14:36:16.423: INFO: Pod downwardapi-volume-9691b41c-b9cf-4394-8c20-9bd6eac35746 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:36:16.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4186" for this suite.
Jun 26 14:36:22.452: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:36:22.663: INFO: namespace projected-4186 deletion completed in 6.231823128s

• [SLOW TEST:8.339 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:36:22.666: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-613a69b0-7567-4f7d-bba3-8f1b18e32913
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:36:22.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8363" for this suite.
Jun 26 14:36:28.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:36:28.857: INFO: namespace configmap-8363 deletion completed in 6.105035493s

• [SLOW TEST:6.191 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:36:28.857: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-7b45bacd-02e4-485d-9fa9-970df044e75b
STEP: Creating a pod to test consume configMaps
Jun 26 14:36:28.899: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b01d7f40-ff74-4f9b-be4a-e9c7d89d4a28" in namespace "projected-4196" to be "success or failure"
Jun 26 14:36:28.902: INFO: Pod "pod-projected-configmaps-b01d7f40-ff74-4f9b-be4a-e9c7d89d4a28": Phase="Pending", Reason="", readiness=false. Elapsed: 3.324803ms
Jun 26 14:36:30.910: INFO: Pod "pod-projected-configmaps-b01d7f40-ff74-4f9b-be4a-e9c7d89d4a28": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011109707s
STEP: Saw pod success
Jun 26 14:36:30.910: INFO: Pod "pod-projected-configmaps-b01d7f40-ff74-4f9b-be4a-e9c7d89d4a28" satisfied condition "success or failure"
Jun 26 14:36:30.915: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-projected-configmaps-b01d7f40-ff74-4f9b-be4a-e9c7d89d4a28 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 26 14:36:30.954: INFO: Waiting for pod pod-projected-configmaps-b01d7f40-ff74-4f9b-be4a-e9c7d89d4a28 to disappear
Jun 26 14:36:30.961: INFO: Pod pod-projected-configmaps-b01d7f40-ff74-4f9b-be4a-e9c7d89d4a28 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:36:30.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4196" for this suite.
Jun 26 14:36:36.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:36:37.224: INFO: namespace projected-4196 deletion completed in 6.253074383s

• [SLOW TEST:8.367 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:36:37.225: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4850
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-4850
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4850
Jun 26 14:36:37.361: INFO: Found 0 stateful pods, waiting for 1
Jun 26 14:36:47.373: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun 26 14:36:47.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 exec --namespace=statefulset-4850 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 26 14:36:48.023: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 26 14:36:48.023: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 26 14:36:48.023: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 26 14:36:48.031: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 26 14:36:58.040: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 26 14:36:58.040: INFO: Waiting for statefulset status.replicas updated to 0
Jun 26 14:36:58.070: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999063s
Jun 26 14:36:59.079: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993490208s
Jun 26 14:37:00.087: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.983845058s
Jun 26 14:37:01.097: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.976579563s
Jun 26 14:37:02.101: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.966643251s
Jun 26 14:37:03.110: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.96231366s
Jun 26 14:37:04.119: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.953262171s
Jun 26 14:37:05.128: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.943836095s
Jun 26 14:37:06.139: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.934863198s
Jun 26 14:37:07.148: INFO: Verifying statefulset ss doesn't scale past 1 for another 924.096885ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4850
Jun 26 14:37:08.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 exec --namespace=statefulset-4850 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 26 14:37:08.733: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 26 14:37:08.733: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 26 14:37:08.733: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 26 14:37:08.736: INFO: Found 1 stateful pods, waiting for 3
Jun 26 14:37:18.748: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 26 14:37:18.749: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 26 14:37:18.749: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun 26 14:37:18.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 exec --namespace=statefulset-4850 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 26 14:37:19.308: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 26 14:37:19.308: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 26 14:37:19.308: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 26 14:37:19.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 exec --namespace=statefulset-4850 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 26 14:37:19.878: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 26 14:37:19.878: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 26 14:37:19.878: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 26 14:37:19.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 exec --namespace=statefulset-4850 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 26 14:37:20.194: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 26 14:37:20.194: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 26 14:37:20.194: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 26 14:37:20.194: INFO: Waiting for statefulset status.replicas updated to 0
Jun 26 14:37:20.197: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jun 26 14:37:30.204: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 26 14:37:30.205: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 26 14:37:30.205: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 26 14:37:30.216: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999541s
Jun 26 14:37:31.221: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994353653s
Jun 26 14:37:32.231: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989497742s
Jun 26 14:37:33.240: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.979460082s
Jun 26 14:37:34.249: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970079936s
Jun 26 14:37:35.260: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.960797576s
Jun 26 14:37:36.269: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.950545011s
Jun 26 14:37:37.273: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.941499421s
Jun 26 14:37:38.278: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.936884453s
Jun 26 14:37:39.284: INFO: Verifying statefulset ss doesn't scale past 3 for another 932.010547ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4850
Jun 26 14:37:40.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 exec --namespace=statefulset-4850 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 26 14:37:40.793: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 26 14:37:40.793: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 26 14:37:40.793: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 26 14:37:40.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 exec --namespace=statefulset-4850 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 26 14:37:41.101: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 26 14:37:41.101: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 26 14:37:41.101: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 26 14:37:41.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 exec --namespace=statefulset-4850 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 26 14:37:41.470: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 26 14:37:41.470: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 26 14:37:41.470: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 26 14:37:41.470: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 26 14:38:01.485: INFO: Deleting all statefulset in ns statefulset-4850
Jun 26 14:38:01.488: INFO: Scaling statefulset ss to 0
Jun 26 14:38:01.495: INFO: Waiting for statefulset status.replicas updated to 0
Jun 26 14:38:01.497: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:38:01.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4850" for this suite.
Jun 26 14:38:07.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:38:07.601: INFO: namespace statefulset-4850 deletion completed in 6.089510557s

• [SLOW TEST:90.377 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:38:07.603: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-689b9eb4-10fb-450f-94e6-57ee6769d07b in namespace container-probe-8869
Jun 26 14:38:11.661: INFO: Started pod liveness-689b9eb4-10fb-450f-94e6-57ee6769d07b in namespace container-probe-8869
STEP: checking the pod's current state and verifying that restartCount is present
Jun 26 14:38:11.666: INFO: Initial restart count of pod liveness-689b9eb4-10fb-450f-94e6-57ee6769d07b is 0
Jun 26 14:38:23.718: INFO: Restart count of pod container-probe-8869/liveness-689b9eb4-10fb-450f-94e6-57ee6769d07b is now 1 (12.051648816s elapsed)
Jun 26 14:38:43.790: INFO: Restart count of pod container-probe-8869/liveness-689b9eb4-10fb-450f-94e6-57ee6769d07b is now 2 (32.12367096s elapsed)
Jun 26 14:39:03.853: INFO: Restart count of pod container-probe-8869/liveness-689b9eb4-10fb-450f-94e6-57ee6769d07b is now 3 (52.186774863s elapsed)
Jun 26 14:39:25.921: INFO: Restart count of pod container-probe-8869/liveness-689b9eb4-10fb-450f-94e6-57ee6769d07b is now 4 (1m14.254578419s elapsed)
Jun 26 14:40:34.108: INFO: Restart count of pod container-probe-8869/liveness-689b9eb4-10fb-450f-94e6-57ee6769d07b is now 5 (2m22.441498074s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:40:34.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8869" for this suite.
Jun 26 14:40:40.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:40:40.323: INFO: namespace container-probe-8869 deletion completed in 6.170602535s

• [SLOW TEST:152.720 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:40:40.325: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7120.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-7120.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7120.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-7120.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-7120.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7120.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 26 14:40:58.474: INFO: DNS probes using dns-7120/dns-test-6b0ae27a-9abd-4257-91c1-b598474f99cb succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:40:58.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7120" for this suite.
Jun 26 14:41:04.540: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:41:04.736: INFO: namespace dns-7120 deletion completed in 6.225195324s

• [SLOW TEST:24.411 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:41:04.738: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:41:04.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7172" for this suite.
Jun 26 14:41:26.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:41:27.098: INFO: namespace kubelet-test-7172 deletion completed in 22.237041498s

• [SLOW TEST:22.361 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:41:27.108: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:41:30.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4237" for this suite.
Jun 26 14:41:52.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:41:52.435: INFO: namespace replication-controller-4237 deletion completed in 22.194375791s

• [SLOW TEST:25.328 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:41:52.436: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 26 14:41:52.525: INFO: Waiting up to 5m0s for pod "pod-b21245a6-b16f-48a3-b1eb-8a0eeaeb6b20" in namespace "emptydir-1711" to be "success or failure"
Jun 26 14:41:52.531: INFO: Pod "pod-b21245a6-b16f-48a3-b1eb-8a0eeaeb6b20": Phase="Pending", Reason="", readiness=false. Elapsed: 6.62966ms
Jun 26 14:41:54.536: INFO: Pod "pod-b21245a6-b16f-48a3-b1eb-8a0eeaeb6b20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010712013s
STEP: Saw pod success
Jun 26 14:41:54.536: INFO: Pod "pod-b21245a6-b16f-48a3-b1eb-8a0eeaeb6b20" satisfied condition "success or failure"
Jun 26 14:41:54.538: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-b21245a6-b16f-48a3-b1eb-8a0eeaeb6b20 container test-container: <nil>
STEP: delete the pod
Jun 26 14:41:54.553: INFO: Waiting for pod pod-b21245a6-b16f-48a3-b1eb-8a0eeaeb6b20 to disappear
Jun 26 14:41:54.556: INFO: Pod pod-b21245a6-b16f-48a3-b1eb-8a0eeaeb6b20 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:41:54.556: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1711" for this suite.
Jun 26 14:42:00.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:42:00.791: INFO: namespace emptydir-1711 deletion completed in 6.231765587s

• [SLOW TEST:8.355 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:42:00.794: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 26 14:42:00.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4050'
Jun 26 14:42:01.107: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 26 14:42:01.108: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Jun 26 14:42:01.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 delete deployment e2e-test-nginx-deployment --namespace=kubectl-4050'
Jun 26 14:42:01.296: INFO: stderr: ""
Jun 26 14:42:01.296: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:42:01.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4050" for this suite.
Jun 26 14:42:07.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:42:07.561: INFO: namespace kubectl-4050 deletion completed in 6.258418035s

• [SLOW TEST:6.768 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:42:07.564: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-e7568478-bcdd-4060-aadc-9b805ef8221e
STEP: Creating a pod to test consume configMaps
Jun 26 14:42:07.666: INFO: Waiting up to 5m0s for pod "pod-configmaps-81b6bf73-43b0-444c-a17d-08a64e6a4ba7" in namespace "configmap-4268" to be "success or failure"
Jun 26 14:42:07.676: INFO: Pod "pod-configmaps-81b6bf73-43b0-444c-a17d-08a64e6a4ba7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.996763ms
Jun 26 14:42:09.681: INFO: Pod "pod-configmaps-81b6bf73-43b0-444c-a17d-08a64e6a4ba7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015347121s
STEP: Saw pod success
Jun 26 14:42:09.682: INFO: Pod "pod-configmaps-81b6bf73-43b0-444c-a17d-08a64e6a4ba7" satisfied condition "success or failure"
Jun 26 14:42:09.685: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-configmaps-81b6bf73-43b0-444c-a17d-08a64e6a4ba7 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 26 14:42:09.703: INFO: Waiting for pod pod-configmaps-81b6bf73-43b0-444c-a17d-08a64e6a4ba7 to disappear
Jun 26 14:42:09.706: INFO: Pod pod-configmaps-81b6bf73-43b0-444c-a17d-08a64e6a4ba7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:42:09.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4268" for this suite.
Jun 26 14:42:15.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:42:15.922: INFO: namespace configmap-4268 deletion completed in 6.212959265s

• [SLOW TEST:8.358 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:42:15.923: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 26 14:42:15.997: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:42:17.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6720" for this suite.
Jun 26 14:42:23.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:42:23.260: INFO: namespace custom-resource-definition-6720 deletion completed in 6.103445485s

• [SLOW TEST:7.337 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:42:23.262: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-8d068683-c67a-4bdb-bc66-772146f80c46 in namespace container-probe-4134
Jun 26 14:42:27.305: INFO: Started pod liveness-8d068683-c67a-4bdb-bc66-772146f80c46 in namespace container-probe-4134
STEP: checking the pod's current state and verifying that restartCount is present
Jun 26 14:42:27.312: INFO: Initial restart count of pod liveness-8d068683-c67a-4bdb-bc66-772146f80c46 is 0
Jun 26 14:42:47.387: INFO: Restart count of pod container-probe-4134/liveness-8d068683-c67a-4bdb-bc66-772146f80c46 is now 1 (20.074834048s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:42:47.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4134" for this suite.
Jun 26 14:42:53.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:42:53.698: INFO: namespace container-probe-4134 deletion completed in 6.274949744s

• [SLOW TEST:30.436 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:42:53.701: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 26 14:42:53.810: INFO: Waiting up to 5m0s for pod "downwardapi-volume-552705e8-6bf2-46d1-bb78-4c45009af115" in namespace "downward-api-9794" to be "success or failure"
Jun 26 14:42:53.819: INFO: Pod "downwardapi-volume-552705e8-6bf2-46d1-bb78-4c45009af115": Phase="Pending", Reason="", readiness=false. Elapsed: 8.792301ms
Jun 26 14:42:55.829: INFO: Pod "downwardapi-volume-552705e8-6bf2-46d1-bb78-4c45009af115": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018442247s
STEP: Saw pod success
Jun 26 14:42:55.829: INFO: Pod "downwardapi-volume-552705e8-6bf2-46d1-bb78-4c45009af115" satisfied condition "success or failure"
Jun 26 14:42:55.841: INFO: Trying to get logs from node k8s-worker-1-dev pod downwardapi-volume-552705e8-6bf2-46d1-bb78-4c45009af115 container client-container: <nil>
STEP: delete the pod
Jun 26 14:42:55.890: INFO: Waiting for pod downwardapi-volume-552705e8-6bf2-46d1-bb78-4c45009af115 to disappear
Jun 26 14:42:55.897: INFO: Pod downwardapi-volume-552705e8-6bf2-46d1-bb78-4c45009af115 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:42:55.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9794" for this suite.
Jun 26 14:43:01.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:43:02.128: INFO: namespace downward-api-9794 deletion completed in 6.220706089s

• [SLOW TEST:8.428 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:43:02.130: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 26 14:43:02.214: INFO: Waiting up to 5m0s for pod "downwardapi-volume-490ab19e-8306-41e2-a18b-064bb0dc6e6e" in namespace "projected-6527" to be "success or failure"
Jun 26 14:43:02.220: INFO: Pod "downwardapi-volume-490ab19e-8306-41e2-a18b-064bb0dc6e6e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.51575ms
Jun 26 14:43:04.225: INFO: Pod "downwardapi-volume-490ab19e-8306-41e2-a18b-064bb0dc6e6e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010918774s
Jun 26 14:43:06.229: INFO: Pod "downwardapi-volume-490ab19e-8306-41e2-a18b-064bb0dc6e6e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015039613s
STEP: Saw pod success
Jun 26 14:43:06.229: INFO: Pod "downwardapi-volume-490ab19e-8306-41e2-a18b-064bb0dc6e6e" satisfied condition "success or failure"
Jun 26 14:43:06.231: INFO: Trying to get logs from node k8s-worker-2-dev pod downwardapi-volume-490ab19e-8306-41e2-a18b-064bb0dc6e6e container client-container: <nil>
STEP: delete the pod
Jun 26 14:43:06.253: INFO: Waiting for pod downwardapi-volume-490ab19e-8306-41e2-a18b-064bb0dc6e6e to disappear
Jun 26 14:43:06.256: INFO: Pod downwardapi-volume-490ab19e-8306-41e2-a18b-064bb0dc6e6e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:43:06.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6527" for this suite.
Jun 26 14:43:12.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:43:12.487: INFO: namespace projected-6527 deletion completed in 6.227651659s

• [SLOW TEST:10.357 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:43:12.496: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:43:21.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3952" for this suite.
Jun 26 14:43:27.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:43:27.937: INFO: namespace namespaces-3952 deletion completed in 6.158602289s
STEP: Destroying namespace "nsdeletetest-481" for this suite.
Jun 26 14:43:27.939: INFO: Namespace nsdeletetest-481 was already deleted
STEP: Destroying namespace "nsdeletetest-3283" for this suite.
Jun 26 14:43:33.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:43:34.027: INFO: namespace nsdeletetest-3283 deletion completed in 6.088039235s

• [SLOW TEST:21.532 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:43:34.028: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jun 26 14:43:36.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 exec pod-sharedvolume-01e4ce9a-2ffd-482a-83a8-e2b0a63b269f -c busybox-main-container --namespace=emptydir-8474 -- cat /usr/share/volumeshare/shareddata.txt'
Jun 26 14:43:36.780: INFO: stderr: ""
Jun 26 14:43:36.780: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:43:36.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8474" for this suite.
Jun 26 14:43:42.815: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:43:43.012: INFO: namespace emptydir-8474 deletion completed in 6.222020583s

• [SLOW TEST:8.984 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:43:43.012: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0626 14:43:44.193138      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 26 14:43:44.194: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:43:44.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8843" for this suite.
Jun 26 14:43:50.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:43:50.369: INFO: namespace gc-8843 deletion completed in 6.164726343s

• [SLOW TEST:7.357 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:43:50.370: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-6e933f5d-1059-48ec-8a12-21fcd171b1c8
STEP: Creating a pod to test consume configMaps
Jun 26 14:43:50.462: INFO: Waiting up to 5m0s for pod "pod-configmaps-87a735a2-bd2c-48be-902a-d2da5a9d3e42" in namespace "configmap-9108" to be "success or failure"
Jun 26 14:43:50.475: INFO: Pod "pod-configmaps-87a735a2-bd2c-48be-902a-d2da5a9d3e42": Phase="Pending", Reason="", readiness=false. Elapsed: 13.13999ms
Jun 26 14:43:52.481: INFO: Pod "pod-configmaps-87a735a2-bd2c-48be-902a-d2da5a9d3e42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019454717s
STEP: Saw pod success
Jun 26 14:43:52.482: INFO: Pod "pod-configmaps-87a735a2-bd2c-48be-902a-d2da5a9d3e42" satisfied condition "success or failure"
Jun 26 14:43:52.488: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-configmaps-87a735a2-bd2c-48be-902a-d2da5a9d3e42 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 26 14:43:52.529: INFO: Waiting for pod pod-configmaps-87a735a2-bd2c-48be-902a-d2da5a9d3e42 to disappear
Jun 26 14:43:52.534: INFO: Pod pod-configmaps-87a735a2-bd2c-48be-902a-d2da5a9d3e42 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:43:52.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9108" for this suite.
Jun 26 14:43:58.570: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:43:58.787: INFO: namespace configmap-9108 deletion completed in 6.245016905s

• [SLOW TEST:8.417 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:43:58.787: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 26 14:43:58.879: INFO: (0) /api/v1/nodes/k8s-worker-1-dev/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 13.889316ms)
Jun 26 14:43:58.889: INFO: (1) /api/v1/nodes/k8s-worker-1-dev/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 9.68807ms)
Jun 26 14:43:58.896: INFO: (2) /api/v1/nodes/k8s-worker-1-dev/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.552276ms)
Jun 26 14:43:58.904: INFO: (3) /api/v1/nodes/k8s-worker-1-dev/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.067855ms)
Jun 26 14:43:58.911: INFO: (4) /api/v1/nodes/k8s-worker-1-dev/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.662807ms)
Jun 26 14:43:58.917: INFO: (5) /api/v1/nodes/k8s-worker-1-dev/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.409852ms)
Jun 26 14:43:58.925: INFO: (6) /api/v1/nodes/k8s-worker-1-dev/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.483837ms)
Jun 26 14:43:58.932: INFO: (7) /api/v1/nodes/k8s-worker-1-dev/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.194175ms)
Jun 26 14:43:58.939: INFO: (8) /api/v1/nodes/k8s-worker-1-dev/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.577716ms)
Jun 26 14:43:58.947: INFO: (9) /api/v1/nodes/k8s-worker-1-dev/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.252294ms)
Jun 26 14:43:58.955: INFO: (10) /api/v1/nodes/k8s-worker-1-dev/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.183058ms)
Jun 26 14:43:58.962: INFO: (11) /api/v1/nodes/k8s-worker-1-dev/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.599307ms)
Jun 26 14:43:58.971: INFO: (12) /api/v1/nodes/k8s-worker-1-dev/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 9.356531ms)
Jun 26 14:43:58.979: INFO: (13) /api/v1/nodes/k8s-worker-1-dev/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.549649ms)
Jun 26 14:43:58.987: INFO: (14) /api/v1/nodes/k8s-worker-1-dev/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 7.986995ms)
Jun 26 14:43:58.993: INFO: (15) /api/v1/nodes/k8s-worker-1-dev/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 6.050256ms)
Jun 26 14:43:59.001: INFO: (16) /api/v1/nodes/k8s-worker-1-dev/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.075912ms)
Jun 26 14:43:59.010: INFO: (17) /api/v1/nodes/k8s-worker-1-dev/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.408954ms)
Jun 26 14:43:59.020: INFO: (18) /api/v1/nodes/k8s-worker-1-dev/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 9.662934ms)
Jun 26 14:43:59.029: INFO: (19) /api/v1/nodes/k8s-worker-1-dev/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 8.471181ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:43:59.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1560" for this suite.
Jun 26 14:44:05.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:44:05.285: INFO: namespace proxy-1560 deletion completed in 6.249297796s

• [SLOW TEST:6.498 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:44:05.287: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-rznk
STEP: Creating a pod to test atomic-volume-subpath
Jun 26 14:44:05.388: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-rznk" in namespace "subpath-5" to be "success or failure"
Jun 26 14:44:05.394: INFO: Pod "pod-subpath-test-downwardapi-rznk": Phase="Pending", Reason="", readiness=false. Elapsed: 5.487681ms
Jun 26 14:44:07.400: INFO: Pod "pod-subpath-test-downwardapi-rznk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012032638s
Jun 26 14:44:09.407: INFO: Pod "pod-subpath-test-downwardapi-rznk": Phase="Running", Reason="", readiness=true. Elapsed: 4.018281725s
Jun 26 14:44:11.411: INFO: Pod "pod-subpath-test-downwardapi-rznk": Phase="Running", Reason="", readiness=true. Elapsed: 6.022102033s
Jun 26 14:44:13.419: INFO: Pod "pod-subpath-test-downwardapi-rznk": Phase="Running", Reason="", readiness=true. Elapsed: 8.030515102s
Jun 26 14:44:15.423: INFO: Pod "pod-subpath-test-downwardapi-rznk": Phase="Running", Reason="", readiness=true. Elapsed: 10.034168327s
Jun 26 14:44:17.426: INFO: Pod "pod-subpath-test-downwardapi-rznk": Phase="Running", Reason="", readiness=true. Elapsed: 12.038012683s
Jun 26 14:44:19.433: INFO: Pod "pod-subpath-test-downwardapi-rznk": Phase="Running", Reason="", readiness=true. Elapsed: 14.044195812s
Jun 26 14:44:21.440: INFO: Pod "pod-subpath-test-downwardapi-rznk": Phase="Running", Reason="", readiness=true. Elapsed: 16.05163478s
Jun 26 14:44:23.447: INFO: Pod "pod-subpath-test-downwardapi-rznk": Phase="Running", Reason="", readiness=true. Elapsed: 18.058249726s
Jun 26 14:44:25.455: INFO: Pod "pod-subpath-test-downwardapi-rznk": Phase="Running", Reason="", readiness=true. Elapsed: 20.066657598s
Jun 26 14:44:27.464: INFO: Pod "pod-subpath-test-downwardapi-rznk": Phase="Running", Reason="", readiness=true. Elapsed: 22.075534479s
Jun 26 14:44:29.468: INFO: Pod "pod-subpath-test-downwardapi-rznk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.0795335s
STEP: Saw pod success
Jun 26 14:44:29.468: INFO: Pod "pod-subpath-test-downwardapi-rznk" satisfied condition "success or failure"
Jun 26 14:44:29.470: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-subpath-test-downwardapi-rznk container test-container-subpath-downwardapi-rznk: <nil>
STEP: delete the pod
Jun 26 14:44:29.487: INFO: Waiting for pod pod-subpath-test-downwardapi-rznk to disappear
Jun 26 14:44:29.489: INFO: Pod pod-subpath-test-downwardapi-rznk no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-rznk
Jun 26 14:44:29.490: INFO: Deleting pod "pod-subpath-test-downwardapi-rznk" in namespace "subpath-5"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:44:29.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5" for this suite.
Jun 26 14:44:35.510: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:44:35.729: INFO: namespace subpath-5 deletion completed in 6.234590302s

• [SLOW TEST:30.442 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:44:35.732: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 26 14:44:35.863: INFO: Create a RollingUpdate DaemonSet
Jun 26 14:44:35.873: INFO: Check that daemon pods launch on every node of the cluster
Jun 26 14:44:35.884: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:44:35.890: INFO: Number of nodes with available pods: 0
Jun 26 14:44:35.890: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 14:44:36.902: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:44:36.916: INFO: Number of nodes with available pods: 0
Jun 26 14:44:36.916: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 14:44:37.895: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:44:37.897: INFO: Number of nodes with available pods: 1
Jun 26 14:44:37.898: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 14:44:38.901: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:44:38.910: INFO: Number of nodes with available pods: 3
Jun 26 14:44:38.911: INFO: Number of running nodes: 3, number of available pods: 3
Jun 26 14:44:38.911: INFO: Update the DaemonSet to trigger a rollout
Jun 26 14:44:38.927: INFO: Updating DaemonSet daemon-set
Jun 26 14:44:42.962: INFO: Roll back the DaemonSet before rollout is complete
Jun 26 14:44:42.977: INFO: Updating DaemonSet daemon-set
Jun 26 14:44:42.977: INFO: Make sure DaemonSet rollback is complete
Jun 26 14:44:42.988: INFO: Wrong image for pod: daemon-set-rxbg8. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 26 14:44:42.988: INFO: Pod daemon-set-rxbg8 is not available
Jun 26 14:44:42.998: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:44:44.003: INFO: Wrong image for pod: daemon-set-rxbg8. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 26 14:44:44.003: INFO: Pod daemon-set-rxbg8 is not available
Jun 26 14:44:44.007: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:44:45.008: INFO: Pod daemon-set-jtqlf is not available
Jun 26 14:44:45.017: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2582, will wait for the garbage collector to delete the pods
Jun 26 14:44:45.107: INFO: Deleting DaemonSet.extensions daemon-set took: 18.007468ms
Jun 26 14:44:45.407: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.433969ms
Jun 26 14:46:02.417: INFO: Number of nodes with available pods: 0
Jun 26 14:46:02.418: INFO: Number of running nodes: 0, number of available pods: 0
Jun 26 14:46:02.431: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2582/daemonsets","resourceVersion":"18282"},"items":null}

Jun 26 14:46:02.437: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2582/pods","resourceVersion":"18282"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:46:02.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2582" for this suite.
Jun 26 14:46:08.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:46:08.764: INFO: namespace daemonsets-2582 deletion completed in 6.288572835s

• [SLOW TEST:93.032 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:46:08.773: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jun 26 14:46:08.850: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:46:13.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6026" for this suite.
Jun 26 14:46:19.793: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:46:19.880: INFO: namespace init-container-6026 deletion completed in 6.111510011s

• [SLOW TEST:11.107 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:46:19.881: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-6060
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6060 to expose endpoints map[]
Jun 26 14:46:19.929: INFO: Get endpoints failed (4.447556ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jun 26 14:46:20.936: INFO: successfully validated that service multi-endpoint-test in namespace services-6060 exposes endpoints map[] (1.011084111s elapsed)
STEP: Creating pod pod1 in namespace services-6060
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6060 to expose endpoints map[pod1:[100]]
Jun 26 14:46:22.992: INFO: successfully validated that service multi-endpoint-test in namespace services-6060 exposes endpoints map[pod1:[100]] (2.041501962s elapsed)
STEP: Creating pod pod2 in namespace services-6060
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6060 to expose endpoints map[pod1:[100] pod2:[101]]
Jun 26 14:46:25.037: INFO: successfully validated that service multi-endpoint-test in namespace services-6060 exposes endpoints map[pod1:[100] pod2:[101]] (2.033870864s elapsed)
STEP: Deleting pod pod1 in namespace services-6060
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6060 to expose endpoints map[pod2:[101]]
Jun 26 14:46:26.053: INFO: successfully validated that service multi-endpoint-test in namespace services-6060 exposes endpoints map[pod2:[101]] (1.011728559s elapsed)
STEP: Deleting pod pod2 in namespace services-6060
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6060 to expose endpoints map[]
Jun 26 14:46:27.065: INFO: successfully validated that service multi-endpoint-test in namespace services-6060 exposes endpoints map[] (1.00755895s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:46:27.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6060" for this suite.
Jun 26 14:46:33.113: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:46:33.312: INFO: namespace services-6060 deletion completed in 6.215651962s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:13.432 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:46:33.318: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:46:56.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-76" for this suite.
Jun 26 14:47:02.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:47:03.134: INFO: namespace container-runtime-76 deletion completed in 6.423011874s

• [SLOW TEST:29.816 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:47:03.137: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 26 14:47:03.199: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 version'
Jun 26 14:47:03.405: INFO: stderr: ""
Jun 26 14:47:03.405: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:40:16Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:32:14Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:47:03.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8792" for this suite.
Jun 26 14:47:09.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:47:09.519: INFO: namespace kubectl-8792 deletion completed in 6.098911607s

• [SLOW TEST:6.382 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:47:09.519: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-3772/secret-test-e0faf169-2c22-4d31-836f-a52b3a2753c0
STEP: Creating a pod to test consume secrets
Jun 26 14:47:09.553: INFO: Waiting up to 5m0s for pod "pod-configmaps-c0122ebf-45d7-4802-8093-d41c3716887b" in namespace "secrets-3772" to be "success or failure"
Jun 26 14:47:09.557: INFO: Pod "pod-configmaps-c0122ebf-45d7-4802-8093-d41c3716887b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.154059ms
Jun 26 14:47:11.566: INFO: Pod "pod-configmaps-c0122ebf-45d7-4802-8093-d41c3716887b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012125398s
STEP: Saw pod success
Jun 26 14:47:11.566: INFO: Pod "pod-configmaps-c0122ebf-45d7-4802-8093-d41c3716887b" satisfied condition "success or failure"
Jun 26 14:47:11.572: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-configmaps-c0122ebf-45d7-4802-8093-d41c3716887b container env-test: <nil>
STEP: delete the pod
Jun 26 14:47:11.612: INFO: Waiting for pod pod-configmaps-c0122ebf-45d7-4802-8093-d41c3716887b to disappear
Jun 26 14:47:11.617: INFO: Pod pod-configmaps-c0122ebf-45d7-4802-8093-d41c3716887b no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:47:11.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3772" for this suite.
Jun 26 14:47:17.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:47:17.852: INFO: namespace secrets-3772 deletion completed in 6.22327471s

• [SLOW TEST:8.333 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:47:17.862: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-9771301f-381c-43d3-87d7-542552bb8e61
STEP: Creating a pod to test consume secrets
Jun 26 14:47:17.934: INFO: Waiting up to 5m0s for pod "pod-secrets-05358fc8-8e00-4505-af38-0fa8855fa065" in namespace "secrets-4732" to be "success or failure"
Jun 26 14:47:17.939: INFO: Pod "pod-secrets-05358fc8-8e00-4505-af38-0fa8855fa065": Phase="Pending", Reason="", readiness=false. Elapsed: 4.455721ms
Jun 26 14:47:19.943: INFO: Pod "pod-secrets-05358fc8-8e00-4505-af38-0fa8855fa065": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008302192s
STEP: Saw pod success
Jun 26 14:47:19.943: INFO: Pod "pod-secrets-05358fc8-8e00-4505-af38-0fa8855fa065" satisfied condition "success or failure"
Jun 26 14:47:19.945: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-secrets-05358fc8-8e00-4505-af38-0fa8855fa065 container secret-volume-test: <nil>
STEP: delete the pod
Jun 26 14:47:19.967: INFO: Waiting for pod pod-secrets-05358fc8-8e00-4505-af38-0fa8855fa065 to disappear
Jun 26 14:47:19.971: INFO: Pod pod-secrets-05358fc8-8e00-4505-af38-0fa8855fa065 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:47:19.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4732" for this suite.
Jun 26 14:47:25.991: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:47:26.191: INFO: namespace secrets-4732 deletion completed in 6.21730748s

• [SLOW TEST:8.330 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:47:26.194: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 26 14:47:26.285: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e46e6aa3-9b74-44d5-814e-3d0ff535c202" in namespace "downward-api-811" to be "success or failure"
Jun 26 14:47:26.291: INFO: Pod "downwardapi-volume-e46e6aa3-9b74-44d5-814e-3d0ff535c202": Phase="Pending", Reason="", readiness=false. Elapsed: 5.78157ms
Jun 26 14:47:28.300: INFO: Pod "downwardapi-volume-e46e6aa3-9b74-44d5-814e-3d0ff535c202": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01448321s
Jun 26 14:47:30.304: INFO: Pod "downwardapi-volume-e46e6aa3-9b74-44d5-814e-3d0ff535c202": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018464099s
STEP: Saw pod success
Jun 26 14:47:30.304: INFO: Pod "downwardapi-volume-e46e6aa3-9b74-44d5-814e-3d0ff535c202" satisfied condition "success or failure"
Jun 26 14:47:30.306: INFO: Trying to get logs from node k8s-worker-1-dev pod downwardapi-volume-e46e6aa3-9b74-44d5-814e-3d0ff535c202 container client-container: <nil>
STEP: delete the pod
Jun 26 14:47:30.323: INFO: Waiting for pod downwardapi-volume-e46e6aa3-9b74-44d5-814e-3d0ff535c202 to disappear
Jun 26 14:47:30.325: INFO: Pod downwardapi-volume-e46e6aa3-9b74-44d5-814e-3d0ff535c202 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:47:30.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-811" for this suite.
Jun 26 14:47:36.340: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:47:36.537: INFO: namespace downward-api-811 deletion completed in 6.208391682s

• [SLOW TEST:10.343 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:47:36.537: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Jun 26 14:47:36.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 create -f - --namespace=kubectl-9326'
Jun 26 14:47:37.016: INFO: stderr: ""
Jun 26 14:47:37.016: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Jun 26 14:47:38.020: INFO: Selector matched 1 pods for map[app:redis]
Jun 26 14:47:38.020: INFO: Found 0 / 1
Jun 26 14:47:39.024: INFO: Selector matched 1 pods for map[app:redis]
Jun 26 14:47:39.024: INFO: Found 1 / 1
Jun 26 14:47:39.024: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 26 14:47:39.030: INFO: Selector matched 1 pods for map[app:redis]
Jun 26 14:47:39.030: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jun 26 14:47:39.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 logs redis-master-6bkjz redis-master --namespace=kubectl-9326'
Jun 26 14:47:39.281: INFO: stderr: ""
Jun 26 14:47:39.282: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 26 Jun 14:47:37.886 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 26 Jun 14:47:37.886 # Server started, Redis version 3.2.12\n1:M 26 Jun 14:47:37.886 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 26 Jun 14:47:37.886 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jun 26 14:47:39.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 log redis-master-6bkjz redis-master --namespace=kubectl-9326 --tail=1'
Jun 26 14:47:39.502: INFO: stderr: ""
Jun 26 14:47:39.503: INFO: stdout: "1:M 26 Jun 14:47:37.886 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jun 26 14:47:39.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 log redis-master-6bkjz redis-master --namespace=kubectl-9326 --limit-bytes=1'
Jun 26 14:47:39.703: INFO: stderr: ""
Jun 26 14:47:39.703: INFO: stdout: " "
STEP: exposing timestamps
Jun 26 14:47:39.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 log redis-master-6bkjz redis-master --namespace=kubectl-9326 --tail=1 --timestamps'
Jun 26 14:47:39.826: INFO: stderr: ""
Jun 26 14:47:39.826: INFO: stdout: "2019-06-26T16:47:37.886729929+02:00 1:M 26 Jun 14:47:37.886 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jun 26 14:47:42.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 log redis-master-6bkjz redis-master --namespace=kubectl-9326 --since=1s'
Jun 26 14:47:42.542: INFO: stderr: ""
Jun 26 14:47:42.542: INFO: stdout: ""
Jun 26 14:47:42.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 log redis-master-6bkjz redis-master --namespace=kubectl-9326 --since=24h'
Jun 26 14:47:42.789: INFO: stderr: ""
Jun 26 14:47:42.790: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 26 Jun 14:47:37.886 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 26 Jun 14:47:37.886 # Server started, Redis version 3.2.12\n1:M 26 Jun 14:47:37.886 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 26 Jun 14:47:37.886 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Jun 26 14:47:42.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 delete --grace-period=0 --force -f - --namespace=kubectl-9326'
Jun 26 14:47:42.981: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 26 14:47:42.981: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jun 26 14:47:42.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get rc,svc -l name=nginx --no-headers --namespace=kubectl-9326'
Jun 26 14:47:43.194: INFO: stderr: "No resources found.\n"
Jun 26 14:47:43.194: INFO: stdout: ""
Jun 26 14:47:43.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods -l name=nginx --namespace=kubectl-9326 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 26 14:47:43.399: INFO: stderr: ""
Jun 26 14:47:43.399: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:47:43.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9326" for this suite.
Jun 26 14:47:49.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:47:49.525: INFO: namespace kubectl-9326 deletion completed in 6.114610779s

• [SLOW TEST:12.988 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:47:49.526: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-05a8a967-ca72-4e21-9c95-b974d5b8bc38
Jun 26 14:47:49.565: INFO: Pod name my-hostname-basic-05a8a967-ca72-4e21-9c95-b974d5b8bc38: Found 0 pods out of 1
Jun 26 14:47:54.573: INFO: Pod name my-hostname-basic-05a8a967-ca72-4e21-9c95-b974d5b8bc38: Found 1 pods out of 1
Jun 26 14:47:54.573: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-05a8a967-ca72-4e21-9c95-b974d5b8bc38" are running
Jun 26 14:47:54.581: INFO: Pod "my-hostname-basic-05a8a967-ca72-4e21-9c95-b974d5b8bc38-k6ttd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-26 14:47:49 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-26 14:47:51 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-26 14:47:51 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-26 14:47:49 +0000 UTC Reason: Message:}])
Jun 26 14:47:54.581: INFO: Trying to dial the pod
Jun 26 14:47:59.595: INFO: Controller my-hostname-basic-05a8a967-ca72-4e21-9c95-b974d5b8bc38: Got expected result from replica 1 [my-hostname-basic-05a8a967-ca72-4e21-9c95-b974d5b8bc38-k6ttd]: "my-hostname-basic-05a8a967-ca72-4e21-9c95-b974d5b8bc38-k6ttd", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:47:59.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1583" for this suite.
Jun 26 14:48:05.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:48:05.787: INFO: namespace replication-controller-1583 deletion completed in 6.188245261s

• [SLOW TEST:16.261 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:48:05.789: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jun 26 14:48:05.819: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 26 14:48:05.827: INFO: Waiting for terminating namespaces to be deleted...
Jun 26 14:48:05.829: INFO: 
Logging pods the kubelet thinks is on node k8s-worker-1-dev before test
Jun 26 14:48:05.836: INFO: coredns-5c98db65d4-kvsnz from kube-system started at 2019-06-26 12:38:56 +0000 UTC (1 container statuses recorded)
Jun 26 14:48:05.836: INFO: 	Container coredns ready: true, restart count 0
Jun 26 14:48:05.836: INFO: nginx-7bb7cd8db5-kt54d from default started at 2019-06-26 12:40:54 +0000 UTC (1 container statuses recorded)
Jun 26 14:48:05.836: INFO: 	Container nginx ready: true, restart count 0
Jun 26 14:48:05.836: INFO: sonobuoy-systemd-logs-daemon-set-750e9b7608a045f9-dm828 from heptio-sonobuoy started at 2019-06-26 14:04:39 +0000 UTC (2 container statuses recorded)
Jun 26 14:48:05.836: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 26 14:48:05.836: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 26 14:48:05.836: INFO: kube-proxy-kt9gc from kube-system started at 2019-06-26 12:36:48 +0000 UTC (1 container statuses recorded)
Jun 26 14:48:05.836: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 26 14:48:05.836: INFO: nginx-proxy-k8s-worker-1-dev from kube-system started at 2019-06-26 12:36:55 +0000 UTC (1 container statuses recorded)
Jun 26 14:48:05.836: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 26 14:48:05.836: INFO: calico-node-qkw4j from kube-system started at 2019-06-26 12:38:32 +0000 UTC (1 container statuses recorded)
Jun 26 14:48:05.836: INFO: 	Container calico-node ready: true, restart count 0
Jun 26 14:48:05.836: INFO: 
Logging pods the kubelet thinks is on node k8s-worker-2-dev before test
Jun 26 14:48:05.843: INFO: kube-proxy-rgh7t from kube-system started at 2019-06-26 12:36:49 +0000 UTC (1 container statuses recorded)
Jun 26 14:48:05.843: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 26 14:48:05.843: INFO: calico-node-glqjd from kube-system started at 2019-06-26 12:38:32 +0000 UTC (1 container statuses recorded)
Jun 26 14:48:05.843: INFO: 	Container calico-node ready: true, restart count 0
Jun 26 14:48:05.843: INFO: calico-kube-controllers-6fb584dd97-wfj5r from kube-system started at 2019-06-26 12:38:56 +0000 UTC (1 container statuses recorded)
Jun 26 14:48:05.843: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 26 14:48:05.843: INFO: sonobuoy-systemd-logs-daemon-set-750e9b7608a045f9-tm998 from heptio-sonobuoy started at 2019-06-26 14:04:39 +0000 UTC (2 container statuses recorded)
Jun 26 14:48:05.843: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 26 14:48:05.843: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 26 14:48:05.843: INFO: nginx-proxy-k8s-worker-2-dev from kube-system started at 2019-06-26 12:37:22 +0000 UTC (1 container statuses recorded)
Jun 26 14:48:05.843: INFO: 	Container nginx-proxy ready: true, restart count 0
Jun 26 14:48:05.843: INFO: 
Logging pods the kubelet thinks is on node k8s-worker-3-dev before test
Jun 26 14:48:05.849: INFO: calico-node-c28zf from kube-system started at 2019-06-26 12:38:32 +0000 UTC (1 container statuses recorded)
Jun 26 14:48:05.849: INFO: 	Container calico-node ready: true, restart count 0
Jun 26 14:48:05.849: INFO: coredns-5c98db65d4-s8zsb from kube-system started at 2019-06-26 12:38:56 +0000 UTC (1 container statuses recorded)
Jun 26 14:48:05.849: INFO: 	Container coredns ready: true, restart count 0
Jun 26 14:48:05.849: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-26 14:04:36 +0000 UTC (1 container statuses recorded)
Jun 26 14:48:05.849: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 26 14:48:05.849: INFO: sonobuoy-e2e-job-75c21c1d55e6417f from heptio-sonobuoy started at 2019-06-26 14:04:39 +0000 UTC (2 container statuses recorded)
Jun 26 14:48:05.849: INFO: 	Container e2e ready: true, restart count 0
Jun 26 14:48:05.849: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 26 14:48:05.850: INFO: sonobuoy-systemd-logs-daemon-set-750e9b7608a045f9-kgkw4 from heptio-sonobuoy started at 2019-06-26 14:04:39 +0000 UTC (2 container statuses recorded)
Jun 26 14:48:05.850: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 26 14:48:05.850: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 26 14:48:05.850: INFO: kube-proxy-ngw7p from kube-system started at 2019-06-26 12:36:48 +0000 UTC (1 container statuses recorded)
Jun 26 14:48:05.850: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 26 14:48:05.850: INFO: nginx-proxy-k8s-worker-3-dev from kube-system started at 2019-06-26 12:36:56 +0000 UTC (1 container statuses recorded)
Jun 26 14:48:05.850: INFO: 	Container nginx-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15abc6fd07c98400], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:48:06.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4392" for this suite.
Jun 26 14:48:12.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:48:13.103: INFO: namespace sched-pred-4392 deletion completed in 6.219112545s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.315 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:48:13.108: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Jun 26 14:48:13.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 api-versions'
Jun 26 14:48:13.370: INFO: stderr: ""
Jun 26 14:48:13.370: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:48:13.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1401" for this suite.
Jun 26 14:48:19.404: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:48:19.489: INFO: namespace kubectl-1401 deletion completed in 6.108614327s

• [SLOW TEST:6.382 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:48:19.490: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 26 14:48:21.554: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:48:21.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1393" for this suite.
Jun 26 14:48:27.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:48:27.790: INFO: namespace container-runtime-1393 deletion completed in 6.191841801s

• [SLOW TEST:8.300 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:48:27.790: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 26 14:48:27.823: INFO: (0) /api/v1/nodes/k8s-worker-1-dev:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.857188ms)
Jun 26 14:48:27.826: INFO: (1) /api/v1/nodes/k8s-worker-1-dev:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.901546ms)
Jun 26 14:48:27.830: INFO: (2) /api/v1/nodes/k8s-worker-1-dev:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.507574ms)
Jun 26 14:48:27.833: INFO: (3) /api/v1/nodes/k8s-worker-1-dev:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.890184ms)
Jun 26 14:48:27.836: INFO: (4) /api/v1/nodes/k8s-worker-1-dev:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.9528ms)
Jun 26 14:48:27.839: INFO: (5) /api/v1/nodes/k8s-worker-1-dev:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.959367ms)
Jun 26 14:48:27.842: INFO: (6) /api/v1/nodes/k8s-worker-1-dev:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.972919ms)
Jun 26 14:48:27.845: INFO: (7) /api/v1/nodes/k8s-worker-1-dev:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.825254ms)
Jun 26 14:48:27.848: INFO: (8) /api/v1/nodes/k8s-worker-1-dev:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.990847ms)
Jun 26 14:48:27.851: INFO: (9) /api/v1/nodes/k8s-worker-1-dev:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.856422ms)
Jun 26 14:48:27.854: INFO: (10) /api/v1/nodes/k8s-worker-1-dev:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.677402ms)
Jun 26 14:48:27.858: INFO: (11) /api/v1/nodes/k8s-worker-1-dev:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.407191ms)
Jun 26 14:48:27.861: INFO: (12) /api/v1/nodes/k8s-worker-1-dev:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.591099ms)
Jun 26 14:48:27.866: INFO: (13) /api/v1/nodes/k8s-worker-1-dev:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 4.191668ms)
Jun 26 14:48:27.870: INFO: (14) /api/v1/nodes/k8s-worker-1-dev:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.994384ms)
Jun 26 14:48:27.873: INFO: (15) /api/v1/nodes/k8s-worker-1-dev:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.355848ms)
Jun 26 14:48:27.877: INFO: (16) /api/v1/nodes/k8s-worker-1-dev:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.532011ms)
Jun 26 14:48:27.880: INFO: (17) /api/v1/nodes/k8s-worker-1-dev:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.436603ms)
Jun 26 14:48:27.883: INFO: (18) /api/v1/nodes/k8s-worker-1-dev:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.850293ms)
Jun 26 14:48:27.887: INFO: (19) /api/v1/nodes/k8s-worker-1-dev:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.944299ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:48:27.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3441" for this suite.
Jun 26 14:48:33.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:48:33.985: INFO: namespace proxy-3441 deletion completed in 6.093372188s

• [SLOW TEST:6.195 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:48:33.985: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun 26 14:48:34.020: INFO: Waiting up to 5m0s for pod "pod-27ca5827-ea22-47b5-921d-ed91073f1967" in namespace "emptydir-1028" to be "success or failure"
Jun 26 14:48:34.023: INFO: Pod "pod-27ca5827-ea22-47b5-921d-ed91073f1967": Phase="Pending", Reason="", readiness=false. Elapsed: 3.017093ms
Jun 26 14:48:36.026: INFO: Pod "pod-27ca5827-ea22-47b5-921d-ed91073f1967": Phase="Running", Reason="", readiness=true. Elapsed: 2.006431623s
Jun 26 14:48:38.030: INFO: Pod "pod-27ca5827-ea22-47b5-921d-ed91073f1967": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009896815s
STEP: Saw pod success
Jun 26 14:48:38.030: INFO: Pod "pod-27ca5827-ea22-47b5-921d-ed91073f1967" satisfied condition "success or failure"
Jun 26 14:48:38.032: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-27ca5827-ea22-47b5-921d-ed91073f1967 container test-container: <nil>
STEP: delete the pod
Jun 26 14:48:38.056: INFO: Waiting for pod pod-27ca5827-ea22-47b5-921d-ed91073f1967 to disappear
Jun 26 14:48:38.058: INFO: Pod pod-27ca5827-ea22-47b5-921d-ed91073f1967 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:48:38.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1028" for this suite.
Jun 26 14:48:44.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:48:44.282: INFO: namespace emptydir-1028 deletion completed in 6.220387643s

• [SLOW TEST:10.297 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:48:44.284: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun 26 14:48:44.363: INFO: Pod name pod-release: Found 0 pods out of 1
Jun 26 14:48:49.367: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:48:49.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7828" for this suite.
Jun 26 14:48:55.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:48:55.627: INFO: namespace replication-controller-7828 deletion completed in 6.230675284s

• [SLOW TEST:11.343 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:48:55.633: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Jun 26 14:48:56.265: INFO: created pod pod-service-account-defaultsa
Jun 26 14:48:56.265: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun 26 14:48:56.280: INFO: created pod pod-service-account-mountsa
Jun 26 14:48:56.280: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun 26 14:48:56.297: INFO: created pod pod-service-account-nomountsa
Jun 26 14:48:56.297: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun 26 14:48:56.308: INFO: created pod pod-service-account-defaultsa-mountspec
Jun 26 14:48:56.308: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun 26 14:48:56.326: INFO: created pod pod-service-account-mountsa-mountspec
Jun 26 14:48:56.326: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun 26 14:48:56.341: INFO: created pod pod-service-account-nomountsa-mountspec
Jun 26 14:48:56.341: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun 26 14:48:56.381: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun 26 14:48:56.381: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun 26 14:48:56.404: INFO: created pod pod-service-account-mountsa-nomountspec
Jun 26 14:48:56.404: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun 26 14:48:56.428: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun 26 14:48:56.428: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:48:56.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6277" for this suite.
Jun 26 14:49:02.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:49:02.560: INFO: namespace svcaccounts-6277 deletion completed in 6.114895552s

• [SLOW TEST:6.928 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:49:02.561: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Jun 26 14:49:02.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 create -f - --namespace=kubectl-8476'
Jun 26 14:49:02.852: INFO: stderr: ""
Jun 26 14:49:02.852: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 26 14:49:02.852: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8476'
Jun 26 14:49:02.939: INFO: stderr: ""
Jun 26 14:49:02.939: INFO: stdout: "update-demo-nautilus-gfc2z update-demo-nautilus-sk4gn "
Jun 26 14:49:02.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-gfc2z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8476'
Jun 26 14:49:03.027: INFO: stderr: ""
Jun 26 14:49:03.027: INFO: stdout: ""
Jun 26 14:49:03.027: INFO: update-demo-nautilus-gfc2z is created but not running
Jun 26 14:49:08.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8476'
Jun 26 14:49:08.224: INFO: stderr: ""
Jun 26 14:49:08.224: INFO: stdout: "update-demo-nautilus-gfc2z update-demo-nautilus-sk4gn "
Jun 26 14:49:08.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-gfc2z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8476'
Jun 26 14:49:08.408: INFO: stderr: ""
Jun 26 14:49:08.408: INFO: stdout: "true"
Jun 26 14:49:08.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-gfc2z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8476'
Jun 26 14:49:08.611: INFO: stderr: ""
Jun 26 14:49:08.611: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 26 14:49:08.611: INFO: validating pod update-demo-nautilus-gfc2z
Jun 26 14:49:08.629: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 26 14:49:08.629: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 26 14:49:08.629: INFO: update-demo-nautilus-gfc2z is verified up and running
Jun 26 14:49:08.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-sk4gn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8476'
Jun 26 14:49:08.862: INFO: stderr: ""
Jun 26 14:49:08.862: INFO: stdout: "true"
Jun 26 14:49:08.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-sk4gn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8476'
Jun 26 14:49:09.092: INFO: stderr: ""
Jun 26 14:49:09.092: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 26 14:49:09.092: INFO: validating pod update-demo-nautilus-sk4gn
Jun 26 14:49:09.103: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 26 14:49:09.103: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 26 14:49:09.103: INFO: update-demo-nautilus-sk4gn is verified up and running
STEP: rolling-update to new replication controller
Jun 26 14:49:09.116: INFO: scanned /root for discovery docs: <nil>
Jun 26 14:49:09.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-8476'
Jun 26 14:49:31.771: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 26 14:49:31.771: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 26 14:49:31.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8476'
Jun 26 14:49:31.877: INFO: stderr: ""
Jun 26 14:49:31.877: INFO: stdout: "update-demo-kitten-dc9jm update-demo-kitten-vgg9x "
Jun 26 14:49:31.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-kitten-dc9jm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8476'
Jun 26 14:49:31.960: INFO: stderr: ""
Jun 26 14:49:31.960: INFO: stdout: "true"
Jun 26 14:49:31.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-kitten-dc9jm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8476'
Jun 26 14:49:32.048: INFO: stderr: ""
Jun 26 14:49:32.048: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 26 14:49:32.048: INFO: validating pod update-demo-kitten-dc9jm
Jun 26 14:49:32.052: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 26 14:49:32.052: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 26 14:49:32.052: INFO: update-demo-kitten-dc9jm is verified up and running
Jun 26 14:49:32.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-kitten-vgg9x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8476'
Jun 26 14:49:32.140: INFO: stderr: ""
Jun 26 14:49:32.140: INFO: stdout: "true"
Jun 26 14:49:32.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-kitten-vgg9x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8476'
Jun 26 14:49:32.227: INFO: stderr: ""
Jun 26 14:49:32.227: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 26 14:49:32.227: INFO: validating pod update-demo-kitten-vgg9x
Jun 26 14:49:32.231: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 26 14:49:32.231: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 26 14:49:32.231: INFO: update-demo-kitten-vgg9x is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:49:32.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8476" for this suite.
Jun 26 14:49:56.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:49:56.470: INFO: namespace kubectl-8476 deletion completed in 24.234566749s

• [SLOW TEST:53.909 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:49:56.473: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 26 14:49:56.584: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun 26 14:49:56.603: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:49:56.611: INFO: Number of nodes with available pods: 0
Jun 26 14:49:56.612: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 14:49:57.626: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:49:57.634: INFO: Number of nodes with available pods: 0
Jun 26 14:49:57.634: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 14:49:58.618: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:49:58.623: INFO: Number of nodes with available pods: 1
Jun 26 14:49:58.623: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 14:49:59.617: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:49:59.620: INFO: Number of nodes with available pods: 3
Jun 26 14:49:59.620: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun 26 14:49:59.647: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:49:59.647: INFO: Wrong image for pod: daemon-set-q22dk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:49:59.647: INFO: Wrong image for pod: daemon-set-vhw7w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:49:59.652: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:00.656: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:00.656: INFO: Wrong image for pod: daemon-set-q22dk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:00.656: INFO: Wrong image for pod: daemon-set-vhw7w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:00.659: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:01.676: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:01.676: INFO: Wrong image for pod: daemon-set-q22dk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:01.677: INFO: Wrong image for pod: daemon-set-vhw7w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:01.688: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:02.661: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:02.661: INFO: Wrong image for pod: daemon-set-q22dk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:02.662: INFO: Pod daemon-set-q22dk is not available
Jun 26 14:50:02.662: INFO: Wrong image for pod: daemon-set-vhw7w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:02.670: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:03.664: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:03.664: INFO: Wrong image for pod: daemon-set-q22dk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:03.664: INFO: Pod daemon-set-q22dk is not available
Jun 26 14:50:03.665: INFO: Wrong image for pod: daemon-set-vhw7w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:03.674: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:04.663: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:04.663: INFO: Wrong image for pod: daemon-set-q22dk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:04.663: INFO: Pod daemon-set-q22dk is not available
Jun 26 14:50:04.663: INFO: Wrong image for pod: daemon-set-vhw7w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:04.671: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:05.661: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:05.661: INFO: Wrong image for pod: daemon-set-q22dk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:05.661: INFO: Pod daemon-set-q22dk is not available
Jun 26 14:50:05.661: INFO: Wrong image for pod: daemon-set-vhw7w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:05.668: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:06.661: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:06.662: INFO: Wrong image for pod: daemon-set-q22dk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:06.662: INFO: Pod daemon-set-q22dk is not available
Jun 26 14:50:06.662: INFO: Wrong image for pod: daemon-set-vhw7w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:06.669: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:07.660: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:07.661: INFO: Wrong image for pod: daemon-set-q22dk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:07.661: INFO: Pod daemon-set-q22dk is not available
Jun 26 14:50:07.661: INFO: Wrong image for pod: daemon-set-vhw7w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:07.670: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:08.661: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:08.661: INFO: Wrong image for pod: daemon-set-q22dk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:08.661: INFO: Pod daemon-set-q22dk is not available
Jun 26 14:50:08.661: INFO: Wrong image for pod: daemon-set-vhw7w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:08.670: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:09.656: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:09.656: INFO: Wrong image for pod: daemon-set-q22dk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:09.656: INFO: Pod daemon-set-q22dk is not available
Jun 26 14:50:09.656: INFO: Wrong image for pod: daemon-set-vhw7w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:09.660: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:10.656: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:10.656: INFO: Wrong image for pod: daemon-set-q22dk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:10.656: INFO: Pod daemon-set-q22dk is not available
Jun 26 14:50:10.656: INFO: Wrong image for pod: daemon-set-vhw7w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:10.660: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:11.660: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:11.660: INFO: Wrong image for pod: daemon-set-q22dk. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:11.660: INFO: Pod daemon-set-q22dk is not available
Jun 26 14:50:11.660: INFO: Wrong image for pod: daemon-set-vhw7w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:11.669: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:12.662: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:12.662: INFO: Wrong image for pod: daemon-set-vhw7w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:12.662: INFO: Pod daemon-set-zlw7t is not available
Jun 26 14:50:12.671: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:13.656: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:13.656: INFO: Wrong image for pod: daemon-set-vhw7w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:13.656: INFO: Pod daemon-set-zlw7t is not available
Jun 26 14:50:13.659: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:14.656: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:14.656: INFO: Wrong image for pod: daemon-set-vhw7w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:14.656: INFO: Pod daemon-set-zlw7t is not available
Jun 26 14:50:14.659: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:15.656: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:15.656: INFO: Wrong image for pod: daemon-set-vhw7w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:15.660: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:16.655: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:16.656: INFO: Wrong image for pod: daemon-set-vhw7w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:16.656: INFO: Pod daemon-set-vhw7w is not available
Jun 26 14:50:16.659: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:17.661: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:17.662: INFO: Wrong image for pod: daemon-set-vhw7w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:17.662: INFO: Pod daemon-set-vhw7w is not available
Jun 26 14:50:17.669: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:18.661: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:18.661: INFO: Wrong image for pod: daemon-set-vhw7w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:18.661: INFO: Pod daemon-set-vhw7w is not available
Jun 26 14:50:18.670: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:19.660: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:19.661: INFO: Wrong image for pod: daemon-set-vhw7w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:19.661: INFO: Pod daemon-set-vhw7w is not available
Jun 26 14:50:19.669: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:20.658: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:20.658: INFO: Wrong image for pod: daemon-set-vhw7w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:20.658: INFO: Pod daemon-set-vhw7w is not available
Jun 26 14:50:20.664: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:21.663: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:21.663: INFO: Wrong image for pod: daemon-set-vhw7w. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:21.663: INFO: Pod daemon-set-vhw7w is not available
Jun 26 14:50:21.674: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:22.661: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:22.661: INFO: Pod daemon-set-qdbm6 is not available
Jun 26 14:50:22.670: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:23.662: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:23.671: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:24.661: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:24.670: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:25.662: INFO: Wrong image for pod: daemon-set-9lsqx. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 26 14:50:25.662: INFO: Pod daemon-set-9lsqx is not available
Jun 26 14:50:25.671: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:26.664: INFO: Pod daemon-set-bj4mm is not available
Jun 26 14:50:26.672: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jun 26 14:50:26.681: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:26.689: INFO: Number of nodes with available pods: 2
Jun 26 14:50:26.689: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 14:50:27.696: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:27.703: INFO: Number of nodes with available pods: 2
Jun 26 14:50:27.703: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 14:50:28.702: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 14:50:28.710: INFO: Number of nodes with available pods: 3
Jun 26 14:50:28.711: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4363, will wait for the garbage collector to delete the pods
Jun 26 14:50:28.817: INFO: Deleting DaemonSet.extensions daemon-set took: 12.482ms
Jun 26 14:50:29.118: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.316766ms
Jun 26 14:50:42.225: INFO: Number of nodes with available pods: 0
Jun 26 14:50:42.225: INFO: Number of running nodes: 0, number of available pods: 0
Jun 26 14:50:42.232: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4363/daemonsets","resourceVersion":"19735"},"items":null}

Jun 26 14:50:42.238: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4363/pods","resourceVersion":"19735"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:50:42.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4363" for this suite.
Jun 26 14:50:48.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:50:48.507: INFO: namespace daemonsets-4363 deletion completed in 6.242021225s

• [SLOW TEST:52.035 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:50:48.509: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3929
I0626 14:50:48.591855      18 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3929, replica count: 1
I0626 14:50:49.643987      18 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0626 14:50:50.644345      18 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0626 14:50:51.644807      18 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 26 14:50:51.761: INFO: Created: latency-svc-hfvhd
Jun 26 14:50:51.775: INFO: Got endpoints: latency-svc-hfvhd [29.933293ms]
Jun 26 14:50:51.843: INFO: Created: latency-svc-dm2tx
Jun 26 14:50:51.850: INFO: Got endpoints: latency-svc-dm2tx [73.898324ms]
Jun 26 14:50:51.876: INFO: Created: latency-svc-r9gnj
Jun 26 14:50:51.884: INFO: Got endpoints: latency-svc-r9gnj [107.337667ms]
Jun 26 14:50:51.921: INFO: Created: latency-svc-t4j6g
Jun 26 14:50:51.928: INFO: Got endpoints: latency-svc-t4j6g [149.438677ms]
Jun 26 14:50:51.944: INFO: Created: latency-svc-fkjth
Jun 26 14:50:51.950: INFO: Got endpoints: latency-svc-fkjth [173.425407ms]
Jun 26 14:50:51.974: INFO: Created: latency-svc-f6p57
Jun 26 14:50:51.980: INFO: Got endpoints: latency-svc-f6p57 [202.513543ms]
Jun 26 14:50:52.006: INFO: Created: latency-svc-xvzzk
Jun 26 14:50:52.013: INFO: Got endpoints: latency-svc-xvzzk [236.035766ms]
Jun 26 14:50:52.055: INFO: Created: latency-svc-448jd
Jun 26 14:50:52.060: INFO: Got endpoints: latency-svc-448jd [282.631749ms]
Jun 26 14:50:52.088: INFO: Created: latency-svc-vtbbf
Jun 26 14:50:52.093: INFO: Got endpoints: latency-svc-vtbbf [316.177771ms]
Jun 26 14:50:52.126: INFO: Created: latency-svc-pwd8w
Jun 26 14:50:52.140: INFO: Got endpoints: latency-svc-pwd8w [361.729842ms]
Jun 26 14:50:52.165: INFO: Created: latency-svc-96v6b
Jun 26 14:50:52.168: INFO: Got endpoints: latency-svc-96v6b [388.89588ms]
Jun 26 14:50:52.212: INFO: Created: latency-svc-gs985
Jun 26 14:50:52.224: INFO: Got endpoints: latency-svc-gs985 [445.976935ms]
Jun 26 14:50:52.231: INFO: Created: latency-svc-2dls8
Jun 26 14:50:52.235: INFO: Got endpoints: latency-svc-2dls8 [456.675182ms]
Jun 26 14:50:52.258: INFO: Created: latency-svc-zn54c
Jun 26 14:50:52.264: INFO: Got endpoints: latency-svc-zn54c [486.93251ms]
Jun 26 14:50:52.287: INFO: Created: latency-svc-cwkjs
Jun 26 14:50:52.294: INFO: Got endpoints: latency-svc-cwkjs [515.164443ms]
Jun 26 14:50:52.331: INFO: Created: latency-svc-w7xx2
Jun 26 14:50:52.335: INFO: Got endpoints: latency-svc-w7xx2 [559.215245ms]
Jun 26 14:50:52.350: INFO: Created: latency-svc-lkszd
Jun 26 14:50:52.364: INFO: Got endpoints: latency-svc-lkszd [513.78767ms]
Jun 26 14:50:52.377: INFO: Created: latency-svc-gb4zk
Jun 26 14:50:52.386: INFO: Got endpoints: latency-svc-gb4zk [501.910156ms]
Jun 26 14:50:52.405: INFO: Created: latency-svc-fn9xv
Jun 26 14:50:52.409: INFO: Got endpoints: latency-svc-fn9xv [481.102936ms]
Jun 26 14:50:52.419: INFO: Created: latency-svc-5kr6h
Jun 26 14:50:52.426: INFO: Got endpoints: latency-svc-5kr6h [475.265128ms]
Jun 26 14:50:52.443: INFO: Created: latency-svc-gd8nr
Jun 26 14:50:52.477: INFO: Got endpoints: latency-svc-gd8nr [497.072007ms]
Jun 26 14:50:52.491: INFO: Created: latency-svc-n2x4s
Jun 26 14:50:52.502: INFO: Got endpoints: latency-svc-n2x4s [488.714862ms]
Jun 26 14:50:52.519: INFO: Created: latency-svc-ztkd9
Jun 26 14:50:52.526: INFO: Got endpoints: latency-svc-ztkd9 [465.96649ms]
Jun 26 14:50:52.565: INFO: Created: latency-svc-zkvlx
Jun 26 14:50:52.566: INFO: Got endpoints: latency-svc-zkvlx [472.355752ms]
Jun 26 14:50:52.592: INFO: Created: latency-svc-pjfpn
Jun 26 14:50:52.602: INFO: Got endpoints: latency-svc-pjfpn [462.009425ms]
Jun 26 14:50:52.631: INFO: Created: latency-svc-srgcv
Jun 26 14:50:52.632: INFO: Got endpoints: latency-svc-srgcv [463.74831ms]
Jun 26 14:50:52.649: INFO: Created: latency-svc-p7wh9
Jun 26 14:50:52.654: INFO: Got endpoints: latency-svc-p7wh9 [430.00693ms]
Jun 26 14:50:52.684: INFO: Created: latency-svc-qkrjt
Jun 26 14:50:52.687: INFO: Got endpoints: latency-svc-qkrjt [451.688721ms]
Jun 26 14:50:52.742: INFO: Created: latency-svc-2fv6w
Jun 26 14:50:52.783: INFO: Got endpoints: latency-svc-2fv6w [517.992766ms]
Jun 26 14:50:52.821: INFO: Created: latency-svc-7jw8d
Jun 26 14:50:52.862: INFO: Got endpoints: latency-svc-7jw8d [568.38491ms]
Jun 26 14:50:52.919: INFO: Created: latency-svc-956s9
Jun 26 14:50:52.932: INFO: Got endpoints: latency-svc-956s9 [597.096749ms]
Jun 26 14:50:52.953: INFO: Created: latency-svc-96rbm
Jun 26 14:50:52.968: INFO: Got endpoints: latency-svc-96rbm [602.785056ms]
Jun 26 14:50:52.977: INFO: Created: latency-svc-ldzzv
Jun 26 14:50:52.980: INFO: Got endpoints: latency-svc-ldzzv [593.068948ms]
Jun 26 14:50:52.994: INFO: Created: latency-svc-swxsm
Jun 26 14:50:53.004: INFO: Got endpoints: latency-svc-swxsm [594.759877ms]
Jun 26 14:50:53.020: INFO: Created: latency-svc-5hnzl
Jun 26 14:50:53.024: INFO: Got endpoints: latency-svc-5hnzl [598.50675ms]
Jun 26 14:50:53.049: INFO: Created: latency-svc-ct4lh
Jun 26 14:50:53.053: INFO: Got endpoints: latency-svc-ct4lh [575.717498ms]
Jun 26 14:50:53.089: INFO: Created: latency-svc-2r2ht
Jun 26 14:50:53.101: INFO: Got endpoints: latency-svc-2r2ht [599.505934ms]
Jun 26 14:50:53.115: INFO: Created: latency-svc-2t48z
Jun 26 14:50:53.119: INFO: Got endpoints: latency-svc-2t48z [591.795969ms]
Jun 26 14:50:53.138: INFO: Created: latency-svc-d6jmm
Jun 26 14:50:53.144: INFO: Got endpoints: latency-svc-d6jmm [577.190245ms]
Jun 26 14:50:53.177: INFO: Created: latency-svc-crlhn
Jun 26 14:50:53.178: INFO: Got endpoints: latency-svc-crlhn [575.942644ms]
Jun 26 14:50:53.198: INFO: Created: latency-svc-p2496
Jun 26 14:50:53.204: INFO: Got endpoints: latency-svc-p2496 [571.09701ms]
Jun 26 14:50:53.228: INFO: Created: latency-svc-jtd78
Jun 26 14:50:53.230: INFO: Got endpoints: latency-svc-jtd78 [575.162763ms]
Jun 26 14:50:53.260: INFO: Created: latency-svc-lqz9c
Jun 26 14:50:53.272: INFO: Got endpoints: latency-svc-lqz9c [585.35919ms]
Jun 26 14:50:53.273: INFO: Created: latency-svc-2nhc7
Jun 26 14:50:53.282: INFO: Got endpoints: latency-svc-2nhc7 [498.334342ms]
Jun 26 14:50:53.289: INFO: Created: latency-svc-x5z2n
Jun 26 14:50:53.298: INFO: Got endpoints: latency-svc-x5z2n [434.923781ms]
Jun 26 14:50:53.315: INFO: Created: latency-svc-zh8xm
Jun 26 14:50:53.317: INFO: Got endpoints: latency-svc-zh8xm [383.407725ms]
Jun 26 14:50:53.324: INFO: Created: latency-svc-kzlz7
Jun 26 14:50:53.332: INFO: Got endpoints: latency-svc-kzlz7 [364.450216ms]
Jun 26 14:50:53.346: INFO: Created: latency-svc-kfqwn
Jun 26 14:50:53.353: INFO: Got endpoints: latency-svc-kfqwn [372.254563ms]
Jun 26 14:50:53.391: INFO: Created: latency-svc-k2j58
Jun 26 14:50:53.401: INFO: Got endpoints: latency-svc-k2j58 [396.954623ms]
Jun 26 14:50:53.415: INFO: Created: latency-svc-4j6x5
Jun 26 14:50:53.421: INFO: Got endpoints: latency-svc-4j6x5 [396.018042ms]
Jun 26 14:50:53.443: INFO: Created: latency-svc-fw8l2
Jun 26 14:50:53.449: INFO: Got endpoints: latency-svc-fw8l2 [395.096534ms]
Jun 26 14:50:53.514: INFO: Created: latency-svc-nqprx
Jun 26 14:50:53.522: INFO: Got endpoints: latency-svc-nqprx [420.078666ms]
Jun 26 14:50:53.535: INFO: Created: latency-svc-5wcfx
Jun 26 14:50:53.547: INFO: Got endpoints: latency-svc-5wcfx [427.47237ms]
Jun 26 14:50:53.558: INFO: Created: latency-svc-g7xh9
Jun 26 14:50:53.569: INFO: Got endpoints: latency-svc-g7xh9 [425.003379ms]
Jun 26 14:50:53.580: INFO: Created: latency-svc-bnt96
Jun 26 14:50:53.588: INFO: Got endpoints: latency-svc-bnt96 [408.873668ms]
Jun 26 14:50:53.608: INFO: Created: latency-svc-hwfls
Jun 26 14:50:53.623: INFO: Got endpoints: latency-svc-hwfls [418.061569ms]
Jun 26 14:50:53.645: INFO: Created: latency-svc-zr88f
Jun 26 14:50:53.645: INFO: Got endpoints: latency-svc-zr88f [414.999463ms]
Jun 26 14:50:53.658: INFO: Created: latency-svc-cl4jw
Jun 26 14:50:53.666: INFO: Got endpoints: latency-svc-cl4jw [393.187642ms]
Jun 26 14:50:53.686: INFO: Created: latency-svc-j4z6g
Jun 26 14:50:53.694: INFO: Got endpoints: latency-svc-j4z6g [411.460022ms]
Jun 26 14:50:53.710: INFO: Created: latency-svc-4nnsp
Jun 26 14:50:53.710: INFO: Got endpoints: latency-svc-4nnsp [412.29494ms]
Jun 26 14:50:53.721: INFO: Created: latency-svc-tm5c9
Jun 26 14:50:53.726: INFO: Got endpoints: latency-svc-tm5c9 [408.955566ms]
Jun 26 14:50:53.742: INFO: Created: latency-svc-zjg57
Jun 26 14:50:53.749: INFO: Got endpoints: latency-svc-zjg57 [416.048704ms]
Jun 26 14:50:53.763: INFO: Created: latency-svc-sx5k6
Jun 26 14:50:53.770: INFO: Got endpoints: latency-svc-sx5k6 [416.681904ms]
Jun 26 14:50:53.797: INFO: Created: latency-svc-zr7p7
Jun 26 14:50:53.806: INFO: Got endpoints: latency-svc-zr7p7 [403.795919ms]
Jun 26 14:50:53.836: INFO: Created: latency-svc-dfhv7
Jun 26 14:50:53.860: INFO: Got endpoints: latency-svc-dfhv7 [439.047583ms]
Jun 26 14:50:53.872: INFO: Created: latency-svc-5wnsd
Jun 26 14:50:53.879: INFO: Got endpoints: latency-svc-5wnsd [430.157166ms]
Jun 26 14:50:53.949: INFO: Created: latency-svc-p2xtd
Jun 26 14:50:53.972: INFO: Got endpoints: latency-svc-p2xtd [450.298853ms]
Jun 26 14:50:54.016: INFO: Created: latency-svc-r6gjq
Jun 26 14:50:54.016: INFO: Got endpoints: latency-svc-r6gjq [469.271784ms]
Jun 26 14:50:54.023: INFO: Created: latency-svc-kn4c8
Jun 26 14:50:54.031: INFO: Got endpoints: latency-svc-kn4c8 [58.030662ms]
Jun 26 14:50:54.050: INFO: Created: latency-svc-q7twx
Jun 26 14:50:54.100: INFO: Got endpoints: latency-svc-q7twx [530.862312ms]
Jun 26 14:50:54.300: INFO: Created: latency-svc-w6lrb
Jun 26 14:50:54.309: INFO: Got endpoints: latency-svc-w6lrb [720.740921ms]
Jun 26 14:50:54.331: INFO: Created: latency-svc-6blql
Jun 26 14:50:54.342: INFO: Got endpoints: latency-svc-6blql [718.900313ms]
Jun 26 14:50:54.354: INFO: Created: latency-svc-m25d4
Jun 26 14:50:54.370: INFO: Got endpoints: latency-svc-m25d4 [724.314151ms]
Jun 26 14:50:54.386: INFO: Created: latency-svc-wzdk4
Jun 26 14:50:54.400: INFO: Got endpoints: latency-svc-wzdk4 [732.993503ms]
Jun 26 14:50:54.412: INFO: Created: latency-svc-kp9wk
Jun 26 14:50:54.421: INFO: Got endpoints: latency-svc-kp9wk [727.079647ms]
Jun 26 14:50:54.433: INFO: Created: latency-svc-spscl
Jun 26 14:50:54.441: INFO: Got endpoints: latency-svc-spscl [731.028309ms]
Jun 26 14:50:54.469: INFO: Created: latency-svc-rpbjq
Jun 26 14:50:54.479: INFO: Got endpoints: latency-svc-rpbjq [753.20394ms]
Jun 26 14:50:54.490: INFO: Created: latency-svc-nsck2
Jun 26 14:50:54.496: INFO: Got endpoints: latency-svc-nsck2 [747.111858ms]
Jun 26 14:50:54.523: INFO: Created: latency-svc-kmqfh
Jun 26 14:50:54.530: INFO: Got endpoints: latency-svc-kmqfh [760.167502ms]
Jun 26 14:50:54.550: INFO: Created: latency-svc-7vdd7
Jun 26 14:50:54.553: INFO: Got endpoints: latency-svc-7vdd7 [747.434388ms]
Jun 26 14:50:54.573: INFO: Created: latency-svc-78rfd
Jun 26 14:50:54.577: INFO: Got endpoints: latency-svc-78rfd [716.19048ms]
Jun 26 14:50:54.594: INFO: Created: latency-svc-bwf92
Jun 26 14:50:54.606: INFO: Got endpoints: latency-svc-bwf92 [726.857238ms]
Jun 26 14:50:54.634: INFO: Created: latency-svc-f4vj7
Jun 26 14:50:54.644: INFO: Got endpoints: latency-svc-f4vj7 [628.186997ms]
Jun 26 14:50:54.666: INFO: Created: latency-svc-xz4sq
Jun 26 14:50:54.683: INFO: Got endpoints: latency-svc-xz4sq [652.349533ms]
Jun 26 14:50:54.704: INFO: Created: latency-svc-tlvp9
Jun 26 14:50:54.710: INFO: Got endpoints: latency-svc-tlvp9 [368.316836ms]
Jun 26 14:50:54.736: INFO: Created: latency-svc-z7mgg
Jun 26 14:50:54.788: INFO: Got endpoints: latency-svc-z7mgg [687.663144ms]
Jun 26 14:50:54.803: INFO: Created: latency-svc-zb27v
Jun 26 14:50:54.813: INFO: Got endpoints: latency-svc-zb27v [503.61178ms]
Jun 26 14:50:54.835: INFO: Created: latency-svc-d5c82
Jun 26 14:50:54.839: INFO: Got endpoints: latency-svc-d5c82 [469.203114ms]
Jun 26 14:50:54.857: INFO: Created: latency-svc-r974d
Jun 26 14:50:54.863: INFO: Got endpoints: latency-svc-r974d [462.109549ms]
Jun 26 14:50:54.884: INFO: Created: latency-svc-xmdgq
Jun 26 14:50:54.894: INFO: Got endpoints: latency-svc-xmdgq [472.243776ms]
Jun 26 14:50:54.916: INFO: Created: latency-svc-lnf4b
Jun 26 14:50:54.920: INFO: Got endpoints: latency-svc-lnf4b [478.626807ms]
Jun 26 14:50:54.962: INFO: Created: latency-svc-h2wr4
Jun 26 14:50:54.979: INFO: Got endpoints: latency-svc-h2wr4 [500.054442ms]
Jun 26 14:50:55.019: INFO: Created: latency-svc-27n4f
Jun 26 14:50:55.021: INFO: Got endpoints: latency-svc-27n4f [525.193491ms]
Jun 26 14:50:55.058: INFO: Created: latency-svc-p6pfs
Jun 26 14:50:55.075: INFO: Got endpoints: latency-svc-p6pfs [545.471394ms]
Jun 26 14:50:55.094: INFO: Created: latency-svc-rhz4r
Jun 26 14:50:55.094: INFO: Got endpoints: latency-svc-rhz4r [541.055424ms]
Jun 26 14:50:55.113: INFO: Created: latency-svc-ds55z
Jun 26 14:50:55.125: INFO: Got endpoints: latency-svc-ds55z [547.635262ms]
Jun 26 14:50:55.142: INFO: Created: latency-svc-jfbrv
Jun 26 14:50:55.154: INFO: Got endpoints: latency-svc-jfbrv [547.088731ms]
Jun 26 14:50:55.169: INFO: Created: latency-svc-mlh5r
Jun 26 14:50:55.200: INFO: Created: latency-svc-5ps9r
Jun 26 14:50:55.203: INFO: Got endpoints: latency-svc-mlh5r [558.28581ms]
Jun 26 14:50:55.241: INFO: Created: latency-svc-vsf5r
Jun 26 14:50:55.252: INFO: Got endpoints: latency-svc-5ps9r [568.453934ms]
Jun 26 14:50:55.272: INFO: Created: latency-svc-brgzd
Jun 26 14:50:55.316: INFO: Got endpoints: latency-svc-vsf5r [605.059035ms]
Jun 26 14:50:55.326: INFO: Created: latency-svc-5x2qw
Jun 26 14:50:55.349: INFO: Created: latency-svc-8td2w
Jun 26 14:50:55.356: INFO: Got endpoints: latency-svc-brgzd [568.216432ms]
Jun 26 14:50:55.371: INFO: Created: latency-svc-bzz8k
Jun 26 14:50:55.404: INFO: Got endpoints: latency-svc-5x2qw [590.266621ms]
Jun 26 14:50:55.408: INFO: Created: latency-svc-wtrps
Jun 26 14:50:55.437: INFO: Created: latency-svc-g9895
Jun 26 14:50:55.456: INFO: Got endpoints: latency-svc-8td2w [616.475346ms]
Jun 26 14:50:55.459: INFO: Created: latency-svc-m9gv5
Jun 26 14:50:55.481: INFO: Created: latency-svc-5zqjv
Jun 26 14:50:55.501: INFO: Got endpoints: latency-svc-bzz8k [637.82079ms]
Jun 26 14:50:55.515: INFO: Created: latency-svc-9l5jb
Jun 26 14:50:55.543: INFO: Created: latency-svc-fl64k
Jun 26 14:50:55.613: INFO: Got endpoints: latency-svc-g9895 [693.107845ms]
Jun 26 14:50:55.614: INFO: Got endpoints: latency-svc-wtrps [719.566324ms]
Jun 26 14:50:55.709: INFO: Got endpoints: latency-svc-m9gv5 [729.586782ms]
Jun 26 14:50:55.715: INFO: Got endpoints: latency-svc-5zqjv [693.372462ms]
Jun 26 14:50:55.722: INFO: Created: latency-svc-pc6lr
Jun 26 14:50:55.747: INFO: Created: latency-svc-2dx5m
Jun 26 14:50:55.750: INFO: Got endpoints: latency-svc-9l5jb [674.690085ms]
Jun 26 14:50:55.768: INFO: Created: latency-svc-7h24f
Jun 26 14:50:55.787: INFO: Created: latency-svc-kgltk
Jun 26 14:50:55.804: INFO: Got endpoints: latency-svc-fl64k [709.736025ms]
Jun 26 14:50:55.812: INFO: Created: latency-svc-z9cpc
Jun 26 14:50:55.836: INFO: Created: latency-svc-9k9f5
Jun 26 14:50:55.854: INFO: Got endpoints: latency-svc-pc6lr [728.980226ms]
Jun 26 14:50:55.873: INFO: Created: latency-svc-dskj6
Jun 26 14:50:55.909: INFO: Got endpoints: latency-svc-2dx5m [753.966348ms]
Jun 26 14:50:55.926: INFO: Created: latency-svc-j9mww
Jun 26 14:50:55.954: INFO: Got endpoints: latency-svc-7h24f [750.717344ms]
Jun 26 14:50:56.161: INFO: Got endpoints: latency-svc-kgltk [909.118264ms]
Jun 26 14:50:56.166: INFO: Got endpoints: latency-svc-dskj6 [762.313047ms]
Jun 26 14:50:56.172: INFO: Got endpoints: latency-svc-9k9f5 [815.604102ms]
Jun 26 14:50:56.173: INFO: Got endpoints: latency-svc-z9cpc [856.267538ms]
Jun 26 14:50:56.184: INFO: Created: latency-svc-7qtlz
Jun 26 14:50:56.204: INFO: Got endpoints: latency-svc-j9mww [747.107922ms]
Jun 26 14:50:56.214: INFO: Created: latency-svc-2f9xd
Jun 26 14:50:56.232: INFO: Created: latency-svc-q6tlm
Jun 26 14:50:56.252: INFO: Got endpoints: latency-svc-7qtlz [750.726547ms]
Jun 26 14:50:56.272: INFO: Created: latency-svc-hmvhl
Jun 26 14:50:56.301: INFO: Created: latency-svc-5sb2v
Jun 26 14:50:56.305: INFO: Got endpoints: latency-svc-2f9xd [691.006071ms]
Jun 26 14:50:56.325: INFO: Created: latency-svc-rqlgk
Jun 26 14:50:56.344: INFO: Created: latency-svc-mrzhn
Jun 26 14:50:56.348: INFO: Got endpoints: latency-svc-q6tlm [733.704713ms]
Jun 26 14:50:56.375: INFO: Created: latency-svc-tqwz6
Jun 26 14:50:56.400: INFO: Got endpoints: latency-svc-hmvhl [690.614861ms]
Jun 26 14:50:56.408: INFO: Created: latency-svc-d8kfw
Jun 26 14:50:56.422: INFO: Created: latency-svc-8657w
Jun 26 14:50:56.459: INFO: Got endpoints: latency-svc-5sb2v [743.752938ms]
Jun 26 14:50:56.463: INFO: Created: latency-svc-z6dfn
Jun 26 14:50:56.483: INFO: Created: latency-svc-2rjcq
Jun 26 14:50:56.506: INFO: Got endpoints: latency-svc-rqlgk [755.413253ms]
Jun 26 14:50:56.520: INFO: Created: latency-svc-lmcmm
Jun 26 14:50:56.535: INFO: Created: latency-svc-v2jjt
Jun 26 14:50:56.549: INFO: Got endpoints: latency-svc-mrzhn [745.1315ms]
Jun 26 14:50:56.557: INFO: Created: latency-svc-5hsvg
Jun 26 14:50:56.581: INFO: Created: latency-svc-qgnvd
Jun 26 14:50:56.600: INFO: Got endpoints: latency-svc-tqwz6 [745.386747ms]
Jun 26 14:50:56.631: INFO: Created: latency-svc-xlgg2
Jun 26 14:50:56.646: INFO: Created: latency-svc-wjxwh
Jun 26 14:50:56.658: INFO: Got endpoints: latency-svc-d8kfw [749.392128ms]
Jun 26 14:50:56.678: INFO: Created: latency-svc-qbdjx
Jun 26 14:50:56.703: INFO: Got endpoints: latency-svc-8657w [748.938175ms]
Jun 26 14:50:56.705: INFO: Created: latency-svc-lhxpm
Jun 26 14:50:56.751: INFO: Got endpoints: latency-svc-z6dfn [589.590113ms]
Jun 26 14:50:56.762: INFO: Created: latency-svc-9kpq6
Jun 26 14:50:56.769: INFO: Created: latency-svc-67pl5
Jun 26 14:50:56.785: INFO: Created: latency-svc-tt22x
Jun 26 14:50:56.790: INFO: Created: latency-svc-p4wcz
Jun 26 14:50:56.798: INFO: Got endpoints: latency-svc-2rjcq [631.150009ms]
Jun 26 14:50:56.799: INFO: Created: latency-svc-t7m4h
Jun 26 14:50:56.808: INFO: Created: latency-svc-xmxjw
Jun 26 14:50:56.818: INFO: Created: latency-svc-q76nd
Jun 26 14:50:56.854: INFO: Got endpoints: latency-svc-lmcmm [681.056644ms]
Jun 26 14:50:56.865: INFO: Created: latency-svc-jclns
Jun 26 14:50:56.896: INFO: Got endpoints: latency-svc-v2jjt [723.236543ms]
Jun 26 14:50:56.907: INFO: Created: latency-svc-dp689
Jun 26 14:50:56.946: INFO: Got endpoints: latency-svc-5hsvg [742.246334ms]
Jun 26 14:50:56.961: INFO: Created: latency-svc-wcpv9
Jun 26 14:50:56.998: INFO: Got endpoints: latency-svc-qgnvd [746.600207ms]
Jun 26 14:50:57.013: INFO: Created: latency-svc-d29hv
Jun 26 14:50:57.045: INFO: Got endpoints: latency-svc-xlgg2 [740.653499ms]
Jun 26 14:50:57.057: INFO: Created: latency-svc-mwp82
Jun 26 14:50:57.095: INFO: Got endpoints: latency-svc-wjxwh [746.531506ms]
Jun 26 14:50:57.108: INFO: Created: latency-svc-982f2
Jun 26 14:50:57.145: INFO: Got endpoints: latency-svc-qbdjx [745.054689ms]
Jun 26 14:50:57.157: INFO: Created: latency-svc-ktlbd
Jun 26 14:50:57.197: INFO: Got endpoints: latency-svc-lhxpm [737.790594ms]
Jun 26 14:50:57.208: INFO: Created: latency-svc-cbd6l
Jun 26 14:50:57.245: INFO: Got endpoints: latency-svc-9kpq6 [739.153616ms]
Jun 26 14:50:57.260: INFO: Created: latency-svc-gp477
Jun 26 14:50:57.296: INFO: Got endpoints: latency-svc-67pl5 [746.582215ms]
Jun 26 14:50:57.309: INFO: Created: latency-svc-26ss2
Jun 26 14:50:57.345: INFO: Got endpoints: latency-svc-tt22x [744.998518ms]
Jun 26 14:50:57.362: INFO: Created: latency-svc-kp7dt
Jun 26 14:50:57.395: INFO: Got endpoints: latency-svc-p4wcz [737.021722ms]
Jun 26 14:50:57.407: INFO: Created: latency-svc-6qbfd
Jun 26 14:50:57.446: INFO: Got endpoints: latency-svc-t7m4h [743.234984ms]
Jun 26 14:50:57.463: INFO: Created: latency-svc-dptnh
Jun 26 14:50:57.501: INFO: Got endpoints: latency-svc-xmxjw [749.32829ms]
Jun 26 14:50:57.514: INFO: Created: latency-svc-jj5q8
Jun 26 14:50:57.545: INFO: Got endpoints: latency-svc-q76nd [747.342156ms]
Jun 26 14:50:57.567: INFO: Created: latency-svc-zf5cd
Jun 26 14:50:57.596: INFO: Got endpoints: latency-svc-jclns [741.854656ms]
Jun 26 14:50:57.607: INFO: Created: latency-svc-79smg
Jun 26 14:50:57.646: INFO: Got endpoints: latency-svc-dp689 [749.846148ms]
Jun 26 14:50:57.660: INFO: Created: latency-svc-9rvkx
Jun 26 14:50:57.697: INFO: Got endpoints: latency-svc-wcpv9 [750.970559ms]
Jun 26 14:50:57.710: INFO: Created: latency-svc-7pnvh
Jun 26 14:50:57.745: INFO: Got endpoints: latency-svc-d29hv [747.068693ms]
Jun 26 14:50:57.756: INFO: Created: latency-svc-zcwnt
Jun 26 14:50:57.797: INFO: Got endpoints: latency-svc-mwp82 [751.108977ms]
Jun 26 14:50:57.814: INFO: Created: latency-svc-t2gjh
Jun 26 14:50:57.846: INFO: Got endpoints: latency-svc-982f2 [750.958192ms]
Jun 26 14:50:57.864: INFO: Created: latency-svc-7grn6
Jun 26 14:50:57.896: INFO: Got endpoints: latency-svc-ktlbd [750.371454ms]
Jun 26 14:50:57.908: INFO: Created: latency-svc-zdwcs
Jun 26 14:50:57.946: INFO: Got endpoints: latency-svc-cbd6l [748.808543ms]
Jun 26 14:50:57.959: INFO: Created: latency-svc-h75lc
Jun 26 14:50:57.996: INFO: Got endpoints: latency-svc-gp477 [751.083576ms]
Jun 26 14:50:58.009: INFO: Created: latency-svc-vpphb
Jun 26 14:50:58.046: INFO: Got endpoints: latency-svc-26ss2 [749.966633ms]
Jun 26 14:50:58.059: INFO: Created: latency-svc-d74jt
Jun 26 14:50:58.099: INFO: Got endpoints: latency-svc-kp7dt [754.230996ms]
Jun 26 14:50:58.111: INFO: Created: latency-svc-hjkhg
Jun 26 14:50:58.146: INFO: Got endpoints: latency-svc-6qbfd [750.645833ms]
Jun 26 14:50:58.159: INFO: Created: latency-svc-l7wjz
Jun 26 14:50:58.196: INFO: Got endpoints: latency-svc-dptnh [749.536288ms]
Jun 26 14:50:58.208: INFO: Created: latency-svc-rt9px
Jun 26 14:50:58.246: INFO: Got endpoints: latency-svc-jj5q8 [744.791503ms]
Jun 26 14:50:58.260: INFO: Created: latency-svc-9gnqf
Jun 26 14:50:58.296: INFO: Got endpoints: latency-svc-zf5cd [750.59874ms]
Jun 26 14:50:58.316: INFO: Created: latency-svc-qp45d
Jun 26 14:50:58.347: INFO: Got endpoints: latency-svc-79smg [751.34979ms]
Jun 26 14:50:58.360: INFO: Created: latency-svc-rxklg
Jun 26 14:50:58.396: INFO: Got endpoints: latency-svc-9rvkx [749.15507ms]
Jun 26 14:50:58.407: INFO: Created: latency-svc-phfc4
Jun 26 14:50:58.445: INFO: Got endpoints: latency-svc-7pnvh [748.062685ms]
Jun 26 14:50:58.459: INFO: Created: latency-svc-5wnwq
Jun 26 14:50:58.495: INFO: Got endpoints: latency-svc-zcwnt [749.650763ms]
Jun 26 14:50:58.508: INFO: Created: latency-svc-gt4x8
Jun 26 14:50:58.546: INFO: Got endpoints: latency-svc-t2gjh [749.702568ms]
Jun 26 14:50:58.561: INFO: Created: latency-svc-4b8jm
Jun 26 14:50:58.596: INFO: Got endpoints: latency-svc-7grn6 [749.849123ms]
Jun 26 14:50:58.609: INFO: Created: latency-svc-hlcns
Jun 26 14:50:58.647: INFO: Got endpoints: latency-svc-zdwcs [750.88139ms]
Jun 26 14:50:58.672: INFO: Created: latency-svc-5rgqz
Jun 26 14:50:58.696: INFO: Got endpoints: latency-svc-h75lc [750.615186ms]
Jun 26 14:50:58.713: INFO: Created: latency-svc-zfnr4
Jun 26 14:50:58.746: INFO: Got endpoints: latency-svc-vpphb [749.227821ms]
Jun 26 14:50:58.760: INFO: Created: latency-svc-crsdt
Jun 26 14:50:58.795: INFO: Got endpoints: latency-svc-d74jt [748.981873ms]
Jun 26 14:50:58.810: INFO: Created: latency-svc-mccfm
Jun 26 14:50:58.846: INFO: Got endpoints: latency-svc-hjkhg [746.624823ms]
Jun 26 14:50:58.867: INFO: Created: latency-svc-m7s45
Jun 26 14:50:58.896: INFO: Got endpoints: latency-svc-l7wjz [749.677084ms]
Jun 26 14:50:58.908: INFO: Created: latency-svc-x9qfz
Jun 26 14:50:58.946: INFO: Got endpoints: latency-svc-rt9px [749.87488ms]
Jun 26 14:50:58.962: INFO: Created: latency-svc-jkdf9
Jun 26 14:50:58.996: INFO: Got endpoints: latency-svc-9gnqf [749.850007ms]
Jun 26 14:50:59.036: INFO: Created: latency-svc-cqrmx
Jun 26 14:50:59.049: INFO: Got endpoints: latency-svc-qp45d [752.954885ms]
Jun 26 14:50:59.097: INFO: Created: latency-svc-tnlzg
Jun 26 14:50:59.098: INFO: Got endpoints: latency-svc-rxklg [750.467602ms]
Jun 26 14:50:59.112: INFO: Created: latency-svc-f7fwf
Jun 26 14:50:59.145: INFO: Got endpoints: latency-svc-phfc4 [749.543989ms]
Jun 26 14:50:59.161: INFO: Created: latency-svc-pnh95
Jun 26 14:50:59.196: INFO: Got endpoints: latency-svc-5wnwq [749.710407ms]
Jun 26 14:50:59.207: INFO: Created: latency-svc-dmrgr
Jun 26 14:50:59.246: INFO: Got endpoints: latency-svc-gt4x8 [750.831494ms]
Jun 26 14:50:59.259: INFO: Created: latency-svc-kw8sk
Jun 26 14:50:59.300: INFO: Got endpoints: latency-svc-4b8jm [754.039933ms]
Jun 26 14:50:59.326: INFO: Created: latency-svc-24t6z
Jun 26 14:50:59.348: INFO: Got endpoints: latency-svc-hlcns [752.47854ms]
Jun 26 14:50:59.376: INFO: Created: latency-svc-4jdsz
Jun 26 14:50:59.399: INFO: Got endpoints: latency-svc-5rgqz [752.155327ms]
Jun 26 14:50:59.423: INFO: Created: latency-svc-nflg9
Jun 26 14:50:59.449: INFO: Got endpoints: latency-svc-zfnr4 [752.823297ms]
Jun 26 14:50:59.478: INFO: Created: latency-svc-h5g2x
Jun 26 14:50:59.499: INFO: Got endpoints: latency-svc-crsdt [752.885961ms]
Jun 26 14:50:59.526: INFO: Created: latency-svc-jbzvx
Jun 26 14:50:59.548: INFO: Got endpoints: latency-svc-mccfm [752.311838ms]
Jun 26 14:50:59.567: INFO: Created: latency-svc-2qqt8
Jun 26 14:50:59.599: INFO: Got endpoints: latency-svc-m7s45 [752.759806ms]
Jun 26 14:50:59.621: INFO: Created: latency-svc-mxdhb
Jun 26 14:50:59.650: INFO: Got endpoints: latency-svc-x9qfz [753.820386ms]
Jun 26 14:50:59.702: INFO: Got endpoints: latency-svc-jkdf9 [756.580039ms]
Jun 26 14:50:59.748: INFO: Got endpoints: latency-svc-cqrmx [752.048955ms]
Jun 26 14:50:59.797: INFO: Got endpoints: latency-svc-tnlzg [748.050802ms]
Jun 26 14:50:59.849: INFO: Got endpoints: latency-svc-f7fwf [750.966171ms]
Jun 26 14:51:00.002: INFO: Got endpoints: latency-svc-pnh95 [856.279683ms]
Jun 26 14:51:00.002: INFO: Got endpoints: latency-svc-dmrgr [806.723279ms]
Jun 26 14:51:00.003: INFO: Got endpoints: latency-svc-kw8sk [756.974372ms]
Jun 26 14:51:00.046: INFO: Got endpoints: latency-svc-24t6z [745.589913ms]
Jun 26 14:51:00.096: INFO: Got endpoints: latency-svc-4jdsz [747.581191ms]
Jun 26 14:51:00.145: INFO: Got endpoints: latency-svc-nflg9 [746.326007ms]
Jun 26 14:51:00.196: INFO: Got endpoints: latency-svc-h5g2x [746.605906ms]
Jun 26 14:51:00.245: INFO: Got endpoints: latency-svc-jbzvx [746.207339ms]
Jun 26 14:51:00.296: INFO: Got endpoints: latency-svc-2qqt8 [747.811696ms]
Jun 26 14:51:00.346: INFO: Got endpoints: latency-svc-mxdhb [746.827808ms]
Jun 26 14:51:00.346: INFO: Latencies: [58.030662ms 73.898324ms 107.337667ms 149.438677ms 173.425407ms 202.513543ms 236.035766ms 282.631749ms 316.177771ms 361.729842ms 364.450216ms 368.316836ms 372.254563ms 383.407725ms 388.89588ms 393.187642ms 395.096534ms 396.018042ms 396.954623ms 403.795919ms 408.873668ms 408.955566ms 411.460022ms 412.29494ms 414.999463ms 416.048704ms 416.681904ms 418.061569ms 420.078666ms 425.003379ms 427.47237ms 430.00693ms 430.157166ms 434.923781ms 439.047583ms 445.976935ms 450.298853ms 451.688721ms 456.675182ms 462.009425ms 462.109549ms 463.74831ms 465.96649ms 469.203114ms 469.271784ms 472.243776ms 472.355752ms 475.265128ms 478.626807ms 481.102936ms 486.93251ms 488.714862ms 497.072007ms 498.334342ms 500.054442ms 501.910156ms 503.61178ms 513.78767ms 515.164443ms 517.992766ms 525.193491ms 530.862312ms 541.055424ms 545.471394ms 547.088731ms 547.635262ms 558.28581ms 559.215245ms 568.216432ms 568.38491ms 568.453934ms 571.09701ms 575.162763ms 575.717498ms 575.942644ms 577.190245ms 585.35919ms 589.590113ms 590.266621ms 591.795969ms 593.068948ms 594.759877ms 597.096749ms 598.50675ms 599.505934ms 602.785056ms 605.059035ms 616.475346ms 628.186997ms 631.150009ms 637.82079ms 652.349533ms 674.690085ms 681.056644ms 687.663144ms 690.614861ms 691.006071ms 693.107845ms 693.372462ms 709.736025ms 716.19048ms 718.900313ms 719.566324ms 720.740921ms 723.236543ms 724.314151ms 726.857238ms 727.079647ms 728.980226ms 729.586782ms 731.028309ms 732.993503ms 733.704713ms 737.021722ms 737.790594ms 739.153616ms 740.653499ms 741.854656ms 742.246334ms 743.234984ms 743.752938ms 744.791503ms 744.998518ms 745.054689ms 745.1315ms 745.386747ms 745.589913ms 746.207339ms 746.326007ms 746.531506ms 746.582215ms 746.600207ms 746.605906ms 746.624823ms 746.827808ms 747.068693ms 747.107922ms 747.111858ms 747.342156ms 747.434388ms 747.581191ms 747.811696ms 748.050802ms 748.062685ms 748.808543ms 748.938175ms 748.981873ms 749.15507ms 749.227821ms 749.32829ms 749.392128ms 749.536288ms 749.543989ms 749.650763ms 749.677084ms 749.702568ms 749.710407ms 749.846148ms 749.849123ms 749.850007ms 749.87488ms 749.966633ms 750.371454ms 750.467602ms 750.59874ms 750.615186ms 750.645833ms 750.717344ms 750.726547ms 750.831494ms 750.88139ms 750.958192ms 750.966171ms 750.970559ms 751.083576ms 751.108977ms 751.34979ms 752.048955ms 752.155327ms 752.311838ms 752.47854ms 752.759806ms 752.823297ms 752.885961ms 752.954885ms 753.20394ms 753.820386ms 753.966348ms 754.039933ms 754.230996ms 755.413253ms 756.580039ms 756.974372ms 760.167502ms 762.313047ms 806.723279ms 815.604102ms 856.267538ms 856.279683ms 909.118264ms]
Jun 26 14:51:00.346: INFO: 50 %ile: 716.19048ms
Jun 26 14:51:00.346: INFO: 90 %ile: 752.47854ms
Jun 26 14:51:00.346: INFO: 99 %ile: 856.279683ms
Jun 26 14:51:00.346: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:51:00.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3929" for this suite.
Jun 26 14:51:18.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:51:18.448: INFO: namespace svc-latency-3929 deletion completed in 18.097156949s

• [SLOW TEST:29.939 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:51:18.448: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 26 14:51:18.483: INFO: Waiting up to 5m0s for pod "downwardapi-volume-efc5ba32-58dc-4a90-b64e-c9d1183b92f9" in namespace "downward-api-9073" to be "success or failure"
Jun 26 14:51:18.485: INFO: Pod "downwardapi-volume-efc5ba32-58dc-4a90-b64e-c9d1183b92f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.782635ms
Jun 26 14:51:20.493: INFO: Pod "downwardapi-volume-efc5ba32-58dc-4a90-b64e-c9d1183b92f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010411154s
Jun 26 14:51:22.501: INFO: Pod "downwardapi-volume-efc5ba32-58dc-4a90-b64e-c9d1183b92f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018200427s
STEP: Saw pod success
Jun 26 14:51:22.501: INFO: Pod "downwardapi-volume-efc5ba32-58dc-4a90-b64e-c9d1183b92f9" satisfied condition "success or failure"
Jun 26 14:51:22.507: INFO: Trying to get logs from node k8s-worker-1-dev pod downwardapi-volume-efc5ba32-58dc-4a90-b64e-c9d1183b92f9 container client-container: <nil>
STEP: delete the pod
Jun 26 14:51:22.544: INFO: Waiting for pod downwardapi-volume-efc5ba32-58dc-4a90-b64e-c9d1183b92f9 to disappear
Jun 26 14:51:22.549: INFO: Pod downwardapi-volume-efc5ba32-58dc-4a90-b64e-c9d1183b92f9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:51:22.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9073" for this suite.
Jun 26 14:51:28.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:51:28.673: INFO: namespace downward-api-9073 deletion completed in 6.114495626s

• [SLOW TEST:10.225 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:51:28.673: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun 26 14:51:28.716: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-280,SelfLink:/api/v1/namespaces/watch-280/configmaps/e2e-watch-test-configmap-a,UID:666b4a60-21d8-4e5a-8e1a-2b48efadcfbd,ResourceVersion:21129,Generation:0,CreationTimestamp:2019-06-26 14:51:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 26 14:51:28.716: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-280,SelfLink:/api/v1/namespaces/watch-280/configmaps/e2e-watch-test-configmap-a,UID:666b4a60-21d8-4e5a-8e1a-2b48efadcfbd,ResourceVersion:21129,Generation:0,CreationTimestamp:2019-06-26 14:51:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun 26 14:51:38.741: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-280,SelfLink:/api/v1/namespaces/watch-280/configmaps/e2e-watch-test-configmap-a,UID:666b4a60-21d8-4e5a-8e1a-2b48efadcfbd,ResourceVersion:21145,Generation:0,CreationTimestamp:2019-06-26 14:51:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 26 14:51:38.742: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-280,SelfLink:/api/v1/namespaces/watch-280/configmaps/e2e-watch-test-configmap-a,UID:666b4a60-21d8-4e5a-8e1a-2b48efadcfbd,ResourceVersion:21145,Generation:0,CreationTimestamp:2019-06-26 14:51:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun 26 14:51:48.759: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-280,SelfLink:/api/v1/namespaces/watch-280/configmaps/e2e-watch-test-configmap-a,UID:666b4a60-21d8-4e5a-8e1a-2b48efadcfbd,ResourceVersion:21162,Generation:0,CreationTimestamp:2019-06-26 14:51:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 26 14:51:48.760: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-280,SelfLink:/api/v1/namespaces/watch-280/configmaps/e2e-watch-test-configmap-a,UID:666b4a60-21d8-4e5a-8e1a-2b48efadcfbd,ResourceVersion:21162,Generation:0,CreationTimestamp:2019-06-26 14:51:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun 26 14:51:58.775: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-280,SelfLink:/api/v1/namespaces/watch-280/configmaps/e2e-watch-test-configmap-a,UID:666b4a60-21d8-4e5a-8e1a-2b48efadcfbd,ResourceVersion:21178,Generation:0,CreationTimestamp:2019-06-26 14:51:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 26 14:51:58.776: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-280,SelfLink:/api/v1/namespaces/watch-280/configmaps/e2e-watch-test-configmap-a,UID:666b4a60-21d8-4e5a-8e1a-2b48efadcfbd,ResourceVersion:21178,Generation:0,CreationTimestamp:2019-06-26 14:51:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun 26 14:52:08.783: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-280,SelfLink:/api/v1/namespaces/watch-280/configmaps/e2e-watch-test-configmap-b,UID:e5a9d401-fafd-449a-80c2-2e2a3ce3c2c5,ResourceVersion:21194,Generation:0,CreationTimestamp:2019-06-26 14:52:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 26 14:52:08.783: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-280,SelfLink:/api/v1/namespaces/watch-280/configmaps/e2e-watch-test-configmap-b,UID:e5a9d401-fafd-449a-80c2-2e2a3ce3c2c5,ResourceVersion:21194,Generation:0,CreationTimestamp:2019-06-26 14:52:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun 26 14:52:18.791: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-280,SelfLink:/api/v1/namespaces/watch-280/configmaps/e2e-watch-test-configmap-b,UID:e5a9d401-fafd-449a-80c2-2e2a3ce3c2c5,ResourceVersion:21210,Generation:0,CreationTimestamp:2019-06-26 14:52:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 26 14:52:18.791: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-280,SelfLink:/api/v1/namespaces/watch-280/configmaps/e2e-watch-test-configmap-b,UID:e5a9d401-fafd-449a-80c2-2e2a3ce3c2c5,ResourceVersion:21210,Generation:0,CreationTimestamp:2019-06-26 14:52:08 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:52:28.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-280" for this suite.
Jun 26 14:52:34.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:52:35.034: INFO: namespace watch-280 deletion completed in 6.227352036s

• [SLOW TEST:66.362 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:52:35.038: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 26 14:52:35.120: INFO: Waiting up to 5m0s for pod "pod-7dd1b681-0d60-4a6f-81f7-fbd830a2cb81" in namespace "emptydir-6289" to be "success or failure"
Jun 26 14:52:35.126: INFO: Pod "pod-7dd1b681-0d60-4a6f-81f7-fbd830a2cb81": Phase="Pending", Reason="", readiness=false. Elapsed: 5.803908ms
Jun 26 14:52:37.130: INFO: Pod "pod-7dd1b681-0d60-4a6f-81f7-fbd830a2cb81": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009983884s
Jun 26 14:52:39.134: INFO: Pod "pod-7dd1b681-0d60-4a6f-81f7-fbd830a2cb81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.0137103s
STEP: Saw pod success
Jun 26 14:52:39.134: INFO: Pod "pod-7dd1b681-0d60-4a6f-81f7-fbd830a2cb81" satisfied condition "success or failure"
Jun 26 14:52:39.136: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-7dd1b681-0d60-4a6f-81f7-fbd830a2cb81 container test-container: <nil>
STEP: delete the pod
Jun 26 14:52:39.163: INFO: Waiting for pod pod-7dd1b681-0d60-4a6f-81f7-fbd830a2cb81 to disappear
Jun 26 14:52:39.166: INFO: Pod pod-7dd1b681-0d60-4a6f-81f7-fbd830a2cb81 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:52:39.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6289" for this suite.
Jun 26 14:52:45.183: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:52:45.385: INFO: namespace emptydir-6289 deletion completed in 6.216383011s

• [SLOW TEST:10.348 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:52:45.389: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-91b6b566-99c5-4bb8-955f-242c92160300
STEP: Creating a pod to test consume configMaps
Jun 26 14:52:45.509: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5e85dd5f-1bd6-4aa5-bde9-ca06a535e73a" in namespace "projected-3757" to be "success or failure"
Jun 26 14:52:45.517: INFO: Pod "pod-projected-configmaps-5e85dd5f-1bd6-4aa5-bde9-ca06a535e73a": Phase="Pending", Reason="", readiness=false. Elapsed: 8.267197ms
Jun 26 14:52:47.526: INFO: Pod "pod-projected-configmaps-5e85dd5f-1bd6-4aa5-bde9-ca06a535e73a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01695889s
Jun 26 14:52:49.530: INFO: Pod "pod-projected-configmaps-5e85dd5f-1bd6-4aa5-bde9-ca06a535e73a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020992261s
STEP: Saw pod success
Jun 26 14:52:49.530: INFO: Pod "pod-projected-configmaps-5e85dd5f-1bd6-4aa5-bde9-ca06a535e73a" satisfied condition "success or failure"
Jun 26 14:52:49.532: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-projected-configmaps-5e85dd5f-1bd6-4aa5-bde9-ca06a535e73a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 26 14:52:49.562: INFO: Waiting for pod pod-projected-configmaps-5e85dd5f-1bd6-4aa5-bde9-ca06a535e73a to disappear
Jun 26 14:52:49.566: INFO: Pod pod-projected-configmaps-5e85dd5f-1bd6-4aa5-bde9-ca06a535e73a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:52:49.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3757" for this suite.
Jun 26 14:52:55.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:52:55.804: INFO: namespace projected-3757 deletion completed in 6.233183745s

• [SLOW TEST:10.415 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:52:55.808: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Jun 26 14:52:55.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 cluster-info'
Jun 26 14:52:56.100: INFO: stderr: ""
Jun 26 14:52:56.100: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:52:56.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9863" for this suite.
Jun 26 14:53:02.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:53:02.342: INFO: namespace kubectl-9863 deletion completed in 6.22954547s

• [SLOW TEST:6.534 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:53:02.347: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-060126ed-02f9-48c8-83fa-589e6a77e345
STEP: Creating a pod to test consume configMaps
Jun 26 14:53:02.440: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c6058adb-822f-4e34-ad28-20d2eed3a84c" in namespace "projected-8360" to be "success or failure"
Jun 26 14:53:02.447: INFO: Pod "pod-projected-configmaps-c6058adb-822f-4e34-ad28-20d2eed3a84c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.126485ms
Jun 26 14:53:04.454: INFO: Pod "pod-projected-configmaps-c6058adb-822f-4e34-ad28-20d2eed3a84c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013466997s
STEP: Saw pod success
Jun 26 14:53:04.454: INFO: Pod "pod-projected-configmaps-c6058adb-822f-4e34-ad28-20d2eed3a84c" satisfied condition "success or failure"
Jun 26 14:53:04.461: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-projected-configmaps-c6058adb-822f-4e34-ad28-20d2eed3a84c container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 26 14:53:04.500: INFO: Waiting for pod pod-projected-configmaps-c6058adb-822f-4e34-ad28-20d2eed3a84c to disappear
Jun 26 14:53:04.506: INFO: Pod pod-projected-configmaps-c6058adb-822f-4e34-ad28-20d2eed3a84c no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:53:04.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8360" for this suite.
Jun 26 14:53:10.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:53:10.777: INFO: namespace projected-8360 deletion completed in 6.257254577s

• [SLOW TEST:8.429 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:53:10.777: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 26 14:53:13.914: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:53:13.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6108" for this suite.
Jun 26 14:53:19.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:53:20.073: INFO: namespace container-runtime-6108 deletion completed in 6.108295871s

• [SLOW TEST:9.296 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:53:20.074: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-ff8deff4-3da9-4c54-bea9-47f5dccaac38
STEP: Creating a pod to test consume secrets
Jun 26 14:53:20.126: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-9c2642a8-394d-490b-a2ea-bbe46fc62cf3" in namespace "projected-7294" to be "success or failure"
Jun 26 14:53:20.129: INFO: Pod "pod-projected-secrets-9c2642a8-394d-490b-a2ea-bbe46fc62cf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.881264ms
Jun 26 14:53:22.137: INFO: Pod "pod-projected-secrets-9c2642a8-394d-490b-a2ea-bbe46fc62cf3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011341143s
Jun 26 14:53:24.145: INFO: Pod "pod-projected-secrets-9c2642a8-394d-490b-a2ea-bbe46fc62cf3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019689045s
STEP: Saw pod success
Jun 26 14:53:24.145: INFO: Pod "pod-projected-secrets-9c2642a8-394d-490b-a2ea-bbe46fc62cf3" satisfied condition "success or failure"
Jun 26 14:53:24.152: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-projected-secrets-9c2642a8-394d-490b-a2ea-bbe46fc62cf3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 26 14:53:24.190: INFO: Waiting for pod pod-projected-secrets-9c2642a8-394d-490b-a2ea-bbe46fc62cf3 to disappear
Jun 26 14:53:24.200: INFO: Pod pod-projected-secrets-9c2642a8-394d-490b-a2ea-bbe46fc62cf3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:53:24.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7294" for this suite.
Jun 26 14:53:30.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:53:30.350: INFO: namespace projected-7294 deletion completed in 6.141997824s

• [SLOW TEST:10.277 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:53:30.352: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-8223
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8223 to expose endpoints map[]
Jun 26 14:53:30.401: INFO: Get endpoints failed (3.487289ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jun 26 14:53:31.404: INFO: successfully validated that service endpoint-test2 in namespace services-8223 exposes endpoints map[] (1.006955337s elapsed)
STEP: Creating pod pod1 in namespace services-8223
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8223 to expose endpoints map[pod1:[80]]
Jun 26 14:53:33.429: INFO: successfully validated that service endpoint-test2 in namespace services-8223 exposes endpoints map[pod1:[80]] (2.018005165s elapsed)
STEP: Creating pod pod2 in namespace services-8223
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8223 to expose endpoints map[pod1:[80] pod2:[80]]
Jun 26 14:53:35.464: INFO: successfully validated that service endpoint-test2 in namespace services-8223 exposes endpoints map[pod1:[80] pod2:[80]] (2.028400996s elapsed)
STEP: Deleting pod pod1 in namespace services-8223
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8223 to expose endpoints map[pod2:[80]]
Jun 26 14:53:36.489: INFO: successfully validated that service endpoint-test2 in namespace services-8223 exposes endpoints map[pod2:[80]] (1.021633379s elapsed)
STEP: Deleting pod pod2 in namespace services-8223
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8223 to expose endpoints map[]
Jun 26 14:53:37.516: INFO: successfully validated that service endpoint-test2 in namespace services-8223 exposes endpoints map[] (1.014232221s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:53:37.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8223" for this suite.
Jun 26 14:53:43.636: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:53:43.854: INFO: namespace services-8223 deletion completed in 6.251809286s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:13.503 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:53:43.858: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Jun 26 14:53:43.936: INFO: Waiting up to 5m0s for pod "var-expansion-f26324f5-29f3-440c-a345-6aef50e10e8c" in namespace "var-expansion-3705" to be "success or failure"
Jun 26 14:53:43.943: INFO: Pod "var-expansion-f26324f5-29f3-440c-a345-6aef50e10e8c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.750519ms
Jun 26 14:53:45.951: INFO: Pod "var-expansion-f26324f5-29f3-440c-a345-6aef50e10e8c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015181771s
Jun 26 14:53:47.956: INFO: Pod "var-expansion-f26324f5-29f3-440c-a345-6aef50e10e8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.019954601s
STEP: Saw pod success
Jun 26 14:53:47.956: INFO: Pod "var-expansion-f26324f5-29f3-440c-a345-6aef50e10e8c" satisfied condition "success or failure"
Jun 26 14:53:47.959: INFO: Trying to get logs from node k8s-worker-1-dev pod var-expansion-f26324f5-29f3-440c-a345-6aef50e10e8c container dapi-container: <nil>
STEP: delete the pod
Jun 26 14:53:47.976: INFO: Waiting for pod var-expansion-f26324f5-29f3-440c-a345-6aef50e10e8c to disappear
Jun 26 14:53:47.978: INFO: Pod var-expansion-f26324f5-29f3-440c-a345-6aef50e10e8c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:53:47.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3705" for this suite.
Jun 26 14:53:53.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:53:54.209: INFO: namespace var-expansion-3705 deletion completed in 6.226671195s

• [SLOW TEST:10.351 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:53:54.213: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 26 14:53:54.298: INFO: Waiting up to 5m0s for pod "pod-79b51a60-5ed3-4e31-baa2-559b6cd0146b" in namespace "emptydir-1275" to be "success or failure"
Jun 26 14:53:54.306: INFO: Pod "pod-79b51a60-5ed3-4e31-baa2-559b6cd0146b": Phase="Pending", Reason="", readiness=false. Elapsed: 8.305179ms
Jun 26 14:53:56.314: INFO: Pod "pod-79b51a60-5ed3-4e31-baa2-559b6cd0146b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016499418s
STEP: Saw pod success
Jun 26 14:53:56.314: INFO: Pod "pod-79b51a60-5ed3-4e31-baa2-559b6cd0146b" satisfied condition "success or failure"
Jun 26 14:53:56.322: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-79b51a60-5ed3-4e31-baa2-559b6cd0146b container test-container: <nil>
STEP: delete the pod
Jun 26 14:53:56.365: INFO: Waiting for pod pod-79b51a60-5ed3-4e31-baa2-559b6cd0146b to disappear
Jun 26 14:53:56.370: INFO: Pod pod-79b51a60-5ed3-4e31-baa2-559b6cd0146b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:53:56.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1275" for this suite.
Jun 26 14:54:02.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:54:02.476: INFO: namespace emptydir-1275 deletion completed in 6.097875026s

• [SLOW TEST:8.263 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:54:02.476: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Jun 26 14:54:02.516: INFO: Waiting up to 5m0s for pod "pod-64613cbe-3d7c-42f4-a3eb-023ff32b57cb" in namespace "emptydir-5311" to be "success or failure"
Jun 26 14:54:02.520: INFO: Pod "pod-64613cbe-3d7c-42f4-a3eb-023ff32b57cb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.423348ms
Jun 26 14:54:04.524: INFO: Pod "pod-64613cbe-3d7c-42f4-a3eb-023ff32b57cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007488353s
STEP: Saw pod success
Jun 26 14:54:04.524: INFO: Pod "pod-64613cbe-3d7c-42f4-a3eb-023ff32b57cb" satisfied condition "success or failure"
Jun 26 14:54:04.526: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-64613cbe-3d7c-42f4-a3eb-023ff32b57cb container test-container: <nil>
STEP: delete the pod
Jun 26 14:54:04.542: INFO: Waiting for pod pod-64613cbe-3d7c-42f4-a3eb-023ff32b57cb to disappear
Jun 26 14:54:04.544: INFO: Pod pod-64613cbe-3d7c-42f4-a3eb-023ff32b57cb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:54:04.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5311" for this suite.
Jun 26 14:54:10.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:54:10.639: INFO: namespace emptydir-5311 deletion completed in 6.091378888s

• [SLOW TEST:8.163 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:54:10.640: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6697.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6697.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6697.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6697.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6697.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6697.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6697.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6697.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6697.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6697.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6697.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6697.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6697.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 33.216.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.216.33_udp@PTR;check="$$(dig +tcp +noall +answer +search 33.216.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.216.33_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6697.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6697.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6697.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6697.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6697.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6697.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6697.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6697.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6697.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6697.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6697.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6697.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6697.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 33.216.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.216.33_udp@PTR;check="$$(dig +tcp +noall +answer +search 33.216.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.216.33_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 26 14:54:14.811: INFO: DNS probes using dns-6697/dns-test-f6e0f6ce-ebdb-438f-ac85-f05152b5b902 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:54:14.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6697" for this suite.
Jun 26 14:54:20.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:54:21.106: INFO: namespace dns-6697 deletion completed in 6.216982372s

• [SLOW TEST:10.466 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:54:21.108: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-dccccb5a-6346-4187-8e49-0bc7655301bb
STEP: Creating a pod to test consume secrets
Jun 26 14:54:21.210: INFO: Waiting up to 5m0s for pod "pod-secrets-6591d829-8b7b-457b-90cf-6f560633ec90" in namespace "secrets-8198" to be "success or failure"
Jun 26 14:54:21.221: INFO: Pod "pod-secrets-6591d829-8b7b-457b-90cf-6f560633ec90": Phase="Pending", Reason="", readiness=false. Elapsed: 11.12262ms
Jun 26 14:54:23.225: INFO: Pod "pod-secrets-6591d829-8b7b-457b-90cf-6f560633ec90": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014852729s
STEP: Saw pod success
Jun 26 14:54:23.225: INFO: Pod "pod-secrets-6591d829-8b7b-457b-90cf-6f560633ec90" satisfied condition "success or failure"
Jun 26 14:54:23.227: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-secrets-6591d829-8b7b-457b-90cf-6f560633ec90 container secret-volume-test: <nil>
STEP: delete the pod
Jun 26 14:54:23.248: INFO: Waiting for pod pod-secrets-6591d829-8b7b-457b-90cf-6f560633ec90 to disappear
Jun 26 14:54:23.251: INFO: Pod pod-secrets-6591d829-8b7b-457b-90cf-6f560633ec90 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:54:23.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8198" for this suite.
Jun 26 14:54:29.266: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:54:29.347: INFO: namespace secrets-8198 deletion completed in 6.091522187s

• [SLOW TEST:8.239 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:54:29.348: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8443.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-8443.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 26 14:54:43.429: INFO: DNS probes using dns-8443/dns-test-dead01ad-387f-4a2b-93d8-21b9c30dbef1 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:54:43.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8443" for this suite.
Jun 26 14:54:49.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:54:49.679: INFO: namespace dns-8443 deletion completed in 6.230301598s

• [SLOW TEST:20.332 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:54:49.682: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 26 14:54:51.792: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:54:51.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7605" for this suite.
Jun 26 14:54:57.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:54:57.959: INFO: namespace container-runtime-7605 deletion completed in 6.113436548s

• [SLOW TEST:8.278 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:54:57.961: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jun 26 14:55:00.022: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-254532258 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jun 26 14:55:15.118: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:55:15.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8227" for this suite.
Jun 26 14:55:21.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:55:21.222: INFO: namespace pods-8227 deletion completed in 6.096211137s

• [SLOW TEST:23.260 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:55:21.222: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Jun 26 14:55:21.251: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jun 26 14:55:21.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 create -f - --namespace=kubectl-4576'
Jun 26 14:55:21.452: INFO: stderr: ""
Jun 26 14:55:21.452: INFO: stdout: "service/redis-slave created\n"
Jun 26 14:55:21.453: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jun 26 14:55:21.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 create -f - --namespace=kubectl-4576'
Jun 26 14:55:21.682: INFO: stderr: ""
Jun 26 14:55:21.682: INFO: stdout: "service/redis-master created\n"
Jun 26 14:55:21.682: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun 26 14:55:21.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 create -f - --namespace=kubectl-4576'
Jun 26 14:55:21.889: INFO: stderr: ""
Jun 26 14:55:21.890: INFO: stdout: "service/frontend created\n"
Jun 26 14:55:21.891: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jun 26 14:55:21.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 create -f - --namespace=kubectl-4576'
Jun 26 14:55:22.072: INFO: stderr: ""
Jun 26 14:55:22.072: INFO: stdout: "deployment.apps/frontend created\n"
Jun 26 14:55:22.072: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 26 14:55:22.072: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 create -f - --namespace=kubectl-4576'
Jun 26 14:55:22.255: INFO: stderr: ""
Jun 26 14:55:22.255: INFO: stdout: "deployment.apps/redis-master created\n"
Jun 26 14:55:22.256: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jun 26 14:55:22.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 create -f - --namespace=kubectl-4576'
Jun 26 14:55:22.430: INFO: stderr: ""
Jun 26 14:55:22.430: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jun 26 14:55:22.430: INFO: Waiting for all frontend pods to be Running.
Jun 26 14:56:02.483: INFO: Waiting for frontend to serve content.
Jun 26 14:56:02.503: INFO: Trying to add a new entry to the guestbook.
Jun 26 14:56:02.515: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jun 26 14:56:02.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 delete --grace-period=0 --force -f - --namespace=kubectl-4576'
Jun 26 14:56:02.648: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 26 14:56:02.648: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jun 26 14:56:02.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 delete --grace-period=0 --force -f - --namespace=kubectl-4576'
Jun 26 14:56:02.759: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 26 14:56:02.759: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 26 14:56:02.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 delete --grace-period=0 --force -f - --namespace=kubectl-4576'
Jun 26 14:56:02.872: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 26 14:56:02.872: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 26 14:56:02.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 delete --grace-period=0 --force -f - --namespace=kubectl-4576'
Jun 26 14:56:02.973: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 26 14:56:02.973: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 26 14:56:02.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 delete --grace-period=0 --force -f - --namespace=kubectl-4576'
Jun 26 14:56:03.054: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 26 14:56:03.054: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 26 14:56:03.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 delete --grace-period=0 --force -f - --namespace=kubectl-4576'
Jun 26 14:56:03.133: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 26 14:56:03.133: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:56:03.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4576" for this suite.
Jun 26 14:56:43.151: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:56:43.368: INFO: namespace kubectl-4576 deletion completed in 40.230130953s

• [SLOW TEST:82.146 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:56:43.369: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 26 14:56:43.472: INFO: Waiting up to 5m0s for pod "pod-47db6665-3074-414a-80e0-40299878c1b0" in namespace "emptydir-6645" to be "success or failure"
Jun 26 14:56:43.491: INFO: Pod "pod-47db6665-3074-414a-80e0-40299878c1b0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.488247ms
Jun 26 14:56:45.499: INFO: Pod "pod-47db6665-3074-414a-80e0-40299878c1b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026482216s
STEP: Saw pod success
Jun 26 14:56:45.499: INFO: Pod "pod-47db6665-3074-414a-80e0-40299878c1b0" satisfied condition "success or failure"
Jun 26 14:56:45.505: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-47db6665-3074-414a-80e0-40299878c1b0 container test-container: <nil>
STEP: delete the pod
Jun 26 14:56:45.556: INFO: Waiting for pod pod-47db6665-3074-414a-80e0-40299878c1b0 to disappear
Jun 26 14:56:45.561: INFO: Pod pod-47db6665-3074-414a-80e0-40299878c1b0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:56:45.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6645" for this suite.
Jun 26 14:56:51.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:56:51.735: INFO: namespace emptydir-6645 deletion completed in 6.163977959s

• [SLOW TEST:8.366 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:56:51.736: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-l5prf in namespace proxy-1970
I0626 14:56:51.798658      18 runners.go:180] Created replication controller with name: proxy-service-l5prf, namespace: proxy-1970, replica count: 1
I0626 14:56:52.849874      18 runners.go:180] proxy-service-l5prf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0626 14:56:53.850215      18 runners.go:180] proxy-service-l5prf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0626 14:56:54.850461      18 runners.go:180] proxy-service-l5prf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0626 14:56:55.850792      18 runners.go:180] proxy-service-l5prf Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0626 14:56:56.851102      18 runners.go:180] proxy-service-l5prf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0626 14:56:57.851388      18 runners.go:180] proxy-service-l5prf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0626 14:56:58.851940      18 runners.go:180] proxy-service-l5prf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0626 14:56:59.852296      18 runners.go:180] proxy-service-l5prf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0626 14:57:00.852845      18 runners.go:180] proxy-service-l5prf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0626 14:57:01.853153      18 runners.go:180] proxy-service-l5prf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0626 14:57:02.853495      18 runners.go:180] proxy-service-l5prf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0626 14:57:03.853813      18 runners.go:180] proxy-service-l5prf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0626 14:57:04.853977      18 runners.go:180] proxy-service-l5prf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0626 14:57:05.854261      18 runners.go:180] proxy-service-l5prf Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0626 14:57:06.854663      18 runners.go:180] proxy-service-l5prf Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 26 14:57:06.858: INFO: setup took 15.079674099s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun 26 14:57:06.863: INFO: (0) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 5.033286ms)
Jun 26 14:57:06.865: INFO: (0) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 6.264399ms)
Jun 26 14:57:06.866: INFO: (0) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 7.799977ms)
Jun 26 14:57:06.867: INFO: (0) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/rewriteme">test</a> (200; 7.77514ms)
Jun 26 14:57:06.870: INFO: (0) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname1/proxy/: foo (200; 10.922847ms)
Jun 26 14:57:06.871: INFO: (0) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 11.957444ms)
Jun 26 14:57:06.872: INFO: (0) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname2/proxy/: bar (200; 12.567858ms)
Jun 26 14:57:06.872: INFO: (0) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">... (200; 12.855612ms)
Jun 26 14:57:06.872: INFO: (0) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">test<... (200; 12.723205ms)
Jun 26 14:57:06.872: INFO: (0) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname2/proxy/: bar (200; 12.911307ms)
Jun 26 14:57:06.872: INFO: (0) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:460/proxy/: tls baz (200; 13.370823ms)
Jun 26 14:57:06.872: INFO: (0) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname1/proxy/: foo (200; 13.409745ms)
Jun 26 14:57:06.873: INFO: (0) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname1/proxy/: tls baz (200; 14.058689ms)
Jun 26 14:57:06.873: INFO: (0) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname2/proxy/: tls qux (200; 14.115345ms)
Jun 26 14:57:06.873: INFO: (0) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/tlsrewritem... (200; 14.847073ms)
Jun 26 14:57:06.876: INFO: (0) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:462/proxy/: tls qux (200; 17.041682ms)
Jun 26 14:57:06.883: INFO: (1) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/rewriteme">test</a> (200; 7.418382ms)
Jun 26 14:57:06.883: INFO: (1) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">... (200; 7.705802ms)
Jun 26 14:57:06.884: INFO: (1) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:462/proxy/: tls qux (200; 7.858773ms)
Jun 26 14:57:06.884: INFO: (1) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 7.687784ms)
Jun 26 14:57:06.884: INFO: (1) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:460/proxy/: tls baz (200; 8.10688ms)
Jun 26 14:57:06.884: INFO: (1) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 8.285021ms)
Jun 26 14:57:06.884: INFO: (1) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">test<... (200; 8.324993ms)
Jun 26 14:57:06.884: INFO: (1) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/tlsrewritem... (200; 8.07379ms)
Jun 26 14:57:06.885: INFO: (1) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 8.780993ms)
Jun 26 14:57:06.887: INFO: (1) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 10.591056ms)
Jun 26 14:57:06.887: INFO: (1) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname2/proxy/: bar (200; 10.419658ms)
Jun 26 14:57:06.888: INFO: (1) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname1/proxy/: tls baz (200; 11.897493ms)
Jun 26 14:57:06.891: INFO: (1) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname1/proxy/: foo (200; 15.052686ms)
Jun 26 14:57:06.891: INFO: (1) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname2/proxy/: tls qux (200; 15.604019ms)
Jun 26 14:57:06.891: INFO: (1) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname1/proxy/: foo (200; 15.599982ms)
Jun 26 14:57:06.892: INFO: (1) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname2/proxy/: bar (200; 16.742371ms)
Jun 26 14:57:06.901: INFO: (2) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:462/proxy/: tls qux (200; 7.817002ms)
Jun 26 14:57:06.901: INFO: (2) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 8.157053ms)
Jun 26 14:57:06.901: INFO: (2) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/rewriteme">test</a> (200; 8.517423ms)
Jun 26 14:57:06.902: INFO: (2) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 8.352054ms)
Jun 26 14:57:06.902: INFO: (2) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">test<... (200; 8.336224ms)
Jun 26 14:57:06.902: INFO: (2) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:460/proxy/: tls baz (200; 9.374887ms)
Jun 26 14:57:06.902: INFO: (2) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/tlsrewritem... (200; 9.160187ms)
Jun 26 14:57:06.902: INFO: (2) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 9.189388ms)
Jun 26 14:57:06.903: INFO: (2) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname1/proxy/: foo (200; 9.568772ms)
Jun 26 14:57:06.903: INFO: (2) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 10.155975ms)
Jun 26 14:57:06.903: INFO: (2) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">... (200; 9.609765ms)
Jun 26 14:57:06.903: INFO: (2) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname2/proxy/: bar (200; 9.784835ms)
Jun 26 14:57:06.906: INFO: (2) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname1/proxy/: foo (200; 12.582807ms)
Jun 26 14:57:06.906: INFO: (2) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname2/proxy/: tls qux (200; 12.512681ms)
Jun 26 14:57:06.906: INFO: (2) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname2/proxy/: bar (200; 13.301423ms)
Jun 26 14:57:06.907: INFO: (2) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname1/proxy/: tls baz (200; 14.03315ms)
Jun 26 14:57:06.914: INFO: (3) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:462/proxy/: tls qux (200; 6.375589ms)
Jun 26 14:57:06.914: INFO: (3) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname1/proxy/: foo (200; 6.737941ms)
Jun 26 14:57:06.914: INFO: (3) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 7.124972ms)
Jun 26 14:57:06.916: INFO: (3) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:460/proxy/: tls baz (200; 8.084732ms)
Jun 26 14:57:06.916: INFO: (3) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">... (200; 6.764749ms)
Jun 26 14:57:06.916: INFO: (3) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/rewriteme">test</a> (200; 7.33164ms)
Jun 26 14:57:06.916: INFO: (3) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 7.713507ms)
Jun 26 14:57:06.916: INFO: (3) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 8.438011ms)
Jun 26 14:57:06.916: INFO: (3) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">test<... (200; 8.037471ms)
Jun 26 14:57:06.916: INFO: (3) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/tlsrewritem... (200; 7.792692ms)
Jun 26 14:57:06.917: INFO: (3) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 9.31779ms)
Jun 26 14:57:06.918: INFO: (3) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname1/proxy/: tls baz (200; 9.858925ms)
Jun 26 14:57:06.918: INFO: (3) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname2/proxy/: tls qux (200; 9.943775ms)
Jun 26 14:57:06.918: INFO: (3) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname2/proxy/: bar (200; 9.577047ms)
Jun 26 14:57:06.919: INFO: (3) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname1/proxy/: foo (200; 9.846785ms)
Jun 26 14:57:06.919: INFO: (3) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname2/proxy/: bar (200; 9.659412ms)
Jun 26 14:57:06.923: INFO: (4) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/rewriteme">test</a> (200; 3.605901ms)
Jun 26 14:57:06.923: INFO: (4) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 4.391423ms)
Jun 26 14:57:06.923: INFO: (4) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">... (200; 3.503085ms)
Jun 26 14:57:06.930: INFO: (4) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">test<... (200; 9.722563ms)
Jun 26 14:57:06.931: INFO: (4) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname2/proxy/: bar (200; 11.332459ms)
Jun 26 14:57:06.931: INFO: (4) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname2/proxy/: tls qux (200; 11.990545ms)
Jun 26 14:57:06.931: INFO: (4) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:460/proxy/: tls baz (200; 10.882934ms)
Jun 26 14:57:06.931: INFO: (4) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname1/proxy/: tls baz (200; 11.092303ms)
Jun 26 14:57:06.932: INFO: (4) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname1/proxy/: foo (200; 12.314853ms)
Jun 26 14:57:06.932: INFO: (4) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 11.316444ms)
Jun 26 14:57:06.932: INFO: (4) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 11.973433ms)
Jun 26 14:57:06.932: INFO: (4) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname1/proxy/: foo (200; 12.35626ms)
Jun 26 14:57:06.932: INFO: (4) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:462/proxy/: tls qux (200; 12.123135ms)
Jun 26 14:57:06.932: INFO: (4) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/tlsrewritem... (200; 13.207475ms)
Jun 26 14:57:06.933: INFO: (4) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 12.785501ms)
Jun 26 14:57:06.933: INFO: (4) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname2/proxy/: bar (200; 13.113369ms)
Jun 26 14:57:06.936: INFO: (5) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">... (200; 3.906123ms)
Jun 26 14:57:06.937: INFO: (5) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 3.912812ms)
Jun 26 14:57:06.937: INFO: (5) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:460/proxy/: tls baz (200; 4.596686ms)
Jun 26 14:57:06.940: INFO: (5) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 6.697314ms)
Jun 26 14:57:06.940: INFO: (5) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 6.604234ms)
Jun 26 14:57:06.940: INFO: (5) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:462/proxy/: tls qux (200; 7.14811ms)
Jun 26 14:57:06.940: INFO: (5) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">test<... (200; 6.978151ms)
Jun 26 14:57:06.940: INFO: (5) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 7.640959ms)
Jun 26 14:57:06.940: INFO: (5) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/tlsrewritem... (200; 7.265306ms)
Jun 26 14:57:06.940: INFO: (5) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/rewriteme">test</a> (200; 7.254588ms)
Jun 26 14:57:06.943: INFO: (5) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname2/proxy/: bar (200; 10.105985ms)
Jun 26 14:57:06.943: INFO: (5) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname1/proxy/: foo (200; 10.190381ms)
Jun 26 14:57:06.943: INFO: (5) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname2/proxy/: tls qux (200; 9.959196ms)
Jun 26 14:57:06.943: INFO: (5) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname1/proxy/: tls baz (200; 9.959363ms)
Jun 26 14:57:06.943: INFO: (5) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname1/proxy/: foo (200; 10.381013ms)
Jun 26 14:57:06.945: INFO: (5) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname2/proxy/: bar (200; 12.620863ms)
Jun 26 14:57:06.951: INFO: (6) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">... (200; 5.665457ms)
Jun 26 14:57:06.952: INFO: (6) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/rewriteme">test</a> (200; 6.218983ms)
Jun 26 14:57:06.953: INFO: (6) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:462/proxy/: tls qux (200; 7.032129ms)
Jun 26 14:57:06.953: INFO: (6) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 7.517618ms)
Jun 26 14:57:06.955: INFO: (6) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 8.78313ms)
Jun 26 14:57:06.955: INFO: (6) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:460/proxy/: tls baz (200; 9.273332ms)
Jun 26 14:57:06.955: INFO: (6) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 9.285218ms)
Jun 26 14:57:06.956: INFO: (6) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 10.013314ms)
Jun 26 14:57:06.956: INFO: (6) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">test<... (200; 10.392287ms)
Jun 26 14:57:06.957: INFO: (6) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/tlsrewritem... (200; 11.060221ms)
Jun 26 14:57:06.960: INFO: (6) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname2/proxy/: bar (200; 14.111063ms)
Jun 26 14:57:06.960: INFO: (6) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname1/proxy/: tls baz (200; 14.646415ms)
Jun 26 14:57:06.961: INFO: (6) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname1/proxy/: foo (200; 14.68998ms)
Jun 26 14:57:06.961: INFO: (6) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname2/proxy/: bar (200; 15.586895ms)
Jun 26 14:57:06.964: INFO: (6) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname1/proxy/: foo (200; 18.543235ms)
Jun 26 14:57:06.965: INFO: (6) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname2/proxy/: tls qux (200; 18.912854ms)
Jun 26 14:57:06.971: INFO: (7) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">test<... (200; 5.151015ms)
Jun 26 14:57:06.971: INFO: (7) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:462/proxy/: tls qux (200; 5.253483ms)
Jun 26 14:57:06.971: INFO: (7) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">... (200; 5.523259ms)
Jun 26 14:57:06.971: INFO: (7) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 5.882466ms)
Jun 26 14:57:06.971: INFO: (7) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 6.15498ms)
Jun 26 14:57:06.971: INFO: (7) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/tlsrewritem... (200; 5.797339ms)
Jun 26 14:57:06.971: INFO: (7) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:460/proxy/: tls baz (200; 5.439862ms)
Jun 26 14:57:06.971: INFO: (7) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 6.053359ms)
Jun 26 14:57:06.973: INFO: (7) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname2/proxy/: bar (200; 8.284686ms)
Jun 26 14:57:06.973: INFO: (7) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/rewriteme">test</a> (200; 8.461012ms)
Jun 26 14:57:06.974: INFO: (7) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname2/proxy/: bar (200; 8.298288ms)
Jun 26 14:57:06.974: INFO: (7) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname1/proxy/: foo (200; 8.445743ms)
Jun 26 14:57:06.974: INFO: (7) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 8.188026ms)
Jun 26 14:57:06.974: INFO: (7) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname2/proxy/: tls qux (200; 8.668272ms)
Jun 26 14:57:06.974: INFO: (7) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname1/proxy/: foo (200; 8.851928ms)
Jun 26 14:57:06.974: INFO: (7) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname1/proxy/: tls baz (200; 8.382434ms)
Jun 26 14:57:06.981: INFO: (8) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">... (200; 6.061767ms)
Jun 26 14:57:06.981: INFO: (8) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 6.638634ms)
Jun 26 14:57:06.981: INFO: (8) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 6.553715ms)
Jun 26 14:57:06.981: INFO: (8) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:460/proxy/: tls baz (200; 7.425633ms)
Jun 26 14:57:06.982: INFO: (8) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:462/proxy/: tls qux (200; 7.68276ms)
Jun 26 14:57:06.982: INFO: (8) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/tlsrewritem... (200; 7.944567ms)
Jun 26 14:57:06.983: INFO: (8) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/rewriteme">test</a> (200; 7.775584ms)
Jun 26 14:57:06.983: INFO: (8) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">test<... (200; 7.888725ms)
Jun 26 14:57:06.983: INFO: (8) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 8.098648ms)
Jun 26 14:57:06.983: INFO: (8) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname2/proxy/: bar (200; 9.009802ms)
Jun 26 14:57:06.983: INFO: (8) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 8.06278ms)
Jun 26 14:57:06.984: INFO: (8) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname1/proxy/: tls baz (200; 8.766007ms)
Jun 26 14:57:06.984: INFO: (8) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname1/proxy/: foo (200; 9.858944ms)
Jun 26 14:57:06.984: INFO: (8) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname2/proxy/: bar (200; 9.251163ms)
Jun 26 14:57:06.985: INFO: (8) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname2/proxy/: tls qux (200; 9.670276ms)
Jun 26 14:57:06.985: INFO: (8) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname1/proxy/: foo (200; 9.947921ms)
Jun 26 14:57:06.988: INFO: (9) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:462/proxy/: tls qux (200; 2.920539ms)
Jun 26 14:57:06.989: INFO: (9) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 4.242489ms)
Jun 26 14:57:06.989: INFO: (9) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">... (200; 3.97865ms)
Jun 26 14:57:06.990: INFO: (9) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/tlsrewritem... (200; 4.457619ms)
Jun 26 14:57:06.990: INFO: (9) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">test<... (200; 4.67891ms)
Jun 26 14:57:06.991: INFO: (9) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 5.867043ms)
Jun 26 14:57:06.991: INFO: (9) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 6.240348ms)
Jun 26 14:57:06.991: INFO: (9) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:460/proxy/: tls baz (200; 5.923954ms)
Jun 26 14:57:06.993: INFO: (9) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/rewriteme">test</a> (200; 7.738279ms)
Jun 26 14:57:06.993: INFO: (9) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname1/proxy/: foo (200; 8.640703ms)
Jun 26 14:57:06.994: INFO: (9) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname1/proxy/: tls baz (200; 8.749436ms)
Jun 26 14:57:06.994: INFO: (9) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 8.5781ms)
Jun 26 14:57:06.994: INFO: (9) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname2/proxy/: tls qux (200; 8.603701ms)
Jun 26 14:57:06.994: INFO: (9) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname2/proxy/: bar (200; 8.917171ms)
Jun 26 14:57:06.995: INFO: (9) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname2/proxy/: bar (200; 9.771047ms)
Jun 26 14:57:06.995: INFO: (9) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname1/proxy/: foo (200; 9.643019ms)
Jun 26 14:57:07.001: INFO: (10) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 5.982877ms)
Jun 26 14:57:07.001: INFO: (10) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">... (200; 5.442157ms)
Jun 26 14:57:07.002: INFO: (10) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 5.00678ms)
Jun 26 14:57:07.002: INFO: (10) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:460/proxy/: tls baz (200; 5.480923ms)
Jun 26 14:57:07.002: INFO: (10) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 5.630435ms)
Jun 26 14:57:07.002: INFO: (10) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">test<... (200; 5.598012ms)
Jun 26 14:57:07.003: INFO: (10) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/tlsrewritem... (200; 7.076575ms)
Jun 26 14:57:07.003: INFO: (10) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/rewriteme">test</a> (200; 7.238602ms)
Jun 26 14:57:07.003: INFO: (10) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:462/proxy/: tls qux (200; 6.274211ms)
Jun 26 14:57:07.003: INFO: (10) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 6.752315ms)
Jun 26 14:57:07.004: INFO: (10) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname2/proxy/: tls qux (200; 9.261717ms)
Jun 26 14:57:07.005: INFO: (10) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname1/proxy/: foo (200; 8.974901ms)
Jun 26 14:57:07.005: INFO: (10) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname1/proxy/: tls baz (200; 8.439377ms)
Jun 26 14:57:07.005: INFO: (10) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname1/proxy/: foo (200; 8.9573ms)
Jun 26 14:57:07.005: INFO: (10) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname2/proxy/: bar (200; 9.297817ms)
Jun 26 14:57:07.006: INFO: (10) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname2/proxy/: bar (200; 10.231769ms)
Jun 26 14:57:07.008: INFO: (11) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 2.270884ms)
Jun 26 14:57:07.012: INFO: (11) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">test<... (200; 5.218849ms)
Jun 26 14:57:07.012: INFO: (11) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/rewriteme">test</a> (200; 5.047718ms)
Jun 26 14:57:07.013: INFO: (11) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:460/proxy/: tls baz (200; 6.158236ms)
Jun 26 14:57:07.013: INFO: (11) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 6.359638ms)
Jun 26 14:57:07.013: INFO: (11) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">... (200; 5.350069ms)
Jun 26 14:57:07.013: INFO: (11) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:462/proxy/: tls qux (200; 6.315983ms)
Jun 26 14:57:07.013: INFO: (11) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 6.222114ms)
Jun 26 14:57:07.014: INFO: (11) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 6.776959ms)
Jun 26 14:57:07.014: INFO: (11) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/tlsrewritem... (200; 6.305237ms)
Jun 26 14:57:07.015: INFO: (11) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname2/proxy/: tls qux (200; 7.390309ms)
Jun 26 14:57:07.015: INFO: (11) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname1/proxy/: foo (200; 7.694679ms)
Jun 26 14:57:07.015: INFO: (11) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname2/proxy/: bar (200; 7.885498ms)
Jun 26 14:57:07.015: INFO: (11) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname2/proxy/: bar (200; 8.112895ms)
Jun 26 14:57:07.016: INFO: (11) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname1/proxy/: foo (200; 9.364272ms)
Jun 26 14:57:07.016: INFO: (11) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname1/proxy/: tls baz (200; 8.892278ms)
Jun 26 14:57:07.021: INFO: (12) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname2/proxy/: bar (200; 4.980031ms)
Jun 26 14:57:07.021: INFO: (12) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:462/proxy/: tls qux (200; 5.035127ms)
Jun 26 14:57:07.021: INFO: (12) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 5.112559ms)
Jun 26 14:57:07.024: INFO: (12) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/rewriteme">test</a> (200; 7.705905ms)
Jun 26 14:57:07.024: INFO: (12) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 8.259705ms)
Jun 26 14:57:07.024: INFO: (12) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 7.770492ms)
Jun 26 14:57:07.025: INFO: (12) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname2/proxy/: bar (200; 8.351107ms)
Jun 26 14:57:07.025: INFO: (12) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname2/proxy/: tls qux (200; 8.557156ms)
Jun 26 14:57:07.025: INFO: (12) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:460/proxy/: tls baz (200; 9.038018ms)
Jun 26 14:57:07.025: INFO: (12) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">... (200; 8.832592ms)
Jun 26 14:57:07.025: INFO: (12) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/tlsrewritem... (200; 8.70422ms)
Jun 26 14:57:07.025: INFO: (12) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname1/proxy/: tls baz (200; 9.115119ms)
Jun 26 14:57:07.026: INFO: (12) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">test<... (200; 9.559777ms)
Jun 26 14:57:07.026: INFO: (12) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 9.696374ms)
Jun 26 14:57:07.027: INFO: (12) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname1/proxy/: foo (200; 10.884344ms)
Jun 26 14:57:07.027: INFO: (12) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname1/proxy/: foo (200; 10.933357ms)
Jun 26 14:57:07.033: INFO: (13) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">test<... (200; 5.280794ms)
Jun 26 14:57:07.033: INFO: (13) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 4.460544ms)
Jun 26 14:57:07.033: INFO: (13) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:462/proxy/: tls qux (200; 6.123963ms)
Jun 26 14:57:07.034: INFO: (13) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 5.824985ms)
Jun 26 14:57:07.034: INFO: (13) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:460/proxy/: tls baz (200; 5.096116ms)
Jun 26 14:57:07.034: INFO: (13) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">... (200; 5.744517ms)
Jun 26 14:57:07.035: INFO: (13) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 7.177297ms)
Jun 26 14:57:07.035: INFO: (13) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 7.22856ms)
Jun 26 14:57:07.035: INFO: (13) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/tlsrewritem... (200; 6.896641ms)
Jun 26 14:57:07.035: INFO: (13) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/rewriteme">test</a> (200; 7.028722ms)
Jun 26 14:57:07.038: INFO: (13) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname1/proxy/: tls baz (200; 10.721786ms)
Jun 26 14:57:07.039: INFO: (13) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname2/proxy/: bar (200; 10.830595ms)
Jun 26 14:57:07.039: INFO: (13) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname2/proxy/: tls qux (200; 11.431946ms)
Jun 26 14:57:07.039: INFO: (13) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname1/proxy/: foo (200; 11.012669ms)
Jun 26 14:57:07.039: INFO: (13) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname1/proxy/: foo (200; 10.601844ms)
Jun 26 14:57:07.039: INFO: (13) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname2/proxy/: bar (200; 10.845876ms)
Jun 26 14:57:07.044: INFO: (14) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:460/proxy/: tls baz (200; 4.567459ms)
Jun 26 14:57:07.045: INFO: (14) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 5.107314ms)
Jun 26 14:57:07.046: INFO: (14) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname2/proxy/: bar (200; 6.027081ms)
Jun 26 14:57:07.046: INFO: (14) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 5.8831ms)
Jun 26 14:57:07.047: INFO: (14) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/tlsrewritem... (200; 6.080821ms)
Jun 26 14:57:07.047: INFO: (14) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 5.502426ms)
Jun 26 14:57:07.047: INFO: (14) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 6.650592ms)
Jun 26 14:57:07.047: INFO: (14) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">test<... (200; 6.336145ms)
Jun 26 14:57:07.047: INFO: (14) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:462/proxy/: tls qux (200; 7.132303ms)
Jun 26 14:57:07.047: INFO: (14) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname1/proxy/: foo (200; 7.028104ms)
Jun 26 14:57:07.047: INFO: (14) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">... (200; 5.800329ms)
Jun 26 14:57:07.047: INFO: (14) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/rewriteme">test</a> (200; 6.07477ms)
Jun 26 14:57:07.050: INFO: (14) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname2/proxy/: tls qux (200; 8.991607ms)
Jun 26 14:57:07.050: INFO: (14) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname1/proxy/: foo (200; 8.69824ms)
Jun 26 14:57:07.050: INFO: (14) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname1/proxy/: tls baz (200; 10.000798ms)
Jun 26 14:57:07.051: INFO: (14) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname2/proxy/: bar (200; 9.436325ms)
Jun 26 14:57:07.057: INFO: (15) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/rewriteme">test</a> (200; 6.275544ms)
Jun 26 14:57:07.057: INFO: (15) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 6.762232ms)
Jun 26 14:57:07.058: INFO: (15) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 7.665132ms)
Jun 26 14:57:07.059: INFO: (15) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 7.696949ms)
Jun 26 14:57:07.059: INFO: (15) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:460/proxy/: tls baz (200; 7.897602ms)
Jun 26 14:57:07.059: INFO: (15) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:462/proxy/: tls qux (200; 8.49473ms)
Jun 26 14:57:07.060: INFO: (15) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/tlsrewritem... (200; 9.387596ms)
Jun 26 14:57:07.061: INFO: (15) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 10.076848ms)
Jun 26 14:57:07.062: INFO: (15) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname2/proxy/: bar (200; 11.21919ms)
Jun 26 14:57:07.062: INFO: (15) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname1/proxy/: foo (200; 11.612554ms)
Jun 26 14:57:07.062: INFO: (15) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname2/proxy/: bar (200; 11.180565ms)
Jun 26 14:57:07.063: INFO: (15) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">... (200; 11.963578ms)
Jun 26 14:57:07.063: INFO: (15) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname2/proxy/: tls qux (200; 12.155016ms)
Jun 26 14:57:07.063: INFO: (15) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">test<... (200; 12.641014ms)
Jun 26 14:57:07.064: INFO: (15) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname1/proxy/: foo (200; 12.454192ms)
Jun 26 14:57:07.064: INFO: (15) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname1/proxy/: tls baz (200; 13.06598ms)
Jun 26 14:57:07.073: INFO: (16) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 8.067491ms)
Jun 26 14:57:07.073: INFO: (16) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname2/proxy/: bar (200; 8.528812ms)
Jun 26 14:57:07.073: INFO: (16) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/tlsrewritem... (200; 8.772202ms)
Jun 26 14:57:07.073: INFO: (16) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname1/proxy/: foo (200; 8.725327ms)
Jun 26 14:57:07.073: INFO: (16) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 8.108018ms)
Jun 26 14:57:07.074: INFO: (16) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 7.917512ms)
Jun 26 14:57:07.074: INFO: (16) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname2/proxy/: bar (200; 9.143039ms)
Jun 26 14:57:07.074: INFO: (16) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:460/proxy/: tls baz (200; 9.022656ms)
Jun 26 14:57:07.074: INFO: (16) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/rewriteme">test</a> (200; 8.140989ms)
Jun 26 14:57:07.074: INFO: (16) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:462/proxy/: tls qux (200; 8.870649ms)
Jun 26 14:57:07.074: INFO: (16) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">... (200; 9.479161ms)
Jun 26 14:57:07.074: INFO: (16) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname1/proxy/: foo (200; 9.289782ms)
Jun 26 14:57:07.075: INFO: (16) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname2/proxy/: tls qux (200; 9.717425ms)
Jun 26 14:57:07.076: INFO: (16) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 10.062502ms)
Jun 26 14:57:07.076: INFO: (16) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">test<... (200; 10.248608ms)
Jun 26 14:57:07.076: INFO: (16) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname1/proxy/: tls baz (200; 10.655008ms)
Jun 26 14:57:07.080: INFO: (17) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 4.072437ms)
Jun 26 14:57:07.081: INFO: (17) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">test<... (200; 4.467125ms)
Jun 26 14:57:07.081: INFO: (17) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/tlsrewritem... (200; 4.442375ms)
Jun 26 14:57:07.085: INFO: (17) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 8.075939ms)
Jun 26 14:57:07.087: INFO: (17) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 9.963505ms)
Jun 26 14:57:07.087: INFO: (17) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/rewriteme">test</a> (200; 10.532006ms)
Jun 26 14:57:07.087: INFO: (17) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:462/proxy/: tls qux (200; 9.656586ms)
Jun 26 14:57:07.088: INFO: (17) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname2/proxy/: bar (200; 11.26147ms)
Jun 26 14:57:07.088: INFO: (17) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname2/proxy/: bar (200; 11.798747ms)
Jun 26 14:57:07.088: INFO: (17) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname1/proxy/: foo (200; 11.583956ms)
Jun 26 14:57:07.089: INFO: (17) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname2/proxy/: tls qux (200; 12.590513ms)
Jun 26 14:57:07.090: INFO: (17) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname1/proxy/: foo (200; 13.424095ms)
Jun 26 14:57:07.091: INFO: (17) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:460/proxy/: tls baz (200; 13.754693ms)
Jun 26 14:57:07.091: INFO: (17) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname1/proxy/: tls baz (200; 14.557762ms)
Jun 26 14:57:07.091: INFO: (17) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">... (200; 14.757206ms)
Jun 26 14:57:07.091: INFO: (17) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 14.729199ms)
Jun 26 14:57:07.101: INFO: (18) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/rewriteme">test</a> (200; 7.59966ms)
Jun 26 14:57:07.102: INFO: (18) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 7.893355ms)
Jun 26 14:57:07.102: INFO: (18) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/tlsrewritem... (200; 10.803424ms)
Jun 26 14:57:07.102: INFO: (18) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 9.623782ms)
Jun 26 14:57:07.103: INFO: (18) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:462/proxy/: tls qux (200; 10.11097ms)
Jun 26 14:57:07.103: INFO: (18) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 10.654532ms)
Jun 26 14:57:07.104: INFO: (18) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">... (200; 12.070846ms)
Jun 26 14:57:07.104: INFO: (18) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 10.928197ms)
Jun 26 14:57:07.104: INFO: (18) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname2/proxy/: tls qux (200; 10.713626ms)
Jun 26 14:57:07.104: INFO: (18) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname1/proxy/: foo (200; 12.736368ms)
Jun 26 14:57:07.105: INFO: (18) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">test<... (200; 11.135357ms)
Jun 26 14:57:07.105: INFO: (18) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname1/proxy/: tls baz (200; 11.31219ms)
Jun 26 14:57:07.105: INFO: (18) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname1/proxy/: foo (200; 12.754931ms)
Jun 26 14:57:07.107: INFO: (18) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname2/proxy/: bar (200; 14.959882ms)
Jun 26 14:57:07.108: INFO: (18) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname2/proxy/: bar (200; 15.659755ms)
Jun 26 14:57:07.108: INFO: (18) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:460/proxy/: tls baz (200; 15.1236ms)
Jun 26 14:57:07.111: INFO: (19) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:443/proxy/tlsrewritem... (200; 3.264502ms)
Jun 26 14:57:07.112: INFO: (19) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">... (200; 3.686778ms)
Jun 26 14:57:07.115: INFO: (19) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 5.971178ms)
Jun 26 14:57:07.115: INFO: (19) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv/proxy/rewriteme">test</a> (200; 4.875925ms)
Jun 26 14:57:07.117: INFO: (19) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname1/proxy/: tls baz (200; 7.839244ms)
Jun 26 14:57:07.117: INFO: (19) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname2/proxy/: bar (200; 8.915849ms)
Jun 26 14:57:07.118: INFO: (19) /api/v1/namespaces/proxy-1970/services/proxy-service-l5prf:portname1/proxy/: foo (200; 9.198876ms)
Jun 26 14:57:07.119: INFO: (19) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:462/proxy/: tls qux (200; 9.266724ms)
Jun 26 14:57:07.119: INFO: (19) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname1/proxy/: foo (200; 10.322295ms)
Jun 26 14:57:07.119: INFO: (19) /api/v1/namespaces/proxy-1970/services/http:proxy-service-l5prf:portname2/proxy/: bar (200; 10.12517ms)
Jun 26 14:57:07.119: INFO: (19) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 9.938038ms)
Jun 26 14:57:07.119: INFO: (19) /api/v1/namespaces/proxy-1970/pods/https:proxy-service-l5prf-7fdvv:460/proxy/: tls baz (200; 9.927138ms)
Jun 26 14:57:07.119: INFO: (19) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:162/proxy/: bar (200; 9.432142ms)
Jun 26 14:57:07.118: INFO: (19) /api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/: <a href="/api/v1/namespaces/proxy-1970/pods/proxy-service-l5prf-7fdvv:1080/proxy/rewriteme">test<... (200; 8.610731ms)
Jun 26 14:57:07.119: INFO: (19) /api/v1/namespaces/proxy-1970/services/https:proxy-service-l5prf:tlsportname2/proxy/: tls qux (200; 9.587071ms)
Jun 26 14:57:07.120: INFO: (19) /api/v1/namespaces/proxy-1970/pods/http:proxy-service-l5prf-7fdvv:160/proxy/: foo (200; 10.695607ms)
STEP: deleting ReplicationController proxy-service-l5prf in namespace proxy-1970, will wait for the garbage collector to delete the pods
Jun 26 14:57:07.182: INFO: Deleting ReplicationController proxy-service-l5prf took: 8.320607ms
Jun 26 14:57:07.482: INFO: Terminating ReplicationController proxy-service-l5prf pods took: 300.500955ms
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:57:12.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1970" for this suite.
Jun 26 14:57:18.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:57:18.275: INFO: namespace proxy-1970 deletion completed in 6.087675442s

• [SLOW TEST:26.539 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:57:18.276: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-12ce1614-a0c2-4cd1-a9bb-aa3823392e77
STEP: Creating a pod to test consume configMaps
Jun 26 14:57:18.317: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-a5f8679a-b054-4fb0-9480-85e9613d63b6" in namespace "projected-4220" to be "success or failure"
Jun 26 14:57:18.321: INFO: Pod "pod-projected-configmaps-a5f8679a-b054-4fb0-9480-85e9613d63b6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.618602ms
Jun 26 14:57:20.325: INFO: Pod "pod-projected-configmaps-a5f8679a-b054-4fb0-9480-85e9613d63b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007688043s
STEP: Saw pod success
Jun 26 14:57:20.325: INFO: Pod "pod-projected-configmaps-a5f8679a-b054-4fb0-9480-85e9613d63b6" satisfied condition "success or failure"
Jun 26 14:57:20.327: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-projected-configmaps-a5f8679a-b054-4fb0-9480-85e9613d63b6 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 26 14:57:20.361: INFO: Waiting for pod pod-projected-configmaps-a5f8679a-b054-4fb0-9480-85e9613d63b6 to disappear
Jun 26 14:57:20.366: INFO: Pod pod-projected-configmaps-a5f8679a-b054-4fb0-9480-85e9613d63b6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:57:20.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4220" for this suite.
Jun 26 14:57:26.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:57:26.461: INFO: namespace projected-4220 deletion completed in 6.08876096s

• [SLOW TEST:8.186 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:57:26.463: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0626 14:58:06.543263      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 26 14:58:06.544: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:58:06.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7750" for this suite.
Jun 26 14:58:14.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:58:14.662: INFO: namespace gc-7750 deletion completed in 8.106145488s

• [SLOW TEST:48.199 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:58:14.665: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-b9bf3799-4bfa-4487-bdb0-1deda26735f2
STEP: Creating a pod to test consume secrets
Jun 26 14:58:14.705: INFO: Waiting up to 5m0s for pod "pod-secrets-6d8bda56-7863-4443-b896-7a1e4b61b09a" in namespace "secrets-3196" to be "success or failure"
Jun 26 14:58:14.708: INFO: Pod "pod-secrets-6d8bda56-7863-4443-b896-7a1e4b61b09a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.374555ms
Jun 26 14:58:16.711: INFO: Pod "pod-secrets-6d8bda56-7863-4443-b896-7a1e4b61b09a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006259361s
STEP: Saw pod success
Jun 26 14:58:16.711: INFO: Pod "pod-secrets-6d8bda56-7863-4443-b896-7a1e4b61b09a" satisfied condition "success or failure"
Jun 26 14:58:16.714: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-secrets-6d8bda56-7863-4443-b896-7a1e4b61b09a container secret-volume-test: <nil>
STEP: delete the pod
Jun 26 14:58:16.730: INFO: Waiting for pod pod-secrets-6d8bda56-7863-4443-b896-7a1e4b61b09a to disappear
Jun 26 14:58:16.732: INFO: Pod pod-secrets-6d8bda56-7863-4443-b896-7a1e4b61b09a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:58:16.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3196" for this suite.
Jun 26 14:58:22.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:58:22.821: INFO: namespace secrets-3196 deletion completed in 6.085675842s

• [SLOW TEST:8.157 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:58:22.823: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-9624c014-5e7f-404f-986d-84f101daca36
STEP: Creating a pod to test consume configMaps
Jun 26 14:58:22.860: INFO: Waiting up to 5m0s for pod "pod-configmaps-35e3696e-6f29-4dc6-a8fd-fcb3f4f2af0d" in namespace "configmap-2696" to be "success or failure"
Jun 26 14:58:22.862: INFO: Pod "pod-configmaps-35e3696e-6f29-4dc6-a8fd-fcb3f4f2af0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.642394ms
Jun 26 14:58:24.866: INFO: Pod "pod-configmaps-35e3696e-6f29-4dc6-a8fd-fcb3f4f2af0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006037218s
STEP: Saw pod success
Jun 26 14:58:24.866: INFO: Pod "pod-configmaps-35e3696e-6f29-4dc6-a8fd-fcb3f4f2af0d" satisfied condition "success or failure"
Jun 26 14:58:24.868: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-configmaps-35e3696e-6f29-4dc6-a8fd-fcb3f4f2af0d container configmap-volume-test: <nil>
STEP: delete the pod
Jun 26 14:58:24.886: INFO: Waiting for pod pod-configmaps-35e3696e-6f29-4dc6-a8fd-fcb3f4f2af0d to disappear
Jun 26 14:58:24.889: INFO: Pod pod-configmaps-35e3696e-6f29-4dc6-a8fd-fcb3f4f2af0d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:58:24.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2696" for this suite.
Jun 26 14:58:30.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:58:30.988: INFO: namespace configmap-2696 deletion completed in 6.094866366s

• [SLOW TEST:8.165 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:58:30.988: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jun 26 14:58:31.015: INFO: PodSpec: initContainers in spec.initContainers
Jun 26 14:59:16.489: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-bfe581b5-49d3-4008-8b12-e855be0253b3", GenerateName:"", Namespace:"init-container-8312", SelfLink:"/api/v1/namespaces/init-container-8312/pods/pod-init-bfe581b5-49d3-4008-8b12-e855be0253b3", UID:"6d5186c8-57d5-4b54-b7af-ffe608b9e02b", ResourceVersion:"23086", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63697157911, loc:(*time.Location)(0x80bb5c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"15022218"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"172.16.146.198/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-tjw64", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0023bc040), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-tjw64", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-tjw64", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-tjw64", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002e440c8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-worker-2-dev", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001f1a300), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002e44210)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002e44230)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002e44238), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002e4423c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697157911, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697157911, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697157911, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697157911, loc:(*time.Location)(0x80bb5c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.9.247", PodIP:"172.16.146.198", StartTime:(*v1.Time)(0xc0033b80c0), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000d7a070)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000d7a0e0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://55c69bfce824d7d0231c6d4f7a3bf6d658f0722762150209719631e8d50c31b3"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0033b8100), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0033b80e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:59:16.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8312" for this suite.
Jun 26 14:59:38.528: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:59:38.622: INFO: namespace init-container-8312 deletion completed in 22.11293046s

• [SLOW TEST:67.633 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:59:38.622: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-57eba6b7-371c-42c0-8016-a8b5469790ed
STEP: Creating a pod to test consume configMaps
Jun 26 14:59:38.676: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fb208a30-5d52-4519-9ecb-bd4840294116" in namespace "projected-9461" to be "success or failure"
Jun 26 14:59:38.680: INFO: Pod "pod-projected-configmaps-fb208a30-5d52-4519-9ecb-bd4840294116": Phase="Pending", Reason="", readiness=false. Elapsed: 3.688891ms
Jun 26 14:59:40.688: INFO: Pod "pod-projected-configmaps-fb208a30-5d52-4519-9ecb-bd4840294116": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011705386s
STEP: Saw pod success
Jun 26 14:59:40.688: INFO: Pod "pod-projected-configmaps-fb208a30-5d52-4519-9ecb-bd4840294116" satisfied condition "success or failure"
Jun 26 14:59:40.693: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-projected-configmaps-fb208a30-5d52-4519-9ecb-bd4840294116 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 26 14:59:40.739: INFO: Waiting for pod pod-projected-configmaps-fb208a30-5d52-4519-9ecb-bd4840294116 to disappear
Jun 26 14:59:40.749: INFO: Pod pod-projected-configmaps-fb208a30-5d52-4519-9ecb-bd4840294116 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 14:59:40.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9461" for this suite.
Jun 26 14:59:46.785: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 14:59:46.993: INFO: namespace projected-9461 deletion completed in 6.231917152s

• [SLOW TEST:8.371 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 14:59:46.994: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:00:07.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3396" for this suite.
Jun 26 15:00:51.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:00:51.318: INFO: namespace kubelet-test-3396 deletion completed in 44.21017128s

• [SLOW TEST:64.325 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:00:51.324: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 26 15:00:51.454: INFO: Creating ReplicaSet my-hostname-basic-86573542-9535-42d7-9413-cb0daabee18a
Jun 26 15:00:51.474: INFO: Pod name my-hostname-basic-86573542-9535-42d7-9413-cb0daabee18a: Found 0 pods out of 1
Jun 26 15:00:56.479: INFO: Pod name my-hostname-basic-86573542-9535-42d7-9413-cb0daabee18a: Found 1 pods out of 1
Jun 26 15:00:56.479: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-86573542-9535-42d7-9413-cb0daabee18a" is running
Jun 26 15:00:56.481: INFO: Pod "my-hostname-basic-86573542-9535-42d7-9413-cb0daabee18a-prgfm" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-26 15:00:51 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-26 15:00:56 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-26 15:00:56 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-26 15:00:51 +0000 UTC Reason: Message:}])
Jun 26 15:00:56.481: INFO: Trying to dial the pod
Jun 26 15:01:01.493: INFO: Controller my-hostname-basic-86573542-9535-42d7-9413-cb0daabee18a: Got expected result from replica 1 [my-hostname-basic-86573542-9535-42d7-9413-cb0daabee18a-prgfm]: "my-hostname-basic-86573542-9535-42d7-9413-cb0daabee18a-prgfm", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:01:01.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2894" for this suite.
Jun 26 15:01:07.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:01:07.592: INFO: namespace replicaset-2894 deletion completed in 6.09240616s

• [SLOW TEST:16.269 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:01:07.594: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 26 15:01:12.152: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ea26fddf-4139-4536-bd4d-ff99fca600bd"
Jun 26 15:01:12.152: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-ea26fddf-4139-4536-bd4d-ff99fca600bd" in namespace "pods-3749" to be "terminated due to deadline exceeded"
Jun 26 15:01:12.155: INFO: Pod "pod-update-activedeadlineseconds-ea26fddf-4139-4536-bd4d-ff99fca600bd": Phase="Running", Reason="", readiness=true. Elapsed: 2.75829ms
Jun 26 15:01:14.160: INFO: Pod "pod-update-activedeadlineseconds-ea26fddf-4139-4536-bd4d-ff99fca600bd": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.007339266s
Jun 26 15:01:14.160: INFO: Pod "pod-update-activedeadlineseconds-ea26fddf-4139-4536-bd4d-ff99fca600bd" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:01:14.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3749" for this suite.
Jun 26 15:01:20.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:01:20.261: INFO: namespace pods-3749 deletion completed in 6.094067969s

• [SLOW TEST:12.668 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:01:20.263: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 26 15:01:20.327: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 15:01:20.331: INFO: Number of nodes with available pods: 0
Jun 26 15:01:20.331: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:01:21.341: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 15:01:21.349: INFO: Number of nodes with available pods: 0
Jun 26 15:01:21.349: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:01:22.340: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 15:01:22.347: INFO: Number of nodes with available pods: 2
Jun 26 15:01:22.347: INFO: Node k8s-worker-2-dev is running more than one daemon pod
Jun 26 15:01:23.340: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 15:01:23.347: INFO: Number of nodes with available pods: 3
Jun 26 15:01:23.347: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun 26 15:01:23.398: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 15:01:23.409: INFO: Number of nodes with available pods: 2
Jun 26 15:01:23.410: INFO: Node k8s-worker-2-dev is running more than one daemon pod
Jun 26 15:01:24.415: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 15:01:24.419: INFO: Number of nodes with available pods: 2
Jun 26 15:01:24.419: INFO: Node k8s-worker-2-dev is running more than one daemon pod
Jun 26 15:01:25.416: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 15:01:25.419: INFO: Number of nodes with available pods: 2
Jun 26 15:01:25.419: INFO: Node k8s-worker-2-dev is running more than one daemon pod
Jun 26 15:01:26.415: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 15:01:26.418: INFO: Number of nodes with available pods: 3
Jun 26 15:01:26.419: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2401, will wait for the garbage collector to delete the pods
Jun 26 15:01:26.485: INFO: Deleting DaemonSet.extensions daemon-set took: 8.028993ms
Jun 26 15:01:26.785: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.343005ms
Jun 26 15:01:32.192: INFO: Number of nodes with available pods: 0
Jun 26 15:01:32.192: INFO: Number of running nodes: 0, number of available pods: 0
Jun 26 15:01:32.196: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2401/daemonsets","resourceVersion":"23556"},"items":null}

Jun 26 15:01:32.200: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2401/pods","resourceVersion":"23556"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:01:32.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2401" for this suite.
Jun 26 15:01:38.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:01:38.321: INFO: namespace daemonsets-2401 deletion completed in 6.095001233s

• [SLOW TEST:18.058 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:01:38.322: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 26 15:01:40.376: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:01:40.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7732" for this suite.
Jun 26 15:01:46.406: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:01:46.485: INFO: namespace container-runtime-7732 deletion completed in 6.091633011s

• [SLOW TEST:8.164 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:01:46.487: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jun 26 15:01:49.050: INFO: Successfully updated pod "labelsupdateb176eff8-88ca-40f5-a0a6-f9c704b0cbd4"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:01:53.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1812" for this suite.
Jun 26 15:02:15.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:02:15.294: INFO: namespace downward-api-1812 deletion completed in 22.214686427s

• [SLOW TEST:28.808 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:02:15.297: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun 26 15:02:15.406: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-8445,SelfLink:/api/v1/namespaces/watch-8445/configmaps/e2e-watch-test-resource-version,UID:98d924b5-3341-4839-8934-b1f9f343de58,ResourceVersion:23720,Generation:0,CreationTimestamp:2019-06-26 15:02:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 26 15:02:15.407: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-8445,SelfLink:/api/v1/namespaces/watch-8445/configmaps/e2e-watch-test-resource-version,UID:98d924b5-3341-4839-8934-b1f9f343de58,ResourceVersion:23721,Generation:0,CreationTimestamp:2019-06-26 15:02:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:02:15.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-8445" for this suite.
Jun 26 15:02:21.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:02:21.511: INFO: namespace watch-8445 deletion completed in 6.095586886s

• [SLOW TEST:6.214 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:02:21.512: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jun 26 15:02:21.548: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Jun 26 15:02:22.438: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun 26 15:02:24.532: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 26 15:02:26.541: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 26 15:02:28.543: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 26 15:02:30.541: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 26 15:02:32.541: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 26 15:02:34.543: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 26 15:02:36.537: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 26 15:02:38.541: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697158142, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 26 15:02:42.184: INFO: Waited 1.625652921s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:02:42.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9164" for this suite.
Jun 26 15:02:48.781: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:02:48.985: INFO: namespace aggregator-9164 deletion completed in 6.301924016s

• [SLOW TEST:27.474 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:02:48.992: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 26 15:02:49.105: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun 26 15:02:49.121: INFO: Number of nodes with available pods: 0
Jun 26 15:02:49.121: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jun 26 15:02:49.172: INFO: Number of nodes with available pods: 0
Jun 26 15:02:49.181: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:02:50.185: INFO: Number of nodes with available pods: 0
Jun 26 15:02:50.185: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:02:51.186: INFO: Number of nodes with available pods: 1
Jun 26 15:02:51.186: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun 26 15:02:51.207: INFO: Number of nodes with available pods: 1
Jun 26 15:02:51.207: INFO: Number of running nodes: 0, number of available pods: 1
Jun 26 15:02:52.211: INFO: Number of nodes with available pods: 0
Jun 26 15:02:52.211: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun 26 15:02:52.218: INFO: Number of nodes with available pods: 0
Jun 26 15:02:52.218: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:02:53.221: INFO: Number of nodes with available pods: 0
Jun 26 15:02:53.221: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:02:54.224: INFO: Number of nodes with available pods: 0
Jun 26 15:02:54.224: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:02:55.227: INFO: Number of nodes with available pods: 0
Jun 26 15:02:55.227: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:02:56.225: INFO: Number of nodes with available pods: 0
Jun 26 15:02:56.225: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:02:57.224: INFO: Number of nodes with available pods: 0
Jun 26 15:02:57.225: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:02:58.225: INFO: Number of nodes with available pods: 0
Jun 26 15:02:58.226: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:02:59.228: INFO: Number of nodes with available pods: 0
Jun 26 15:02:59.228: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:03:00.225: INFO: Number of nodes with available pods: 0
Jun 26 15:03:00.225: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:03:01.221: INFO: Number of nodes with available pods: 0
Jun 26 15:03:01.221: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:03:02.225: INFO: Number of nodes with available pods: 0
Jun 26 15:03:02.225: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:03:03.226: INFO: Number of nodes with available pods: 0
Jun 26 15:03:03.227: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:03:04.222: INFO: Number of nodes with available pods: 1
Jun 26 15:03:04.222: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9232, will wait for the garbage collector to delete the pods
Jun 26 15:03:04.292: INFO: Deleting DaemonSet.extensions daemon-set took: 13.10126ms
Jun 26 15:03:04.592: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.689106ms
Jun 26 15:03:11.800: INFO: Number of nodes with available pods: 0
Jun 26 15:03:11.801: INFO: Number of running nodes: 0, number of available pods: 0
Jun 26 15:03:11.807: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9232/daemonsets","resourceVersion":"23971"},"items":null}

Jun 26 15:03:11.815: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9232/pods","resourceVersion":"23971"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:03:11.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9232" for this suite.
Jun 26 15:03:17.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:03:18.079: INFO: namespace daemonsets-9232 deletion completed in 6.196919423s

• [SLOW TEST:29.087 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:03:18.081: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun 26 15:03:30.218: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1737 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 26 15:03:30.218: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
Jun 26 15:03:30.435: INFO: Exec stderr: ""
Jun 26 15:03:30.435: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1737 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 26 15:03:30.435: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
Jun 26 15:03:30.620: INFO: Exec stderr: ""
Jun 26 15:03:30.620: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1737 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 26 15:03:30.620: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
Jun 26 15:03:30.869: INFO: Exec stderr: ""
Jun 26 15:03:30.869: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1737 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 26 15:03:30.869: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
Jun 26 15:03:31.273: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun 26 15:03:31.273: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1737 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 26 15:03:31.273: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
Jun 26 15:03:31.453: INFO: Exec stderr: ""
Jun 26 15:03:31.453: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1737 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 26 15:03:31.453: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
Jun 26 15:03:31.633: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun 26 15:03:31.633: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1737 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 26 15:03:31.633: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
Jun 26 15:03:31.856: INFO: Exec stderr: ""
Jun 26 15:03:31.856: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1737 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 26 15:03:31.856: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
Jun 26 15:03:32.294: INFO: Exec stderr: ""
Jun 26 15:03:32.294: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1737 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 26 15:03:32.294: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
Jun 26 15:03:32.487: INFO: Exec stderr: ""
Jun 26 15:03:32.487: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1737 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 26 15:03:32.487: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
Jun 26 15:03:32.685: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:03:32.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1737" for this suite.
Jun 26 15:04:10.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:04:10.781: INFO: namespace e2e-kubelet-etc-hosts-1737 deletion completed in 38.09144711s

• [SLOW TEST:52.700 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:04:10.783: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-9d5ff0db-bddd-4a2c-a2c1-ec9a256e6df0
STEP: Creating a pod to test consume secrets
Jun 26 15:04:10.822: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7a78a638-80e0-4803-9c73-c4b9eff18ed3" in namespace "projected-5032" to be "success or failure"
Jun 26 15:04:10.825: INFO: Pod "pod-projected-secrets-7a78a638-80e0-4803-9c73-c4b9eff18ed3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.777107ms
Jun 26 15:04:12.831: INFO: Pod "pod-projected-secrets-7a78a638-80e0-4803-9c73-c4b9eff18ed3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008511137s
STEP: Saw pod success
Jun 26 15:04:12.831: INFO: Pod "pod-projected-secrets-7a78a638-80e0-4803-9c73-c4b9eff18ed3" satisfied condition "success or failure"
Jun 26 15:04:12.839: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-projected-secrets-7a78a638-80e0-4803-9c73-c4b9eff18ed3 container secret-volume-test: <nil>
STEP: delete the pod
Jun 26 15:04:12.886: INFO: Waiting for pod pod-projected-secrets-7a78a638-80e0-4803-9c73-c4b9eff18ed3 to disappear
Jun 26 15:04:12.892: INFO: Pod pod-projected-secrets-7a78a638-80e0-4803-9c73-c4b9eff18ed3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:04:12.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5032" for this suite.
Jun 26 15:04:18.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:04:19.015: INFO: namespace projected-5032 deletion completed in 6.113072085s

• [SLOW TEST:8.233 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:04:19.016: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jun 26 15:04:21.603: INFO: Successfully updated pod "labelsupdate344ee085-5e13-4728-a9b0-2aeb961b3010"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:04:23.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1078" for this suite.
Jun 26 15:04:45.677: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:04:45.811: INFO: namespace projected-1078 deletion completed in 22.16383339s

• [SLOW TEST:26.795 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:04:45.811: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-0bc3205d-59f9-4937-b964-f0009166353e
STEP: Creating secret with name secret-projected-all-test-volume-d93ae849-4ec0-493d-bab1-de82789c0873
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun 26 15:04:45.857: INFO: Waiting up to 5m0s for pod "projected-volume-3313f6b0-ecbd-4717-a1e1-4029ee60d291" in namespace "projected-4390" to be "success or failure"
Jun 26 15:04:45.861: INFO: Pod "projected-volume-3313f6b0-ecbd-4717-a1e1-4029ee60d291": Phase="Pending", Reason="", readiness=false. Elapsed: 4.002635ms
Jun 26 15:04:47.865: INFO: Pod "projected-volume-3313f6b0-ecbd-4717-a1e1-4029ee60d291": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007706089s
STEP: Saw pod success
Jun 26 15:04:47.865: INFO: Pod "projected-volume-3313f6b0-ecbd-4717-a1e1-4029ee60d291" satisfied condition "success or failure"
Jun 26 15:04:47.867: INFO: Trying to get logs from node k8s-worker-2-dev pod projected-volume-3313f6b0-ecbd-4717-a1e1-4029ee60d291 container projected-all-volume-test: <nil>
STEP: delete the pod
Jun 26 15:04:47.885: INFO: Waiting for pod projected-volume-3313f6b0-ecbd-4717-a1e1-4029ee60d291 to disappear
Jun 26 15:04:47.887: INFO: Pod projected-volume-3313f6b0-ecbd-4717-a1e1-4029ee60d291 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:04:47.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4390" for this suite.
Jun 26 15:04:53.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:04:53.978: INFO: namespace projected-4390 deletion completed in 6.086719499s

• [SLOW TEST:8.167 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:04:53.978: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun 26 15:04:57.034: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:04:58.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5207" for this suite.
Jun 26 15:05:20.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:05:20.166: INFO: namespace replicaset-5207 deletion completed in 22.099166401s

• [SLOW TEST:26.188 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:05:20.167: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jun 26 15:05:30.310: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:05:30.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0626 15:05:30.310254      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-3721" for this suite.
Jun 26 15:05:36.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:05:36.594: INFO: namespace gc-3721 deletion completed in 6.273950177s

• [SLOW TEST:16.426 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:05:36.597: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 26 15:05:36.701: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ecacc098-06ab-46a3-a52f-4dc1b045303b" in namespace "downward-api-3421" to be "success or failure"
Jun 26 15:05:36.723: INFO: Pod "downwardapi-volume-ecacc098-06ab-46a3-a52f-4dc1b045303b": Phase="Pending", Reason="", readiness=false. Elapsed: 21.788882ms
Jun 26 15:05:38.733: INFO: Pod "downwardapi-volume-ecacc098-06ab-46a3-a52f-4dc1b045303b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032241434s
Jun 26 15:05:40.737: INFO: Pod "downwardapi-volume-ecacc098-06ab-46a3-a52f-4dc1b045303b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036209967s
STEP: Saw pod success
Jun 26 15:05:40.737: INFO: Pod "downwardapi-volume-ecacc098-06ab-46a3-a52f-4dc1b045303b" satisfied condition "success or failure"
Jun 26 15:05:40.740: INFO: Trying to get logs from node k8s-worker-1-dev pod downwardapi-volume-ecacc098-06ab-46a3-a52f-4dc1b045303b container client-container: <nil>
STEP: delete the pod
Jun 26 15:05:40.755: INFO: Waiting for pod downwardapi-volume-ecacc098-06ab-46a3-a52f-4dc1b045303b to disappear
Jun 26 15:05:40.758: INFO: Pod downwardapi-volume-ecacc098-06ab-46a3-a52f-4dc1b045303b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:05:40.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3421" for this suite.
Jun 26 15:05:46.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:05:46.982: INFO: namespace downward-api-3421 deletion completed in 6.219970018s

• [SLOW TEST:10.385 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:05:46.991: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Jun 26 15:05:47.071: INFO: Waiting up to 5m0s for pod "var-expansion-4c965e39-b9f9-4e15-926a-cb3ba924888d" in namespace "var-expansion-8941" to be "success or failure"
Jun 26 15:05:47.077: INFO: Pod "var-expansion-4c965e39-b9f9-4e15-926a-cb3ba924888d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.782547ms
Jun 26 15:05:49.084: INFO: Pod "var-expansion-4c965e39-b9f9-4e15-926a-cb3ba924888d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012439893s
STEP: Saw pod success
Jun 26 15:05:49.084: INFO: Pod "var-expansion-4c965e39-b9f9-4e15-926a-cb3ba924888d" satisfied condition "success or failure"
Jun 26 15:05:49.091: INFO: Trying to get logs from node k8s-worker-2-dev pod var-expansion-4c965e39-b9f9-4e15-926a-cb3ba924888d container dapi-container: <nil>
STEP: delete the pod
Jun 26 15:05:49.137: INFO: Waiting for pod var-expansion-4c965e39-b9f9-4e15-926a-cb3ba924888d to disappear
Jun 26 15:05:49.147: INFO: Pod var-expansion-4c965e39-b9f9-4e15-926a-cb3ba924888d no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:05:49.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8941" for this suite.
Jun 26 15:05:55.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:05:55.247: INFO: namespace var-expansion-8941 deletion completed in 6.08975605s

• [SLOW TEST:8.256 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:05:55.249: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-7980
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jun 26 15:05:55.293: INFO: Found 0 stateful pods, waiting for 3
Jun 26 15:06:05.298: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 26 15:06:05.298: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 26 15:06:05.298: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun 26 15:06:05.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 exec --namespace=statefulset-7980 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 26 15:06:05.727: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 26 15:06:05.727: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 26 15:06:05.727: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jun 26 15:06:15.776: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun 26 15:06:25.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 exec --namespace=statefulset-7980 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 26 15:06:26.555: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 26 15:06:26.555: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 26 15:06:26.555: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 26 15:06:26.596: INFO: Waiting for StatefulSet statefulset-7980/ss2 to complete update
Jun 26 15:06:26.596: INFO: Waiting for Pod statefulset-7980/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 26 15:06:26.596: INFO: Waiting for Pod statefulset-7980/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 26 15:06:26.596: INFO: Waiting for Pod statefulset-7980/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 26 15:06:36.606: INFO: Waiting for StatefulSet statefulset-7980/ss2 to complete update
Jun 26 15:06:36.606: INFO: Waiting for Pod statefulset-7980/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Jun 26 15:06:46.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 exec --namespace=statefulset-7980 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 26 15:06:47.365: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 26 15:06:47.365: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 26 15:06:47.365: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 26 15:06:47.410: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun 26 15:06:57.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 exec --namespace=statefulset-7980 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 26 15:06:58.149: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 26 15:06:58.149: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 26 15:06:58.149: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 26 15:07:08.204: INFO: Waiting for StatefulSet statefulset-7980/ss2 to complete update
Jun 26 15:07:08.204: INFO: Waiting for Pod statefulset-7980/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jun 26 15:07:08.204: INFO: Waiting for Pod statefulset-7980/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jun 26 15:07:08.204: INFO: Waiting for Pod statefulset-7980/ss2-2 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jun 26 15:07:18.216: INFO: Waiting for StatefulSet statefulset-7980/ss2 to complete update
Jun 26 15:07:18.217: INFO: Waiting for Pod statefulset-7980/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jun 26 15:07:28.220: INFO: Waiting for StatefulSet statefulset-7980/ss2 to complete update
Jun 26 15:07:28.220: INFO: Waiting for Pod statefulset-7980/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 26 15:07:38.222: INFO: Deleting all statefulset in ns statefulset-7980
Jun 26 15:07:38.230: INFO: Scaling statefulset ss2 to 0
Jun 26 15:08:08.260: INFO: Waiting for statefulset status.replicas updated to 0
Jun 26 15:08:08.265: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:08:08.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7980" for this suite.
Jun 26 15:08:14.323: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:08:14.418: INFO: namespace statefulset-7980 deletion completed in 6.125341974s

• [SLOW TEST:139.169 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:08:14.421: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-b6b23f0b-9e30-4804-97f9-40ec9f9b8ec1
STEP: Creating a pod to test consume configMaps
Jun 26 15:08:14.467: INFO: Waiting up to 5m0s for pod "pod-configmaps-e9164583-caf0-4a35-86fa-8322e4676b2f" in namespace "configmap-7135" to be "success or failure"
Jun 26 15:08:14.470: INFO: Pod "pod-configmaps-e9164583-caf0-4a35-86fa-8322e4676b2f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.765245ms
Jun 26 15:08:16.474: INFO: Pod "pod-configmaps-e9164583-caf0-4a35-86fa-8322e4676b2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006637498s
STEP: Saw pod success
Jun 26 15:08:16.474: INFO: Pod "pod-configmaps-e9164583-caf0-4a35-86fa-8322e4676b2f" satisfied condition "success or failure"
Jun 26 15:08:16.476: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-configmaps-e9164583-caf0-4a35-86fa-8322e4676b2f container configmap-volume-test: <nil>
STEP: delete the pod
Jun 26 15:08:16.490: INFO: Waiting for pod pod-configmaps-e9164583-caf0-4a35-86fa-8322e4676b2f to disappear
Jun 26 15:08:16.492: INFO: Pod pod-configmaps-e9164583-caf0-4a35-86fa-8322e4676b2f no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:08:16.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7135" for this suite.
Jun 26 15:08:22.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:08:22.713: INFO: namespace configmap-7135 deletion completed in 6.217323185s

• [SLOW TEST:8.292 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:08:22.715: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-6606
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 26 15:08:22.784: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 26 15:08:42.970: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.249.33:8080/dial?request=hostName&protocol=http&host=172.16.249.32&port=8080&tries=1'] Namespace:pod-network-test-6606 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 26 15:08:42.971: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
Jun 26 15:08:43.260: INFO: Waiting for endpoints: map[]
Jun 26 15:08:43.263: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.249.33:8080/dial?request=hostName&protocol=http&host=172.16.97.216&port=8080&tries=1'] Namespace:pod-network-test-6606 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 26 15:08:43.263: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
Jun 26 15:08:43.461: INFO: Waiting for endpoints: map[]
Jun 26 15:08:43.469: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.249.33:8080/dial?request=hostName&protocol=http&host=172.16.146.216&port=8080&tries=1'] Namespace:pod-network-test-6606 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 26 15:08:43.469: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
Jun 26 15:08:43.678: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:08:43.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6606" for this suite.
Jun 26 15:09:07.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:09:07.922: INFO: namespace pod-network-test-6606 deletion completed in 24.237719941s

• [SLOW TEST:45.207 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:09:07.924: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jun 26 15:09:08.006: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:09:11.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9815" for this suite.
Jun 26 15:09:17.835: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:09:18.054: INFO: namespace init-container-9815 deletion completed in 6.242153049s

• [SLOW TEST:10.129 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:09:18.059: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 26 15:09:18.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2795'
Jun 26 15:09:18.378: INFO: stderr: ""
Jun 26 15:09:18.380: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Jun 26 15:09:18.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 delete pods e2e-test-nginx-pod --namespace=kubectl-2795'
Jun 26 15:09:31.704: INFO: stderr: ""
Jun 26 15:09:31.705: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:09:31.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2795" for this suite.
Jun 26 15:09:37.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:09:37.940: INFO: namespace kubectl-2795 deletion completed in 6.225714384s

• [SLOW TEST:19.881 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:09:37.941: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Jun 26 15:09:38.009: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-254532258 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:09:38.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8810" for this suite.
Jun 26 15:09:44.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:09:44.420: INFO: namespace kubectl-8810 deletion completed in 6.23508497s

• [SLOW TEST:6.480 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:09:44.421: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 26 15:09:44.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7922'
Jun 26 15:09:44.728: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 26 15:09:44.728: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Jun 26 15:09:44.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 delete jobs e2e-test-nginx-job --namespace=kubectl-7922'
Jun 26 15:09:44.958: INFO: stderr: ""
Jun 26 15:09:44.958: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:09:44.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7922" for this suite.
Jun 26 15:10:07.002: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:10:07.082: INFO: namespace kubectl-7922 deletion completed in 22.107177487s

• [SLOW TEST:22.661 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:10:07.082: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jun 26 15:10:07.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 create -f - --namespace=kubectl-1307'
Jun 26 15:10:07.596: INFO: stderr: ""
Jun 26 15:10:07.596: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 26 15:10:07.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1307'
Jun 26 15:10:07.801: INFO: stderr: ""
Jun 26 15:10:07.801: INFO: stdout: "update-demo-nautilus-4r9sf update-demo-nautilus-6rcq4 "
Jun 26 15:10:07.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-4r9sf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1307'
Jun 26 15:10:07.890: INFO: stderr: ""
Jun 26 15:10:07.890: INFO: stdout: ""
Jun 26 15:10:07.891: INFO: update-demo-nautilus-4r9sf is created but not running
Jun 26 15:10:12.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1307'
Jun 26 15:10:13.090: INFO: stderr: ""
Jun 26 15:10:13.090: INFO: stdout: "update-demo-nautilus-4r9sf update-demo-nautilus-6rcq4 "
Jun 26 15:10:13.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-4r9sf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1307'
Jun 26 15:10:13.278: INFO: stderr: ""
Jun 26 15:10:13.278: INFO: stdout: "true"
Jun 26 15:10:13.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-4r9sf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1307'
Jun 26 15:10:13.496: INFO: stderr: ""
Jun 26 15:10:13.496: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 26 15:10:13.496: INFO: validating pod update-demo-nautilus-4r9sf
Jun 26 15:10:13.506: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 26 15:10:13.507: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 26 15:10:13.507: INFO: update-demo-nautilus-4r9sf is verified up and running
Jun 26 15:10:13.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-6rcq4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1307'
Jun 26 15:10:13.710: INFO: stderr: ""
Jun 26 15:10:13.710: INFO: stdout: "true"
Jun 26 15:10:13.710: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-6rcq4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1307'
Jun 26 15:10:13.925: INFO: stderr: ""
Jun 26 15:10:13.925: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 26 15:10:13.925: INFO: validating pod update-demo-nautilus-6rcq4
Jun 26 15:10:13.935: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 26 15:10:13.935: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 26 15:10:13.935: INFO: update-demo-nautilus-6rcq4 is verified up and running
STEP: using delete to clean up resources
Jun 26 15:10:13.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 delete --grace-period=0 --force -f - --namespace=kubectl-1307'
Jun 26 15:10:14.133: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 26 15:10:14.133: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 26 15:10:14.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-1307'
Jun 26 15:10:14.357: INFO: stderr: "No resources found.\n"
Jun 26 15:10:14.357: INFO: stdout: ""
Jun 26 15:10:14.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods -l name=update-demo --namespace=kubectl-1307 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 26 15:10:14.571: INFO: stderr: ""
Jun 26 15:10:14.571: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:10:14.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1307" for this suite.
Jun 26 15:10:36.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:10:36.686: INFO: namespace kubectl-1307 deletion completed in 22.103290487s

• [SLOW TEST:29.604 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:10:36.687: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:10:38.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4030" for this suite.
Jun 26 15:10:44.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:10:44.990: INFO: namespace emptydir-wrapper-4030 deletion completed in 6.217419732s

• [SLOW TEST:8.303 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:10:44.991: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 26 15:10:49.618: INFO: Successfully updated pod "pod-update-2bdfb384-a567-49c8-b8c4-732754593dc4"
STEP: verifying the updated pod is in kubernetes
Jun 26 15:10:49.630: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:10:49.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8591" for this suite.
Jun 26 15:11:11.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:11:11.895: INFO: namespace pods-8591 deletion completed in 22.254860635s

• [SLOW TEST:26.904 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:11:11.898: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:11:11.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4182" for this suite.
Jun 26 15:11:18.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:11:18.225: INFO: namespace services-4182 deletion completed in 6.21873795s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.329 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:11:18.227: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Jun 26 15:11:18.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 --namespace=kubectl-5266 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jun 26 15:11:19.844: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jun 26 15:11:19.844: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:11:21.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5266" for this suite.
Jun 26 15:11:33.870: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:11:34.104: INFO: namespace kubectl-5266 deletion completed in 12.249814407s

• [SLOW TEST:15.876 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:11:34.107: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 26 15:11:34.199: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun 26 15:11:34.216: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 26 15:11:39.225: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 26 15:11:39.226: INFO: Creating deployment "test-rolling-update-deployment"
Jun 26 15:11:39.236: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun 26 15:11:39.250: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun 26 15:11:41.266: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun 26 15:11:41.274: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 26 15:11:41.296: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-3572,SelfLink:/apis/apps/v1/namespaces/deployment-3572/deployments/test-rolling-update-deployment,UID:1187889a-e5f5-4711-aba4-be3e836b2adb,ResourceVersion:26265,Generation:1,CreationTimestamp:2019-06-26 15:11:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-06-26 15:11:39 +0000 UTC 2019-06-26 15:11:39 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-06-26 15:11:40 +0000 UTC 2019-06-26 15:11:39 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jun 26 15:11:41.305: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-3572,SelfLink:/apis/apps/v1/namespaces/deployment-3572/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:37be7069-c5d9-4fe5-add9-34d1187fe7e2,ResourceVersion:26254,Generation:1,CreationTimestamp:2019-06-26 15:11:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 1187889a-e5f5-4711-aba4-be3e836b2adb 0xc00318fa17 0xc00318fa18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 26 15:11:41.305: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun 26 15:11:41.306: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-3572,SelfLink:/apis/apps/v1/namespaces/deployment-3572/replicasets/test-rolling-update-controller,UID:22532eb4-9536-4d38-a0c0-1c9f69468838,ResourceVersion:26263,Generation:2,CreationTimestamp:2019-06-26 15:11:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 1187889a-e5f5-4711-aba4-be3e836b2adb 0xc00318f937 0xc00318f938}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 26 15:11:41.314: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-68c25" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-68c25,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-3572,SelfLink:/api/v1/namespaces/deployment-3572/pods/test-rolling-update-deployment-79f6b9d75c-68c25,UID:a6a0284e-5120-453d-a63f-9afadeed652b,ResourceVersion:26253,Generation:0,CreationTimestamp:2019-06-26 15:11:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.146.220/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 37be7069-c5d9-4fe5-add9-34d1187fe7e2 0xc000e68367 0xc000e68368}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hnvgx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hnvgx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-hnvgx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2-dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000e683e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000e68400}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:11:39 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:11:40 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:11:40 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:11:39 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.247,PodIP:172.16.146.220,StartTime:2019-06-26 15:11:39 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-06-26 15:11:40 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://0e9a7e05dfd0b37380cee6cbce6b45bfdc5505ac4350e42b2d804dd63b455b37}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:11:41.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3572" for this suite.
Jun 26 15:11:47.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:11:47.587: INFO: namespace deployment-3572 deletion completed in 6.261574649s

• [SLOW TEST:13.480 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:11:47.593: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 26 15:11:47.699: INFO: Waiting up to 5m0s for pod "downward-api-280d6814-9521-4153-a98a-d04282e1dd57" in namespace "downward-api-3098" to be "success or failure"
Jun 26 15:11:47.709: INFO: Pod "downward-api-280d6814-9521-4153-a98a-d04282e1dd57": Phase="Pending", Reason="", readiness=false. Elapsed: 10.584428ms
Jun 26 15:11:49.713: INFO: Pod "downward-api-280d6814-9521-4153-a98a-d04282e1dd57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014510706s
STEP: Saw pod success
Jun 26 15:11:49.713: INFO: Pod "downward-api-280d6814-9521-4153-a98a-d04282e1dd57" satisfied condition "success or failure"
Jun 26 15:11:49.715: INFO: Trying to get logs from node k8s-worker-1-dev pod downward-api-280d6814-9521-4153-a98a-d04282e1dd57 container dapi-container: <nil>
STEP: delete the pod
Jun 26 15:11:49.733: INFO: Waiting for pod downward-api-280d6814-9521-4153-a98a-d04282e1dd57 to disappear
Jun 26 15:11:49.735: INFO: Pod downward-api-280d6814-9521-4153-a98a-d04282e1dd57 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:11:49.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3098" for this suite.
Jun 26 15:11:55.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:11:55.948: INFO: namespace downward-api-3098 deletion completed in 6.208502927s

• [SLOW TEST:8.355 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:11:55.953: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-e1cfabb9-d5b9-4fd9-a209-5c8889cf2007
STEP: Creating a pod to test consume configMaps
Jun 26 15:11:56.050: INFO: Waiting up to 5m0s for pod "pod-configmaps-4acddb7b-e8c3-4fa6-9217-fa6e5073f236" in namespace "configmap-2232" to be "success or failure"
Jun 26 15:11:56.057: INFO: Pod "pod-configmaps-4acddb7b-e8c3-4fa6-9217-fa6e5073f236": Phase="Pending", Reason="", readiness=false. Elapsed: 6.836482ms
Jun 26 15:11:58.064: INFO: Pod "pod-configmaps-4acddb7b-e8c3-4fa6-9217-fa6e5073f236": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013904262s
STEP: Saw pod success
Jun 26 15:11:58.064: INFO: Pod "pod-configmaps-4acddb7b-e8c3-4fa6-9217-fa6e5073f236" satisfied condition "success or failure"
Jun 26 15:11:58.070: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-configmaps-4acddb7b-e8c3-4fa6-9217-fa6e5073f236 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 26 15:11:58.109: INFO: Waiting for pod pod-configmaps-4acddb7b-e8c3-4fa6-9217-fa6e5073f236 to disappear
Jun 26 15:11:58.116: INFO: Pod pod-configmaps-4acddb7b-e8c3-4fa6-9217-fa6e5073f236 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:11:58.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2232" for this suite.
Jun 26 15:12:04.156: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:12:04.242: INFO: namespace configmap-2232 deletion completed in 6.112523518s

• [SLOW TEST:8.288 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:12:04.243: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-b3e8671e-312c-4596-8838-f1439257b694
STEP: Creating a pod to test consume secrets
Jun 26 15:12:04.283: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7b81cf42-56af-4c2c-a717-03a0973879eb" in namespace "projected-56" to be "success or failure"
Jun 26 15:12:04.286: INFO: Pod "pod-projected-secrets-7b81cf42-56af-4c2c-a717-03a0973879eb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.069298ms
Jun 26 15:12:06.294: INFO: Pod "pod-projected-secrets-7b81cf42-56af-4c2c-a717-03a0973879eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011029924s
STEP: Saw pod success
Jun 26 15:12:06.294: INFO: Pod "pod-projected-secrets-7b81cf42-56af-4c2c-a717-03a0973879eb" satisfied condition "success or failure"
Jun 26 15:12:06.300: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-projected-secrets-7b81cf42-56af-4c2c-a717-03a0973879eb container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 26 15:12:06.333: INFO: Waiting for pod pod-projected-secrets-7b81cf42-56af-4c2c-a717-03a0973879eb to disappear
Jun 26 15:12:06.338: INFO: Pod pod-projected-secrets-7b81cf42-56af-4c2c-a717-03a0973879eb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:12:06.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-56" for this suite.
Jun 26 15:12:12.378: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:12:12.603: INFO: namespace projected-56 deletion completed in 6.252847007s

• [SLOW TEST:8.359 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:12:12.607: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jun 26 15:12:18.733: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:12:18.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0626 15:12:18.733056      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-3697" for this suite.
Jun 26 15:12:24.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:12:24.968: INFO: namespace gc-3697 deletion completed in 6.228242276s

• [SLOW TEST:12.362 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:12:24.972: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jun 26 15:12:25.046: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:12:30.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-988" for this suite.
Jun 26 15:12:52.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:12:52.829: INFO: namespace init-container-988 deletion completed in 22.223573888s

• [SLOW TEST:27.857 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:12:52.830: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 26 15:12:52.948: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"54c9180d-84c4-43b6-afcb-d4eeda2a747c", Controller:(*bool)(0xc002a90326), BlockOwnerDeletion:(*bool)(0xc002a90327)}}
Jun 26 15:12:52.986: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"6308af50-02f1-4a57-8744-71ecf263964e", Controller:(*bool)(0xc002a904d6), BlockOwnerDeletion:(*bool)(0xc002a904d7)}}
Jun 26 15:12:53.001: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"6f339cd2-3822-491d-b6cd-145f2a85c287", Controller:(*bool)(0xc001e45f5a), BlockOwnerDeletion:(*bool)(0xc001e45f5b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:12:58.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1714" for this suite.
Jun 26 15:13:04.064: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:13:04.291: INFO: namespace gc-1714 deletion completed in 6.253890532s

• [SLOW TEST:11.461 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:13:04.294: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jun 26 15:13:04.424: INFO: namespace kubectl-6549
Jun 26 15:13:04.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 create -f - --namespace=kubectl-6549'
Jun 26 15:13:04.831: INFO: stderr: ""
Jun 26 15:13:04.831: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 26 15:13:05.839: INFO: Selector matched 1 pods for map[app:redis]
Jun 26 15:13:05.839: INFO: Found 0 / 1
Jun 26 15:13:06.845: INFO: Selector matched 1 pods for map[app:redis]
Jun 26 15:13:06.845: INFO: Found 1 / 1
Jun 26 15:13:06.845: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 26 15:13:06.854: INFO: Selector matched 1 pods for map[app:redis]
Jun 26 15:13:06.854: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 26 15:13:06.854: INFO: wait on redis-master startup in kubectl-6549 
Jun 26 15:13:06.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 logs redis-master-c4hwg redis-master --namespace=kubectl-6549'
Jun 26 15:13:07.083: INFO: stderr: ""
Jun 26 15:13:07.083: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 26 Jun 15:13:05.974 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 26 Jun 15:13:05.974 # Server started, Redis version 3.2.12\n1:M 26 Jun 15:13:05.974 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 26 Jun 15:13:05.974 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jun 26 15:13:07.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-6549'
Jun 26 15:13:07.388: INFO: stderr: ""
Jun 26 15:13:07.388: INFO: stdout: "service/rm2 exposed\n"
Jun 26 15:13:07.397: INFO: Service rm2 in namespace kubectl-6549 found.
STEP: exposing service
Jun 26 15:13:09.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-6549'
Jun 26 15:13:09.507: INFO: stderr: ""
Jun 26 15:13:09.507: INFO: stdout: "service/rm3 exposed\n"
Jun 26 15:13:09.511: INFO: Service rm3 in namespace kubectl-6549 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:13:11.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6549" for this suite.
Jun 26 15:13:33.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:13:33.822: INFO: namespace kubectl-6549 deletion completed in 22.300675687s

• [SLOW TEST:29.528 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:13:33.826: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Jun 26 15:13:35.953: INFO: Pod pod-hostip-70d54712-26ce-4555-8060-ce427b95cc00 has hostIP: 192.168.9.245
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:13:35.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2807" for this suite.
Jun 26 15:13:57.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:13:58.068: INFO: namespace pods-2807 deletion completed in 22.103845782s

• [SLOW TEST:24.242 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:13:58.069: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 26 15:13:58.136: INFO: Waiting up to 5m0s for pod "pod-70863403-0d16-4591-8012-ac007b96984b" in namespace "emptydir-6779" to be "success or failure"
Jun 26 15:13:58.141: INFO: Pod "pod-70863403-0d16-4591-8012-ac007b96984b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.80787ms
Jun 26 15:14:00.149: INFO: Pod "pod-70863403-0d16-4591-8012-ac007b96984b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01259172s
Jun 26 15:14:02.157: INFO: Pod "pod-70863403-0d16-4591-8012-ac007b96984b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021087869s
STEP: Saw pod success
Jun 26 15:14:02.157: INFO: Pod "pod-70863403-0d16-4591-8012-ac007b96984b" satisfied condition "success or failure"
Jun 26 15:14:02.163: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-70863403-0d16-4591-8012-ac007b96984b container test-container: <nil>
STEP: delete the pod
Jun 26 15:14:02.209: INFO: Waiting for pod pod-70863403-0d16-4591-8012-ac007b96984b to disappear
Jun 26 15:14:02.215: INFO: Pod pod-70863403-0d16-4591-8012-ac007b96984b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:14:02.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6779" for this suite.
Jun 26 15:14:08.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:14:08.435: INFO: namespace emptydir-6779 deletion completed in 6.212088173s

• [SLOW TEST:10.367 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:14:08.436: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 26 15:14:08.510: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cbb928c2-d02f-4eee-b5c8-60b669e75b1b" in namespace "downward-api-6569" to be "success or failure"
Jun 26 15:14:08.517: INFO: Pod "downwardapi-volume-cbb928c2-d02f-4eee-b5c8-60b669e75b1b": Phase="Pending", Reason="", readiness=false. Elapsed: 7.186347ms
Jun 26 15:14:10.523: INFO: Pod "downwardapi-volume-cbb928c2-d02f-4eee-b5c8-60b669e75b1b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013438025s
Jun 26 15:14:12.528: INFO: Pod "downwardapi-volume-cbb928c2-d02f-4eee-b5c8-60b669e75b1b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01799302s
STEP: Saw pod success
Jun 26 15:14:12.528: INFO: Pod "downwardapi-volume-cbb928c2-d02f-4eee-b5c8-60b669e75b1b" satisfied condition "success or failure"
Jun 26 15:14:12.530: INFO: Trying to get logs from node k8s-worker-1-dev pod downwardapi-volume-cbb928c2-d02f-4eee-b5c8-60b669e75b1b container client-container: <nil>
STEP: delete the pod
Jun 26 15:14:12.547: INFO: Waiting for pod downwardapi-volume-cbb928c2-d02f-4eee-b5c8-60b669e75b1b to disappear
Jun 26 15:14:12.550: INFO: Pod downwardapi-volume-cbb928c2-d02f-4eee-b5c8-60b669e75b1b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:14:12.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6569" for this suite.
Jun 26 15:14:18.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:14:18.650: INFO: namespace downward-api-6569 deletion completed in 6.096454112s

• [SLOW TEST:10.214 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:14:18.651: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 26 15:14:18.693: INFO: Waiting up to 5m0s for pod "downwardapi-volume-397fb000-661c-49b6-8f6f-c86bbdb3e2e2" in namespace "downward-api-182" to be "success or failure"
Jun 26 15:14:18.697: INFO: Pod "downwardapi-volume-397fb000-661c-49b6-8f6f-c86bbdb3e2e2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.749015ms
Jun 26 15:14:20.701: INFO: Pod "downwardapi-volume-397fb000-661c-49b6-8f6f-c86bbdb3e2e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008207999s
STEP: Saw pod success
Jun 26 15:14:20.702: INFO: Pod "downwardapi-volume-397fb000-661c-49b6-8f6f-c86bbdb3e2e2" satisfied condition "success or failure"
Jun 26 15:14:20.704: INFO: Trying to get logs from node k8s-worker-1-dev pod downwardapi-volume-397fb000-661c-49b6-8f6f-c86bbdb3e2e2 container client-container: <nil>
STEP: delete the pod
Jun 26 15:14:20.722: INFO: Waiting for pod downwardapi-volume-397fb000-661c-49b6-8f6f-c86bbdb3e2e2 to disappear
Jun 26 15:14:20.724: INFO: Pod downwardapi-volume-397fb000-661c-49b6-8f6f-c86bbdb3e2e2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:14:20.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-182" for this suite.
Jun 26 15:14:26.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:14:26.822: INFO: namespace downward-api-182 deletion completed in 6.093685115s

• [SLOW TEST:8.171 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:14:26.823: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 26 15:14:26.857: INFO: Waiting up to 5m0s for pod "pod-ab785a16-dbbd-4cf9-a947-62a08a6f731a" in namespace "emptydir-9876" to be "success or failure"
Jun 26 15:14:26.861: INFO: Pod "pod-ab785a16-dbbd-4cf9-a947-62a08a6f731a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.1953ms
Jun 26 15:14:28.865: INFO: Pod "pod-ab785a16-dbbd-4cf9-a947-62a08a6f731a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007854692s
STEP: Saw pod success
Jun 26 15:14:28.865: INFO: Pod "pod-ab785a16-dbbd-4cf9-a947-62a08a6f731a" satisfied condition "success or failure"
Jun 26 15:14:28.868: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-ab785a16-dbbd-4cf9-a947-62a08a6f731a container test-container: <nil>
STEP: delete the pod
Jun 26 15:14:28.887: INFO: Waiting for pod pod-ab785a16-dbbd-4cf9-a947-62a08a6f731a to disappear
Jun 26 15:14:28.890: INFO: Pod pod-ab785a16-dbbd-4cf9-a947-62a08a6f731a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:14:28.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9876" for this suite.
Jun 26 15:14:34.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:14:35.080: INFO: namespace emptydir-9876 deletion completed in 6.186542406s

• [SLOW TEST:8.257 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:14:35.082: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 26 15:14:35.140: INFO: Creating deployment "nginx-deployment"
Jun 26 15:14:35.149: INFO: Waiting for observed generation 1
Jun 26 15:14:37.165: INFO: Waiting for all required pods to come up
Jun 26 15:14:37.177: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun 26 15:14:39.220: INFO: Waiting for deployment "nginx-deployment" to complete
Jun 26 15:14:39.229: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jun 26 15:14:39.239: INFO: Updating deployment nginx-deployment
Jun 26 15:14:39.239: INFO: Waiting for observed generation 2
Jun 26 15:14:41.249: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun 26 15:14:41.251: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun 26 15:14:41.253: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jun 26 15:14:41.259: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun 26 15:14:41.259: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun 26 15:14:41.261: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jun 26 15:14:41.265: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jun 26 15:14:41.265: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jun 26 15:14:41.271: INFO: Updating deployment nginx-deployment
Jun 26 15:14:41.271: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jun 26 15:14:41.275: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun 26 15:14:41.279: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 26 15:14:41.292: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-8771,SelfLink:/apis/apps/v1/namespaces/deployment-8771/deployments/nginx-deployment,UID:909b1916-afa4-42d8-9936-e0269a1e0876,ResourceVersion:27501,Generation:3,CreationTimestamp:2019-06-26 15:14:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Progressing True 2019-06-26 15:14:39 +0000 UTC 2019-06-26 15:14:35 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.} {Available False 2019-06-26 15:14:41 +0000 UTC 2019-06-26 15:14:41 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Jun 26 15:14:41.302: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-8771,SelfLink:/apis/apps/v1/namespaces/deployment-8771/replicasets/nginx-deployment-55fb7cb77f,UID:5c137922-632d-43b1-87fa-b72bddcfa786,ResourceVersion:27495,Generation:3,CreationTimestamp:2019-06-26 15:14:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 909b1916-afa4-42d8-9936-e0269a1e0876 0xc00206ed27 0xc00206ed28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 26 15:14:41.302: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jun 26 15:14:41.302: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-8771,SelfLink:/apis/apps/v1/namespaces/deployment-8771/replicasets/nginx-deployment-7b8c6f4498,UID:94ba55cc-7d08-45ba-95f2-1d54d2e0aa16,ResourceVersion:27493,Generation:3,CreationTimestamp:2019-06-26 15:14:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 909b1916-afa4-42d8-9936-e0269a1e0876 0xc00206edf7 0xc00206edf8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jun 26 15:14:41.322: INFO: Pod "nginx-deployment-55fb7cb77f-4xkg4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-4xkg4,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8771,SelfLink:/api/v1/namespaces/deployment-8771/pods/nginx-deployment-55fb7cb77f-4xkg4,UID:f4f68002-1117-419f-a0f4-2068712e2988,ResourceVersion:27478,Generation:0,CreationTimestamp:2019-06-26 15:14:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.97.223/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5c137922-632d-43b1-87fa-b72bddcfa786 0xc00339fbd7 0xc00339fbd8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6pm6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6pm6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-c6pm6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3-dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00339fc50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00339fc70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:39 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.249,PodIP:,StartTime:2019-06-26 15:14:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 26 15:14:41.323: INFO: Pod "nginx-deployment-55fb7cb77f-8jsfz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-8jsfz,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8771,SelfLink:/api/v1/namespaces/deployment-8771/pods/nginx-deployment-55fb7cb77f-8jsfz,UID:538fbb19-fec4-405f-890e-95a04b3956e6,ResourceVersion:27479,Generation:0,CreationTimestamp:2019-06-26 15:14:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.146.233/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5c137922-632d-43b1-87fa-b72bddcfa786 0xc00339fd40 0xc00339fd41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6pm6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6pm6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-c6pm6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2-dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00339fdc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00339fde0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:39 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.247,PodIP:,StartTime:2019-06-26 15:14:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 26 15:14:41.323: INFO: Pod "nginx-deployment-55fb7cb77f-8nv7w" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-8nv7w,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8771,SelfLink:/api/v1/namespaces/deployment-8771/pods/nginx-deployment-55fb7cb77f-8nv7w,UID:adaeb98f-e587-49da-bc68-5d208fe2cf5b,ResourceVersion:27484,Generation:0,CreationTimestamp:2019-06-26 15:14:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.249.57/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5c137922-632d-43b1-87fa-b72bddcfa786 0xc00339fec0 0xc00339fec1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6pm6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6pm6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-c6pm6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1-dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00339ff40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00339ff60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:39 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.245,PodIP:,StartTime:2019-06-26 15:14:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 26 15:14:41.323: INFO: Pod "nginx-deployment-55fb7cb77f-cxxbv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-cxxbv,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8771,SelfLink:/api/v1/namespaces/deployment-8771/pods/nginx-deployment-55fb7cb77f-cxxbv,UID:f39a4d6a-7a02-480c-9c5d-692f07fe9af1,ResourceVersion:27474,Generation:0,CreationTimestamp:2019-06-26 15:14:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.146.232/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5c137922-632d-43b1-87fa-b72bddcfa786 0xc00344a030 0xc00344a031}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6pm6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6pm6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-c6pm6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2-dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00344a0b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00344a0d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:39 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.247,PodIP:,StartTime:2019-06-26 15:14:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 26 15:14:41.324: INFO: Pod "nginx-deployment-55fb7cb77f-l9ptc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-l9ptc,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8771,SelfLink:/api/v1/namespaces/deployment-8771/pods/nginx-deployment-55fb7cb77f-l9ptc,UID:91cd1c82-79dc-40ba-8b9d-03c0f2c76fbf,ResourceVersion:27506,Generation:0,CreationTimestamp:2019-06-26 15:14:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5c137922-632d-43b1-87fa-b72bddcfa786 0xc00344a1a0 0xc00344a1a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6pm6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6pm6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-c6pm6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3-dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00344a220} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00344a240}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:41 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 26 15:14:41.324: INFO: Pod "nginx-deployment-55fb7cb77f-qv79h" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-qv79h,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8771,SelfLink:/api/v1/namespaces/deployment-8771/pods/nginx-deployment-55fb7cb77f-qv79h,UID:f17ebf4a-e916-4851-a87f-a74f0444bfe5,ResourceVersion:27511,Generation:0,CreationTimestamp:2019-06-26 15:14:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5c137922-632d-43b1-87fa-b72bddcfa786 0xc00344a2c0 0xc00344a2c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6pm6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6pm6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-c6pm6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00344a330} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00344a350}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 26 15:14:41.325: INFO: Pod "nginx-deployment-55fb7cb77f-wgsdl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-wgsdl,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8771,SelfLink:/api/v1/namespaces/deployment-8771/pods/nginx-deployment-55fb7cb77f-wgsdl,UID:cd31c785-ab83-4b8a-b6b7-019c820ad6dc,ResourceVersion:27475,Generation:0,CreationTimestamp:2019-06-26 15:14:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.249.56/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5c137922-632d-43b1-87fa-b72bddcfa786 0xc00344a3c7 0xc00344a3c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6pm6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6pm6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-c6pm6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1-dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00344a440} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00344a460}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:39 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:39 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.245,PodIP:,StartTime:2019-06-26 15:14:39 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 26 15:14:41.325: INFO: Pod "nginx-deployment-55fb7cb77f-wt9f9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-wt9f9,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-8771,SelfLink:/api/v1/namespaces/deployment-8771/pods/nginx-deployment-55fb7cb77f-wt9f9,UID:69a11a46-1019-4094-9071-9bb40dc5016f,ResourceVersion:27509,Generation:0,CreationTimestamp:2019-06-26 15:14:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 5c137922-632d-43b1-87fa-b72bddcfa786 0xc00344a530 0xc00344a531}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6pm6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6pm6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-c6pm6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00344a5a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00344a5c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 26 15:14:41.325: INFO: Pod "nginx-deployment-7b8c6f4498-458sl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-458sl,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8771,SelfLink:/api/v1/namespaces/deployment-8771/pods/nginx-deployment-7b8c6f4498-458sl,UID:d06c93d1-1bd0-4ec4-a1e3-dc4e2457402b,ResourceVersion:27510,Generation:0,CreationTimestamp:2019-06-26 15:14:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94ba55cc-7d08-45ba-95f2-1d54d2e0aa16 0xc00344a627 0xc00344a628}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6pm6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6pm6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-c6pm6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1-dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00344a6a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00344a6c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:41 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 26 15:14:41.326: INFO: Pod "nginx-deployment-7b8c6f4498-6l8f4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-6l8f4,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8771,SelfLink:/api/v1/namespaces/deployment-8771/pods/nginx-deployment-7b8c6f4498-6l8f4,UID:e6f47485-d45d-40b0-833d-330b5f30df92,ResourceVersion:27507,Generation:0,CreationTimestamp:2019-06-26 15:14:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94ba55cc-7d08-45ba-95f2-1d54d2e0aa16 0xc00344a740 0xc00344a741}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6pm6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6pm6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-c6pm6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00344a7a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00344a7c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 26 15:14:41.326: INFO: Pod "nginx-deployment-7b8c6f4498-8ml8w" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8ml8w,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8771,SelfLink:/api/v1/namespaces/deployment-8771/pods/nginx-deployment-7b8c6f4498-8ml8w,UID:e7f9f9b7-a284-45b5-8104-4838968d10a9,ResourceVersion:27399,Generation:0,CreationTimestamp:2019-06-26 15:14:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.249.55/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94ba55cc-7d08-45ba-95f2-1d54d2e0aa16 0xc00344a837 0xc00344a838}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6pm6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6pm6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-c6pm6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1-dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00344a8b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00344a8d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:35 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.245,PodIP:172.16.249.55,StartTime:2019-06-26 15:14:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-26 15:14:37 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://5d20bded6199de8517fd000ab60c81d2ccac99f3f52bf3aaba5984081193eb48}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 26 15:14:41.327: INFO: Pod "nginx-deployment-7b8c6f4498-9vvg2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-9vvg2,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8771,SelfLink:/api/v1/namespaces/deployment-8771/pods/nginx-deployment-7b8c6f4498-9vvg2,UID:48efcc02-25a5-46b6-876e-d413204cb684,ResourceVersion:27408,Generation:0,CreationTimestamp:2019-06-26 15:14:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.97.222/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94ba55cc-7d08-45ba-95f2-1d54d2e0aa16 0xc00344a9b7 0xc00344a9b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6pm6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6pm6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-c6pm6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3-dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00344aa30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00344aa50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:38 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:38 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:35 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.249,PodIP:172.16.97.222,StartTime:2019-06-26 15:14:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-26 15:14:37 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://02bc6ccae2a11baea46161621d70ea473feca09b68ad58893f6d903e3b266650}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 26 15:14:41.327: INFO: Pod "nginx-deployment-7b8c6f4498-c59pk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-c59pk,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8771,SelfLink:/api/v1/namespaces/deployment-8771/pods/nginx-deployment-7b8c6f4498-c59pk,UID:4413a174-7455-4d24-8861-89f79de29186,ResourceVersion:27374,Generation:0,CreationTimestamp:2019-06-26 15:14:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.146.229/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94ba55cc-7d08-45ba-95f2-1d54d2e0aa16 0xc00344ab27 0xc00344ab28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6pm6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6pm6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-c6pm6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2-dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00344aba0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00344abc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:35 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.247,PodIP:172.16.146.229,StartTime:2019-06-26 15:14:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-26 15:14:37 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://3017a5e2d4778cd5b82eefd90a8f01fd6c65f641cff6a0731d0189a14818ad41}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 26 15:14:41.327: INFO: Pod "nginx-deployment-7b8c6f4498-d7djc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-d7djc,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8771,SelfLink:/api/v1/namespaces/deployment-8771/pods/nginx-deployment-7b8c6f4498-d7djc,UID:22b946eb-312b-44ed-932a-eeb48a960a7d,ResourceVersion:27513,Generation:0,CreationTimestamp:2019-06-26 15:14:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94ba55cc-7d08-45ba-95f2-1d54d2e0aa16 0xc00344ac97 0xc00344ac98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6pm6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6pm6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-c6pm6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00344ad00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00344ad20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 26 15:14:41.328: INFO: Pod "nginx-deployment-7b8c6f4498-fr7wh" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fr7wh,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8771,SelfLink:/api/v1/namespaces/deployment-8771/pods/nginx-deployment-7b8c6f4498-fr7wh,UID:4a44d6d0-e8cc-4f14-9fc9-e31b84ebefbd,ResourceVersion:27391,Generation:0,CreationTimestamp:2019-06-26 15:14:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.97.221/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94ba55cc-7d08-45ba-95f2-1d54d2e0aa16 0xc00344ad97 0xc00344ad98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6pm6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6pm6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-c6pm6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3-dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00344ae10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00344ae30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:35 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.249,PodIP:172.16.97.221,StartTime:2019-06-26 15:14:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-26 15:14:37 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://96c2ddfd148905a202cabfe9d30843bd71573791b869e73910de7306262c63d4}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 26 15:14:41.329: INFO: Pod "nginx-deployment-7b8c6f4498-j6kb7" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-j6kb7,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8771,SelfLink:/api/v1/namespaces/deployment-8771/pods/nginx-deployment-7b8c6f4498-j6kb7,UID:9e2a429a-bea1-40cd-83fc-33c63edccf45,ResourceVersion:27377,Generation:0,CreationTimestamp:2019-06-26 15:14:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.146.228/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94ba55cc-7d08-45ba-95f2-1d54d2e0aa16 0xc00344af07 0xc00344af08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6pm6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6pm6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-c6pm6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2-dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00344af80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00344afa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:35 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.247,PodIP:172.16.146.228,StartTime:2019-06-26 15:14:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-26 15:14:37 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://fdfdcf95b5037154b7bb583a00b42030a5fc1d1da04e166632d3256b09fd8a19}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 26 15:14:41.329: INFO: Pod "nginx-deployment-7b8c6f4498-jpcl8" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-jpcl8,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8771,SelfLink:/api/v1/namespaces/deployment-8771/pods/nginx-deployment-7b8c6f4498-jpcl8,UID:ed949177-6452-4580-b16b-2f54b8f5bb7c,ResourceVersion:27387,Generation:0,CreationTimestamp:2019-06-26 15:14:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.97.220/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94ba55cc-7d08-45ba-95f2-1d54d2e0aa16 0xc00344b087 0xc00344b088}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6pm6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6pm6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-c6pm6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-3-dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00344b110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00344b130}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:35 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.249,PodIP:172.16.97.220,StartTime:2019-06-26 15:14:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-26 15:14:37 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://9d9ccb2f270948c16cc6e8d0c17bce5d5c5e1de76db101a2ae9ba8f5dd6f23ae}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 26 15:14:41.329: INFO: Pod "nginx-deployment-7b8c6f4498-l6cwq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-l6cwq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8771,SelfLink:/api/v1/namespaces/deployment-8771/pods/nginx-deployment-7b8c6f4498-l6cwq,UID:1a5add9e-e6c9-4684-b8ba-bba89e2db497,ResourceVersion:27508,Generation:0,CreationTimestamp:2019-06-26 15:14:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94ba55cc-7d08-45ba-95f2-1d54d2e0aa16 0xc00344b207 0xc00344b208}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6pm6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6pm6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-c6pm6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1-dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00344b280} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00344b2a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:41 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 26 15:14:41.330: INFO: Pod "nginx-deployment-7b8c6f4498-q4w45" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-q4w45,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8771,SelfLink:/api/v1/namespaces/deployment-8771/pods/nginx-deployment-7b8c6f4498-q4w45,UID:55b7f091-4831-4f49-af3a-8bea82fc4435,ResourceVersion:27382,Generation:0,CreationTimestamp:2019-06-26 15:14:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.146.231/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94ba55cc-7d08-45ba-95f2-1d54d2e0aa16 0xc00344b320 0xc00344b321}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6pm6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6pm6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-c6pm6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2-dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00344b390} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00344b3b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:35 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.247,PodIP:172.16.146.231,StartTime:2019-06-26 15:14:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-26 15:14:37 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://67c9e45dbc56adaa95b52bdc6a780a260010d06c4b71a88a3a8de3946ba5917d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 26 15:14:41.330: INFO: Pod "nginx-deployment-7b8c6f4498-r7slr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-r7slr,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8771,SelfLink:/api/v1/namespaces/deployment-8771/pods/nginx-deployment-7b8c6f4498-r7slr,UID:39983856-dd5e-4815-89e4-84e8c04bdd30,ResourceVersion:27380,Generation:0,CreationTimestamp:2019-06-26 15:14:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.146.230/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94ba55cc-7d08-45ba-95f2-1d54d2e0aa16 0xc00344b487 0xc00344b488}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6pm6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6pm6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-c6pm6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2-dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00344b500} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00344b520}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:35 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:37 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:37 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:35 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.247,PodIP:172.16.146.230,StartTime:2019-06-26 15:14:35 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-26 15:14:37 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://a9ba7436cabe74fed4c316db32627cdcf80320e967d68dde001c78c432f1f0d5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 26 15:14:41.331: INFO: Pod "nginx-deployment-7b8c6f4498-rcf5p" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-rcf5p,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8771,SelfLink:/api/v1/namespaces/deployment-8771/pods/nginx-deployment-7b8c6f4498-rcf5p,UID:d8211797-6cc8-4b64-aed2-1470a4e989bd,ResourceVersion:27500,Generation:0,CreationTimestamp:2019-06-26 15:14:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94ba55cc-7d08-45ba-95f2-1d54d2e0aa16 0xc00344b5f7 0xc00344b5f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6pm6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6pm6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-c6pm6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-1-dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00344b670} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00344b690}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:14:41 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 26 15:14:41.331: INFO: Pod "nginx-deployment-7b8c6f4498-sm4pw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-sm4pw,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8771,SelfLink:/api/v1/namespaces/deployment-8771/pods/nginx-deployment-7b8c6f4498-sm4pw,UID:0c9e0e52-9f7e-4a5b-b268-f2e27557cc6c,ResourceVersion:27514,Generation:0,CreationTimestamp:2019-06-26 15:14:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94ba55cc-7d08-45ba-95f2-1d54d2e0aa16 0xc00344b710 0xc00344b711}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6pm6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6pm6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-c6pm6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00344b770} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00344b790}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 26 15:14:41.331: INFO: Pod "nginx-deployment-7b8c6f4498-x7wgs" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-x7wgs,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-8771,SelfLink:/api/v1/namespaces/deployment-8771/pods/nginx-deployment-7b8c6f4498-x7wgs,UID:27786ac1-b6f2-498b-ab94-3a137bfc535d,ResourceVersion:27512,Generation:0,CreationTimestamp:2019-06-26 15:14:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 94ba55cc-7d08-45ba-95f2-1d54d2e0aa16 0xc00344b7f7 0xc00344b7f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6pm6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6pm6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-c6pm6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00344b860} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00344b880}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:14:41.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8771" for this suite.
Jun 26 15:14:49.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:14:49.539: INFO: namespace deployment-8771 deletion completed in 8.198823889s

• [SLOW TEST:14.457 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:14:49.540: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 26 15:14:49.595: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jun 26 15:14:50.634: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:14:51.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3297" for this suite.
Jun 26 15:14:57.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:14:57.890: INFO: namespace replication-controller-3297 deletion completed in 6.234787996s

• [SLOW TEST:8.350 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:14:57.891: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Jun 26 15:14:57.983: INFO: Waiting up to 5m0s for pod "var-expansion-768d4870-3cbc-4682-a3d9-9627d3de87df" in namespace "var-expansion-7732" to be "success or failure"
Jun 26 15:14:57.990: INFO: Pod "var-expansion-768d4870-3cbc-4682-a3d9-9627d3de87df": Phase="Pending", Reason="", readiness=false. Elapsed: 6.715652ms
Jun 26 15:14:59.998: INFO: Pod "var-expansion-768d4870-3cbc-4682-a3d9-9627d3de87df": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014229584s
Jun 26 15:15:02.002: INFO: Pod "var-expansion-768d4870-3cbc-4682-a3d9-9627d3de87df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01845457s
STEP: Saw pod success
Jun 26 15:15:02.002: INFO: Pod "var-expansion-768d4870-3cbc-4682-a3d9-9627d3de87df" satisfied condition "success or failure"
Jun 26 15:15:02.004: INFO: Trying to get logs from node k8s-worker-1-dev pod var-expansion-768d4870-3cbc-4682-a3d9-9627d3de87df container dapi-container: <nil>
STEP: delete the pod
Jun 26 15:15:02.023: INFO: Waiting for pod var-expansion-768d4870-3cbc-4682-a3d9-9627d3de87df to disappear
Jun 26 15:15:02.026: INFO: Pod var-expansion-768d4870-3cbc-4682-a3d9-9627d3de87df no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:15:02.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7732" for this suite.
Jun 26 15:15:08.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:15:08.202: INFO: namespace var-expansion-7732 deletion completed in 6.172075103s

• [SLOW TEST:10.311 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:15:08.205: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jun 26 15:15:12.815: INFO: Successfully updated pod "annotationupdate84fefbed-965b-45e6-9916-475c8a0d8340"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:15:14.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7729" for this suite.
Jun 26 15:15:34.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:15:35.086: INFO: namespace projected-7729 deletion completed in 20.231001683s

• [SLOW TEST:26.882 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:15:35.089: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 26 15:15:35.161: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:15:39.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-737" for this suite.
Jun 26 15:16:25.232: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:16:25.449: INFO: namespace pods-737 deletion completed in 46.230355916s

• [SLOW TEST:50.361 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:16:25.450: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 26 15:16:35.592: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 26 15:16:35.595: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 26 15:16:37.596: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 26 15:16:37.603: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 26 15:16:39.596: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 26 15:16:39.599: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 26 15:16:41.596: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 26 15:16:41.604: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 26 15:16:43.596: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 26 15:16:43.607: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 26 15:16:45.596: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 26 15:16:45.604: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 26 15:16:47.595: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 26 15:16:47.599: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 26 15:16:49.595: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 26 15:16:49.599: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 26 15:16:51.596: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 26 15:16:51.606: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 26 15:16:53.596: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 26 15:16:53.607: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 26 15:16:55.596: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 26 15:16:55.603: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:16:55.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4730" for this suite.
Jun 26 15:17:17.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:17:17.798: INFO: namespace container-lifecycle-hook-4730 deletion completed in 22.16873921s

• [SLOW TEST:52.349 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:17:17.801: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jun 26 15:17:17.835: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 create -f - --namespace=kubectl-385'
Jun 26 15:17:18.147: INFO: stderr: ""
Jun 26 15:17:18.147: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 26 15:17:18.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-385'
Jun 26 15:17:18.309: INFO: stderr: ""
Jun 26 15:17:18.309: INFO: stdout: "update-demo-nautilus-4cbhg update-demo-nautilus-f696q "
Jun 26 15:17:18.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-4cbhg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-385'
Jun 26 15:17:18.393: INFO: stderr: ""
Jun 26 15:17:18.393: INFO: stdout: ""
Jun 26 15:17:18.393: INFO: update-demo-nautilus-4cbhg is created but not running
Jun 26 15:17:23.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-385'
Jun 26 15:17:23.596: INFO: stderr: ""
Jun 26 15:17:23.596: INFO: stdout: "update-demo-nautilus-4cbhg update-demo-nautilus-f696q "
Jun 26 15:17:23.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-4cbhg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-385'
Jun 26 15:17:23.797: INFO: stderr: ""
Jun 26 15:17:23.797: INFO: stdout: "true"
Jun 26 15:17:23.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-4cbhg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-385'
Jun 26 15:17:24.004: INFO: stderr: ""
Jun 26 15:17:24.004: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 26 15:17:24.005: INFO: validating pod update-demo-nautilus-4cbhg
Jun 26 15:17:24.017: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 26 15:17:24.018: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 26 15:17:24.018: INFO: update-demo-nautilus-4cbhg is verified up and running
Jun 26 15:17:24.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-f696q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-385'
Jun 26 15:17:24.212: INFO: stderr: ""
Jun 26 15:17:24.212: INFO: stdout: "true"
Jun 26 15:17:24.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-f696q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-385'
Jun 26 15:17:24.410: INFO: stderr: ""
Jun 26 15:17:24.410: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 26 15:17:24.410: INFO: validating pod update-demo-nautilus-f696q
Jun 26 15:17:24.420: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 26 15:17:24.420: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 26 15:17:24.420: INFO: update-demo-nautilus-f696q is verified up and running
STEP: scaling down the replication controller
Jun 26 15:17:24.426: INFO: scanned /root for discovery docs: <nil>
Jun 26 15:17:24.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-385'
Jun 26 15:17:25.674: INFO: stderr: ""
Jun 26 15:17:25.674: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 26 15:17:25.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-385'
Jun 26 15:17:25.870: INFO: stderr: ""
Jun 26 15:17:25.870: INFO: stdout: "update-demo-nautilus-4cbhg update-demo-nautilus-f696q "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 26 15:17:30.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-385'
Jun 26 15:17:30.962: INFO: stderr: ""
Jun 26 15:17:30.962: INFO: stdout: "update-demo-nautilus-4cbhg update-demo-nautilus-f696q "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 26 15:17:35.963: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-385'
Jun 26 15:17:36.191: INFO: stderr: ""
Jun 26 15:17:36.191: INFO: stdout: "update-demo-nautilus-4cbhg "
Jun 26 15:17:36.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-4cbhg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-385'
Jun 26 15:17:36.383: INFO: stderr: ""
Jun 26 15:17:36.383: INFO: stdout: "true"
Jun 26 15:17:36.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-4cbhg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-385'
Jun 26 15:17:36.571: INFO: stderr: ""
Jun 26 15:17:36.571: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 26 15:17:36.571: INFO: validating pod update-demo-nautilus-4cbhg
Jun 26 15:17:36.582: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 26 15:17:36.583: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 26 15:17:36.583: INFO: update-demo-nautilus-4cbhg is verified up and running
STEP: scaling up the replication controller
Jun 26 15:17:36.592: INFO: scanned /root for discovery docs: <nil>
Jun 26 15:17:36.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-385'
Jun 26 15:17:37.860: INFO: stderr: ""
Jun 26 15:17:37.860: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 26 15:17:37.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-385'
Jun 26 15:17:38.072: INFO: stderr: ""
Jun 26 15:17:38.073: INFO: stdout: "update-demo-nautilus-4cbhg update-demo-nautilus-ntjpb "
Jun 26 15:17:38.073: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-4cbhg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-385'
Jun 26 15:17:38.271: INFO: stderr: ""
Jun 26 15:17:38.271: INFO: stdout: "true"
Jun 26 15:17:38.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-4cbhg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-385'
Jun 26 15:17:38.476: INFO: stderr: ""
Jun 26 15:17:38.476: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 26 15:17:38.476: INFO: validating pod update-demo-nautilus-4cbhg
Jun 26 15:17:38.488: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 26 15:17:38.488: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 26 15:17:38.488: INFO: update-demo-nautilus-4cbhg is verified up and running
Jun 26 15:17:38.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-ntjpb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-385'
Jun 26 15:17:38.737: INFO: stderr: ""
Jun 26 15:17:38.738: INFO: stdout: ""
Jun 26 15:17:38.738: INFO: update-demo-nautilus-ntjpb is created but not running
Jun 26 15:17:43.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-385'
Jun 26 15:17:43.943: INFO: stderr: ""
Jun 26 15:17:43.943: INFO: stdout: "update-demo-nautilus-4cbhg update-demo-nautilus-ntjpb "
Jun 26 15:17:43.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-4cbhg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-385'
Jun 26 15:17:44.136: INFO: stderr: ""
Jun 26 15:17:44.136: INFO: stdout: "true"
Jun 26 15:17:44.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-4cbhg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-385'
Jun 26 15:17:44.321: INFO: stderr: ""
Jun 26 15:17:44.321: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 26 15:17:44.321: INFO: validating pod update-demo-nautilus-4cbhg
Jun 26 15:17:44.327: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 26 15:17:44.327: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 26 15:17:44.328: INFO: update-demo-nautilus-4cbhg is verified up and running
Jun 26 15:17:44.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-ntjpb -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-385'
Jun 26 15:17:44.488: INFO: stderr: ""
Jun 26 15:17:44.488: INFO: stdout: "true"
Jun 26 15:17:44.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods update-demo-nautilus-ntjpb -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-385'
Jun 26 15:17:44.665: INFO: stderr: ""
Jun 26 15:17:44.665: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 26 15:17:44.665: INFO: validating pod update-demo-nautilus-ntjpb
Jun 26 15:17:44.675: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 26 15:17:44.675: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 26 15:17:44.675: INFO: update-demo-nautilus-ntjpb is verified up and running
STEP: using delete to clean up resources
Jun 26 15:17:44.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 delete --grace-period=0 --force -f - --namespace=kubectl-385'
Jun 26 15:17:44.805: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 26 15:17:44.805: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 26 15:17:44.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-385'
Jun 26 15:17:44.898: INFO: stderr: "No resources found.\n"
Jun 26 15:17:44.898: INFO: stdout: ""
Jun 26 15:17:44.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods -l name=update-demo --namespace=kubectl-385 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 26 15:17:44.978: INFO: stderr: ""
Jun 26 15:17:44.978: INFO: stdout: "update-demo-nautilus-4cbhg\nupdate-demo-nautilus-ntjpb\n"
Jun 26 15:17:45.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-385'
Jun 26 15:17:45.583: INFO: stderr: "No resources found.\n"
Jun 26 15:17:45.583: INFO: stdout: ""
Jun 26 15:17:45.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods -l name=update-demo --namespace=kubectl-385 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 26 15:17:45.675: INFO: stderr: ""
Jun 26 15:17:45.675: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:17:45.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-385" for this suite.
Jun 26 15:18:07.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:18:07.934: INFO: namespace kubectl-385 deletion completed in 22.253838242s

• [SLOW TEST:50.133 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:18:07.935: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-faf285a8-6724-484f-97fc-8643a8d21867
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:18:10.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2332" for this suite.
Jun 26 15:18:32.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:18:32.302: INFO: namespace configmap-2332 deletion completed in 22.222664848s

• [SLOW TEST:24.368 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:18:32.306: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun 26 15:18:32.385: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jun 26 15:18:39.465: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:18:39.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6518" for this suite.
Jun 26 15:18:45.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:18:45.728: INFO: namespace pods-6518 deletion completed in 6.243000407s

• [SLOW TEST:13.422 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:18:45.729: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Jun 26 15:18:48.357: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5152 pod-service-account-62a64dd2-6ad0-43bf-b758-0ae596b4ee88 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun 26 15:18:48.666: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5152 pod-service-account-62a64dd2-6ad0-43bf-b758-0ae596b4ee88 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun 26 15:18:49.048: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5152 pod-service-account-62a64dd2-6ad0-43bf-b758-0ae596b4ee88 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:18:49.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5152" for this suite.
Jun 26 15:18:55.482: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:18:55.562: INFO: namespace svcaccounts-5152 deletion completed in 6.089964188s

• [SLOW TEST:9.834 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:18:55.563: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-087792b6-4363-41d1-9263-30cee9e5769b
STEP: Creating a pod to test consume configMaps
Jun 26 15:18:55.600: INFO: Waiting up to 5m0s for pod "pod-configmaps-3324b202-dc2c-436d-8c67-d08a9a3b7a14" in namespace "configmap-9709" to be "success or failure"
Jun 26 15:18:55.604: INFO: Pod "pod-configmaps-3324b202-dc2c-436d-8c67-d08a9a3b7a14": Phase="Pending", Reason="", readiness=false. Elapsed: 3.784687ms
Jun 26 15:18:57.608: INFO: Pod "pod-configmaps-3324b202-dc2c-436d-8c67-d08a9a3b7a14": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007157906s
STEP: Saw pod success
Jun 26 15:18:57.608: INFO: Pod "pod-configmaps-3324b202-dc2c-436d-8c67-d08a9a3b7a14" satisfied condition "success or failure"
Jun 26 15:18:57.611: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-configmaps-3324b202-dc2c-436d-8c67-d08a9a3b7a14 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 26 15:18:57.647: INFO: Waiting for pod pod-configmaps-3324b202-dc2c-436d-8c67-d08a9a3b7a14 to disappear
Jun 26 15:18:57.651: INFO: Pod pod-configmaps-3324b202-dc2c-436d-8c67-d08a9a3b7a14 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:18:57.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9709" for this suite.
Jun 26 15:19:03.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:19:03.927: INFO: namespace configmap-9709 deletion completed in 6.268100742s

• [SLOW TEST:8.365 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:19:03.930: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 26 15:19:04.069: INFO: Waiting up to 5m0s for pod "downward-api-d8a78832-f7ed-4566-a42f-12775a1b247c" in namespace "downward-api-8590" to be "success or failure"
Jun 26 15:19:04.075: INFO: Pod "downward-api-d8a78832-f7ed-4566-a42f-12775a1b247c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.807922ms
Jun 26 15:19:06.083: INFO: Pod "downward-api-d8a78832-f7ed-4566-a42f-12775a1b247c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014059457s
Jun 26 15:19:08.092: INFO: Pod "downward-api-d8a78832-f7ed-4566-a42f-12775a1b247c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022362043s
STEP: Saw pod success
Jun 26 15:19:08.092: INFO: Pod "downward-api-d8a78832-f7ed-4566-a42f-12775a1b247c" satisfied condition "success or failure"
Jun 26 15:19:08.099: INFO: Trying to get logs from node k8s-worker-1-dev pod downward-api-d8a78832-f7ed-4566-a42f-12775a1b247c container dapi-container: <nil>
STEP: delete the pod
Jun 26 15:19:08.142: INFO: Waiting for pod downward-api-d8a78832-f7ed-4566-a42f-12775a1b247c to disappear
Jun 26 15:19:08.148: INFO: Pod downward-api-d8a78832-f7ed-4566-a42f-12775a1b247c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:19:08.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8590" for this suite.
Jun 26 15:19:14.186: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:19:14.404: INFO: namespace downward-api-8590 deletion completed in 6.244590247s

• [SLOW TEST:10.473 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:19:14.405: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:19:14.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-304" for this suite.
Jun 26 15:19:36.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:19:36.767: INFO: namespace pods-304 deletion completed in 22.232790087s

• [SLOW TEST:22.363 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:19:36.772: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 26 15:19:36.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-3583'
Jun 26 15:19:37.045: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 26 15:19:37.045: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Jun 26 15:19:37.066: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jun 26 15:19:37.081: INFO: scanned /root for discovery docs: <nil>
Jun 26 15:19:37.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-3583'
Jun 26 15:19:53.070: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 26 15:19:53.070: INFO: stdout: "Created e2e-test-nginx-rc-ae441c64977ffa3a6eb64941dac4e56d\nScaling up e2e-test-nginx-rc-ae441c64977ffa3a6eb64941dac4e56d from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-ae441c64977ffa3a6eb64941dac4e56d up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-ae441c64977ffa3a6eb64941dac4e56d to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jun 26 15:19:53.070: INFO: stdout: "Created e2e-test-nginx-rc-ae441c64977ffa3a6eb64941dac4e56d\nScaling up e2e-test-nginx-rc-ae441c64977ffa3a6eb64941dac4e56d from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-ae441c64977ffa3a6eb64941dac4e56d up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-ae441c64977ffa3a6eb64941dac4e56d to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jun 26 15:19:53.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-3583'
Jun 26 15:19:53.161: INFO: stderr: ""
Jun 26 15:19:53.161: INFO: stdout: "e2e-test-nginx-rc-ae441c64977ffa3a6eb64941dac4e56d-5mtgs "
Jun 26 15:19:53.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods e2e-test-nginx-rc-ae441c64977ffa3a6eb64941dac4e56d-5mtgs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3583'
Jun 26 15:19:53.248: INFO: stderr: ""
Jun 26 15:19:53.248: INFO: stdout: "true"
Jun 26 15:19:53.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pods e2e-test-nginx-rc-ae441c64977ffa3a6eb64941dac4e56d-5mtgs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3583'
Jun 26 15:19:53.337: INFO: stderr: ""
Jun 26 15:19:53.337: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jun 26 15:19:53.337: INFO: e2e-test-nginx-rc-ae441c64977ffa3a6eb64941dac4e56d-5mtgs is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Jun 26 15:19:53.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 delete rc e2e-test-nginx-rc --namespace=kubectl-3583'
Jun 26 15:19:53.425: INFO: stderr: ""
Jun 26 15:19:53.425: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:19:53.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3583" for this suite.
Jun 26 15:20:15.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:20:15.662: INFO: namespace kubectl-3583 deletion completed in 22.230516993s

• [SLOW TEST:38.891 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:20:15.663: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-3867
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 26 15:20:15.734: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 26 15:20:39.903: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.146.246 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3867 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 26 15:20:39.903: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
Jun 26 15:20:41.272: INFO: Found all expected endpoints: [netserver-0]
Jun 26 15:20:41.276: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.97.227 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3867 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 26 15:20:41.276: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
Jun 26 15:20:42.476: INFO: Found all expected endpoints: [netserver-1]
Jun 26 15:20:42.486: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.249.9 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3867 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 26 15:20:42.486: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
Jun 26 15:20:43.768: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:20:43.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3867" for this suite.
Jun 26 15:21:05.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:21:05.995: INFO: namespace pod-network-test-3867 deletion completed in 22.216984473s

• [SLOW TEST:50.332 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:21:05.996: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-ec2829e0-cf96-48bf-a9b7-33138a4c874d
STEP: Creating a pod to test consume secrets
Jun 26 15:21:06.088: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-39ddf72f-b899-4187-81a3-5c3b527e3897" in namespace "projected-5192" to be "success or failure"
Jun 26 15:21:06.094: INFO: Pod "pod-projected-secrets-39ddf72f-b899-4187-81a3-5c3b527e3897": Phase="Pending", Reason="", readiness=false. Elapsed: 6.291826ms
Jun 26 15:21:08.102: INFO: Pod "pod-projected-secrets-39ddf72f-b899-4187-81a3-5c3b527e3897": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01413522s
Jun 26 15:21:10.106: INFO: Pod "pod-projected-secrets-39ddf72f-b899-4187-81a3-5c3b527e3897": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.018120389s
STEP: Saw pod success
Jun 26 15:21:10.106: INFO: Pod "pod-projected-secrets-39ddf72f-b899-4187-81a3-5c3b527e3897" satisfied condition "success or failure"
Jun 26 15:21:10.109: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-projected-secrets-39ddf72f-b899-4187-81a3-5c3b527e3897 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 26 15:21:10.123: INFO: Waiting for pod pod-projected-secrets-39ddf72f-b899-4187-81a3-5c3b527e3897 to disappear
Jun 26 15:21:10.126: INFO: Pod pod-projected-secrets-39ddf72f-b899-4187-81a3-5c3b527e3897 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:21:10.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5192" for this suite.
Jun 26 15:21:16.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:21:16.227: INFO: namespace projected-5192 deletion completed in 6.096314934s

• [SLOW TEST:10.231 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:21:16.227: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 26 15:21:16.270: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d8cf003b-d217-407f-8df3-b78a45f7a420" in namespace "projected-3673" to be "success or failure"
Jun 26 15:21:16.282: INFO: Pod "downwardapi-volume-d8cf003b-d217-407f-8df3-b78a45f7a420": Phase="Pending", Reason="", readiness=false. Elapsed: 11.767435ms
Jun 26 15:21:18.287: INFO: Pod "downwardapi-volume-d8cf003b-d217-407f-8df3-b78a45f7a420": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015935525s
STEP: Saw pod success
Jun 26 15:21:18.287: INFO: Pod "downwardapi-volume-d8cf003b-d217-407f-8df3-b78a45f7a420" satisfied condition "success or failure"
Jun 26 15:21:18.289: INFO: Trying to get logs from node k8s-worker-2-dev pod downwardapi-volume-d8cf003b-d217-407f-8df3-b78a45f7a420 container client-container: <nil>
STEP: delete the pod
Jun 26 15:21:18.305: INFO: Waiting for pod downwardapi-volume-d8cf003b-d217-407f-8df3-b78a45f7a420 to disappear
Jun 26 15:21:18.309: INFO: Pod downwardapi-volume-d8cf003b-d217-407f-8df3-b78a45f7a420 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:21:18.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3673" for this suite.
Jun 26 15:21:24.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:21:24.536: INFO: namespace projected-3673 deletion completed in 6.21674975s

• [SLOW TEST:8.309 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:21:24.537: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Jun 26 15:21:24.623: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-2286" to be "success or failure"
Jun 26 15:21:24.631: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 7.83398ms
Jun 26 15:21:26.639: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015974544s
Jun 26 15:21:28.645: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022339254s
STEP: Saw pod success
Jun 26 15:21:28.645: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jun 26 15:21:28.649: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jun 26 15:21:28.672: INFO: Waiting for pod pod-host-path-test to disappear
Jun 26 15:21:28.675: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:21:28.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-2286" for this suite.
Jun 26 15:21:34.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:21:34.892: INFO: namespace hostpath-2286 deletion completed in 6.212356345s

• [SLOW TEST:10.356 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:21:34.902: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 26 15:21:35.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-632'
Jun 26 15:21:35.209: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 26 15:21:35.209: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jun 26 15:21:35.244: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-x5bk6]
Jun 26 15:21:35.245: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-x5bk6" in namespace "kubectl-632" to be "running and ready"
Jun 26 15:21:35.255: INFO: Pod "e2e-test-nginx-rc-x5bk6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.892659ms
Jun 26 15:21:37.263: INFO: Pod "e2e-test-nginx-rc-x5bk6": Phase="Running", Reason="", readiness=true. Elapsed: 2.017682604s
Jun 26 15:21:37.263: INFO: Pod "e2e-test-nginx-rc-x5bk6" satisfied condition "running and ready"
Jun 26 15:21:37.263: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-x5bk6]
Jun 26 15:21:37.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 logs rc/e2e-test-nginx-rc --namespace=kubectl-632'
Jun 26 15:21:37.548: INFO: stderr: ""
Jun 26 15:21:37.548: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Jun 26 15:21:37.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 delete rc e2e-test-nginx-rc --namespace=kubectl-632'
Jun 26 15:21:37.761: INFO: stderr: ""
Jun 26 15:21:37.762: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:21:37.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-632" for this suite.
Jun 26 15:21:43.795: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:21:43.993: INFO: namespace kubectl-632 deletion completed in 6.220682476s

• [SLOW TEST:9.091 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:21:43.998: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 26 15:21:44.067: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a2fb13e3-1a6f-4a52-95e9-43f0a86cc32b" in namespace "projected-7503" to be "success or failure"
Jun 26 15:21:44.077: INFO: Pod "downwardapi-volume-a2fb13e3-1a6f-4a52-95e9-43f0a86cc32b": Phase="Pending", Reason="", readiness=false. Elapsed: 9.022851ms
Jun 26 15:21:46.081: INFO: Pod "downwardapi-volume-a2fb13e3-1a6f-4a52-95e9-43f0a86cc32b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013335019s
STEP: Saw pod success
Jun 26 15:21:46.081: INFO: Pod "downwardapi-volume-a2fb13e3-1a6f-4a52-95e9-43f0a86cc32b" satisfied condition "success or failure"
Jun 26 15:21:46.083: INFO: Trying to get logs from node k8s-worker-1-dev pod downwardapi-volume-a2fb13e3-1a6f-4a52-95e9-43f0a86cc32b container client-container: <nil>
STEP: delete the pod
Jun 26 15:21:46.098: INFO: Waiting for pod downwardapi-volume-a2fb13e3-1a6f-4a52-95e9-43f0a86cc32b to disappear
Jun 26 15:21:46.100: INFO: Pod downwardapi-volume-a2fb13e3-1a6f-4a52-95e9-43f0a86cc32b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:21:46.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7503" for this suite.
Jun 26 15:21:52.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:21:52.324: INFO: namespace projected-7503 deletion completed in 6.220095366s

• [SLOW TEST:8.326 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:21:52.327: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-4e76f72d-f9e6-4a4a-b5f8-ba5af364618e
STEP: Creating a pod to test consume secrets
Jun 26 15:21:52.409: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2a27cf2d-cdf1-4520-b6c7-87f2fa5a3c57" in namespace "projected-3592" to be "success or failure"
Jun 26 15:21:52.418: INFO: Pod "pod-projected-secrets-2a27cf2d-cdf1-4520-b6c7-87f2fa5a3c57": Phase="Pending", Reason="", readiness=false. Elapsed: 8.391196ms
Jun 26 15:21:54.425: INFO: Pod "pod-projected-secrets-2a27cf2d-cdf1-4520-b6c7-87f2fa5a3c57": Phase="Running", Reason="", readiness=true. Elapsed: 2.016012899s
Jun 26 15:21:56.432: INFO: Pod "pod-projected-secrets-2a27cf2d-cdf1-4520-b6c7-87f2fa5a3c57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023069579s
STEP: Saw pod success
Jun 26 15:21:56.432: INFO: Pod "pod-projected-secrets-2a27cf2d-cdf1-4520-b6c7-87f2fa5a3c57" satisfied condition "success or failure"
Jun 26 15:21:56.439: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-projected-secrets-2a27cf2d-cdf1-4520-b6c7-87f2fa5a3c57 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 26 15:21:56.476: INFO: Waiting for pod pod-projected-secrets-2a27cf2d-cdf1-4520-b6c7-87f2fa5a3c57 to disappear
Jun 26 15:21:56.482: INFO: Pod pod-projected-secrets-2a27cf2d-cdf1-4520-b6c7-87f2fa5a3c57 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:21:56.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3592" for this suite.
Jun 26 15:22:02.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:22:02.714: INFO: namespace projected-3592 deletion completed in 6.221618349s

• [SLOW TEST:10.387 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:22:02.718: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4201
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jun 26 15:22:02.819: INFO: Found 0 stateful pods, waiting for 3
Jun 26 15:22:12.825: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 26 15:22:12.825: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 26 15:22:12.825: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jun 26 15:22:12.849: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun 26 15:22:22.898: INFO: Updating stateful set ss2
Jun 26 15:22:22.920: INFO: Waiting for Pod statefulset-4201/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Jun 26 15:22:33.049: INFO: Found 2 stateful pods, waiting for 3
Jun 26 15:22:43.054: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 26 15:22:43.054: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 26 15:22:43.054: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun 26 15:22:43.077: INFO: Updating stateful set ss2
Jun 26 15:22:43.087: INFO: Waiting for Pod statefulset-4201/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 26 15:22:53.134: INFO: Updating stateful set ss2
Jun 26 15:22:53.165: INFO: Waiting for StatefulSet statefulset-4201/ss2 to complete update
Jun 26 15:22:53.165: INFO: Waiting for Pod statefulset-4201/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 26 15:23:03.180: INFO: Deleting all statefulset in ns statefulset-4201
Jun 26 15:23:03.188: INFO: Scaling statefulset ss2 to 0
Jun 26 15:23:23.226: INFO: Waiting for statefulset status.replicas updated to 0
Jun 26 15:23:23.234: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:23:23.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4201" for this suite.
Jun 26 15:23:35.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:23:35.543: INFO: namespace statefulset-4201 deletion completed in 12.256450856s

• [SLOW TEST:92.826 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:23:35.548: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 26 15:24:16.716: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 26 15:24:16.722: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 26 15:24:18.723: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 26 15:24:18.728: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 26 15:24:20.723: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 26 15:24:20.732: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 26 15:24:22.723: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 26 15:24:22.727: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 26 15:24:24.723: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 26 15:24:24.732: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 26 15:24:26.723: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 26 15:24:26.731: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 26 15:24:28.723: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 26 15:24:28.731: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 26 15:24:30.723: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 26 15:24:30.733: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 26 15:24:32.723: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 26 15:24:32.730: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 26 15:24:34.723: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 26 15:24:34.726: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 26 15:24:36.723: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 26 15:24:36.727: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 26 15:24:38.723: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 26 15:24:38.726: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 26 15:24:40.723: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 26 15:24:40.734: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 26 15:24:42.723: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 26 15:24:42.732: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:24:42.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-744" for this suite.
Jun 26 15:25:04.764: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:25:04.847: INFO: namespace container-lifecycle-hook-744 deletion completed in 22.105115866s

• [SLOW TEST:89.298 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:25:04.847: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Jun 26 15:25:04.883: INFO: Waiting up to 5m0s for pod "client-containers-7d9afdd6-99f2-49ec-a096-a206be68d78c" in namespace "containers-3999" to be "success or failure"
Jun 26 15:25:04.888: INFO: Pod "client-containers-7d9afdd6-99f2-49ec-a096-a206be68d78c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.964546ms
Jun 26 15:25:06.897: INFO: Pod "client-containers-7d9afdd6-99f2-49ec-a096-a206be68d78c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012994558s
Jun 26 15:25:08.907: INFO: Pod "client-containers-7d9afdd6-99f2-49ec-a096-a206be68d78c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023564269s
STEP: Saw pod success
Jun 26 15:25:08.907: INFO: Pod "client-containers-7d9afdd6-99f2-49ec-a096-a206be68d78c" satisfied condition "success or failure"
Jun 26 15:25:08.914: INFO: Trying to get logs from node k8s-worker-1-dev pod client-containers-7d9afdd6-99f2-49ec-a096-a206be68d78c container test-container: <nil>
STEP: delete the pod
Jun 26 15:25:08.956: INFO: Waiting for pod client-containers-7d9afdd6-99f2-49ec-a096-a206be68d78c to disappear
Jun 26 15:25:08.964: INFO: Pod client-containers-7d9afdd6-99f2-49ec-a096-a206be68d78c no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:25:08.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3999" for this suite.
Jun 26 15:25:14.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:25:15.187: INFO: namespace containers-3999 deletion completed in 6.214498019s

• [SLOW TEST:10.340 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:25:15.197: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 26 15:25:15.272: INFO: Waiting up to 5m0s for pod "pod-488c7327-88fa-404a-a067-2f3cb0c3a8e0" in namespace "emptydir-6162" to be "success or failure"
Jun 26 15:25:15.280: INFO: Pod "pod-488c7327-88fa-404a-a067-2f3cb0c3a8e0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.565037ms
Jun 26 15:25:17.284: INFO: Pod "pod-488c7327-88fa-404a-a067-2f3cb0c3a8e0": Phase="Running", Reason="", readiness=true. Elapsed: 2.011632148s
Jun 26 15:25:19.288: INFO: Pod "pod-488c7327-88fa-404a-a067-2f3cb0c3a8e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015374501s
STEP: Saw pod success
Jun 26 15:25:19.288: INFO: Pod "pod-488c7327-88fa-404a-a067-2f3cb0c3a8e0" satisfied condition "success or failure"
Jun 26 15:25:19.290: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-488c7327-88fa-404a-a067-2f3cb0c3a8e0 container test-container: <nil>
STEP: delete the pod
Jun 26 15:25:19.331: INFO: Waiting for pod pod-488c7327-88fa-404a-a067-2f3cb0c3a8e0 to disappear
Jun 26 15:25:19.334: INFO: Pod pod-488c7327-88fa-404a-a067-2f3cb0c3a8e0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:25:19.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6162" for this suite.
Jun 26 15:25:25.360: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:25:25.554: INFO: namespace emptydir-6162 deletion completed in 6.215467777s

• [SLOW TEST:10.358 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:25:25.556: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-dd4bacdb-42ea-4133-9769-715348c45eaf
STEP: Creating a pod to test consume secrets
Jun 26 15:25:25.707: INFO: Waiting up to 5m0s for pod "pod-secrets-3100b182-8e35-49a6-997d-50bbb8bd2fec" in namespace "secrets-1127" to be "success or failure"
Jun 26 15:25:25.719: INFO: Pod "pod-secrets-3100b182-8e35-49a6-997d-50bbb8bd2fec": Phase="Pending", Reason="", readiness=false. Elapsed: 11.778609ms
Jun 26 15:25:27.727: INFO: Pod "pod-secrets-3100b182-8e35-49a6-997d-50bbb8bd2fec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020021986s
STEP: Saw pod success
Jun 26 15:25:27.727: INFO: Pod "pod-secrets-3100b182-8e35-49a6-997d-50bbb8bd2fec" satisfied condition "success or failure"
Jun 26 15:25:27.733: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-secrets-3100b182-8e35-49a6-997d-50bbb8bd2fec container secret-volume-test: <nil>
STEP: delete the pod
Jun 26 15:25:27.801: INFO: Waiting for pod pod-secrets-3100b182-8e35-49a6-997d-50bbb8bd2fec to disappear
Jun 26 15:25:27.811: INFO: Pod pod-secrets-3100b182-8e35-49a6-997d-50bbb8bd2fec no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:25:27.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1127" for this suite.
Jun 26 15:25:33.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:25:34.068: INFO: namespace secrets-1127 deletion completed in 6.244281488s
STEP: Destroying namespace "secret-namespace-4500" for this suite.
Jun 26 15:25:40.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:25:40.267: INFO: namespace secret-namespace-4500 deletion completed in 6.198207714s

• [SLOW TEST:14.711 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:25:40.268: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jun 26 15:25:50.329: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:25:50.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0626 15:25:50.329605      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-7126" for this suite.
Jun 26 15:25:56.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:25:56.565: INFO: namespace gc-7126 deletion completed in 6.227231291s

• [SLOW TEST:16.298 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:25:56.571: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Jun 26 15:25:56.657: INFO: Waiting up to 5m0s for pod "client-containers-9a90ed6e-3c4e-4587-b03d-ceb31d139686" in namespace "containers-9279" to be "success or failure"
Jun 26 15:25:56.663: INFO: Pod "client-containers-9a90ed6e-3c4e-4587-b03d-ceb31d139686": Phase="Pending", Reason="", readiness=false. Elapsed: 5.76029ms
Jun 26 15:25:58.666: INFO: Pod "client-containers-9a90ed6e-3c4e-4587-b03d-ceb31d139686": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009323215s
STEP: Saw pod success
Jun 26 15:25:58.666: INFO: Pod "client-containers-9a90ed6e-3c4e-4587-b03d-ceb31d139686" satisfied condition "success or failure"
Jun 26 15:25:58.669: INFO: Trying to get logs from node k8s-worker-1-dev pod client-containers-9a90ed6e-3c4e-4587-b03d-ceb31d139686 container test-container: <nil>
STEP: delete the pod
Jun 26 15:25:58.688: INFO: Waiting for pod client-containers-9a90ed6e-3c4e-4587-b03d-ceb31d139686 to disappear
Jun 26 15:25:58.691: INFO: Pod client-containers-9a90ed6e-3c4e-4587-b03d-ceb31d139686 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:25:58.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9279" for this suite.
Jun 26 15:26:04.708: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:26:04.912: INFO: namespace containers-9279 deletion completed in 6.217443486s

• [SLOW TEST:8.342 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:26:04.914: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 26 15:26:05.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-9239'
Jun 26 15:26:05.271: INFO: stderr: ""
Jun 26 15:26:05.271: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jun 26 15:26:10.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 get pod e2e-test-nginx-pod --namespace=kubectl-9239 -o json'
Jun 26 15:26:10.407: INFO: stderr: ""
Jun 26 15:26:10.407: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"172.16.249.21/32\"\n        },\n        \"creationTimestamp\": \"2019-06-26T15:26:05Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-9239\",\n        \"resourceVersion\": \"30421\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-9239/pods/e2e-test-nginx-pod\",\n        \"uid\": \"d2f83352-64ab-4746-8ce3-442d55c5f7c5\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-z28nd\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-worker-1-dev\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-z28nd\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-z28nd\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-26T15:26:05Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-26T15:26:06Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-26T15:26:06Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-26T15:26:05Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://b6c4ad037ede9e9d188a5a7e61db00a3d578212cb50220ca430a49f2e07e0c81\",\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imageID\": \"docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-06-26T15:26:06Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.9.245\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.249.21\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-06-26T15:26:05Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun 26 15:26:10.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 replace -f - --namespace=kubectl-9239'
Jun 26 15:26:10.667: INFO: stderr: ""
Jun 26 15:26:10.667: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Jun 26 15:26:10.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 delete pods e2e-test-nginx-pod --namespace=kubectl-9239'
Jun 26 15:26:21.712: INFO: stderr: ""
Jun 26 15:26:21.712: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:26:21.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9239" for this suite.
Jun 26 15:26:27.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:26:27.949: INFO: namespace kubectl-9239 deletion completed in 6.224909425s

• [SLOW TEST:23.036 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:26:27.950: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 26 15:26:50.071: INFO: Container started at 2019-06-26 15:26:28 +0000 UTC, pod became ready at 2019-06-26 15:26:49 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:26:50.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1448" for this suite.
Jun 26 15:27:12.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:27:12.181: INFO: namespace container-probe-1448 deletion completed in 22.097806157s

• [SLOW TEST:44.231 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:27:12.182: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jun 26 15:27:42.761: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:27:42.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0626 15:27:42.760934      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-1138" for this suite.
Jun 26 15:27:48.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:27:49.051: INFO: namespace gc-1138 deletion completed in 6.275443099s

• [SLOW TEST:36.870 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:27:49.054: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jun 26 15:27:49.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 create -f - --namespace=kubectl-4374'
Jun 26 15:27:49.929: INFO: stderr: ""
Jun 26 15:27:49.929: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 26 15:27:50.936: INFO: Selector matched 1 pods for map[app:redis]
Jun 26 15:27:50.936: INFO: Found 0 / 1
Jun 26 15:27:51.943: INFO: Selector matched 1 pods for map[app:redis]
Jun 26 15:27:51.943: INFO: Found 0 / 1
Jun 26 15:27:52.943: INFO: Selector matched 1 pods for map[app:redis]
Jun 26 15:27:52.944: INFO: Found 1 / 1
Jun 26 15:27:52.944: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun 26 15:27:52.954: INFO: Selector matched 1 pods for map[app:redis]
Jun 26 15:27:52.954: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 26 15:27:52.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 patch pod redis-master-cn6qp --namespace=kubectl-4374 -p {"metadata":{"annotations":{"x":"y"}}}'
Jun 26 15:27:53.174: INFO: stderr: ""
Jun 26 15:27:53.175: INFO: stdout: "pod/redis-master-cn6qp patched\n"
STEP: checking annotations
Jun 26 15:27:53.186: INFO: Selector matched 1 pods for map[app:redis]
Jun 26 15:27:53.186: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:27:53.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4374" for this suite.
Jun 26 15:28:15.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:28:15.409: INFO: namespace kubectl-4374 deletion completed in 22.213023347s

• [SLOW TEST:26.356 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:28:15.414: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 26 15:28:15.500: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ce3839b3-fa2b-4d42-9fd6-66db206e174f" in namespace "projected-2272" to be "success or failure"
Jun 26 15:28:15.509: INFO: Pod "downwardapi-volume-ce3839b3-fa2b-4d42-9fd6-66db206e174f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.336908ms
Jun 26 15:28:17.517: INFO: Pod "downwardapi-volume-ce3839b3-fa2b-4d42-9fd6-66db206e174f": Phase="Running", Reason="", readiness=true. Elapsed: 2.016596409s
Jun 26 15:28:19.521: INFO: Pod "downwardapi-volume-ce3839b3-fa2b-4d42-9fd6-66db206e174f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020385657s
STEP: Saw pod success
Jun 26 15:28:19.521: INFO: Pod "downwardapi-volume-ce3839b3-fa2b-4d42-9fd6-66db206e174f" satisfied condition "success or failure"
Jun 26 15:28:19.523: INFO: Trying to get logs from node k8s-worker-1-dev pod downwardapi-volume-ce3839b3-fa2b-4d42-9fd6-66db206e174f container client-container: <nil>
STEP: delete the pod
Jun 26 15:28:19.540: INFO: Waiting for pod downwardapi-volume-ce3839b3-fa2b-4d42-9fd6-66db206e174f to disappear
Jun 26 15:28:19.543: INFO: Pod downwardapi-volume-ce3839b3-fa2b-4d42-9fd6-66db206e174f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:28:19.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2272" for this suite.
Jun 26 15:28:25.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:28:25.755: INFO: namespace projected-2272 deletion completed in 6.207297849s

• [SLOW TEST:10.341 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:28:25.756: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 26 15:28:25.844: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9ac3c9b3-6cdd-4e6c-b8ea-8e6322c8db66" in namespace "downward-api-140" to be "success or failure"
Jun 26 15:28:25.850: INFO: Pod "downwardapi-volume-9ac3c9b3-6cdd-4e6c-b8ea-8e6322c8db66": Phase="Pending", Reason="", readiness=false. Elapsed: 6.196792ms
Jun 26 15:28:27.857: INFO: Pod "downwardapi-volume-9ac3c9b3-6cdd-4e6c-b8ea-8e6322c8db66": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013645709s
Jun 26 15:28:29.862: INFO: Pod "downwardapi-volume-9ac3c9b3-6cdd-4e6c-b8ea-8e6322c8db66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.017789571s
STEP: Saw pod success
Jun 26 15:28:29.862: INFO: Pod "downwardapi-volume-9ac3c9b3-6cdd-4e6c-b8ea-8e6322c8db66" satisfied condition "success or failure"
Jun 26 15:28:29.864: INFO: Trying to get logs from node k8s-worker-1-dev pod downwardapi-volume-9ac3c9b3-6cdd-4e6c-b8ea-8e6322c8db66 container client-container: <nil>
STEP: delete the pod
Jun 26 15:28:29.877: INFO: Waiting for pod downwardapi-volume-9ac3c9b3-6cdd-4e6c-b8ea-8e6322c8db66 to disappear
Jun 26 15:28:29.879: INFO: Pod downwardapi-volume-9ac3c9b3-6cdd-4e6c-b8ea-8e6322c8db66 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:28:29.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-140" for this suite.
Jun 26 15:28:35.899: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:28:36.100: INFO: namespace downward-api-140 deletion completed in 6.216858233s

• [SLOW TEST:10.344 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:28:36.104: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-1280
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 26 15:28:36.189: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 26 15:28:58.362: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.146.193:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1280 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 26 15:28:58.362: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
Jun 26 15:28:58.583: INFO: Found all expected endpoints: [netserver-0]
Jun 26 15:28:58.586: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.249.25:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1280 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 26 15:28:58.586: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
Jun 26 15:28:58.802: INFO: Found all expected endpoints: [netserver-1]
Jun 26 15:28:58.808: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.97.231:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1280 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 26 15:28:58.808: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
Jun 26 15:28:59.052: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:28:59.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1280" for this suite.
Jun 26 15:29:21.086: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:29:21.162: INFO: namespace pod-network-test-1280 deletion completed in 22.09832651s

• [SLOW TEST:45.058 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:29:21.165: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jun 26 15:29:23.218: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-840ae26f-706f-4f42-93c0-fab63867e616,GenerateName:,Namespace:events-8459,SelfLink:/api/v1/namespaces/events-8459/pods/send-events-840ae26f-706f-4f42-93c0-fab63867e616,UID:f249b590-ed5e-4c84-aab9-674d655d734c,ResourceVersion:31064,Generation:0,CreationTimestamp:2019-06-26 15:29:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 192281348,},Annotations:map[string]string{cni.projectcalico.org/podIP: 172.16.146.195/32,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-6tpcd {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6tpcd,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-6tpcd true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-worker-2-dev,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000770290} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0007702b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:29:21 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:29:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:29:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:29:21 +0000 UTC  }],Message:,Reason:,HostIP:192.168.9.247,PodIP:172.16.146.195,StartTime:2019-06-26 15:29:21 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-06-26 15:29:22 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 containerd://15ed3f4bfbe6d56e87d00f34d6d55d1a75e1fbf6f9e5c06405499d75a62c35fb}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jun 26 15:29:25.223: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jun 26 15:29:27.227: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:29:27.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-8459" for this suite.
Jun 26 15:30:05.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:30:05.452: INFO: namespace events-8459 deletion completed in 38.215261807s

• [SLOW TEST:44.288 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:30:05.454: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-08ac4036-80e4-4fa2-b479-5bdbfd1a7328
STEP: Creating a pod to test consume configMaps
Jun 26 15:30:05.541: INFO: Waiting up to 5m0s for pod "pod-configmaps-60576d3a-09ce-4753-8551-24184537928a" in namespace "configmap-2474" to be "success or failure"
Jun 26 15:30:05.547: INFO: Pod "pod-configmaps-60576d3a-09ce-4753-8551-24184537928a": Phase="Pending", Reason="", readiness=false. Elapsed: 5.088575ms
Jun 26 15:30:07.553: INFO: Pod "pod-configmaps-60576d3a-09ce-4753-8551-24184537928a": Phase="Running", Reason="", readiness=true. Elapsed: 2.011540378s
Jun 26 15:30:09.563: INFO: Pod "pod-configmaps-60576d3a-09ce-4753-8551-24184537928a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.020994803s
STEP: Saw pod success
Jun 26 15:30:09.563: INFO: Pod "pod-configmaps-60576d3a-09ce-4753-8551-24184537928a" satisfied condition "success or failure"
Jun 26 15:30:09.569: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-configmaps-60576d3a-09ce-4753-8551-24184537928a container configmap-volume-test: <nil>
STEP: delete the pod
Jun 26 15:30:09.606: INFO: Waiting for pod pod-configmaps-60576d3a-09ce-4753-8551-24184537928a to disappear
Jun 26 15:30:09.612: INFO: Pod pod-configmaps-60576d3a-09ce-4753-8551-24184537928a no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:30:09.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2474" for this suite.
Jun 26 15:30:15.646: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:30:15.726: INFO: namespace configmap-2474 deletion completed in 6.1045675s

• [SLOW TEST:10.273 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:30:15.727: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-1528
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1528
STEP: Creating statefulset with conflicting port in namespace statefulset-1528
STEP: Waiting until pod test-pod will start running in namespace statefulset-1528
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1528
Jun 26 15:30:19.796: INFO: Observed stateful pod in namespace: statefulset-1528, name: ss-0, uid: 312d7239-98b7-48e7-940a-c4b97677f8d8, status phase: Pending. Waiting for statefulset controller to delete.
Jun 26 15:30:20.180: INFO: Observed stateful pod in namespace: statefulset-1528, name: ss-0, uid: 312d7239-98b7-48e7-940a-c4b97677f8d8, status phase: Failed. Waiting for statefulset controller to delete.
Jun 26 15:30:20.188: INFO: Observed stateful pod in namespace: statefulset-1528, name: ss-0, uid: 312d7239-98b7-48e7-940a-c4b97677f8d8, status phase: Failed. Waiting for statefulset controller to delete.
Jun 26 15:30:20.192: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1528
STEP: Removing pod with conflicting port in namespace statefulset-1528
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1528 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 26 15:30:24.220: INFO: Deleting all statefulset in ns statefulset-1528
Jun 26 15:30:24.222: INFO: Scaling statefulset ss to 0
Jun 26 15:30:34.236: INFO: Waiting for statefulset status.replicas updated to 0
Jun 26 15:30:34.240: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:30:34.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1528" for this suite.
Jun 26 15:30:40.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:30:40.356: INFO: namespace statefulset-1528 deletion completed in 6.097079235s

• [SLOW TEST:24.630 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:30:40.363: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 26 15:30:40.396: INFO: Waiting up to 5m0s for pod "pod-b935d90f-e434-47de-8d0a-b06640dc7a21" in namespace "emptydir-8391" to be "success or failure"
Jun 26 15:30:40.399: INFO: Pod "pod-b935d90f-e434-47de-8d0a-b06640dc7a21": Phase="Pending", Reason="", readiness=false. Elapsed: 3.044291ms
Jun 26 15:30:42.403: INFO: Pod "pod-b935d90f-e434-47de-8d0a-b06640dc7a21": Phase="Pending", Reason="", readiness=false. Elapsed: 2.006851391s
Jun 26 15:30:44.407: INFO: Pod "pod-b935d90f-e434-47de-8d0a-b06640dc7a21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010610884s
STEP: Saw pod success
Jun 26 15:30:44.407: INFO: Pod "pod-b935d90f-e434-47de-8d0a-b06640dc7a21" satisfied condition "success or failure"
Jun 26 15:30:44.409: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-b935d90f-e434-47de-8d0a-b06640dc7a21 container test-container: <nil>
STEP: delete the pod
Jun 26 15:30:44.430: INFO: Waiting for pod pod-b935d90f-e434-47de-8d0a-b06640dc7a21 to disappear
Jun 26 15:30:44.433: INFO: Pod pod-b935d90f-e434-47de-8d0a-b06640dc7a21 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:30:44.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8391" for this suite.
Jun 26 15:30:50.448: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:30:50.524: INFO: namespace emptydir-8391 deletion completed in 6.087131245s

• [SLOW TEST:10.162 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:30:50.525: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 26 15:30:50.563: INFO: Waiting up to 5m0s for pod "pod-a37011bf-e86f-4346-955e-926f69a76a88" in namespace "emptydir-7589" to be "success or failure"
Jun 26 15:30:50.569: INFO: Pod "pod-a37011bf-e86f-4346-955e-926f69a76a88": Phase="Pending", Reason="", readiness=false. Elapsed: 5.333924ms
Jun 26 15:30:52.572: INFO: Pod "pod-a37011bf-e86f-4346-955e-926f69a76a88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008741573s
STEP: Saw pod success
Jun 26 15:30:52.572: INFO: Pod "pod-a37011bf-e86f-4346-955e-926f69a76a88" satisfied condition "success or failure"
Jun 26 15:30:52.575: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-a37011bf-e86f-4346-955e-926f69a76a88 container test-container: <nil>
STEP: delete the pod
Jun 26 15:30:52.596: INFO: Waiting for pod pod-a37011bf-e86f-4346-955e-926f69a76a88 to disappear
Jun 26 15:30:52.599: INFO: Pod pod-a37011bf-e86f-4346-955e-926f69a76a88 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:30:52.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7589" for this suite.
Jun 26 15:30:58.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:30:58.715: INFO: namespace emptydir-7589 deletion completed in 6.111048157s

• [SLOW TEST:8.190 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:30:58.716: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-b7bd55bc-3630-43df-bf26-b5e1095e4ca0
STEP: Creating a pod to test consume secrets
Jun 26 15:30:58.761: INFO: Waiting up to 5m0s for pod "pod-secrets-a7bb4fd8-7bba-4f4f-a9d7-59e67f574d7a" in namespace "secrets-4512" to be "success or failure"
Jun 26 15:30:58.765: INFO: Pod "pod-secrets-a7bb4fd8-7bba-4f4f-a9d7-59e67f574d7a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.908075ms
Jun 26 15:31:00.769: INFO: Pod "pod-secrets-a7bb4fd8-7bba-4f4f-a9d7-59e67f574d7a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008039502s
Jun 26 15:31:02.773: INFO: Pod "pod-secrets-a7bb4fd8-7bba-4f4f-a9d7-59e67f574d7a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01195537s
STEP: Saw pod success
Jun 26 15:31:02.773: INFO: Pod "pod-secrets-a7bb4fd8-7bba-4f4f-a9d7-59e67f574d7a" satisfied condition "success or failure"
Jun 26 15:31:02.776: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-secrets-a7bb4fd8-7bba-4f4f-a9d7-59e67f574d7a container secret-volume-test: <nil>
STEP: delete the pod
Jun 26 15:31:02.800: INFO: Waiting for pod pod-secrets-a7bb4fd8-7bba-4f4f-a9d7-59e67f574d7a to disappear
Jun 26 15:31:02.804: INFO: Pod pod-secrets-a7bb4fd8-7bba-4f4f-a9d7-59e67f574d7a no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:31:02.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4512" for this suite.
Jun 26 15:31:08.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:31:08.912: INFO: namespace secrets-4512 deletion completed in 6.103282248s

• [SLOW TEST:10.195 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:31:08.912: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 26 15:31:13.018: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 26 15:31:13.025: INFO: Pod pod-with-prestop-http-hook still exists
Jun 26 15:31:15.026: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 26 15:31:15.032: INFO: Pod pod-with-prestop-http-hook still exists
Jun 26 15:31:17.028: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 26 15:31:17.035: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:31:17.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2572" for this suite.
Jun 26 15:31:39.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:31:39.174: INFO: namespace container-lifecycle-hook-2572 deletion completed in 22.115680161s

• [SLOW TEST:30.262 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:31:39.174: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-52692276-e850-48ac-9107-197cae2079da
STEP: Creating a pod to test consume secrets
Jun 26 15:31:39.213: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c4717bda-d8f4-4a55-a0cc-77cfa3099654" in namespace "projected-4352" to be "success or failure"
Jun 26 15:31:39.217: INFO: Pod "pod-projected-secrets-c4717bda-d8f4-4a55-a0cc-77cfa3099654": Phase="Pending", Reason="", readiness=false. Elapsed: 3.089043ms
Jun 26 15:31:41.220: INFO: Pod "pod-projected-secrets-c4717bda-d8f4-4a55-a0cc-77cfa3099654": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006605415s
STEP: Saw pod success
Jun 26 15:31:41.220: INFO: Pod "pod-projected-secrets-c4717bda-d8f4-4a55-a0cc-77cfa3099654" satisfied condition "success or failure"
Jun 26 15:31:41.222: INFO: Trying to get logs from node k8s-worker-2-dev pod pod-projected-secrets-c4717bda-d8f4-4a55-a0cc-77cfa3099654 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 26 15:31:41.236: INFO: Waiting for pod pod-projected-secrets-c4717bda-d8f4-4a55-a0cc-77cfa3099654 to disappear
Jun 26 15:31:41.239: INFO: Pod pod-projected-secrets-c4717bda-d8f4-4a55-a0cc-77cfa3099654 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:31:41.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4352" for this suite.
Jun 26 15:31:47.268: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:31:47.491: INFO: namespace projected-4352 deletion completed in 6.244384245s

• [SLOW TEST:8.317 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:31:47.492: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 26 15:31:47.588: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b0fd9772-5d55-4d84-93eb-d3e1b924d379" in namespace "projected-8382" to be "success or failure"
Jun 26 15:31:47.597: INFO: Pod "downwardapi-volume-b0fd9772-5d55-4d84-93eb-d3e1b924d379": Phase="Pending", Reason="", readiness=false. Elapsed: 8.705547ms
Jun 26 15:31:49.600: INFO: Pod "downwardapi-volume-b0fd9772-5d55-4d84-93eb-d3e1b924d379": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012168779s
STEP: Saw pod success
Jun 26 15:31:49.600: INFO: Pod "downwardapi-volume-b0fd9772-5d55-4d84-93eb-d3e1b924d379" satisfied condition "success or failure"
Jun 26 15:31:49.603: INFO: Trying to get logs from node k8s-worker-2-dev pod downwardapi-volume-b0fd9772-5d55-4d84-93eb-d3e1b924d379 container client-container: <nil>
STEP: delete the pod
Jun 26 15:31:49.621: INFO: Waiting for pod downwardapi-volume-b0fd9772-5d55-4d84-93eb-d3e1b924d379 to disappear
Jun 26 15:31:49.626: INFO: Pod downwardapi-volume-b0fd9772-5d55-4d84-93eb-d3e1b924d379 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:31:49.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8382" for this suite.
Jun 26 15:31:55.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:31:55.853: INFO: namespace projected-8382 deletion completed in 6.222768919s

• [SLOW TEST:8.361 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:31:55.853: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun 26 15:31:56.281: INFO: Pod name wrapped-volume-race-ff9bbc65-87d6-46ae-869e-183982068531: Found 0 pods out of 5
Jun 26 15:32:01.288: INFO: Pod name wrapped-volume-race-ff9bbc65-87d6-46ae-869e-183982068531: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-ff9bbc65-87d6-46ae-869e-183982068531 in namespace emptydir-wrapper-4215, will wait for the garbage collector to delete the pods
Jun 26 15:32:11.409: INFO: Deleting ReplicationController wrapped-volume-race-ff9bbc65-87d6-46ae-869e-183982068531 took: 20.153937ms
Jun 26 15:32:11.809: INFO: Terminating ReplicationController wrapped-volume-race-ff9bbc65-87d6-46ae-869e-183982068531 pods took: 400.520305ms
STEP: Creating RC which spawns configmap-volume pods
Jun 26 15:32:51.958: INFO: Pod name wrapped-volume-race-77bb134b-b70f-4910-8a63-ff4d029bc069: Found 0 pods out of 5
Jun 26 15:32:56.976: INFO: Pod name wrapped-volume-race-77bb134b-b70f-4910-8a63-ff4d029bc069: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-77bb134b-b70f-4910-8a63-ff4d029bc069 in namespace emptydir-wrapper-4215, will wait for the garbage collector to delete the pods
Jun 26 15:33:09.099: INFO: Deleting ReplicationController wrapped-volume-race-77bb134b-b70f-4910-8a63-ff4d029bc069 took: 16.089673ms
Jun 26 15:33:09.500: INFO: Terminating ReplicationController wrapped-volume-race-77bb134b-b70f-4910-8a63-ff4d029bc069 pods took: 400.636239ms
STEP: Creating RC which spawns configmap-volume pods
Jun 26 15:33:52.744: INFO: Pod name wrapped-volume-race-4a8f3e92-8af5-400d-a095-eb350e958a23: Found 0 pods out of 5
Jun 26 15:33:57.751: INFO: Pod name wrapped-volume-race-4a8f3e92-8af5-400d-a095-eb350e958a23: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-4a8f3e92-8af5-400d-a095-eb350e958a23 in namespace emptydir-wrapper-4215, will wait for the garbage collector to delete the pods
Jun 26 15:34:07.879: INFO: Deleting ReplicationController wrapped-volume-race-4a8f3e92-8af5-400d-a095-eb350e958a23 took: 16.964946ms
Jun 26 15:34:08.280: INFO: Terminating ReplicationController wrapped-volume-race-4a8f3e92-8af5-400d-a095-eb350e958a23 pods took: 400.979884ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:34:52.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4215" for this suite.
Jun 26 15:34:58.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:34:58.404: INFO: namespace emptydir-wrapper-4215 deletion completed in 6.089630767s

• [SLOW TEST:182.551 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:34:58.405: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:35:22.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-8767" for this suite.
Jun 26 15:35:28.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:35:28.675: INFO: namespace namespaces-8767 deletion completed in 6.15720844s
STEP: Destroying namespace "nsdeletetest-2079" for this suite.
Jun 26 15:35:28.681: INFO: Namespace nsdeletetest-2079 was already deleted
STEP: Destroying namespace "nsdeletetest-7496" for this suite.
Jun 26 15:35:34.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:35:34.775: INFO: namespace nsdeletetest-7496 deletion completed in 6.093976342s

• [SLOW TEST:36.370 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:35:34.776: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-6951
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 26 15:35:34.801: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 26 15:35:56.941: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.146.204:8080/dial?request=hostName&protocol=udp&host=172.16.146.203&port=8081&tries=1'] Namespace:pod-network-test-6951 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 26 15:35:56.941: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
Jun 26 15:35:57.465: INFO: Waiting for endpoints: map[]
Jun 26 15:35:57.474: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.146.204:8080/dial?request=hostName&protocol=udp&host=172.16.249.48&port=8081&tries=1'] Namespace:pod-network-test-6951 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 26 15:35:57.474: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
Jun 26 15:35:57.916: INFO: Waiting for endpoints: map[]
Jun 26 15:35:57.922: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.146.204:8080/dial?request=hostName&protocol=udp&host=172.16.97.232&port=8081&tries=1'] Namespace:pod-network-test-6951 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 26 15:35:57.922: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
Jun 26 15:35:58.427: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:35:58.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6951" for this suite.
Jun 26 15:36:20.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:36:20.540: INFO: namespace pod-network-test-6951 deletion completed in 22.102558909s

• [SLOW TEST:45.764 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:36:20.542: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 26 15:36:20.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 create -f - --namespace=kubectl-7884'
Jun 26 15:36:20.810: INFO: stderr: ""
Jun 26 15:36:20.810: INFO: stdout: "replicationcontroller/redis-master created\n"
Jun 26 15:36:20.810: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 create -f - --namespace=kubectl-7884'
Jun 26 15:36:21.194: INFO: stderr: ""
Jun 26 15:36:21.194: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 26 15:36:22.204: INFO: Selector matched 1 pods for map[app:redis]
Jun 26 15:36:22.204: INFO: Found 0 / 1
Jun 26 15:36:23.202: INFO: Selector matched 1 pods for map[app:redis]
Jun 26 15:36:23.202: INFO: Found 1 / 1
Jun 26 15:36:23.202: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 26 15:36:23.210: INFO: Selector matched 1 pods for map[app:redis]
Jun 26 15:36:23.210: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 26 15:36:23.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 describe pod redis-master-jw2m8 --namespace=kubectl-7884'
Jun 26 15:36:23.464: INFO: stderr: ""
Jun 26 15:36:23.464: INFO: stdout: "Name:           redis-master-jw2m8\nNamespace:      kubectl-7884\nPriority:       0\nNode:           k8s-worker-2-dev/192.168.9.247\nStart Time:     Wed, 26 Jun 2019 15:36:20 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    cni.projectcalico.org/podIP: 172.16.146.205/32\nStatus:         Running\nIP:             172.16.146.205\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   containerd://5e5932b495eb38f36ed96de39e000de769c035773a2dbd98c25008669d9e6bb5\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Wed, 26 Jun 2019 15:36:22 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-rcx6r (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-rcx6r:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-rcx6r\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                       Message\n  ----    ------     ----  ----                       -------\n  Normal  Scheduled  3s    default-scheduler          Successfully assigned kubectl-7884/redis-master-jw2m8 to k8s-worker-2-dev\n  Normal  Pulled     1s    kubelet, k8s-worker-2-dev  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, k8s-worker-2-dev  Created container redis-master\n  Normal  Started    1s    kubelet, k8s-worker-2-dev  Started container redis-master\n"
Jun 26 15:36:23.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 describe rc redis-master --namespace=kubectl-7884'
Jun 26 15:36:23.743: INFO: stderr: ""
Jun 26 15:36:23.743: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-7884\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-jw2m8\n"
Jun 26 15:36:23.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 describe service redis-master --namespace=kubectl-7884'
Jun 26 15:36:23.962: INFO: stderr: ""
Jun 26 15:36:23.962: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-7884\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.103.144.184\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         172.16.146.205:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun 26 15:36:23.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 describe node k8s-master-1-dev'
Jun 26 15:36:24.234: INFO: stderr: ""
Jun 26 15:36:24.234: INFO: stdout: "Name:               k8s-master-1-dev\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-master-1-dev\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.9.243/24\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Wed, 26 Jun 2019 12:10:10 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Wed, 26 Jun 2019 12:38:52 +0000   Wed, 26 Jun 2019 12:38:52 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Wed, 26 Jun 2019 15:35:48 +0000   Wed, 26 Jun 2019 12:10:04 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Wed, 26 Jun 2019 15:35:48 +0000   Wed, 26 Jun 2019 12:10:04 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Wed, 26 Jun 2019 15:35:48 +0000   Wed, 26 Jun 2019 12:10:04 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Wed, 26 Jun 2019 15:35:48 +0000   Wed, 26 Jun 2019 12:38:56 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.9.243\n  Hostname:    k8s-master-1-dev\nCapacity:\n cpu:                4\n ephemeral-storage:  62598392Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16425304Ki\n pods:               110\nAllocatable:\n cpu:                4\n ephemeral-storage:  57690677972\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             16322904Ki\n pods:               110\nSystem Info:\n Machine ID:                 85f8c6a1e1a84ef0a6ae554a398ecd14\n System UUID:                85f8c6a1-e1a8-4ef0-a6ae-554a398ecd14\n Boot ID:                    7922afea-ea8a-4e91-981d-b2e86fda34d1\n Kernel Version:             4.19.43-coreos-r1\n OS Image:                   Container Linux by CoreOS 2079.6.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  containerd://1.2.7\n Kubelet Version:            v1.15.0\n Kube-Proxy Version:         v1.15.0\nPodCIDR:                     172.16.0.0/24\nNon-terminated Pods:         (7 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-750e9b7608a045f9-hvlnq    0 (0%)        0 (0%)      0 (0%)           0 (0%)         91m\n  kube-system                calico-node-554qz                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         177m\n  kube-system                etcd-k8s-master-1-dev                                      0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h25m\n  kube-system                kube-apiserver-k8s-master-1-dev                            250m (6%)     0 (0%)      0 (0%)           0 (0%)         3h25m\n  kube-system                kube-controller-manager-k8s-master-1-dev                   200m (5%)     0 (0%)      0 (0%)           0 (0%)         3h26m\n  kube-system                kube-proxy-dxllt                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         3h25m\n  kube-system                kube-scheduler-k8s-master-1-dev                            100m (2%)     0 (0%)      0 (0%)           0 (0%)         3h25m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                800m (20%)  0 (0%)\n  memory             0 (0%)      0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
Jun 26 15:36:24.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 describe namespace kubectl-7884'
Jun 26 15:36:24.452: INFO: stderr: ""
Jun 26 15:36:24.452: INFO: stdout: "Name:         kubectl-7884\nLabels:       e2e-framework=kubectl\n              e2e-run=634cb3d6-6010-4236-bfd0-1b2039a055b2\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:36:24.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7884" for this suite.
Jun 26 15:36:46.500: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:36:46.738: INFO: namespace kubectl-7884 deletion completed in 22.276713603s

• [SLOW TEST:26.196 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:36:46.739: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-1278/configmap-test-d5fd8eac-27bb-47d0-a7f9-7d03ca143dea
STEP: Creating a pod to test consume configMaps
Jun 26 15:36:46.775: INFO: Waiting up to 5m0s for pod "pod-configmaps-9ec0a38f-5060-454b-9bb9-e207ff60f778" in namespace "configmap-1278" to be "success or failure"
Jun 26 15:36:46.777: INFO: Pod "pod-configmaps-9ec0a38f-5060-454b-9bb9-e207ff60f778": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057089ms
Jun 26 15:36:48.781: INFO: Pod "pod-configmaps-9ec0a38f-5060-454b-9bb9-e207ff60f778": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006026347s
STEP: Saw pod success
Jun 26 15:36:48.781: INFO: Pod "pod-configmaps-9ec0a38f-5060-454b-9bb9-e207ff60f778" satisfied condition "success or failure"
Jun 26 15:36:48.784: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-configmaps-9ec0a38f-5060-454b-9bb9-e207ff60f778 container env-test: <nil>
STEP: delete the pod
Jun 26 15:36:48.800: INFO: Waiting for pod pod-configmaps-9ec0a38f-5060-454b-9bb9-e207ff60f778 to disappear
Jun 26 15:36:48.803: INFO: Pod pod-configmaps-9ec0a38f-5060-454b-9bb9-e207ff60f778 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:36:48.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1278" for this suite.
Jun 26 15:36:54.821: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:36:55.025: INFO: namespace configmap-1278 deletion completed in 6.21875362s

• [SLOW TEST:8.287 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:36:55.031: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Jun 26 15:36:55.099: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-254532258 proxy --unix-socket=/tmp/kubectl-proxy-unix677611485/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:36:55.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2618" for this suite.
Jun 26 15:37:01.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:37:01.374: INFO: namespace kubectl-2618 deletion completed in 6.102504184s

• [SLOW TEST:6.342 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:37:01.374: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-7860
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-7860
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7860
Jun 26 15:37:01.419: INFO: Found 0 stateful pods, waiting for 1
Jun 26 15:37:11.427: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun 26 15:37:11.437: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 exec --namespace=statefulset-7860 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 26 15:37:11.901: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 26 15:37:11.901: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 26 15:37:11.901: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 26 15:37:11.912: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 26 15:37:21.928: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 26 15:37:21.928: INFO: Waiting for statefulset status.replicas updated to 0
Jun 26 15:37:21.961: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jun 26 15:37:21.961: INFO: ss-0  k8s-worker-2-dev  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:12 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:01 +0000 UTC  }]
Jun 26 15:37:21.961: INFO: 
Jun 26 15:37:21.961: INFO: StatefulSet ss has not reached scale 3, at 1
Jun 26 15:37:22.978: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988562778s
Jun 26 15:37:23.988: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.971735066s
Jun 26 15:37:24.999: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.961630108s
Jun 26 15:37:26.010: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.950011172s
Jun 26 15:37:27.021: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.939883151s
Jun 26 15:37:28.041: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.917137597s
Jun 26 15:37:29.046: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.90797971s
Jun 26 15:37:30.057: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.903055961s
Jun 26 15:37:31.062: INFO: Verifying statefulset ss doesn't scale past 3 for another 892.037861ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7860
Jun 26 15:37:32.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 exec --namespace=statefulset-7860 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 26 15:37:32.599: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 26 15:37:32.600: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 26 15:37:32.600: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 26 15:37:32.600: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 exec --namespace=statefulset-7860 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 26 15:37:33.283: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 26 15:37:33.283: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 26 15:37:33.283: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 26 15:37:33.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 exec --namespace=statefulset-7860 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 26 15:37:33.991: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 26 15:37:33.991: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 26 15:37:33.991: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 26 15:37:34.001: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 26 15:37:34.001: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 26 15:37:34.001: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun 26 15:37:34.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 exec --namespace=statefulset-7860 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 26 15:37:34.434: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 26 15:37:34.434: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 26 15:37:34.434: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 26 15:37:34.434: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 exec --namespace=statefulset-7860 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 26 15:37:35.101: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 26 15:37:35.101: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 26 15:37:35.101: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 26 15:37:35.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-254532258 exec --namespace=statefulset-7860 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 26 15:37:35.791: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 26 15:37:35.791: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 26 15:37:35.791: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 26 15:37:35.791: INFO: Waiting for statefulset status.replicas updated to 0
Jun 26 15:37:35.808: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 26 15:37:35.808: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 26 15:37:35.808: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 26 15:37:35.836: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jun 26 15:37:35.836: INFO: ss-0  k8s-worker-2-dev  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:01 +0000 UTC  }]
Jun 26 15:37:35.839: INFO: ss-1  k8s-worker-1-dev  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:21 +0000 UTC  }]
Jun 26 15:37:35.839: INFO: ss-2  k8s-worker-3-dev  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:21 +0000 UTC  }]
Jun 26 15:37:35.839: INFO: 
Jun 26 15:37:35.839: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 26 15:37:36.862: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jun 26 15:37:36.862: INFO: ss-0  k8s-worker-2-dev  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:01 +0000 UTC  }]
Jun 26 15:37:36.862: INFO: ss-1  k8s-worker-1-dev  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:21 +0000 UTC  }]
Jun 26 15:37:36.862: INFO: ss-2  k8s-worker-3-dev  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:21 +0000 UTC  }]
Jun 26 15:37:36.862: INFO: 
Jun 26 15:37:36.862: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 26 15:37:37.871: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jun 26 15:37:37.871: INFO: ss-0  k8s-worker-2-dev  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:01 +0000 UTC  }]
Jun 26 15:37:37.871: INFO: ss-1  k8s-worker-1-dev  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:21 +0000 UTC  }]
Jun 26 15:37:37.871: INFO: ss-2  k8s-worker-3-dev  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:21 +0000 UTC  }]
Jun 26 15:37:37.871: INFO: 
Jun 26 15:37:37.871: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 26 15:37:38.880: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jun 26 15:37:38.881: INFO: ss-0  k8s-worker-2-dev  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:01 +0000 UTC  }]
Jun 26 15:37:38.881: INFO: ss-1  k8s-worker-1-dev  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:21 +0000 UTC  }]
Jun 26 15:37:38.881: INFO: ss-2  k8s-worker-3-dev  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:21 +0000 UTC  }]
Jun 26 15:37:38.881: INFO: 
Jun 26 15:37:38.881: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 26 15:37:39.886: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jun 26 15:37:39.886: INFO: ss-0  k8s-worker-2-dev  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:01 +0000 UTC  }]
Jun 26 15:37:39.886: INFO: ss-1  k8s-worker-1-dev  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:21 +0000 UTC  }]
Jun 26 15:37:39.886: INFO: ss-2  k8s-worker-3-dev  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:21 +0000 UTC  }]
Jun 26 15:37:39.886: INFO: 
Jun 26 15:37:39.886: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 26 15:37:40.890: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jun 26 15:37:40.890: INFO: ss-0  k8s-worker-2-dev  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:01 +0000 UTC  }]
Jun 26 15:37:40.890: INFO: ss-1  k8s-worker-1-dev  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:21 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:21 +0000 UTC  }]
Jun 26 15:37:40.890: INFO: ss-2  k8s-worker-3-dev  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:22 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:21 +0000 UTC  }]
Jun 26 15:37:40.890: INFO: 
Jun 26 15:37:40.890: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 26 15:37:41.899: INFO: POD   NODE              PHASE    GRACE  CONDITIONS
Jun 26 15:37:41.899: INFO: ss-0  k8s-worker-2-dev  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:01 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-26 15:37:01 +0000 UTC  }]
Jun 26 15:37:41.899: INFO: 
Jun 26 15:37:41.899: INFO: StatefulSet ss has not reached scale 0, at 1
Jun 26 15:37:42.902: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.926929863s
Jun 26 15:37:43.905: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.923605378s
Jun 26 15:37:44.909: INFO: Verifying statefulset ss doesn't scale past 0 for another 920.504245ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7860
Jun 26 15:37:45.912: INFO: Scaling statefulset ss to 0
Jun 26 15:37:45.920: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 26 15:37:45.922: INFO: Deleting all statefulset in ns statefulset-7860
Jun 26 15:37:45.925: INFO: Scaling statefulset ss to 0
Jun 26 15:37:45.931: INFO: Waiting for statefulset status.replicas updated to 0
Jun 26 15:37:45.934: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:37:45.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7860" for this suite.
Jun 26 15:37:51.971: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:37:52.181: INFO: namespace statefulset-7860 deletion completed in 6.227313112s

• [SLOW TEST:50.807 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:37:52.185: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-mfzb
STEP: Creating a pod to test atomic-volume-subpath
Jun 26 15:37:52.286: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-mfzb" in namespace "subpath-9191" to be "success or failure"
Jun 26 15:37:52.295: INFO: Pod "pod-subpath-test-configmap-mfzb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.417376ms
Jun 26 15:37:54.302: INFO: Pod "pod-subpath-test-configmap-mfzb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.015808032s
Jun 26 15:37:56.309: INFO: Pod "pod-subpath-test-configmap-mfzb": Phase="Running", Reason="", readiness=true. Elapsed: 4.023074248s
Jun 26 15:37:58.313: INFO: Pod "pod-subpath-test-configmap-mfzb": Phase="Running", Reason="", readiness=true. Elapsed: 6.026912237s
Jun 26 15:38:00.321: INFO: Pod "pod-subpath-test-configmap-mfzb": Phase="Running", Reason="", readiness=true. Elapsed: 8.035147895s
Jun 26 15:38:02.330: INFO: Pod "pod-subpath-test-configmap-mfzb": Phase="Running", Reason="", readiness=true. Elapsed: 10.043915873s
Jun 26 15:38:04.338: INFO: Pod "pod-subpath-test-configmap-mfzb": Phase="Running", Reason="", readiness=true. Elapsed: 12.052178447s
Jun 26 15:38:06.346: INFO: Pod "pod-subpath-test-configmap-mfzb": Phase="Running", Reason="", readiness=true. Elapsed: 14.059694963s
Jun 26 15:38:08.353: INFO: Pod "pod-subpath-test-configmap-mfzb": Phase="Running", Reason="", readiness=true. Elapsed: 16.066482771s
Jun 26 15:38:10.357: INFO: Pod "pod-subpath-test-configmap-mfzb": Phase="Running", Reason="", readiness=true. Elapsed: 18.071070097s
Jun 26 15:38:12.366: INFO: Pod "pod-subpath-test-configmap-mfzb": Phase="Running", Reason="", readiness=true. Elapsed: 20.079798168s
Jun 26 15:38:14.370: INFO: Pod "pod-subpath-test-configmap-mfzb": Phase="Running", Reason="", readiness=true. Elapsed: 22.084302397s
Jun 26 15:38:16.381: INFO: Pod "pod-subpath-test-configmap-mfzb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.094938029s
STEP: Saw pod success
Jun 26 15:38:16.381: INFO: Pod "pod-subpath-test-configmap-mfzb" satisfied condition "success or failure"
Jun 26 15:38:16.388: INFO: Trying to get logs from node k8s-worker-1-dev pod pod-subpath-test-configmap-mfzb container test-container-subpath-configmap-mfzb: <nil>
STEP: delete the pod
Jun 26 15:38:16.425: INFO: Waiting for pod pod-subpath-test-configmap-mfzb to disappear
Jun 26 15:38:16.434: INFO: Pod pod-subpath-test-configmap-mfzb no longer exists
STEP: Deleting pod pod-subpath-test-configmap-mfzb
Jun 26 15:38:16.434: INFO: Deleting pod "pod-subpath-test-configmap-mfzb" in namespace "subpath-9191"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:38:16.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9191" for this suite.
Jun 26 15:38:22.478: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:38:22.690: INFO: namespace subpath-9191 deletion completed in 6.238675454s

• [SLOW TEST:30.506 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:38:22.691: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7887.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7887.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7887.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7887.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 26 15:38:44.835: INFO: DNS probes using dns-test-c62c17e1-8d35-4316-ac75-3e58bb4341c9 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7887.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7887.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7887.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7887.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 26 15:38:48.946: INFO: File wheezy_udp@dns-test-service-3.dns-7887.svc.cluster.local from pod  dns-7887/dns-test-49eb73a6-bafe-4ce0-9a86-7d4fcb5ce3b4 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 26 15:38:48.957: INFO: File jessie_udp@dns-test-service-3.dns-7887.svc.cluster.local from pod  dns-7887/dns-test-49eb73a6-bafe-4ce0-9a86-7d4fcb5ce3b4 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 26 15:38:48.957: INFO: Lookups using dns-7887/dns-test-49eb73a6-bafe-4ce0-9a86-7d4fcb5ce3b4 failed for: [wheezy_udp@dns-test-service-3.dns-7887.svc.cluster.local jessie_udp@dns-test-service-3.dns-7887.svc.cluster.local]

Jun 26 15:38:53.970: INFO: File wheezy_udp@dns-test-service-3.dns-7887.svc.cluster.local from pod  dns-7887/dns-test-49eb73a6-bafe-4ce0-9a86-7d4fcb5ce3b4 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 26 15:38:53.978: INFO: File jessie_udp@dns-test-service-3.dns-7887.svc.cluster.local from pod  dns-7887/dns-test-49eb73a6-bafe-4ce0-9a86-7d4fcb5ce3b4 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 26 15:38:53.978: INFO: Lookups using dns-7887/dns-test-49eb73a6-bafe-4ce0-9a86-7d4fcb5ce3b4 failed for: [wheezy_udp@dns-test-service-3.dns-7887.svc.cluster.local jessie_udp@dns-test-service-3.dns-7887.svc.cluster.local]

Jun 26 15:38:58.967: INFO: File jessie_udp@dns-test-service-3.dns-7887.svc.cluster.local from pod  dns-7887/dns-test-49eb73a6-bafe-4ce0-9a86-7d4fcb5ce3b4 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 26 15:38:58.967: INFO: Lookups using dns-7887/dns-test-49eb73a6-bafe-4ce0-9a86-7d4fcb5ce3b4 failed for: [jessie_udp@dns-test-service-3.dns-7887.svc.cluster.local]

Jun 26 15:39:03.976: INFO: DNS probes using dns-test-49eb73a6-bafe-4ce0-9a86-7d4fcb5ce3b4 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7887.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7887.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7887.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7887.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 26 15:39:08.136: INFO: DNS probes using dns-test-e766b286-8fa8-4da5-88d6-3b47b57a06c0 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:39:08.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7887" for this suite.
Jun 26 15:39:14.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:39:14.496: INFO: namespace dns-7887 deletion completed in 6.237304153s

• [SLOW TEST:51.805 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 26 15:39:14.499: INFO: >>> kubeConfig: /tmp/kubeconfig-254532258
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 26 15:39:14.659: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 15:39:14.665: INFO: Number of nodes with available pods: 0
Jun 26 15:39:14.665: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:39:15.693: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 15:39:15.705: INFO: Number of nodes with available pods: 0
Jun 26 15:39:15.705: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:39:16.676: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 15:39:16.683: INFO: Number of nodes with available pods: 2
Jun 26 15:39:16.683: INFO: Node k8s-worker-3-dev is running more than one daemon pod
Jun 26 15:39:17.680: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 15:39:17.689: INFO: Number of nodes with available pods: 3
Jun 26 15:39:17.689: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun 26 15:39:17.732: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 15:39:17.740: INFO: Number of nodes with available pods: 2
Jun 26 15:39:17.740: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:39:18.747: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 15:39:18.751: INFO: Number of nodes with available pods: 2
Jun 26 15:39:18.751: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:39:19.750: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 15:39:19.765: INFO: Number of nodes with available pods: 2
Jun 26 15:39:19.765: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:39:20.751: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 15:39:20.759: INFO: Number of nodes with available pods: 2
Jun 26 15:39:20.759: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:39:21.752: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 15:39:21.763: INFO: Number of nodes with available pods: 2
Jun 26 15:39:21.764: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:39:22.746: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 15:39:22.749: INFO: Number of nodes with available pods: 2
Jun 26 15:39:22.749: INFO: Node k8s-worker-1-dev is running more than one daemon pod
Jun 26 15:39:23.746: INFO: DaemonSet pods can't tolerate node k8s-master-1-dev with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 26 15:39:23.749: INFO: Number of nodes with available pods: 3
Jun 26 15:39:23.749: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9266, will wait for the garbage collector to delete the pods
Jun 26 15:39:23.813: INFO: Deleting DaemonSet.extensions daemon-set took: 7.37802ms
Jun 26 15:39:24.113: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.313776ms
Jun 26 15:39:31.717: INFO: Number of nodes with available pods: 0
Jun 26 15:39:31.717: INFO: Number of running nodes: 0, number of available pods: 0
Jun 26 15:39:31.719: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9266/daemonsets","resourceVersion":"34061"},"items":null}

Jun 26 15:39:31.721: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9266/pods","resourceVersion":"34061"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 26 15:39:31.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9266" for this suite.
Jun 26 15:39:37.773: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 26 15:39:37.862: INFO: namespace daemonsets-9266 deletion completed in 6.11437595s

• [SLOW TEST:23.364 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSJun 26 15:39:37.864: INFO: Running AfterSuite actions on all nodes
Jun 26 15:39:37.864: INFO: Running AfterSuite actions on node 1
Jun 26 15:39:37.864: INFO: Skipping dumping logs from cluster

Ran 215 of 4411 Specs in 5667.100 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4196 Skipped
PASS

Ginkgo ran 1 suite in 1h34m29.183062924s
Test Suite Passed
