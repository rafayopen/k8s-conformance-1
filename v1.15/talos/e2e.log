I0627 16:59:23.900022      18 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-307330860
I0627 16:59:23.900177      18 e2e.go:241] Starting e2e run "29bcf2a6-66cb-4dd4-ad10-778e2965a5e8" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1561654760 - Will randomize all specs
Will run 215 of 4411 specs

Jun 27 16:59:24.424: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
Jun 27 16:59:24.429: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun 27 16:59:24.458: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun 27 16:59:24.524: INFO: 15 / 15 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun 27 16:59:24.524: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Jun 27 16:59:24.524: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun 27 16:59:24.542: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jun 27 16:59:24.542: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jun 27 16:59:24.542: INFO: e2e test version: v1.15.0
Jun 27 16:59:24.544: INFO: kube-apiserver version: v1.15.0
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 16:59:24.544: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename containers
Jun 27 16:59:24.592: INFO: Found PodSecurityPolicies; assuming PodSecurityPolicy is enabled.
Jun 27 16:59:24.608: INFO: Found ClusterRoles; assuming RBAC is enabled.
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-8619
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Jun 27 16:59:24.730: INFO: Waiting up to 5m0s for pod "client-containers-72a1171a-527a-4d4e-a58a-61b8b0fb2d70" in namespace "containers-8619" to be "success or failure"
Jun 27 16:59:24.734: INFO: Pod "client-containers-72a1171a-527a-4d4e-a58a-61b8b0fb2d70": Phase="Pending", Reason="", readiness=false. Elapsed: 3.873853ms
Jun 27 16:59:26.738: INFO: Pod "client-containers-72a1171a-527a-4d4e-a58a-61b8b0fb2d70": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007775242s
Jun 27 16:59:28.742: INFO: Pod "client-containers-72a1171a-527a-4d4e-a58a-61b8b0fb2d70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011831021s
STEP: Saw pod success
Jun 27 16:59:28.742: INFO: Pod "client-containers-72a1171a-527a-4d4e-a58a-61b8b0fb2d70" satisfied condition "success or failure"
Jun 27 16:59:28.746: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nzp9p pod client-containers-72a1171a-527a-4d4e-a58a-61b8b0fb2d70 container test-container: <nil>
STEP: delete the pod
Jun 27 16:59:28.792: INFO: Waiting for pod client-containers-72a1171a-527a-4d4e-a58a-61b8b0fb2d70 to disappear
Jun 27 16:59:28.795: INFO: Pod client-containers-72a1171a-527a-4d4e-a58a-61b8b0fb2d70 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 16:59:28.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8619" for this suite.
Jun 27 16:59:34.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 16:59:34.937: INFO: namespace containers-8619 deletion completed in 6.137859803s

• [SLOW TEST:10.393 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 16:59:34.937: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9037
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 27 16:59:35.105: INFO: Waiting up to 5m0s for pod "downwardapi-volume-217cd014-a366-4c37-9302-3dfcee85eb0f" in namespace "projected-9037" to be "success or failure"
Jun 27 16:59:35.110: INFO: Pod "downwardapi-volume-217cd014-a366-4c37-9302-3dfcee85eb0f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.757064ms
Jun 27 16:59:37.114: INFO: Pod "downwardapi-volume-217cd014-a366-4c37-9302-3dfcee85eb0f": Phase="Running", Reason="", readiness=true. Elapsed: 2.008477883s
Jun 27 16:59:39.117: INFO: Pod "downwardapi-volume-217cd014-a366-4c37-9302-3dfcee85eb0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012080891s
STEP: Saw pod success
Jun 27 16:59:39.117: INFO: Pod "downwardapi-volume-217cd014-a366-4c37-9302-3dfcee85eb0f" satisfied condition "success or failure"
Jun 27 16:59:39.121: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod downwardapi-volume-217cd014-a366-4c37-9302-3dfcee85eb0f container client-container: <nil>
STEP: delete the pod
Jun 27 16:59:39.167: INFO: Waiting for pod downwardapi-volume-217cd014-a366-4c37-9302-3dfcee85eb0f to disappear
Jun 27 16:59:39.172: INFO: Pod downwardapi-volume-217cd014-a366-4c37-9302-3dfcee85eb0f no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 16:59:39.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9037" for this suite.
Jun 27 16:59:45.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 16:59:45.319: INFO: namespace projected-9037 deletion completed in 6.142615587s

• [SLOW TEST:10.382 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 16:59:45.319: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-1857
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-cpwf
STEP: Creating a pod to test atomic-volume-subpath
Jun 27 16:59:45.497: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-cpwf" in namespace "subpath-1857" to be "success or failure"
Jun 27 16:59:45.503: INFO: Pod "pod-subpath-test-downwardapi-cpwf": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015544ms
Jun 27 16:59:47.507: INFO: Pod "pod-subpath-test-downwardapi-cpwf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009553723s
Jun 27 16:59:49.511: INFO: Pod "pod-subpath-test-downwardapi-cpwf": Phase="Running", Reason="", readiness=true. Elapsed: 4.013446032s
Jun 27 16:59:51.517: INFO: Pod "pod-subpath-test-downwardapi-cpwf": Phase="Running", Reason="", readiness=true. Elapsed: 6.020031493s
Jun 27 16:59:53.521: INFO: Pod "pod-subpath-test-downwardapi-cpwf": Phase="Running", Reason="", readiness=true. Elapsed: 8.023704761s
Jun 27 16:59:55.525: INFO: Pod "pod-subpath-test-downwardapi-cpwf": Phase="Running", Reason="", readiness=true. Elapsed: 10.02739536s
Jun 27 16:59:57.529: INFO: Pod "pod-subpath-test-downwardapi-cpwf": Phase="Running", Reason="", readiness=true. Elapsed: 12.031834199s
Jun 27 16:59:59.533: INFO: Pod "pod-subpath-test-downwardapi-cpwf": Phase="Running", Reason="", readiness=true. Elapsed: 14.035570358s
Jun 27 17:00:01.537: INFO: Pod "pod-subpath-test-downwardapi-cpwf": Phase="Running", Reason="", readiness=true. Elapsed: 16.039354257s
Jun 27 17:00:03.540: INFO: Pod "pod-subpath-test-downwardapi-cpwf": Phase="Running", Reason="", readiness=true. Elapsed: 18.043005606s
Jun 27 17:00:05.544: INFO: Pod "pod-subpath-test-downwardapi-cpwf": Phase="Running", Reason="", readiness=true. Elapsed: 20.046816334s
Jun 27 17:00:07.548: INFO: Pod "pod-subpath-test-downwardapi-cpwf": Phase="Running", Reason="", readiness=true. Elapsed: 22.050552533s
Jun 27 17:00:09.551: INFO: Pod "pod-subpath-test-downwardapi-cpwf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.054059742s
STEP: Saw pod success
Jun 27 17:00:09.551: INFO: Pod "pod-subpath-test-downwardapi-cpwf" satisfied condition "success or failure"
Jun 27 17:00:09.555: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-mskvs pod pod-subpath-test-downwardapi-cpwf container test-container-subpath-downwardapi-cpwf: <nil>
STEP: delete the pod
Jun 27 17:00:09.599: INFO: Waiting for pod pod-subpath-test-downwardapi-cpwf to disappear
Jun 27 17:00:09.603: INFO: Pod pod-subpath-test-downwardapi-cpwf no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-cpwf
Jun 27 17:00:09.603: INFO: Deleting pod "pod-subpath-test-downwardapi-cpwf" in namespace "subpath-1857"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:00:09.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1857" for this suite.
Jun 27 17:00:15.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:00:15.758: INFO: namespace subpath-1857 deletion completed in 6.14651321s

• [SLOW TEST:30.438 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:00:15.759: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-8495
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:00:22.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8495" for this suite.
Jun 27 17:00:44.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:00:45.105: INFO: namespace replication-controller-8495 deletion completed in 22.13813812s

• [SLOW TEST:29.346 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:00:45.105: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2313
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 27 17:00:45.268: INFO: Waiting up to 5m0s for pod "downward-api-633ad90b-a20b-4ff6-af40-3449127a6445" in namespace "downward-api-2313" to be "success or failure"
Jun 27 17:00:45.273: INFO: Pod "downward-api-633ad90b-a20b-4ff6-af40-3449127a6445": Phase="Pending", Reason="", readiness=false. Elapsed: 5.358744ms
Jun 27 17:00:47.277: INFO: Pod "downward-api-633ad90b-a20b-4ff6-af40-3449127a6445": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008867753s
Jun 27 17:00:49.282: INFO: Pod "downward-api-633ad90b-a20b-4ff6-af40-3449127a6445": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014312223s
STEP: Saw pod success
Jun 27 17:00:49.282: INFO: Pod "downward-api-633ad90b-a20b-4ff6-af40-3449127a6445" satisfied condition "success or failure"
Jun 27 17:00:49.287: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod downward-api-633ad90b-a20b-4ff6-af40-3449127a6445 container dapi-container: <nil>
STEP: delete the pod
Jun 27 17:00:49.346: INFO: Waiting for pod downward-api-633ad90b-a20b-4ff6-af40-3449127a6445 to disappear
Jun 27 17:00:49.350: INFO: Pod downward-api-633ad90b-a20b-4ff6-af40-3449127a6445 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:00:49.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2313" for this suite.
Jun 27 17:00:55.375: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:00:55.497: INFO: namespace downward-api-2313 deletion completed in 6.137547593s

• [SLOW TEST:10.392 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:00:55.498: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-8650
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8650.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8650.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8650.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8650.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 27 17:01:15.686: INFO: DNS probes using dns-test-b68c304d-9872-4ed9-b9a7-8f16ee41243e succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8650.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8650.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8650.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8650.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 27 17:01:33.741: INFO: DNS probes using dns-test-3af28ae7-c7dc-4421-aa7b-4d332da0d524 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8650.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8650.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8650.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8650.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 27 17:01:35.838: INFO: DNS probes using dns-test-75655498-1b98-4b76-b179-39f99034594b succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:01:35.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8650" for this suite.
Jun 27 17:01:41.937: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:01:42.067: INFO: namespace dns-8650 deletion completed in 6.154415325s

• [SLOW TEST:46.569 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:01:42.068: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-5735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jun 27 17:01:42.223: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:01:46.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5735" for this suite.
Jun 27 17:02:08.217: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:02:08.341: INFO: namespace init-container-5735 deletion completed in 22.13780074s

• [SLOW TEST:26.273 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:02:08.342: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2354
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-035e6c4a-d338-48d6-9035-d27921e01d6d
STEP: Creating a pod to test consume secrets
Jun 27 17:02:08.510: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6e584200-05c7-4c96-b3dd-04fa16f1f06a" in namespace "projected-2354" to be "success or failure"
Jun 27 17:02:08.514: INFO: Pod "pod-projected-secrets-6e584200-05c7-4c96-b3dd-04fa16f1f06a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.637133ms
Jun 27 17:02:10.518: INFO: Pod "pod-projected-secrets-6e584200-05c7-4c96-b3dd-04fa16f1f06a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007529302s
STEP: Saw pod success
Jun 27 17:02:10.518: INFO: Pod "pod-projected-secrets-6e584200-05c7-4c96-b3dd-04fa16f1f06a" satisfied condition "success or failure"
Jun 27 17:02:10.522: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-mskvs pod pod-projected-secrets-6e584200-05c7-4c96-b3dd-04fa16f1f06a container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 27 17:02:10.563: INFO: Waiting for pod pod-projected-secrets-6e584200-05c7-4c96-b3dd-04fa16f1f06a to disappear
Jun 27 17:02:10.568: INFO: Pod pod-projected-secrets-6e584200-05c7-4c96-b3dd-04fa16f1f06a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:02:10.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2354" for this suite.
Jun 27 17:02:16.587: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:02:16.707: INFO: namespace projected-2354 deletion completed in 6.134980591s

• [SLOW TEST:8.365 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:02:16.708: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-6701
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:02:43.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-6701" for this suite.
Jun 27 17:02:49.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:02:49.277: INFO: namespace container-runtime-6701 deletion completed in 6.185149229s

• [SLOW TEST:32.569 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:02:49.277: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-6723
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Jun 27 17:02:49.521: INFO: Waiting up to 5m0s for pod "client-containers-1fd676dc-fe1a-4980-ad4a-5df3011f305a" in namespace "containers-6723" to be "success or failure"
Jun 27 17:02:49.525: INFO: Pod "client-containers-1fd676dc-fe1a-4980-ad4a-5df3011f305a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.065624ms
Jun 27 17:02:51.528: INFO: Pod "client-containers-1fd676dc-fe1a-4980-ad4a-5df3011f305a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007707532s
Jun 27 17:02:53.532: INFO: Pod "client-containers-1fd676dc-fe1a-4980-ad4a-5df3011f305a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011446831s
STEP: Saw pod success
Jun 27 17:02:53.532: INFO: Pod "client-containers-1fd676dc-fe1a-4980-ad4a-5df3011f305a" satisfied condition "success or failure"
Jun 27 17:02:53.536: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod client-containers-1fd676dc-fe1a-4980-ad4a-5df3011f305a container test-container: <nil>
STEP: delete the pod
Jun 27 17:02:53.554: INFO: Waiting for pod client-containers-1fd676dc-fe1a-4980-ad4a-5df3011f305a to disappear
Jun 27 17:02:53.558: INFO: Pod client-containers-1fd676dc-fe1a-4980-ad4a-5df3011f305a no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:02:53.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6723" for this suite.
Jun 27 17:02:59.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:02:59.697: INFO: namespace containers-6723 deletion completed in 6.1338246s

• [SLOW TEST:10.419 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:02:59.697: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-3080
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 27 17:02:59.875: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun 27 17:02:59.884: INFO: Number of nodes with available pods: 0
Jun 27 17:02:59.884: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jun 27 17:02:59.903: INFO: Number of nodes with available pods: 0
Jun 27 17:02:59.903: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:03:00.906: INFO: Number of nodes with available pods: 0
Jun 27 17:03:00.906: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:03:01.907: INFO: Number of nodes with available pods: 0
Jun 27 17:03:01.907: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:03:02.907: INFO: Number of nodes with available pods: 0
Jun 27 17:03:02.907: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:03:03.907: INFO: Number of nodes with available pods: 0
Jun 27 17:03:03.907: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:03:04.907: INFO: Number of nodes with available pods: 0
Jun 27 17:03:04.907: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:03:05.907: INFO: Number of nodes with available pods: 1
Jun 27 17:03:05.907: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun 27 17:03:05.925: INFO: Number of nodes with available pods: 1
Jun 27 17:03:05.925: INFO: Number of running nodes: 0, number of available pods: 1
Jun 27 17:03:06.929: INFO: Number of nodes with available pods: 0
Jun 27 17:03:06.929: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun 27 17:03:06.945: INFO: Number of nodes with available pods: 0
Jun 27 17:03:06.945: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:03:07.949: INFO: Number of nodes with available pods: 0
Jun 27 17:03:07.949: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:03:08.949: INFO: Number of nodes with available pods: 0
Jun 27 17:03:08.949: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:03:09.949: INFO: Number of nodes with available pods: 0
Jun 27 17:03:09.949: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:03:10.949: INFO: Number of nodes with available pods: 0
Jun 27 17:03:10.949: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:03:11.949: INFO: Number of nodes with available pods: 0
Jun 27 17:03:11.949: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:03:12.949: INFO: Number of nodes with available pods: 0
Jun 27 17:03:12.949: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:03:13.949: INFO: Number of nodes with available pods: 0
Jun 27 17:03:13.949: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:03:14.949: INFO: Number of nodes with available pods: 0
Jun 27 17:03:14.949: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:03:15.948: INFO: Number of nodes with available pods: 0
Jun 27 17:03:15.949: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:03:16.949: INFO: Number of nodes with available pods: 0
Jun 27 17:03:16.949: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:03:17.949: INFO: Number of nodes with available pods: 0
Jun 27 17:03:17.949: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:03:18.948: INFO: Number of nodes with available pods: 0
Jun 27 17:03:18.949: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:03:19.948: INFO: Number of nodes with available pods: 0
Jun 27 17:03:19.949: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:03:20.949: INFO: Number of nodes with available pods: 0
Jun 27 17:03:20.949: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:03:21.949: INFO: Number of nodes with available pods: 1
Jun 27 17:03:21.949: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3080, will wait for the garbage collector to delete the pods
Jun 27 17:03:22.020: INFO: Deleting DaemonSet.extensions daemon-set took: 9.079617ms
Jun 27 17:03:22.420: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.344786ms
Jun 27 17:03:29.824: INFO: Number of nodes with available pods: 0
Jun 27 17:03:29.824: INFO: Number of running nodes: 0, number of available pods: 0
Jun 27 17:03:29.831: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3080/daemonsets","resourceVersion":"2573"},"items":null}

Jun 27 17:03:29.835: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3080/pods","resourceVersion":"2573"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:03:29.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3080" for this suite.
Jun 27 17:03:35.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:03:36.014: INFO: namespace daemonsets-3080 deletion completed in 6.148995331s

• [SLOW TEST:36.317 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:03:36.015: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-7787
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 27 17:03:36.180: INFO: Waiting up to 5m0s for pod "downward-api-825a3795-a34b-496e-a0c8-88e277787b17" in namespace "downward-api-7787" to be "success or failure"
Jun 27 17:03:36.185: INFO: Pod "downward-api-825a3795-a34b-496e-a0c8-88e277787b17": Phase="Pending", Reason="", readiness=false. Elapsed: 5.327754ms
Jun 27 17:03:38.189: INFO: Pod "downward-api-825a3795-a34b-496e-a0c8-88e277787b17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009091493s
STEP: Saw pod success
Jun 27 17:03:38.189: INFO: Pod "downward-api-825a3795-a34b-496e-a0c8-88e277787b17" satisfied condition "success or failure"
Jun 27 17:03:38.193: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod downward-api-825a3795-a34b-496e-a0c8-88e277787b17 container dapi-container: <nil>
STEP: delete the pod
Jun 27 17:03:38.215: INFO: Waiting for pod downward-api-825a3795-a34b-496e-a0c8-88e277787b17 to disappear
Jun 27 17:03:38.220: INFO: Pod downward-api-825a3795-a34b-496e-a0c8-88e277787b17 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:03:38.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7787" for this suite.
Jun 27 17:03:44.240: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:03:44.380: INFO: namespace downward-api-7787 deletion completed in 6.154015735s

• [SLOW TEST:8.365 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:03:44.380: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4158
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 27 17:03:44.547: INFO: Waiting up to 5m0s for pod "downward-api-9b6bd596-6953-4029-9b73-510a6fe9779c" in namespace "downward-api-4158" to be "success or failure"
Jun 27 17:03:44.551: INFO: Pod "downward-api-9b6bd596-6953-4029-9b73-510a6fe9779c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.292773ms
Jun 27 17:03:46.555: INFO: Pod "downward-api-9b6bd596-6953-4029-9b73-510a6fe9779c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008079472s
STEP: Saw pod success
Jun 27 17:03:46.555: INFO: Pod "downward-api-9b6bd596-6953-4029-9b73-510a6fe9779c" satisfied condition "success or failure"
Jun 27 17:03:46.559: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod downward-api-9b6bd596-6953-4029-9b73-510a6fe9779c container dapi-container: <nil>
STEP: delete the pod
Jun 27 17:03:46.580: INFO: Waiting for pod downward-api-9b6bd596-6953-4029-9b73-510a6fe9779c to disappear
Jun 27 17:03:46.583: INFO: Pod downward-api-9b6bd596-6953-4029-9b73-510a6fe9779c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:03:46.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4158" for this suite.
Jun 27 17:03:52.610: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:03:52.733: INFO: namespace downward-api-4158 deletion completed in 6.144651128s

• [SLOW TEST:8.353 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:03:52.734: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-2257
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:03:58.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2257" for this suite.
Jun 27 17:04:04.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:04:04.665: INFO: namespace watch-2257 deletion completed in 6.223018728s

• [SLOW TEST:11.932 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:04:04.665: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5802
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 27 17:04:04.836: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6912bdf9-c45e-4c4d-8dc7-d5417cfebce2" in namespace "projected-5802" to be "success or failure"
Jun 27 17:04:04.840: INFO: Pod "downwardapi-volume-6912bdf9-c45e-4c4d-8dc7-d5417cfebce2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.935923ms
Jun 27 17:04:06.844: INFO: Pod "downwardapi-volume-6912bdf9-c45e-4c4d-8dc7-d5417cfebce2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007908072s
Jun 27 17:04:08.848: INFO: Pod "downwardapi-volume-6912bdf9-c45e-4c4d-8dc7-d5417cfebce2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011583641s
STEP: Saw pod success
Jun 27 17:04:08.848: INFO: Pod "downwardapi-volume-6912bdf9-c45e-4c4d-8dc7-d5417cfebce2" satisfied condition "success or failure"
Jun 27 17:04:08.851: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod downwardapi-volume-6912bdf9-c45e-4c4d-8dc7-d5417cfebce2 container client-container: <nil>
STEP: delete the pod
Jun 27 17:04:08.870: INFO: Waiting for pod downwardapi-volume-6912bdf9-c45e-4c4d-8dc7-d5417cfebce2 to disappear
Jun 27 17:04:08.875: INFO: Pod downwardapi-volume-6912bdf9-c45e-4c4d-8dc7-d5417cfebce2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:04:08.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5802" for this suite.
Jun 27 17:04:14.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:04:15.008: INFO: namespace projected-5802 deletion completed in 6.128946706s

• [SLOW TEST:10.343 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:04:15.009: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9225
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun 27 17:04:15.170: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9225,SelfLink:/api/v1/namespaces/watch-9225/configmaps/e2e-watch-test-configmap-a,UID:7d985c2e-3b18-4527-9123-9b213267250e,ResourceVersion:2884,Generation:0,CreationTimestamp:2019-06-27 17:04:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 27 17:04:15.171: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9225,SelfLink:/api/v1/namespaces/watch-9225/configmaps/e2e-watch-test-configmap-a,UID:7d985c2e-3b18-4527-9123-9b213267250e,ResourceVersion:2884,Generation:0,CreationTimestamp:2019-06-27 17:04:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun 27 17:04:25.180: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9225,SelfLink:/api/v1/namespaces/watch-9225/configmaps/e2e-watch-test-configmap-a,UID:7d985c2e-3b18-4527-9123-9b213267250e,ResourceVersion:2900,Generation:0,CreationTimestamp:2019-06-27 17:04:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 27 17:04:25.180: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9225,SelfLink:/api/v1/namespaces/watch-9225/configmaps/e2e-watch-test-configmap-a,UID:7d985c2e-3b18-4527-9123-9b213267250e,ResourceVersion:2900,Generation:0,CreationTimestamp:2019-06-27 17:04:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun 27 17:04:35.188: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9225,SelfLink:/api/v1/namespaces/watch-9225/configmaps/e2e-watch-test-configmap-a,UID:7d985c2e-3b18-4527-9123-9b213267250e,ResourceVersion:2917,Generation:0,CreationTimestamp:2019-06-27 17:04:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 27 17:04:35.188: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9225,SelfLink:/api/v1/namespaces/watch-9225/configmaps/e2e-watch-test-configmap-a,UID:7d985c2e-3b18-4527-9123-9b213267250e,ResourceVersion:2917,Generation:0,CreationTimestamp:2019-06-27 17:04:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun 27 17:04:45.196: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9225,SelfLink:/api/v1/namespaces/watch-9225/configmaps/e2e-watch-test-configmap-a,UID:7d985c2e-3b18-4527-9123-9b213267250e,ResourceVersion:2934,Generation:0,CreationTimestamp:2019-06-27 17:04:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 27 17:04:45.196: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-9225,SelfLink:/api/v1/namespaces/watch-9225/configmaps/e2e-watch-test-configmap-a,UID:7d985c2e-3b18-4527-9123-9b213267250e,ResourceVersion:2934,Generation:0,CreationTimestamp:2019-06-27 17:04:15 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun 27 17:04:55.204: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9225,SelfLink:/api/v1/namespaces/watch-9225/configmaps/e2e-watch-test-configmap-b,UID:07ccc20e-06d2-4e97-88db-be93a5c83778,ResourceVersion:2950,Generation:0,CreationTimestamp:2019-06-27 17:04:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 27 17:04:55.204: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9225,SelfLink:/api/v1/namespaces/watch-9225/configmaps/e2e-watch-test-configmap-b,UID:07ccc20e-06d2-4e97-88db-be93a5c83778,ResourceVersion:2950,Generation:0,CreationTimestamp:2019-06-27 17:04:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun 27 17:05:05.213: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9225,SelfLink:/api/v1/namespaces/watch-9225/configmaps/e2e-watch-test-configmap-b,UID:07ccc20e-06d2-4e97-88db-be93a5c83778,ResourceVersion:2968,Generation:0,CreationTimestamp:2019-06-27 17:04:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 27 17:05:05.213: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-9225,SelfLink:/api/v1/namespaces/watch-9225/configmaps/e2e-watch-test-configmap-b,UID:07ccc20e-06d2-4e97-88db-be93a5c83778,ResourceVersion:2968,Generation:0,CreationTimestamp:2019-06-27 17:04:55 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:05:15.213: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9225" for this suite.
Jun 27 17:05:21.230: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:05:21.365: INFO: namespace watch-9225 deletion completed in 6.146820689s

• [SLOW TEST:66.356 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:05:21.365: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1783
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Jun 27 17:05:21.521: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jun 27 17:05:21.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 create -f - --namespace=kubectl-1783'
Jun 27 17:05:22.140: INFO: stderr: ""
Jun 27 17:05:22.140: INFO: stdout: "service/redis-slave created\n"
Jun 27 17:05:22.140: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jun 27 17:05:22.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 create -f - --namespace=kubectl-1783'
Jun 27 17:05:22.581: INFO: stderr: ""
Jun 27 17:05:22.581: INFO: stdout: "service/redis-master created\n"
Jun 27 17:05:22.581: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun 27 17:05:22.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 create -f - --namespace=kubectl-1783'
Jun 27 17:05:22.975: INFO: stderr: ""
Jun 27 17:05:22.976: INFO: stdout: "service/frontend created\n"
Jun 27 17:05:22.976: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jun 27 17:05:22.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 create -f - --namespace=kubectl-1783'
Jun 27 17:05:23.363: INFO: stderr: ""
Jun 27 17:05:23.363: INFO: stdout: "deployment.apps/frontend created\n"
Jun 27 17:05:23.363: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 27 17:05:23.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 create -f - --namespace=kubectl-1783'
Jun 27 17:05:23.760: INFO: stderr: ""
Jun 27 17:05:23.760: INFO: stdout: "deployment.apps/redis-master created\n"
Jun 27 17:05:23.760: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jun 27 17:05:23.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 create -f - --namespace=kubectl-1783'
Jun 27 17:05:24.176: INFO: stderr: ""
Jun 27 17:05:24.177: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jun 27 17:05:24.177: INFO: Waiting for all frontend pods to be Running.
Jun 27 17:05:49.228: INFO: Waiting for frontend to serve content.
Jun 27 17:05:54.255: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Jun 27 17:05:59.277: INFO: Trying to add a new entry to the guestbook.
Jun 27 17:05:59.302: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jun 27 17:05:59.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 delete --grace-period=0 --force -f - --namespace=kubectl-1783'
Jun 27 17:05:59.513: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 27 17:05:59.513: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jun 27 17:05:59.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 delete --grace-period=0 --force -f - --namespace=kubectl-1783'
Jun 27 17:05:59.740: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 27 17:05:59.741: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 27 17:05:59.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 delete --grace-period=0 --force -f - --namespace=kubectl-1783'
Jun 27 17:05:59.989: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 27 17:05:59.989: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 27 17:05:59.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 delete --grace-period=0 --force -f - --namespace=kubectl-1783'
Jun 27 17:06:00.174: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 27 17:06:00.174: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 27 17:06:00.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 delete --grace-period=0 --force -f - --namespace=kubectl-1783'
Jun 27 17:06:00.348: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 27 17:06:00.348: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 27 17:06:00.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 delete --grace-period=0 --force -f - --namespace=kubectl-1783'
Jun 27 17:06:00.548: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 27 17:06:00.548: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:06:00.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1783" for this suite.
Jun 27 17:06:40.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:06:40.707: INFO: namespace kubectl-1783 deletion completed in 40.151432033s

• [SLOW TEST:79.342 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:06:40.708: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-6540
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0627 17:07:20.903332      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 27 17:07:20.903: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:07:20.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6540" for this suite.
Jun 27 17:07:28.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:07:29.040: INFO: namespace gc-6540 deletion completed in 8.132229184s

• [SLOW TEST:48.333 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:07:29.042: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-390
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jun 27 17:07:29.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 create -f - --namespace=kubectl-390'
Jun 27 17:07:29.594: INFO: stderr: ""
Jun 27 17:07:29.594: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 27 17:07:29.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-390'
Jun 27 17:07:29.779: INFO: stderr: ""
Jun 27 17:07:29.779: INFO: stdout: "update-demo-nautilus-5gntn update-demo-nautilus-958rm "
Jun 27 17:07:29.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-nautilus-5gntn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-390'
Jun 27 17:07:29.951: INFO: stderr: ""
Jun 27 17:07:29.951: INFO: stdout: ""
Jun 27 17:07:29.951: INFO: update-demo-nautilus-5gntn is created but not running
Jun 27 17:07:34.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-390'
Jun 27 17:07:35.127: INFO: stderr: ""
Jun 27 17:07:35.127: INFO: stdout: "update-demo-nautilus-5gntn update-demo-nautilus-958rm "
Jun 27 17:07:35.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-nautilus-5gntn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-390'
Jun 27 17:07:35.295: INFO: stderr: ""
Jun 27 17:07:35.295: INFO: stdout: "true"
Jun 27 17:07:35.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-nautilus-5gntn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-390'
Jun 27 17:07:35.466: INFO: stderr: ""
Jun 27 17:07:35.466: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 27 17:07:35.466: INFO: validating pod update-demo-nautilus-5gntn
Jun 27 17:07:35.472: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 27 17:07:35.472: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 27 17:07:35.472: INFO: update-demo-nautilus-5gntn is verified up and running
Jun 27 17:07:35.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-nautilus-958rm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-390'
Jun 27 17:07:35.642: INFO: stderr: ""
Jun 27 17:07:35.642: INFO: stdout: "true"
Jun 27 17:07:35.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-nautilus-958rm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-390'
Jun 27 17:07:35.821: INFO: stderr: ""
Jun 27 17:07:35.821: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 27 17:07:35.821: INFO: validating pod update-demo-nautilus-958rm
Jun 27 17:07:35.826: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 27 17:07:35.826: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 27 17:07:35.826: INFO: update-demo-nautilus-958rm is verified up and running
STEP: scaling down the replication controller
Jun 27 17:07:35.830: INFO: scanned /root for discovery docs: <nil>
Jun 27 17:07:35.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-390'
Jun 27 17:07:37.046: INFO: stderr: ""
Jun 27 17:07:37.046: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 27 17:07:37.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-390'
Jun 27 17:07:37.231: INFO: stderr: ""
Jun 27 17:07:37.231: INFO: stdout: "update-demo-nautilus-5gntn update-demo-nautilus-958rm "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 27 17:07:42.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-390'
Jun 27 17:07:42.414: INFO: stderr: ""
Jun 27 17:07:42.414: INFO: stdout: "update-demo-nautilus-5gntn update-demo-nautilus-958rm "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 27 17:07:47.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-390'
Jun 27 17:07:47.587: INFO: stderr: ""
Jun 27 17:07:47.587: INFO: stdout: "update-demo-nautilus-958rm "
Jun 27 17:07:47.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-nautilus-958rm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-390'
Jun 27 17:07:47.762: INFO: stderr: ""
Jun 27 17:07:47.762: INFO: stdout: "true"
Jun 27 17:07:47.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-nautilus-958rm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-390'
Jun 27 17:07:47.936: INFO: stderr: ""
Jun 27 17:07:47.936: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 27 17:07:47.936: INFO: validating pod update-demo-nautilus-958rm
Jun 27 17:07:47.941: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 27 17:07:47.941: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 27 17:07:47.941: INFO: update-demo-nautilus-958rm is verified up and running
STEP: scaling up the replication controller
Jun 27 17:07:47.944: INFO: scanned /root for discovery docs: <nil>
Jun 27 17:07:47.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-390'
Jun 27 17:07:49.152: INFO: stderr: ""
Jun 27 17:07:49.152: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 27 17:07:49.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-390'
Jun 27 17:07:49.334: INFO: stderr: ""
Jun 27 17:07:49.334: INFO: stdout: "update-demo-nautilus-958rm update-demo-nautilus-cj8th "
Jun 27 17:07:49.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-nautilus-958rm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-390'
Jun 27 17:07:49.502: INFO: stderr: ""
Jun 27 17:07:49.502: INFO: stdout: "true"
Jun 27 17:07:49.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-nautilus-958rm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-390'
Jun 27 17:07:49.682: INFO: stderr: ""
Jun 27 17:07:49.682: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 27 17:07:49.682: INFO: validating pod update-demo-nautilus-958rm
Jun 27 17:07:49.687: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 27 17:07:49.687: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 27 17:07:49.687: INFO: update-demo-nautilus-958rm is verified up and running
Jun 27 17:07:49.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-nautilus-cj8th -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-390'
Jun 27 17:07:49.856: INFO: stderr: ""
Jun 27 17:07:49.856: INFO: stdout: "true"
Jun 27 17:07:49.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-nautilus-cj8th -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-390'
Jun 27 17:07:50.036: INFO: stderr: ""
Jun 27 17:07:50.036: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 27 17:07:50.036: INFO: validating pod update-demo-nautilus-cj8th
Jun 27 17:07:50.041: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 27 17:07:50.042: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 27 17:07:50.042: INFO: update-demo-nautilus-cj8th is verified up and running
STEP: using delete to clean up resources
Jun 27 17:07:50.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 delete --grace-period=0 --force -f - --namespace=kubectl-390'
Jun 27 17:07:50.222: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 27 17:07:50.222: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 27 17:07:50.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-390'
Jun 27 17:07:50.406: INFO: stderr: "No resources found.\n"
Jun 27 17:07:50.406: INFO: stdout: ""
Jun 27 17:07:50.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods -l name=update-demo --namespace=kubectl-390 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 27 17:07:50.584: INFO: stderr: ""
Jun 27 17:07:50.584: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:07:50.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-390" for this suite.
Jun 27 17:07:56.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:07:56.724: INFO: namespace kubectl-390 deletion completed in 6.135135131s

• [SLOW TEST:27.683 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:07:56.725: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9140
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Jun 27 17:07:56.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 create -f - --namespace=kubectl-9140'
Jun 27 17:07:57.268: INFO: stderr: ""
Jun 27 17:07:57.268: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 27 17:07:57.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9140'
Jun 27 17:07:57.456: INFO: stderr: ""
Jun 27 17:07:57.456: INFO: stdout: "update-demo-nautilus-l5r26 update-demo-nautilus-s4wld "
Jun 27 17:07:57.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-nautilus-l5r26 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9140'
Jun 27 17:07:57.626: INFO: stderr: ""
Jun 27 17:07:57.626: INFO: stdout: ""
Jun 27 17:07:57.626: INFO: update-demo-nautilus-l5r26 is created but not running
Jun 27 17:08:02.626: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9140'
Jun 27 17:08:02.806: INFO: stderr: ""
Jun 27 17:08:02.806: INFO: stdout: "update-demo-nautilus-l5r26 update-demo-nautilus-s4wld "
Jun 27 17:08:02.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-nautilus-l5r26 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9140'
Jun 27 17:08:02.975: INFO: stderr: ""
Jun 27 17:08:02.975: INFO: stdout: "true"
Jun 27 17:08:02.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-nautilus-l5r26 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9140'
Jun 27 17:08:03.143: INFO: stderr: ""
Jun 27 17:08:03.143: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 27 17:08:03.143: INFO: validating pod update-demo-nautilus-l5r26
Jun 27 17:08:03.149: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 27 17:08:03.149: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 27 17:08:03.149: INFO: update-demo-nautilus-l5r26 is verified up and running
Jun 27 17:08:03.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-nautilus-s4wld -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9140'
Jun 27 17:08:03.316: INFO: stderr: ""
Jun 27 17:08:03.316: INFO: stdout: "true"
Jun 27 17:08:03.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-nautilus-s4wld -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9140'
Jun 27 17:08:03.491: INFO: stderr: ""
Jun 27 17:08:03.491: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 27 17:08:03.491: INFO: validating pod update-demo-nautilus-s4wld
Jun 27 17:08:03.497: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 27 17:08:03.497: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 27 17:08:03.497: INFO: update-demo-nautilus-s4wld is verified up and running
STEP: rolling-update to new replication controller
Jun 27 17:08:03.501: INFO: scanned /root for discovery docs: <nil>
Jun 27 17:08:03.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-9140'
Jun 27 17:08:26.149: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 27 17:08:26.149: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 27 17:08:26.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-9140'
Jun 27 17:08:26.332: INFO: stderr: ""
Jun 27 17:08:26.332: INFO: stdout: "update-demo-kitten-bw268 update-demo-kitten-z9vq9 "
Jun 27 17:08:26.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-kitten-bw268 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9140'
Jun 27 17:08:26.501: INFO: stderr: ""
Jun 27 17:08:26.501: INFO: stdout: "true"
Jun 27 17:08:26.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-kitten-bw268 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9140'
Jun 27 17:08:26.672: INFO: stderr: ""
Jun 27 17:08:26.672: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 27 17:08:26.672: INFO: validating pod update-demo-kitten-bw268
Jun 27 17:08:26.678: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 27 17:08:26.678: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 27 17:08:26.678: INFO: update-demo-kitten-bw268 is verified up and running
Jun 27 17:08:26.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-kitten-z9vq9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9140'
Jun 27 17:08:26.848: INFO: stderr: ""
Jun 27 17:08:26.848: INFO: stdout: "true"
Jun 27 17:08:26.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-kitten-z9vq9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9140'
Jun 27 17:08:27.025: INFO: stderr: ""
Jun 27 17:08:27.025: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 27 17:08:27.025: INFO: validating pod update-demo-kitten-z9vq9
Jun 27 17:08:27.030: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 27 17:08:27.030: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 27 17:08:27.030: INFO: update-demo-kitten-z9vq9 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:08:27.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9140" for this suite.
Jun 27 17:08:49.049: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:08:49.227: INFO: namespace kubectl-9140 deletion completed in 22.19129421s

• [SLOW TEST:52.502 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:08:49.227: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-7479
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun 27 17:08:52.486: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:08:53.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7479" for this suite.
Jun 27 17:09:15.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:09:15.649: INFO: namespace replicaset-7479 deletion completed in 22.140697952s

• [SLOW TEST:26.421 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:09:15.650: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9649
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-3df3ad7d-d88b-48d2-9481-c43e4863ae31
STEP: Creating a pod to test consume secrets
Jun 27 17:09:15.834: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-eacaa4eb-9e33-47f1-8532-76e45c07df85" in namespace "projected-9649" to be "success or failure"
Jun 27 17:09:15.842: INFO: Pod "pod-projected-secrets-eacaa4eb-9e33-47f1-8532-76e45c07df85": Phase="Pending", Reason="", readiness=false. Elapsed: 7.877376ms
Jun 27 17:09:17.845: INFO: Pod "pod-projected-secrets-eacaa4eb-9e33-47f1-8532-76e45c07df85": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011665154s
Jun 27 17:09:19.849: INFO: Pod "pod-projected-secrets-eacaa4eb-9e33-47f1-8532-76e45c07df85": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015754993s
STEP: Saw pod success
Jun 27 17:09:19.849: INFO: Pod "pod-projected-secrets-eacaa4eb-9e33-47f1-8532-76e45c07df85" satisfied condition "success or failure"
Jun 27 17:09:19.853: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nzp9p pod pod-projected-secrets-eacaa4eb-9e33-47f1-8532-76e45c07df85 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 27 17:09:19.878: INFO: Waiting for pod pod-projected-secrets-eacaa4eb-9e33-47f1-8532-76e45c07df85 to disappear
Jun 27 17:09:19.881: INFO: Pod pod-projected-secrets-eacaa4eb-9e33-47f1-8532-76e45c07df85 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:09:19.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9649" for this suite.
Jun 27 17:09:25.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:09:26.022: INFO: namespace projected-9649 deletion completed in 6.135847912s

• [SLOW TEST:10.372 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:09:26.023: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9350
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Jun 27 17:09:26.178: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 --namespace=kubectl-9350 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jun 27 17:09:28.256: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jun 27 17:09:28.256: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:09:30.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9350" for this suite.
Jun 27 17:09:36.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:09:36.415: INFO: namespace kubectl-9350 deletion completed in 6.14710217s

• [SLOW TEST:10.392 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:09:36.416: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4917
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 27 17:09:36.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-4917'
Jun 27 17:09:36.763: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 27 17:09:36.763: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Jun 27 17:09:36.777: INFO: scanned /root for discovery docs: <nil>
Jun 27 17:09:36.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-4917'
Jun 27 17:09:52.665: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 27 17:09:52.665: INFO: stdout: "Created e2e-test-nginx-rc-80b03b8c9005244613a7865c86566c7f\nScaling up e2e-test-nginx-rc-80b03b8c9005244613a7865c86566c7f from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-80b03b8c9005244613a7865c86566c7f up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-80b03b8c9005244613a7865c86566c7f to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jun 27 17:09:52.665: INFO: stdout: "Created e2e-test-nginx-rc-80b03b8c9005244613a7865c86566c7f\nScaling up e2e-test-nginx-rc-80b03b8c9005244613a7865c86566c7f from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-80b03b8c9005244613a7865c86566c7f up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-80b03b8c9005244613a7865c86566c7f to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jun 27 17:09:52.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-4917'
Jun 27 17:09:52.838: INFO: stderr: ""
Jun 27 17:09:52.838: INFO: stdout: "e2e-test-nginx-rc-80b03b8c9005244613a7865c86566c7f-rm57d "
Jun 27 17:09:52.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods e2e-test-nginx-rc-80b03b8c9005244613a7865c86566c7f-rm57d -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4917'
Jun 27 17:09:53.008: INFO: stderr: ""
Jun 27 17:09:53.008: INFO: stdout: "true"
Jun 27 17:09:53.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods e2e-test-nginx-rc-80b03b8c9005244613a7865c86566c7f-rm57d -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4917'
Jun 27 17:09:53.181: INFO: stderr: ""
Jun 27 17:09:53.181: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jun 27 17:09:53.181: INFO: e2e-test-nginx-rc-80b03b8c9005244613a7865c86566c7f-rm57d is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Jun 27 17:09:53.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 delete rc e2e-test-nginx-rc --namespace=kubectl-4917'
Jun 27 17:09:53.365: INFO: stderr: ""
Jun 27 17:09:53.365: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:09:53.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4917" for this suite.
Jun 27 17:10:15.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:10:15.535: INFO: namespace kubectl-4917 deletion completed in 22.157597795s

• [SLOW TEST:39.119 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:10:15.535: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-9145
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 27 17:10:23.741: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 27 17:10:23.745: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 27 17:10:25.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 27 17:10:25.750: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 27 17:10:27.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 27 17:10:27.750: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 27 17:10:29.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 27 17:10:29.749: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 27 17:10:31.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 27 17:10:31.749: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 27 17:10:33.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 27 17:10:33.749: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 27 17:10:35.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 27 17:10:35.749: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 27 17:10:37.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 27 17:10:37.749: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 27 17:10:39.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 27 17:10:39.749: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 27 17:10:41.745: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 27 17:10:41.749: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:10:41.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9145" for this suite.
Jun 27 17:11:03.778: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:11:03.900: INFO: namespace container-lifecycle-hook-9145 deletion completed in 22.135294928s

• [SLOW TEST:48.365 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:11:03.904: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-68
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 27 17:11:04.058: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun 27 17:11:04.067: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 27 17:11:09.071: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 27 17:11:09.071: INFO: Creating deployment "test-rolling-update-deployment"
Jun 27 17:11:09.076: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun 27 17:11:09.089: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun 27 17:11:11.097: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun 27 17:11:11.100: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697252269, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697252269, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697252269, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697252269, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:11:13.104: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 27 17:11:13.116: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-68,SelfLink:/apis/apps/v1/namespaces/deployment-68/deployments/test-rolling-update-deployment,UID:6d2e24dc-1fc2-486c-bdb4-0f4a80afda43,ResourceVersion:4689,Generation:1,CreationTimestamp:2019-06-27 17:11:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-06-27 17:11:09 +0000 UTC 2019-06-27 17:11:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-06-27 17:11:11 +0000 UTC 2019-06-27 17:11:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jun 27 17:11:13.121: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-68,SelfLink:/apis/apps/v1/namespaces/deployment-68/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:139248af-438d-4cf9-b8b0-6def91f69e5f,ResourceVersion:4678,Generation:1,CreationTimestamp:2019-06-27 17:11:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 6d2e24dc-1fc2-486c-bdb4-0f4a80afda43 0xc002945c97 0xc002945c98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 27 17:11:13.121: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun 27 17:11:13.121: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-68,SelfLink:/apis/apps/v1/namespaces/deployment-68/replicasets/test-rolling-update-controller,UID:c9284135-6915-48c4-9243-6b6dff189f5b,ResourceVersion:4688,Generation:2,CreationTimestamp:2019-06-27 17:11:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 6d2e24dc-1fc2-486c-bdb4-0f4a80afda43 0xc002945bbf 0xc002945bd0}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 27 17:11:13.125: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-qsjrs" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-qsjrs,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-68,SelfLink:/api/v1/namespaces/deployment-68/pods/test-rolling-update-deployment-79f6b9d75c-qsjrs,UID:04810d2c-b558-4516-90d2-f4e90918776a,ResourceVersion:4677,Generation:0,CreationTimestamp:2019-06-27 17:11:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.187.85/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 139248af-438d-4cf9-b8b0-6def91f69e5f 0xc0029c5147 0xc0029c5148}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-5tfmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-5tfmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-5tfmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nk6xg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029c51b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029c51d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:11:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:11:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:11:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:11:09 +0000 UTC  }],Message:,Reason:,HostIP:139.178.70.235,PodIP:10.244.187.85,StartTime:2019-06-27 17:11:09 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-06-27 17:11:10 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://83e06cd0156a49c968cf5233e235fe767bc62ca21f2c7aa4c976de06c883cf5b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:11:13.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-68" for this suite.
Jun 27 17:11:19.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:11:19.232: INFO: namespace deployment-68 deletion completed in 6.154084515s

• [SLOW TEST:15.380 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:11:19.232: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1651
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-06ccdd9f-e1ed-41ba-8f11-ebbada66ef2c
STEP: Creating secret with name secret-projected-all-test-volume-043522c7-b442-4085-b2e9-232729f91fe2
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun 27 17:11:19.430: INFO: Waiting up to 5m0s for pod "projected-volume-c730f2ce-294c-4b87-8676-105ad0b51fcc" in namespace "projected-1651" to be "success or failure"
Jun 27 17:11:19.434: INFO: Pod "projected-volume-c730f2ce-294c-4b87-8676-105ad0b51fcc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.939673ms
Jun 27 17:11:21.438: INFO: Pod "projected-volume-c730f2ce-294c-4b87-8676-105ad0b51fcc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007952102s
STEP: Saw pod success
Jun 27 17:11:21.438: INFO: Pod "projected-volume-c730f2ce-294c-4b87-8676-105ad0b51fcc" satisfied condition "success or failure"
Jun 27 17:11:21.442: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-mskvs pod projected-volume-c730f2ce-294c-4b87-8676-105ad0b51fcc container projected-all-volume-test: <nil>
STEP: delete the pod
Jun 27 17:11:21.467: INFO: Waiting for pod projected-volume-c730f2ce-294c-4b87-8676-105ad0b51fcc to disappear
Jun 27 17:11:21.470: INFO: Pod projected-volume-c730f2ce-294c-4b87-8676-105ad0b51fcc no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:11:21.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1651" for this suite.
Jun 27 17:11:27.489: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:11:27.610: INFO: namespace projected-1651 deletion completed in 6.135198181s

• [SLOW TEST:8.378 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:11:27.611: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-3173
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-25481f35-91e5-482a-8dfe-0adbeae95b3e in namespace container-probe-3173
Jun 27 17:11:31.789: INFO: Started pod liveness-25481f35-91e5-482a-8dfe-0adbeae95b3e in namespace container-probe-3173
STEP: checking the pod's current state and verifying that restartCount is present
Jun 27 17:11:31.792: INFO: Initial restart count of pod liveness-25481f35-91e5-482a-8dfe-0adbeae95b3e is 0
Jun 27 17:11:53.837: INFO: Restart count of pod container-probe-3173/liveness-25481f35-91e5-482a-8dfe-0adbeae95b3e is now 1 (22.044618098s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:11:53.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3173" for this suite.
Jun 27 17:11:59.866: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:11:59.999: INFO: namespace container-probe-3173 deletion completed in 6.145115139s

• [SLOW TEST:32.388 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:11:59.999: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1003
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-fdf5d462-56be-4005-9289-5f26688d787d
STEP: Creating a pod to test consume secrets
Jun 27 17:12:00.174: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-68a1ecff-5a60-41ff-b37d-8c900daef15a" in namespace "projected-1003" to be "success or failure"
Jun 27 17:12:00.179: INFO: Pod "pod-projected-secrets-68a1ecff-5a60-41ff-b37d-8c900daef15a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.392723ms
Jun 27 17:12:02.183: INFO: Pod "pod-projected-secrets-68a1ecff-5a60-41ff-b37d-8c900daef15a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008348092s
STEP: Saw pod success
Jun 27 17:12:02.183: INFO: Pod "pod-projected-secrets-68a1ecff-5a60-41ff-b37d-8c900daef15a" satisfied condition "success or failure"
Jun 27 17:12:02.187: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod pod-projected-secrets-68a1ecff-5a60-41ff-b37d-8c900daef15a container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 27 17:12:02.208: INFO: Waiting for pod pod-projected-secrets-68a1ecff-5a60-41ff-b37d-8c900daef15a to disappear
Jun 27 17:12:02.214: INFO: Pod pod-projected-secrets-68a1ecff-5a60-41ff-b37d-8c900daef15a no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:12:02.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1003" for this suite.
Jun 27 17:12:08.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:12:08.387: INFO: namespace projected-1003 deletion completed in 6.159273609s

• [SLOW TEST:8.388 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:12:08.387: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-6049
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Jun 27 17:12:08.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 create -f - --namespace=kubectl-6049'
Jun 27 17:12:08.919: INFO: stderr: ""
Jun 27 17:12:08.919: INFO: stdout: "pod/pause created\n"
Jun 27 17:12:08.919: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun 27 17:12:08.919: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-6049" to be "running and ready"
Jun 27 17:12:08.923: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.209393ms
Jun 27 17:12:10.927: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.007936172s
Jun 27 17:12:10.927: INFO: Pod "pause" satisfied condition "running and ready"
Jun 27 17:12:10.927: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Jun 27 17:12:10.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 label pods pause testing-label=testing-label-value --namespace=kubectl-6049'
Jun 27 17:12:11.112: INFO: stderr: ""
Jun 27 17:12:11.112: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun 27 17:12:11.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pod pause -L testing-label --namespace=kubectl-6049'
Jun 27 17:12:11.282: INFO: stderr: ""
Jun 27 17:12:11.282: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun 27 17:12:11.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 label pods pause testing-label- --namespace=kubectl-6049'
Jun 27 17:12:11.466: INFO: stderr: ""
Jun 27 17:12:11.466: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun 27 17:12:11.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pod pause -L testing-label --namespace=kubectl-6049'
Jun 27 17:12:11.641: INFO: stderr: ""
Jun 27 17:12:11.641: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Jun 27 17:12:11.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 delete --grace-period=0 --force -f - --namespace=kubectl-6049'
Jun 27 17:12:11.818: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 27 17:12:11.818: INFO: stdout: "pod \"pause\" force deleted\n"
Jun 27 17:12:11.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get rc,svc -l name=pause --no-headers --namespace=kubectl-6049'
Jun 27 17:12:12.019: INFO: stderr: "No resources found.\n"
Jun 27 17:12:12.019: INFO: stdout: ""
Jun 27 17:12:12.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods -l name=pause --namespace=kubectl-6049 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 27 17:12:12.231: INFO: stderr: ""
Jun 27 17:12:12.231: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:12:12.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6049" for this suite.
Jun 27 17:12:18.250: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:12:18.398: INFO: namespace kubectl-6049 deletion completed in 6.162255901s

• [SLOW TEST:10.011 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:12:18.399: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-2987
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Jun 27 17:12:19.079: INFO: created pod pod-service-account-defaultsa
Jun 27 17:12:19.079: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun 27 17:12:19.086: INFO: created pod pod-service-account-mountsa
Jun 27 17:12:19.086: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun 27 17:12:19.094: INFO: created pod pod-service-account-nomountsa
Jun 27 17:12:19.094: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun 27 17:12:19.102: INFO: created pod pod-service-account-defaultsa-mountspec
Jun 27 17:12:19.102: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun 27 17:12:19.114: INFO: created pod pod-service-account-mountsa-mountspec
Jun 27 17:12:19.114: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun 27 17:12:19.131: INFO: created pod pod-service-account-nomountsa-mountspec
Jun 27 17:12:19.131: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun 27 17:12:19.145: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun 27 17:12:19.145: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun 27 17:12:19.160: INFO: created pod pod-service-account-mountsa-nomountspec
Jun 27 17:12:19.160: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun 27 17:12:19.179: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun 27 17:12:19.179: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:12:19.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2987" for this suite.
Jun 27 17:12:25.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:12:25.409: INFO: namespace svcaccounts-2987 deletion completed in 6.192121664s

• [SLOW TEST:7.010 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:12:25.410: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-3176
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0627 17:12:31.600106      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 27 17:12:31.600: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:12:31.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3176" for this suite.
Jun 27 17:12:37.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:12:37.760: INFO: namespace gc-3176 deletion completed in 6.152363594s

• [SLOW TEST:12.350 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:12:37.760: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-5553
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-1ba6c10c-0955-4a36-8432-95fd4c76d7b2 in namespace container-probe-5553
Jun 27 17:12:39.946: INFO: Started pod busybox-1ba6c10c-0955-4a36-8432-95fd4c76d7b2 in namespace container-probe-5553
STEP: checking the pod's current state and verifying that restartCount is present
Jun 27 17:12:39.950: INFO: Initial restart count of pod busybox-1ba6c10c-0955-4a36-8432-95fd4c76d7b2 is 0
Jun 27 17:13:28.047: INFO: Restart count of pod container-probe-5553/busybox-1ba6c10c-0955-4a36-8432-95fd4c76d7b2 is now 1 (48.096823685s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:13:28.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5553" for this suite.
Jun 27 17:13:34.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:13:34.202: INFO: namespace container-probe-5553 deletion completed in 6.138892224s

• [SLOW TEST:56.441 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:13:34.202: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7135
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jun 27 17:13:34.363: INFO: namespace kubectl-7135
Jun 27 17:13:34.363: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 create -f - --namespace=kubectl-7135'
Jun 27 17:13:34.750: INFO: stderr: ""
Jun 27 17:13:34.750: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 27 17:13:35.756: INFO: Selector matched 1 pods for map[app:redis]
Jun 27 17:13:35.756: INFO: Found 0 / 1
Jun 27 17:13:36.754: INFO: Selector matched 1 pods for map[app:redis]
Jun 27 17:13:36.754: INFO: Found 1 / 1
Jun 27 17:13:36.754: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 27 17:13:36.758: INFO: Selector matched 1 pods for map[app:redis]
Jun 27 17:13:36.758: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 27 17:13:36.758: INFO: wait on redis-master startup in kubectl-7135 
Jun 27 17:13:36.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 logs redis-master-vtdpd redis-master --namespace=kubectl-7135'
Jun 27 17:13:36.950: INFO: stderr: ""
Jun 27 17:13:36.950: INFO: stdout: "1:M 27 Jun 17:13:35.654 # You requested maxclients of 10000 requiring at least 10032 max file descriptors.\n1:M 27 Jun 17:13:35.654 # Server can't set maximum open files to 10032 because of OS error: Operation not permitted.\n1:M 27 Jun 17:13:35.654 # Current maximum open files is 4096. maxclients has been reduced to 4064 to compensate for low ulimit. If you need higher maxclients increase 'ulimit -n'.\n                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Jun 17:13:35.655 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Jun 17:13:35.655 # Server started, Redis version 3.2.12\n1:M 27 Jun 17:13:35.655 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jun 27 17:13:36.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-7135'
Jun 27 17:13:37.164: INFO: stderr: ""
Jun 27 17:13:37.164: INFO: stdout: "service/rm2 exposed\n"
Jun 27 17:13:37.170: INFO: Service rm2 in namespace kubectl-7135 found.
STEP: exposing service
Jun 27 17:13:39.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-7135'
Jun 27 17:13:39.400: INFO: stderr: ""
Jun 27 17:13:39.400: INFO: stdout: "service/rm3 exposed\n"
Jun 27 17:13:39.404: INFO: Service rm3 in namespace kubectl-7135 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:13:41.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7135" for this suite.
Jun 27 17:14:03.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:14:03.554: INFO: namespace kubectl-7135 deletion completed in 22.137354289s

• [SLOW TEST:29.352 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:14:03.554: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9118
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9118
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jun 27 17:14:03.727: INFO: Found 0 stateful pods, waiting for 3
Jun 27 17:14:13.732: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 27 17:14:13.732: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 27 17:14:13.732: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jun 27 17:14:13.764: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun 27 17:14:23.800: INFO: Updating stateful set ss2
Jun 27 17:14:23.810: INFO: Waiting for Pod statefulset-9118/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Jun 27 17:14:33.907: INFO: Found 2 stateful pods, waiting for 3
Jun 27 17:14:43.911: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 27 17:14:43.911: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 27 17:14:43.912: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun 27 17:14:43.938: INFO: Updating stateful set ss2
Jun 27 17:14:43.952: INFO: Waiting for Pod statefulset-9118/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 27 17:14:53.961: INFO: Waiting for Pod statefulset-9118/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 27 17:15:03.980: INFO: Updating stateful set ss2
Jun 27 17:15:03.988: INFO: Waiting for StatefulSet statefulset-9118/ss2 to complete update
Jun 27 17:15:03.988: INFO: Waiting for Pod statefulset-9118/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 27 17:15:13.996: INFO: Waiting for StatefulSet statefulset-9118/ss2 to complete update
Jun 27 17:15:13.996: INFO: Waiting for Pod statefulset-9118/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 27 17:15:23.996: INFO: Deleting all statefulset in ns statefulset-9118
Jun 27 17:15:24.000: INFO: Scaling statefulset ss2 to 0
Jun 27 17:15:54.017: INFO: Waiting for statefulset status.replicas updated to 0
Jun 27 17:15:54.021: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:15:54.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9118" for this suite.
Jun 27 17:16:00.054: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:16:00.179: INFO: namespace statefulset-9118 deletion completed in 6.137520213s

• [SLOW TEST:116.626 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:16:00.181: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-4116
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-ca79e00e-2216-4fd2-8947-5a54c019ea4e
STEP: Creating a pod to test consume secrets
Jun 27 17:16:00.357: INFO: Waiting up to 5m0s for pod "pod-secrets-0527c59b-b781-42e0-87f1-1f7084c6b920" in namespace "secrets-4116" to be "success or failure"
Jun 27 17:16:00.363: INFO: Pod "pod-secrets-0527c59b-b781-42e0-87f1-1f7084c6b920": Phase="Pending", Reason="", readiness=false. Elapsed: 5.738654ms
Jun 27 17:16:02.367: INFO: Pod "pod-secrets-0527c59b-b781-42e0-87f1-1f7084c6b920": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010274544s
STEP: Saw pod success
Jun 27 17:16:02.367: INFO: Pod "pod-secrets-0527c59b-b781-42e0-87f1-1f7084c6b920" satisfied condition "success or failure"
Jun 27 17:16:02.372: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-mskvs pod pod-secrets-0527c59b-b781-42e0-87f1-1f7084c6b920 container secret-volume-test: <nil>
STEP: delete the pod
Jun 27 17:16:02.397: INFO: Waiting for pod pod-secrets-0527c59b-b781-42e0-87f1-1f7084c6b920 to disappear
Jun 27 17:16:02.402: INFO: Pod pod-secrets-0527c59b-b781-42e0-87f1-1f7084c6b920 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:16:02.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4116" for this suite.
Jun 27 17:16:08.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:16:08.553: INFO: namespace secrets-4116 deletion completed in 6.145297539s

• [SLOW TEST:8.372 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:16:08.553: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2836
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 27 17:16:08.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2836'
Jun 27 17:16:08.983: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 27 17:16:08.983: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Jun 27 17:16:08.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 delete deployment e2e-test-nginx-deployment --namespace=kubectl-2836'
Jun 27 17:16:09.189: INFO: stderr: ""
Jun 27 17:16:09.189: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:16:09.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2836" for this suite.
Jun 27 17:16:15.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:16:15.353: INFO: namespace kubectl-2836 deletion completed in 6.153066385s

• [SLOW TEST:6.799 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:16:15.353: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename prestop
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in prestop-5476
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-5476
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-5476
STEP: Deleting pre-stop pod
Jun 27 17:16:26.557: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:16:26.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-5476" for this suite.
Jun 27 17:17:04.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:17:04.721: INFO: namespace prestop-5476 deletion completed in 38.150874057s

• [SLOW TEST:49.368 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:17:04.723: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8301
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-882dbc49-cf6f-402c-bfbb-73cfd901f197
STEP: Creating a pod to test consume secrets
Jun 27 17:17:04.892: INFO: Waiting up to 5m0s for pod "pod-secrets-1fba0b8a-7338-40e1-bfd6-349f5797b3e3" in namespace "secrets-8301" to be "success or failure"
Jun 27 17:17:04.896: INFO: Pod "pod-secrets-1fba0b8a-7338-40e1-bfd6-349f5797b3e3": Phase="Pending", Reason="", readiness=false. Elapsed: 3.998793ms
Jun 27 17:17:06.900: INFO: Pod "pod-secrets-1fba0b8a-7338-40e1-bfd6-349f5797b3e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007838882s
STEP: Saw pod success
Jun 27 17:17:06.900: INFO: Pod "pod-secrets-1fba0b8a-7338-40e1-bfd6-349f5797b3e3" satisfied condition "success or failure"
Jun 27 17:17:06.903: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nzp9p pod pod-secrets-1fba0b8a-7338-40e1-bfd6-349f5797b3e3 container secret-volume-test: <nil>
STEP: delete the pod
Jun 27 17:17:06.928: INFO: Waiting for pod pod-secrets-1fba0b8a-7338-40e1-bfd6-349f5797b3e3 to disappear
Jun 27 17:17:06.932: INFO: Pod pod-secrets-1fba0b8a-7338-40e1-bfd6-349f5797b3e3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:17:06.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8301" for this suite.
Jun 27 17:17:12.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:17:13.068: INFO: namespace secrets-8301 deletion completed in 6.132068458s

• [SLOW TEST:8.345 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:17:13.069: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9382
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-7e080903-68fe-4b50-a7f0-ba60d1736e5c
STEP: Creating a pod to test consume configMaps
Jun 27 17:17:13.236: INFO: Waiting up to 5m0s for pod "pod-configmaps-731c8154-2141-45e7-a252-bd6409b583b3" in namespace "configmap-9382" to be "success or failure"
Jun 27 17:17:13.242: INFO: Pod "pod-configmaps-731c8154-2141-45e7-a252-bd6409b583b3": Phase="Pending", Reason="", readiness=false. Elapsed: 5.989834ms
Jun 27 17:17:15.247: INFO: Pod "pod-configmaps-731c8154-2141-45e7-a252-bd6409b583b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010162333s
STEP: Saw pod success
Jun 27 17:17:15.247: INFO: Pod "pod-configmaps-731c8154-2141-45e7-a252-bd6409b583b3" satisfied condition "success or failure"
Jun 27 17:17:15.250: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod pod-configmaps-731c8154-2141-45e7-a252-bd6409b583b3 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 27 17:17:15.276: INFO: Waiting for pod pod-configmaps-731c8154-2141-45e7-a252-bd6409b583b3 to disappear
Jun 27 17:17:15.281: INFO: Pod pod-configmaps-731c8154-2141-45e7-a252-bd6409b583b3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:17:15.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9382" for this suite.
Jun 27 17:17:21.300: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:17:21.432: INFO: namespace configmap-9382 deletion completed in 6.14639652s

• [SLOW TEST:8.363 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:17:21.433: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8436
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 27 17:17:25.693: INFO: Waiting up to 5m0s for pod "client-envvars-d5b9df8a-070d-4457-806f-0682c1acb15d" in namespace "pods-8436" to be "success or failure"
Jun 27 17:17:25.709: INFO: Pod "client-envvars-d5b9df8a-070d-4457-806f-0682c1acb15d": Phase="Pending", Reason="", readiness=false. Elapsed: 15.989653ms
Jun 27 17:17:27.713: INFO: Pod "client-envvars-d5b9df8a-070d-4457-806f-0682c1acb15d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019891392s
Jun 27 17:17:29.717: INFO: Pod "client-envvars-d5b9df8a-070d-4457-806f-0682c1acb15d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02381245s
STEP: Saw pod success
Jun 27 17:17:29.717: INFO: Pod "client-envvars-d5b9df8a-070d-4457-806f-0682c1acb15d" satisfied condition "success or failure"
Jun 27 17:17:29.721: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod client-envvars-d5b9df8a-070d-4457-806f-0682c1acb15d container env3cont: <nil>
STEP: delete the pod
Jun 27 17:17:29.744: INFO: Waiting for pod client-envvars-d5b9df8a-070d-4457-806f-0682c1acb15d to disappear
Jun 27 17:17:29.751: INFO: Pod client-envvars-d5b9df8a-070d-4457-806f-0682c1acb15d no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:17:29.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8436" for this suite.
Jun 27 17:18:07.770: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:18:07.896: INFO: namespace pods-8436 deletion completed in 38.139877679s

• [SLOW TEST:46.463 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:18:07.897: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1842
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-0f4e8d95-4c84-406b-9f02-662de2a7c2e5
Jun 27 17:18:08.060: INFO: Pod name my-hostname-basic-0f4e8d95-4c84-406b-9f02-662de2a7c2e5: Found 0 pods out of 1
Jun 27 17:18:13.064: INFO: Pod name my-hostname-basic-0f4e8d95-4c84-406b-9f02-662de2a7c2e5: Found 1 pods out of 1
Jun 27 17:18:13.064: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-0f4e8d95-4c84-406b-9f02-662de2a7c2e5" are running
Jun 27 17:18:13.068: INFO: Pod "my-hostname-basic-0f4e8d95-4c84-406b-9f02-662de2a7c2e5-rk4vd" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-27 17:18:08 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-27 17:18:10 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-27 17:18:10 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-27 17:18:08 +0000 UTC Reason: Message:}])
Jun 27 17:18:13.068: INFO: Trying to dial the pod
Jun 27 17:18:18.080: INFO: Controller my-hostname-basic-0f4e8d95-4c84-406b-9f02-662de2a7c2e5: Got expected result from replica 1 [my-hostname-basic-0f4e8d95-4c84-406b-9f02-662de2a7c2e5-rk4vd]: "my-hostname-basic-0f4e8d95-4c84-406b-9f02-662de2a7c2e5-rk4vd", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:18:18.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1842" for this suite.
Jun 27 17:18:24.098: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:18:24.227: INFO: namespace replication-controller-1842 deletion completed in 6.142076496s

• [SLOW TEST:16.330 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:18:24.227: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8090
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-vvw7
STEP: Creating a pod to test atomic-volume-subpath
Jun 27 17:18:24.403: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-vvw7" in namespace "subpath-8090" to be "success or failure"
Jun 27 17:18:24.409: INFO: Pod "pod-subpath-test-configmap-vvw7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.100325ms
Jun 27 17:18:26.413: INFO: Pod "pod-subpath-test-configmap-vvw7": Phase="Running", Reason="", readiness=true. Elapsed: 2.010174654s
Jun 27 17:18:28.417: INFO: Pod "pod-subpath-test-configmap-vvw7": Phase="Running", Reason="", readiness=true. Elapsed: 4.014112533s
Jun 27 17:18:30.425: INFO: Pod "pod-subpath-test-configmap-vvw7": Phase="Running", Reason="", readiness=true. Elapsed: 6.022601195s
Jun 27 17:18:32.430: INFO: Pod "pod-subpath-test-configmap-vvw7": Phase="Running", Reason="", readiness=true. Elapsed: 8.026686994s
Jun 27 17:18:34.433: INFO: Pod "pod-subpath-test-configmap-vvw7": Phase="Running", Reason="", readiness=true. Elapsed: 10.030392863s
Jun 27 17:18:36.437: INFO: Pod "pod-subpath-test-configmap-vvw7": Phase="Running", Reason="", readiness=true. Elapsed: 12.034373662s
Jun 27 17:18:38.441: INFO: Pod "pod-subpath-test-configmap-vvw7": Phase="Running", Reason="", readiness=true. Elapsed: 14.03810277s
Jun 27 17:18:40.445: INFO: Pod "pod-subpath-test-configmap-vvw7": Phase="Running", Reason="", readiness=true. Elapsed: 16.041847789s
Jun 27 17:18:42.448: INFO: Pod "pod-subpath-test-configmap-vvw7": Phase="Running", Reason="", readiness=true. Elapsed: 18.045379678s
Jun 27 17:18:44.452: INFO: Pod "pod-subpath-test-configmap-vvw7": Phase="Running", Reason="", readiness=true. Elapsed: 20.049250847s
Jun 27 17:18:46.456: INFO: Pod "pod-subpath-test-configmap-vvw7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.053072465s
STEP: Saw pod success
Jun 27 17:18:46.456: INFO: Pod "pod-subpath-test-configmap-vvw7" satisfied condition "success or failure"
Jun 27 17:18:46.460: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-mskvs pod pod-subpath-test-configmap-vvw7 container test-container-subpath-configmap-vvw7: <nil>
STEP: delete the pod
Jun 27 17:18:46.485: INFO: Waiting for pod pod-subpath-test-configmap-vvw7 to disappear
Jun 27 17:18:46.489: INFO: Pod pod-subpath-test-configmap-vvw7 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-vvw7
Jun 27 17:18:46.489: INFO: Deleting pod "pod-subpath-test-configmap-vvw7" in namespace "subpath-8090"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:18:46.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8090" for this suite.
Jun 27 17:18:52.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:18:52.639: INFO: namespace subpath-8090 deletion completed in 6.142324377s

• [SLOW TEST:28.412 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:18:52.639: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2683
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secret-namespace-9527
STEP: Creating secret with name secret-test-a2649c7c-ea59-4e30-8d53-4d98aee572cd
STEP: Creating a pod to test consume secrets
Jun 27 17:18:52.961: INFO: Waiting up to 5m0s for pod "pod-secrets-25ec66ed-8427-49c4-b63b-264e8ddf4823" in namespace "secrets-2683" to be "success or failure"
Jun 27 17:18:52.969: INFO: Pod "pod-secrets-25ec66ed-8427-49c4-b63b-264e8ddf4823": Phase="Pending", Reason="", readiness=false. Elapsed: 8.177856ms
Jun 27 17:18:54.973: INFO: Pod "pod-secrets-25ec66ed-8427-49c4-b63b-264e8ddf4823": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012110695s
STEP: Saw pod success
Jun 27 17:18:54.973: INFO: Pod "pod-secrets-25ec66ed-8427-49c4-b63b-264e8ddf4823" satisfied condition "success or failure"
Jun 27 17:18:54.976: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-mskvs pod pod-secrets-25ec66ed-8427-49c4-b63b-264e8ddf4823 container secret-volume-test: <nil>
STEP: delete the pod
Jun 27 17:18:54.995: INFO: Waiting for pod pod-secrets-25ec66ed-8427-49c4-b63b-264e8ddf4823 to disappear
Jun 27 17:18:54.999: INFO: Pod pod-secrets-25ec66ed-8427-49c4-b63b-264e8ddf4823 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:18:54.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2683" for this suite.
Jun 27 17:19:01.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:19:01.138: INFO: namespace secrets-2683 deletion completed in 6.13459458s
STEP: Destroying namespace "secret-namespace-9527" for this suite.
Jun 27 17:19:07.152: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:19:07.284: INFO: namespace secret-namespace-9527 deletion completed in 6.146079729s

• [SLOW TEST:14.645 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:19:07.285: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-4744
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4744
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jun 27 17:19:07.464: INFO: Found 0 stateful pods, waiting for 3
Jun 27 17:19:17.469: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 27 17:19:17.469: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 27 17:19:17.469: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun 27 17:19:17.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 exec --namespace=statefulset-4744 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 27 17:19:17.864: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 27 17:19:17.864: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 27 17:19:17.864: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jun 27 17:19:27.906: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun 27 17:19:37.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 exec --namespace=statefulset-4744 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 27 17:19:38.282: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 27 17:19:38.282: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 27 17:19:38.282: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 27 17:19:58.307: INFO: Waiting for StatefulSet statefulset-4744/ss2 to complete update
Jun 27 17:19:58.307: INFO: Waiting for Pod statefulset-4744/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Jun 27 17:20:08.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 exec --namespace=statefulset-4744 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 27 17:20:08.672: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 27 17:20:08.672: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 27 17:20:08.672: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 27 17:20:18.729: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun 27 17:20:28.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 exec --namespace=statefulset-4744 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 27 17:20:29.097: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 27 17:20:29.097: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 27 17:20:29.097: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 27 17:20:39.121: INFO: Waiting for StatefulSet statefulset-4744/ss2 to complete update
Jun 27 17:20:39.121: INFO: Waiting for Pod statefulset-4744/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jun 27 17:20:39.121: INFO: Waiting for Pod statefulset-4744/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jun 27 17:20:39.121: INFO: Waiting for Pod statefulset-4744/ss2-2 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jun 27 17:20:49.129: INFO: Waiting for StatefulSet statefulset-4744/ss2 to complete update
Jun 27 17:20:49.129: INFO: Waiting for Pod statefulset-4744/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jun 27 17:20:49.129: INFO: Waiting for Pod statefulset-4744/ss2-1 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Jun 27 17:20:59.129: INFO: Waiting for StatefulSet statefulset-4744/ss2 to complete update
Jun 27 17:20:59.129: INFO: Waiting for Pod statefulset-4744/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 27 17:21:09.129: INFO: Deleting all statefulset in ns statefulset-4744
Jun 27 17:21:09.133: INFO: Scaling statefulset ss2 to 0
Jun 27 17:21:39.150: INFO: Waiting for statefulset status.replicas updated to 0
Jun 27 17:21:39.154: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:21:39.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4744" for this suite.
Jun 27 17:21:45.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:21:45.325: INFO: namespace statefulset-4744 deletion completed in 6.149935552s

• [SLOW TEST:158.018 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:21:45.325: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-1151
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 27 17:21:45.499: INFO: Waiting up to 5m0s for pod "pod-b90acdbf-ab97-4703-bf98-e287a14f30aa" in namespace "emptydir-1151" to be "success or failure"
Jun 27 17:21:45.505: INFO: Pod "pod-b90acdbf-ab97-4703-bf98-e287a14f30aa": Phase="Pending", Reason="", readiness=false. Elapsed: 5.570424ms
Jun 27 17:21:47.510: INFO: Pod "pod-b90acdbf-ab97-4703-bf98-e287a14f30aa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010417944s
STEP: Saw pod success
Jun 27 17:21:47.510: INFO: Pod "pod-b90acdbf-ab97-4703-bf98-e287a14f30aa" satisfied condition "success or failure"
Jun 27 17:21:47.513: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod pod-b90acdbf-ab97-4703-bf98-e287a14f30aa container test-container: <nil>
STEP: delete the pod
Jun 27 17:21:47.534: INFO: Waiting for pod pod-b90acdbf-ab97-4703-bf98-e287a14f30aa to disappear
Jun 27 17:21:47.538: INFO: Pod pod-b90acdbf-ab97-4703-bf98-e287a14f30aa no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:21:47.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1151" for this suite.
Jun 27 17:21:53.561: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:21:53.684: INFO: namespace emptydir-1151 deletion completed in 6.140598025s

• [SLOW TEST:8.358 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:21:53.684: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-2770
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Jun 27 17:21:53.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 cluster-info'
Jun 27 17:21:54.019: INFO: stderr: ""
Jun 27 17:21:54.019: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:21:54.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2770" for this suite.
Jun 27 17:22:00.038: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:22:00.163: INFO: namespace kubectl-2770 deletion completed in 6.138795724s

• [SLOW TEST:6.479 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:22:00.164: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4893
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Jun 27 17:22:00.318: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 create -f - --namespace=kubectl-4893'
Jun 27 17:22:00.708: INFO: stderr: ""
Jun 27 17:22:00.708: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Jun 27 17:22:01.712: INFO: Selector matched 1 pods for map[app:redis]
Jun 27 17:22:01.713: INFO: Found 0 / 1
Jun 27 17:22:02.712: INFO: Selector matched 1 pods for map[app:redis]
Jun 27 17:22:02.712: INFO: Found 1 / 1
Jun 27 17:22:02.712: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 27 17:22:02.717: INFO: Selector matched 1 pods for map[app:redis]
Jun 27 17:22:02.717: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jun 27 17:22:02.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 logs redis-master-s4cck redis-master --namespace=kubectl-4893'
Jun 27 17:22:02.918: INFO: stderr: ""
Jun 27 17:22:02.918: INFO: stdout: "1:M 27 Jun 17:22:01.625 # You requested maxclients of 10000 requiring at least 10032 max file descriptors.\n1:M 27 Jun 17:22:01.625 # Server can't set maximum open files to 10032 because of OS error: Operation not permitted.\n1:M 27 Jun 17:22:01.625 # Current maximum open files is 4096. maxclients has been reduced to 4064 to compensate for low ulimit. If you need higher maxclients increase 'ulimit -n'.\n                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Jun 17:22:01.626 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Jun 17:22:01.626 # Server started, Redis version 3.2.12\n1:M 27 Jun 17:22:01.626 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jun 27 17:22:02.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 log redis-master-s4cck redis-master --namespace=kubectl-4893 --tail=1'
Jun 27 17:22:03.110: INFO: stderr: ""
Jun 27 17:22:03.111: INFO: stdout: "1:M 27 Jun 17:22:01.626 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jun 27 17:22:03.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 log redis-master-s4cck redis-master --namespace=kubectl-4893 --limit-bytes=1'
Jun 27 17:22:03.312: INFO: stderr: ""
Jun 27 17:22:03.312: INFO: stdout: "1"
STEP: exposing timestamps
Jun 27 17:22:03.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 log redis-master-s4cck redis-master --namespace=kubectl-4893 --tail=1 --timestamps'
Jun 27 17:22:03.511: INFO: stderr: ""
Jun 27 17:22:03.511: INFO: stdout: "2019-06-27T17:22:01.627228508Z 1:M 27 Jun 17:22:01.626 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jun 27 17:22:06.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 log redis-master-s4cck redis-master --namespace=kubectl-4893 --since=1s'
Jun 27 17:22:06.206: INFO: stderr: ""
Jun 27 17:22:06.206: INFO: stdout: ""
Jun 27 17:22:06.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 log redis-master-s4cck redis-master --namespace=kubectl-4893 --since=24h'
Jun 27 17:22:06.404: INFO: stderr: ""
Jun 27 17:22:06.404: INFO: stdout: "1:M 27 Jun 17:22:01.625 # You requested maxclients of 10000 requiring at least 10032 max file descriptors.\n1:M 27 Jun 17:22:01.625 # Server can't set maximum open files to 10032 because of OS error: Operation not permitted.\n1:M 27 Jun 17:22:01.625 # Current maximum open files is 4096. maxclients has been reduced to 4064 to compensate for low ulimit. If you need higher maxclients increase 'ulimit -n'.\n                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 27 Jun 17:22:01.626 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 27 Jun 17:22:01.626 # Server started, Redis version 3.2.12\n1:M 27 Jun 17:22:01.626 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Jun 27 17:22:06.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 delete --grace-period=0 --force -f - --namespace=kubectl-4893'
Jun 27 17:22:06.580: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 27 17:22:06.580: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jun 27 17:22:06.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get rc,svc -l name=nginx --no-headers --namespace=kubectl-4893'
Jun 27 17:22:06.764: INFO: stderr: "No resources found.\n"
Jun 27 17:22:06.764: INFO: stdout: ""
Jun 27 17:22:06.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods -l name=nginx --namespace=kubectl-4893 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 27 17:22:06.942: INFO: stderr: ""
Jun 27 17:22:06.942: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:22:06.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4893" for this suite.
Jun 27 17:22:28.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:22:29.082: INFO: namespace kubectl-4893 deletion completed in 22.134857098s

• [SLOW TEST:28.918 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:22:29.082: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-3698
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun 27 17:22:29.556: INFO: Pod name wrapped-volume-race-f05301b3-262f-4377-abf0-fe56bd956663: Found 0 pods out of 5
Jun 27 17:22:34.564: INFO: Pod name wrapped-volume-race-f05301b3-262f-4377-abf0-fe56bd956663: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f05301b3-262f-4377-abf0-fe56bd956663 in namespace emptydir-wrapper-3698, will wait for the garbage collector to delete the pods
Jun 27 17:22:44.655: INFO: Deleting ReplicationController wrapped-volume-race-f05301b3-262f-4377-abf0-fe56bd956663 took: 10.981058ms
Jun 27 17:22:45.056: INFO: Terminating ReplicationController wrapped-volume-race-f05301b3-262f-4377-abf0-fe56bd956663 pods took: 400.242886ms
STEP: Creating RC which spawns configmap-volume pods
Jun 27 17:23:20.178: INFO: Pod name wrapped-volume-race-f7db232b-bd80-4ea9-9f64-8171fc6dedf0: Found 0 pods out of 5
Jun 27 17:23:25.187: INFO: Pod name wrapped-volume-race-f7db232b-bd80-4ea9-9f64-8171fc6dedf0: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f7db232b-bd80-4ea9-9f64-8171fc6dedf0 in namespace emptydir-wrapper-3698, will wait for the garbage collector to delete the pods
Jun 27 17:23:35.277: INFO: Deleting ReplicationController wrapped-volume-race-f7db232b-bd80-4ea9-9f64-8171fc6dedf0 took: 8.796927ms
Jun 27 17:23:35.677: INFO: Terminating ReplicationController wrapped-volume-race-f7db232b-bd80-4ea9-9f64-8171fc6dedf0 pods took: 400.244145ms
STEP: Creating RC which spawns configmap-volume pods
Jun 27 17:24:19.897: INFO: Pod name wrapped-volume-race-6a8a5794-deac-478d-b63d-2edb9595a3db: Found 0 pods out of 5
Jun 27 17:24:24.905: INFO: Pod name wrapped-volume-race-6a8a5794-deac-478d-b63d-2edb9595a3db: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-6a8a5794-deac-478d-b63d-2edb9595a3db in namespace emptydir-wrapper-3698, will wait for the garbage collector to delete the pods
Jun 27 17:24:34.995: INFO: Deleting ReplicationController wrapped-volume-race-6a8a5794-deac-478d-b63d-2edb9595a3db took: 9.264327ms
Jun 27 17:24:35.395: INFO: Terminating ReplicationController wrapped-volume-race-6a8a5794-deac-478d-b63d-2edb9595a3db pods took: 400.279516ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:25:11.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3698" for this suite.
Jun 27 17:25:19.073: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:25:19.197: INFO: namespace emptydir-wrapper-3698 deletion completed in 8.138213479s

• [SLOW TEST:170.114 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:25:19.198: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9623
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-374fb64d-e1f9-4af1-a04e-051ff5d0d3be
STEP: Creating a pod to test consume configMaps
Jun 27 17:25:19.394: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d1aaacc0-d643-4e0c-bdab-a06a8b157d63" in namespace "projected-9623" to be "success or failure"
Jun 27 17:25:19.401: INFO: Pod "pod-projected-configmaps-d1aaacc0-d643-4e0c-bdab-a06a8b157d63": Phase="Pending", Reason="", readiness=false. Elapsed: 6.746025ms
Jun 27 17:25:21.404: INFO: Pod "pod-projected-configmaps-d1aaacc0-d643-4e0c-bdab-a06a8b157d63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010347354s
STEP: Saw pod success
Jun 27 17:25:21.404: INFO: Pod "pod-projected-configmaps-d1aaacc0-d643-4e0c-bdab-a06a8b157d63" satisfied condition "success or failure"
Jun 27 17:25:21.408: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod pod-projected-configmaps-d1aaacc0-d643-4e0c-bdab-a06a8b157d63 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 27 17:25:21.444: INFO: Waiting for pod pod-projected-configmaps-d1aaacc0-d643-4e0c-bdab-a06a8b157d63 to disappear
Jun 27 17:25:21.449: INFO: Pod pod-projected-configmaps-d1aaacc0-d643-4e0c-bdab-a06a8b157d63 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:25:21.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9623" for this suite.
Jun 27 17:25:27.469: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:25:27.607: INFO: namespace projected-9623 deletion completed in 6.153240675s

• [SLOW TEST:8.409 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:25:27.608: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename aggregator
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in aggregator-3785
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jun 27 17:25:27.761: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Jun 27 17:25:29.206: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun 27 17:25:31.283: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:25:33.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:25:35.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:25:37.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:25:39.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:25:41.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:25:43.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:25:45.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:25:47.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697253129, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:25:51.333: INFO: Waited 2.032067881s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:25:51.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-3785" for this suite.
Jun 27 17:25:57.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:25:58.076: INFO: namespace aggregator-3785 deletion completed in 6.223236968s

• [SLOW TEST:30.469 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:25:58.077: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-4327
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-4327
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4327 to expose endpoints map[]
Jun 27 17:25:58.254: INFO: Get endpoints failed (5.771165ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Jun 27 17:25:59.258: INFO: successfully validated that service endpoint-test2 in namespace services-4327 exposes endpoints map[] (1.009522651s elapsed)
STEP: Creating pod pod1 in namespace services-4327
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4327 to expose endpoints map[pod1:[80]]
Jun 27 17:26:01.307: INFO: successfully validated that service endpoint-test2 in namespace services-4327 exposes endpoints map[pod1:[80]] (2.033618941s elapsed)
STEP: Creating pod pod2 in namespace services-4327
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4327 to expose endpoints map[pod1:[80] pod2:[80]]
Jun 27 17:26:03.351: INFO: successfully validated that service endpoint-test2 in namespace services-4327 exposes endpoints map[pod1:[80] pod2:[80]] (2.036075003s elapsed)
STEP: Deleting pod pod1 in namespace services-4327
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4327 to expose endpoints map[pod2:[80]]
Jun 27 17:26:03.374: INFO: successfully validated that service endpoint-test2 in namespace services-4327 exposes endpoints map[pod2:[80]] (13.09944ms elapsed)
STEP: Deleting pod pod2 in namespace services-4327
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4327 to expose endpoints map[]
Jun 27 17:26:04.400: INFO: successfully validated that service endpoint-test2 in namespace services-4327 exposes endpoints map[] (1.014599114s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:26:04.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4327" for this suite.
Jun 27 17:26:26.507: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:26:26.631: INFO: namespace services-4327 deletion completed in 22.155536084s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:28.554 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:26:26.633: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-4786
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 27 17:26:26.805: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2ef24ffa-4385-48fc-9b2d-1a3b7e13a0ea" in namespace "downward-api-4786" to be "success or failure"
Jun 27 17:26:26.816: INFO: Pod "downwardapi-volume-2ef24ffa-4385-48fc-9b2d-1a3b7e13a0ea": Phase="Pending", Reason="", readiness=false. Elapsed: 10.619198ms
Jun 27 17:26:28.820: INFO: Pod "downwardapi-volume-2ef24ffa-4385-48fc-9b2d-1a3b7e13a0ea": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014584537s
STEP: Saw pod success
Jun 27 17:26:28.820: INFO: Pod "downwardapi-volume-2ef24ffa-4385-48fc-9b2d-1a3b7e13a0ea" satisfied condition "success or failure"
Jun 27 17:26:28.824: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod downwardapi-volume-2ef24ffa-4385-48fc-9b2d-1a3b7e13a0ea container client-container: <nil>
STEP: delete the pod
Jun 27 17:26:28.845: INFO: Waiting for pod downwardapi-volume-2ef24ffa-4385-48fc-9b2d-1a3b7e13a0ea to disappear
Jun 27 17:26:28.848: INFO: Pod downwardapi-volume-2ef24ffa-4385-48fc-9b2d-1a3b7e13a0ea no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:26:28.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4786" for this suite.
Jun 27 17:26:34.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:26:34.996: INFO: namespace downward-api-4786 deletion completed in 6.141619275s

• [SLOW TEST:8.362 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:26:34.997: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-1328
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jun 27 17:26:35.152: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:26:38.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1328" for this suite.
Jun 27 17:26:44.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:26:44.631: INFO: namespace init-container-1328 deletion completed in 6.139996805s

• [SLOW TEST:9.634 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:26:44.632: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9732
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-4514c878-b91c-419d-a9db-b6add961f9ef
STEP: Creating secret with name s-test-opt-upd-cdf2612d-eee3-473b-b289-1272d9e253b8
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-4514c878-b91c-419d-a9db-b6add961f9ef
STEP: Updating secret s-test-opt-upd-cdf2612d-eee3-473b-b289-1272d9e253b8
STEP: Creating secret with name s-test-opt-create-e855f709-47f5-4734-8f37-229564ab8fc7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:26:48.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9732" for this suite.
Jun 27 17:27:10.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:27:11.034: INFO: namespace projected-9732 deletion completed in 22.133953297s

• [SLOW TEST:26.402 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:27:11.035: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-4433
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 27 17:27:11.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-4433'
Jun 27 17:27:11.468: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 27 17:27:11.468: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jun 27 17:27:11.481: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-wklnf]
Jun 27 17:27:11.481: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-wklnf" in namespace "kubectl-4433" to be "running and ready"
Jun 27 17:27:11.493: INFO: Pod "e2e-test-nginx-rc-wklnf": Phase="Pending", Reason="", readiness=false. Elapsed: 11.757879ms
Jun 27 17:27:13.497: INFO: Pod "e2e-test-nginx-rc-wklnf": Phase="Running", Reason="", readiness=true. Elapsed: 2.015570718s
Jun 27 17:27:13.497: INFO: Pod "e2e-test-nginx-rc-wklnf" satisfied condition "running and ready"
Jun 27 17:27:13.497: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-wklnf]
Jun 27 17:27:13.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 logs rc/e2e-test-nginx-rc --namespace=kubectl-4433'
Jun 27 17:27:13.715: INFO: stderr: ""
Jun 27 17:27:13.715: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Jun 27 17:27:13.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 delete rc e2e-test-nginx-rc --namespace=kubectl-4433'
Jun 27 17:27:13.896: INFO: stderr: ""
Jun 27 17:27:13.896: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:27:13.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4433" for this suite.
Jun 27 17:27:35.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:27:36.048: INFO: namespace kubectl-4433 deletion completed in 22.146319397s

• [SLOW TEST:25.014 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:27:36.049: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-7961
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 27 17:27:36.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 version'
Jun 27 17:27:36.376: INFO: stderr: ""
Jun 27 17:27:36.376: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:40:16Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:32:14Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:27:36.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7961" for this suite.
Jun 27 17:27:42.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:27:42.528: INFO: namespace kubectl-7961 deletion completed in 6.1466001s

• [SLOW TEST:6.479 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:27:42.530: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-5035
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:27:42.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5035" for this suite.
Jun 27 17:27:48.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:27:48.840: INFO: namespace services-5035 deletion completed in 6.143886897s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.310 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:27:48.840: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9790
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-9790/secret-test-4af7c983-746f-4c54-810f-fd5a1ea75484
STEP: Creating a pod to test consume secrets
Jun 27 17:27:49.011: INFO: Waiting up to 5m0s for pod "pod-configmaps-05427e8d-34bc-4a53-8051-bb4b2b84346f" in namespace "secrets-9790" to be "success or failure"
Jun 27 17:27:49.015: INFO: Pod "pod-configmaps-05427e8d-34bc-4a53-8051-bb4b2b84346f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.718073ms
Jun 27 17:27:51.019: INFO: Pod "pod-configmaps-05427e8d-34bc-4a53-8051-bb4b2b84346f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007507271s
STEP: Saw pod success
Jun 27 17:27:51.019: INFO: Pod "pod-configmaps-05427e8d-34bc-4a53-8051-bb4b2b84346f" satisfied condition "success or failure"
Jun 27 17:27:51.022: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-mskvs pod pod-configmaps-05427e8d-34bc-4a53-8051-bb4b2b84346f container env-test: <nil>
STEP: delete the pod
Jun 27 17:27:51.045: INFO: Waiting for pod pod-configmaps-05427e8d-34bc-4a53-8051-bb4b2b84346f to disappear
Jun 27 17:27:51.048: INFO: Pod pod-configmaps-05427e8d-34bc-4a53-8051-bb4b2b84346f no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:27:51.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9790" for this suite.
Jun 27 17:27:57.066: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:27:57.187: INFO: namespace secrets-9790 deletion completed in 6.13433461s

• [SLOW TEST:8.347 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:27:57.188: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6092
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-0f74119c-8dd5-4acc-822c-3ea392fad836
STEP: Creating a pod to test consume configMaps
Jun 27 17:27:57.359: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-afa35b6e-34d5-4a3d-be35-2eea5660d461" in namespace "projected-6092" to be "success or failure"
Jun 27 17:27:57.364: INFO: Pod "pod-projected-configmaps-afa35b6e-34d5-4a3d-be35-2eea5660d461": Phase="Pending", Reason="", readiness=false. Elapsed: 5.620495ms
Jun 27 17:27:59.369: INFO: Pod "pod-projected-configmaps-afa35b6e-34d5-4a3d-be35-2eea5660d461": Phase="Running", Reason="", readiness=true. Elapsed: 2.009962574s
Jun 27 17:28:01.373: INFO: Pod "pod-projected-configmaps-afa35b6e-34d5-4a3d-be35-2eea5660d461": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013682223s
STEP: Saw pod success
Jun 27 17:28:01.373: INFO: Pod "pod-projected-configmaps-afa35b6e-34d5-4a3d-be35-2eea5660d461" satisfied condition "success or failure"
Jun 27 17:28:01.376: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nzp9p pod pod-projected-configmaps-afa35b6e-34d5-4a3d-be35-2eea5660d461 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 27 17:28:01.399: INFO: Waiting for pod pod-projected-configmaps-afa35b6e-34d5-4a3d-be35-2eea5660d461 to disappear
Jun 27 17:28:01.402: INFO: Pod pod-projected-configmaps-afa35b6e-34d5-4a3d-be35-2eea5660d461 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:28:01.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6092" for this suite.
Jun 27 17:28:07.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:28:07.566: INFO: namespace projected-6092 deletion completed in 6.158793549s

• [SLOW TEST:10.378 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:28:07.567: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-4182
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-d066ccd6-b692-4852-953f-242445d65c96
STEP: Creating a pod to test consume configMaps
Jun 27 17:28:07.739: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4e21c581-01e9-4bcb-af97-eb6ddc8b8490" in namespace "projected-4182" to be "success or failure"
Jun 27 17:28:07.747: INFO: Pod "pod-projected-configmaps-4e21c581-01e9-4bcb-af97-eb6ddc8b8490": Phase="Pending", Reason="", readiness=false. Elapsed: 7.700106ms
Jun 27 17:28:09.751: INFO: Pod "pod-projected-configmaps-4e21c581-01e9-4bcb-af97-eb6ddc8b8490": Phase="Running", Reason="", readiness=true. Elapsed: 2.012030705s
Jun 27 17:28:11.755: INFO: Pod "pod-projected-configmaps-4e21c581-01e9-4bcb-af97-eb6ddc8b8490": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.015969194s
STEP: Saw pod success
Jun 27 17:28:11.755: INFO: Pod "pod-projected-configmaps-4e21c581-01e9-4bcb-af97-eb6ddc8b8490" satisfied condition "success or failure"
Jun 27 17:28:11.759: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod pod-projected-configmaps-4e21c581-01e9-4bcb-af97-eb6ddc8b8490 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 27 17:28:11.780: INFO: Waiting for pod pod-projected-configmaps-4e21c581-01e9-4bcb-af97-eb6ddc8b8490 to disappear
Jun 27 17:28:11.784: INFO: Pod pod-projected-configmaps-4e21c581-01e9-4bcb-af97-eb6ddc8b8490 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:28:11.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4182" for this suite.
Jun 27 17:28:17.803: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:28:17.937: INFO: namespace projected-4182 deletion completed in 6.14740535s

• [SLOW TEST:10.370 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:28:17.937: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-4587
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-9fe4404a-9609-4045-98cb-02ac0e72428e in namespace container-probe-4587
Jun 27 17:28:22.110: INFO: Started pod test-webserver-9fe4404a-9609-4045-98cb-02ac0e72428e in namespace container-probe-4587
STEP: checking the pod's current state and verifying that restartCount is present
Jun 27 17:28:22.113: INFO: Initial restart count of pod test-webserver-9fe4404a-9609-4045-98cb-02ac0e72428e is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:32:22.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4587" for this suite.
Jun 27 17:32:28.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:32:28.767: INFO: namespace container-probe-4587 deletion completed in 6.16076409s

• [SLOW TEST:250.830 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:32:28.767: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-7260
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7260.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7260.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7260.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7260.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7260.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7260.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7260.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7260.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 67.147.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.147.67_udp@PTR;check="$$(dig +tcp +noall +answer +search 67.147.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.147.67_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7260.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7260.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7260.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7260.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7260.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7260.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7260.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7260.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7260.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7260.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 67.147.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.147.67_udp@PTR;check="$$(dig +tcp +noall +answer +search 67.147.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.147.67_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 27 17:32:32.996: INFO: Unable to read wheezy_udp@dns-test-service.dns-7260.svc.cluster.local from pod dns-7260/dns-test-16035d77-5bd3-456e-a03f-48ceb4b354ee: the server could not find the requested resource (get pods dns-test-16035d77-5bd3-456e-a03f-48ceb4b354ee)
Jun 27 17:32:33.001: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7260.svc.cluster.local from pod dns-7260/dns-test-16035d77-5bd3-456e-a03f-48ceb4b354ee: the server could not find the requested resource (get pods dns-test-16035d77-5bd3-456e-a03f-48ceb4b354ee)
Jun 27 17:32:33.005: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7260.svc.cluster.local from pod dns-7260/dns-test-16035d77-5bd3-456e-a03f-48ceb4b354ee: the server could not find the requested resource (get pods dns-test-16035d77-5bd3-456e-a03f-48ceb4b354ee)
Jun 27 17:32:33.009: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7260.svc.cluster.local from pod dns-7260/dns-test-16035d77-5bd3-456e-a03f-48ceb4b354ee: the server could not find the requested resource (get pods dns-test-16035d77-5bd3-456e-a03f-48ceb4b354ee)
Jun 27 17:32:33.039: INFO: Unable to read jessie_udp@dns-test-service.dns-7260.svc.cluster.local from pod dns-7260/dns-test-16035d77-5bd3-456e-a03f-48ceb4b354ee: the server could not find the requested resource (get pods dns-test-16035d77-5bd3-456e-a03f-48ceb4b354ee)
Jun 27 17:32:33.043: INFO: Unable to read jessie_tcp@dns-test-service.dns-7260.svc.cluster.local from pod dns-7260/dns-test-16035d77-5bd3-456e-a03f-48ceb4b354ee: the server could not find the requested resource (get pods dns-test-16035d77-5bd3-456e-a03f-48ceb4b354ee)
Jun 27 17:32:33.047: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7260.svc.cluster.local from pod dns-7260/dns-test-16035d77-5bd3-456e-a03f-48ceb4b354ee: the server could not find the requested resource (get pods dns-test-16035d77-5bd3-456e-a03f-48ceb4b354ee)
Jun 27 17:32:33.052: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7260.svc.cluster.local from pod dns-7260/dns-test-16035d77-5bd3-456e-a03f-48ceb4b354ee: the server could not find the requested resource (get pods dns-test-16035d77-5bd3-456e-a03f-48ceb4b354ee)
Jun 27 17:32:33.078: INFO: Lookups using dns-7260/dns-test-16035d77-5bd3-456e-a03f-48ceb4b354ee failed for: [wheezy_udp@dns-test-service.dns-7260.svc.cluster.local wheezy_tcp@dns-test-service.dns-7260.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7260.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7260.svc.cluster.local jessie_udp@dns-test-service.dns-7260.svc.cluster.local jessie_tcp@dns-test-service.dns-7260.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7260.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7260.svc.cluster.local]

Jun 27 17:32:38.166: INFO: DNS probes using dns-7260/dns-test-16035d77-5bd3-456e-a03f-48ceb4b354ee succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:32:38.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7260" for this suite.
Jun 27 17:32:44.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:32:44.433: INFO: namespace dns-7260 deletion completed in 6.150249843s

• [SLOW TEST:15.666 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:32:44.433: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9326
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jun 27 17:32:47.137: INFO: Successfully updated pod "labelsupdateec347470-e12a-4a25-bde0-5d5690814e6a"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:32:49.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9326" for this suite.
Jun 27 17:33:11.173: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:33:11.298: INFO: namespace downward-api-9326 deletion completed in 22.13865682s

• [SLOW TEST:26.865 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:33:11.298: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1387
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 27 17:33:11.469: INFO: Waiting up to 5m0s for pod "downwardapi-volume-999bafb2-054e-4d17-82d5-6954945fc6ec" in namespace "downward-api-1387" to be "success or failure"
Jun 27 17:33:11.474: INFO: Pod "downwardapi-volume-999bafb2-054e-4d17-82d5-6954945fc6ec": Phase="Pending", Reason="", readiness=false. Elapsed: 4.713804ms
Jun 27 17:33:13.481: INFO: Pod "downwardapi-volume-999bafb2-054e-4d17-82d5-6954945fc6ec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011519475s
STEP: Saw pod success
Jun 27 17:33:13.481: INFO: Pod "downwardapi-volume-999bafb2-054e-4d17-82d5-6954945fc6ec" satisfied condition "success or failure"
Jun 27 17:33:13.485: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-mskvs pod downwardapi-volume-999bafb2-054e-4d17-82d5-6954945fc6ec container client-container: <nil>
STEP: delete the pod
Jun 27 17:33:13.513: INFO: Waiting for pod downwardapi-volume-999bafb2-054e-4d17-82d5-6954945fc6ec to disappear
Jun 27 17:33:13.517: INFO: Pod downwardapi-volume-999bafb2-054e-4d17-82d5-6954945fc6ec no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:33:13.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1387" for this suite.
Jun 27 17:33:19.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:33:19.666: INFO: namespace downward-api-1387 deletion completed in 6.144882418s

• [SLOW TEST:8.368 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:33:19.667: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-4093
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-4093/configmap-test-393d883d-83d6-481d-94a0-f21bff433412
STEP: Creating a pod to test consume configMaps
Jun 27 17:33:19.837: INFO: Waiting up to 5m0s for pod "pod-configmaps-f91faa2e-8327-4f6d-a32d-af09620fe0a4" in namespace "configmap-4093" to be "success or failure"
Jun 27 17:33:19.847: INFO: Pod "pod-configmaps-f91faa2e-8327-4f6d-a32d-af09620fe0a4": Phase="Pending", Reason="", readiness=false. Elapsed: 10.305588ms
Jun 27 17:33:21.851: INFO: Pod "pod-configmaps-f91faa2e-8327-4f6d-a32d-af09620fe0a4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014227696s
STEP: Saw pod success
Jun 27 17:33:21.851: INFO: Pod "pod-configmaps-f91faa2e-8327-4f6d-a32d-af09620fe0a4" satisfied condition "success or failure"
Jun 27 17:33:21.855: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nzp9p pod pod-configmaps-f91faa2e-8327-4f6d-a32d-af09620fe0a4 container env-test: <nil>
STEP: delete the pod
Jun 27 17:33:21.879: INFO: Waiting for pod pod-configmaps-f91faa2e-8327-4f6d-a32d-af09620fe0a4 to disappear
Jun 27 17:33:21.884: INFO: Pod pod-configmaps-f91faa2e-8327-4f6d-a32d-af09620fe0a4 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:33:21.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4093" for this suite.
Jun 27 17:33:27.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:33:28.031: INFO: namespace configmap-4093 deletion completed in 6.141269646s

• [SLOW TEST:8.363 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:33:28.031: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3578
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 27 17:33:28.193: INFO: Waiting up to 5m0s for pod "downward-api-61baedcf-7467-45ae-8a83-3b3e95a5beb8" in namespace "downward-api-3578" to be "success or failure"
Jun 27 17:33:28.198: INFO: Pod "downward-api-61baedcf-7467-45ae-8a83-3b3e95a5beb8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.204093ms
Jun 27 17:33:30.202: INFO: Pod "downward-api-61baedcf-7467-45ae-8a83-3b3e95a5beb8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009135222s
STEP: Saw pod success
Jun 27 17:33:30.202: INFO: Pod "downward-api-61baedcf-7467-45ae-8a83-3b3e95a5beb8" satisfied condition "success or failure"
Jun 27 17:33:30.206: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod downward-api-61baedcf-7467-45ae-8a83-3b3e95a5beb8 container dapi-container: <nil>
STEP: delete the pod
Jun 27 17:33:30.226: INFO: Waiting for pod downward-api-61baedcf-7467-45ae-8a83-3b3e95a5beb8 to disappear
Jun 27 17:33:30.232: INFO: Pod downward-api-61baedcf-7467-45ae-8a83-3b3e95a5beb8 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:33:30.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3578" for this suite.
Jun 27 17:33:36.251: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:33:36.385: INFO: namespace downward-api-3578 deletion completed in 6.147746961s

• [SLOW TEST:8.354 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:33:36.387: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-8259
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Jun 27 17:33:36.571: INFO: Waiting up to 5m0s for pod "var-expansion-aa5d98bd-0694-49cc-b710-4120d2bbdcd6" in namespace "var-expansion-8259" to be "success or failure"
Jun 27 17:33:36.574: INFO: Pod "var-expansion-aa5d98bd-0694-49cc-b710-4120d2bbdcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.664133ms
Jun 27 17:33:38.578: INFO: Pod "var-expansion-aa5d98bd-0694-49cc-b710-4120d2bbdcd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007793622s
STEP: Saw pod success
Jun 27 17:33:38.578: INFO: Pod "var-expansion-aa5d98bd-0694-49cc-b710-4120d2bbdcd6" satisfied condition "success or failure"
Jun 27 17:33:38.582: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-mskvs pod var-expansion-aa5d98bd-0694-49cc-b710-4120d2bbdcd6 container dapi-container: <nil>
STEP: delete the pod
Jun 27 17:33:38.603: INFO: Waiting for pod var-expansion-aa5d98bd-0694-49cc-b710-4120d2bbdcd6 to disappear
Jun 27 17:33:38.607: INFO: Pod var-expansion-aa5d98bd-0694-49cc-b710-4120d2bbdcd6 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:33:38.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8259" for this suite.
Jun 27 17:33:44.627: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:33:44.754: INFO: namespace var-expansion-8259 deletion completed in 6.141391626s

• [SLOW TEST:8.368 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:33:44.754: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8163
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-e47b5f1f-ed96-4ea2-b659-9b92395302ea
STEP: Creating a pod to test consume configMaps
Jun 27 17:33:44.924: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-758b5745-6f53-41d8-ac73-b96d990370be" in namespace "projected-8163" to be "success or failure"
Jun 27 17:33:44.929: INFO: Pod "pod-projected-configmaps-758b5745-6f53-41d8-ac73-b96d990370be": Phase="Pending", Reason="", readiness=false. Elapsed: 4.457413ms
Jun 27 17:33:46.933: INFO: Pod "pod-projected-configmaps-758b5745-6f53-41d8-ac73-b96d990370be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008609392s
STEP: Saw pod success
Jun 27 17:33:46.933: INFO: Pod "pod-projected-configmaps-758b5745-6f53-41d8-ac73-b96d990370be" satisfied condition "success or failure"
Jun 27 17:33:46.936: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nzp9p pod pod-projected-configmaps-758b5745-6f53-41d8-ac73-b96d990370be container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 27 17:33:46.959: INFO: Waiting for pod pod-projected-configmaps-758b5745-6f53-41d8-ac73-b96d990370be to disappear
Jun 27 17:33:46.963: INFO: Pod pod-projected-configmaps-758b5745-6f53-41d8-ac73-b96d990370be no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:33:46.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8163" for this suite.
Jun 27 17:33:52.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:33:53.101: INFO: namespace projected-8163 deletion completed in 6.131962418s

• [SLOW TEST:8.347 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:33:53.101: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-8674
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-vch8
STEP: Creating a pod to test atomic-volume-subpath
Jun 27 17:33:53.275: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-vch8" in namespace "subpath-8674" to be "success or failure"
Jun 27 17:33:53.279: INFO: Pod "pod-subpath-test-configmap-vch8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.719353ms
Jun 27 17:33:55.283: INFO: Pod "pod-subpath-test-configmap-vch8": Phase="Running", Reason="", readiness=true. Elapsed: 2.007768122s
Jun 27 17:33:57.287: INFO: Pod "pod-subpath-test-configmap-vch8": Phase="Running", Reason="", readiness=true. Elapsed: 4.012029401s
Jun 27 17:33:59.291: INFO: Pod "pod-subpath-test-configmap-vch8": Phase="Running", Reason="", readiness=true. Elapsed: 6.0157896s
Jun 27 17:34:01.295: INFO: Pod "pod-subpath-test-configmap-vch8": Phase="Running", Reason="", readiness=true. Elapsed: 8.019665648s
Jun 27 17:34:03.299: INFO: Pod "pod-subpath-test-configmap-vch8": Phase="Running", Reason="", readiness=true. Elapsed: 10.023714087s
Jun 27 17:34:05.303: INFO: Pod "pod-subpath-test-configmap-vch8": Phase="Running", Reason="", readiness=true. Elapsed: 12.027557926s
Jun 27 17:34:07.307: INFO: Pod "pod-subpath-test-configmap-vch8": Phase="Running", Reason="", readiness=true. Elapsed: 14.031457065s
Jun 27 17:34:09.311: INFO: Pod "pod-subpath-test-configmap-vch8": Phase="Running", Reason="", readiness=true. Elapsed: 16.036274565s
Jun 27 17:34:11.315: INFO: Pod "pod-subpath-test-configmap-vch8": Phase="Running", Reason="", readiness=true. Elapsed: 18.040093973s
Jun 27 17:34:13.319: INFO: Pod "pod-subpath-test-configmap-vch8": Phase="Running", Reason="", readiness=true. Elapsed: 20.044261342s
Jun 27 17:34:15.323: INFO: Pod "pod-subpath-test-configmap-vch8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.048140811s
STEP: Saw pod success
Jun 27 17:34:15.323: INFO: Pod "pod-subpath-test-configmap-vch8" satisfied condition "success or failure"
Jun 27 17:34:15.327: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod pod-subpath-test-configmap-vch8 container test-container-subpath-configmap-vch8: <nil>
STEP: delete the pod
Jun 27 17:34:15.349: INFO: Waiting for pod pod-subpath-test-configmap-vch8 to disappear
Jun 27 17:34:15.354: INFO: Pod pod-subpath-test-configmap-vch8 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-vch8
Jun 27 17:34:15.354: INFO: Deleting pod "pod-subpath-test-configmap-vch8" in namespace "subpath-8674"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:34:15.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8674" for this suite.
Jun 27 17:34:21.393: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:34:21.525: INFO: namespace subpath-8674 deletion completed in 6.145558729s

• [SLOW TEST:28.424 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:34:21.525: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-9685
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 27 17:34:21.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-9685'
Jun 27 17:34:21.931: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 27 17:34:21.931: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Jun 27 17:34:23.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 delete deployment e2e-test-nginx-deployment --namespace=kubectl-9685'
Jun 27 17:34:24.148: INFO: stderr: ""
Jun 27 17:34:24.148: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:34:24.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9685" for this suite.
Jun 27 17:34:46.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:34:46.283: INFO: namespace kubectl-9685 deletion completed in 22.143403284s

• [SLOW TEST:24.772 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:34:46.283: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5265
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-ba3e5a9b-daa9-405c-bea2-7b0aad3dd3da
STEP: Creating configMap with name cm-test-opt-upd-dde2cb7b-c95f-48ea-bc74-ccd044202a86
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-ba3e5a9b-daa9-405c-bea2-7b0aad3dd3da
STEP: Updating configmap cm-test-opt-upd-dde2cb7b-c95f-48ea-bc74-ccd044202a86
STEP: Creating configMap with name cm-test-opt-create-85d4a116-2ed6-401c-9841-b1d5c206ee95
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:36:00.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5265" for this suite.
Jun 27 17:36:22.905: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:36:23.030: INFO: namespace projected-5265 deletion completed in 22.13831626s

• [SLOW TEST:96.747 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:36:23.031: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename job
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in job-1980
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-1980, will wait for the garbage collector to delete the pods
Jun 27 17:36:25.262: INFO: Deleting Job.batch foo took: 7.809826ms
Jun 27 17:36:25.662: INFO: Terminating Job.batch foo pods took: 400.390045ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:37:05.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1980" for this suite.
Jun 27 17:37:11.184: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:37:11.310: INFO: namespace job-1980 deletion completed in 6.139669584s

• [SLOW TEST:48.279 seconds]
[sig-apps] Job
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:37:11.311: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-1484
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1484.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1484.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1484.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1484.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1484.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1484.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 27 17:37:13.532: INFO: DNS probes using dns-1484/dns-test-c1d09323-4744-497a-ac53-8f2732a50375 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:37:13.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1484" for this suite.
Jun 27 17:37:19.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:37:19.688: INFO: namespace dns-1484 deletion completed in 6.13464984s

• [SLOW TEST:8.377 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:37:19.688: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-4210
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 27 17:37:19.843: INFO: Creating deployment "nginx-deployment"
Jun 27 17:37:19.848: INFO: Waiting for observed generation 1
Jun 27 17:37:21.857: INFO: Waiting for all required pods to come up
Jun 27 17:37:21.862: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun 27 17:37:23.877: INFO: Waiting for deployment "nginx-deployment" to complete
Jun 27 17:37:23.884: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jun 27 17:37:23.893: INFO: Updating deployment nginx-deployment
Jun 27 17:37:23.893: INFO: Waiting for observed generation 2
Jun 27 17:37:25.902: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun 27 17:37:25.906: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun 27 17:37:25.910: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jun 27 17:37:25.920: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun 27 17:37:25.920: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun 27 17:37:25.924: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jun 27 17:37:25.931: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jun 27 17:37:25.931: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jun 27 17:37:25.942: INFO: Updating deployment nginx-deployment
Jun 27 17:37:25.942: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jun 27 17:37:25.989: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun 27 17:37:28.040: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 27 17:37:28.048: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-4210,SelfLink:/apis/apps/v1/namespaces/deployment-4210/deployments/nginx-deployment,UID:0814e732-0b3b-4cba-849d-1fe742de3c15,ResourceVersion:11367,Generation:3,CreationTimestamp:2019-06-27 17:37:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-06-27 17:37:25 +0000 UTC 2019-06-27 17:37:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-06-27 17:37:26 +0000 UTC 2019-06-27 17:37:19 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Jun 27 17:37:28.053: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-4210,SelfLink:/apis/apps/v1/namespaces/deployment-4210/replicasets/nginx-deployment-55fb7cb77f,UID:7e2a62e9-5e45-497e-a6da-e384627d16e5,ResourceVersion:11366,Generation:3,CreationTimestamp:2019-06-27 17:37:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 0814e732-0b3b-4cba-849d-1fe742de3c15 0xc001d167a7 0xc001d167a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 27 17:37:28.053: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jun 27 17:37:28.053: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-4210,SelfLink:/apis/apps/v1/namespaces/deployment-4210/replicasets/nginx-deployment-7b8c6f4498,UID:5acfb9a0-3171-4602-abb3-5faf1ffae314,ResourceVersion:11336,Generation:3,CreationTimestamp:2019-06-27 17:37:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 0814e732-0b3b-4cba-849d-1fe742de3c15 0xc001d16877 0xc001d16878}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jun 27 17:37:28.063: INFO: Pod "nginx-deployment-55fb7cb77f-64b6b" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-64b6b,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-55fb7cb77f-64b6b,UID:472f1f21-a1c9-41a3-8584-569209b6fddc,ResourceVersion:11395,Generation:0,CreationTimestamp:2019-06-27 17:37:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.20.198/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e2a62e9-5e45-497e-a6da-e384627d16e5 0xc0024ed087 0xc0024ed088}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-mskvs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024ed0f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024ed120}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  }],Message:,Reason:,HostIP:147.75.109.65,PodIP:,StartTime:2019-06-27 17:37:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.063: INFO: Pod "nginx-deployment-55fb7cb77f-bkwvg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-bkwvg,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-55fb7cb77f-bkwvg,UID:9d562653-12c8-4f49-bce5-781f589b159a,ResourceVersion:11281,Generation:0,CreationTimestamp:2019-06-27 17:37:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.187.121/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e2a62e9-5e45-497e-a6da-e384627d16e5 0xc0024ed1f0 0xc0024ed1f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nk6xg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024ed260} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024ed280}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:23 +0000 UTC  }],Message:,Reason:,HostIP:139.178.70.235,PodIP:10.244.187.121,StartTime:2019-06-27 17:37:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to resolve image "docker.io/library/nginx:404": no available registry endpoint: docker.io/library/nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.064: INFO: Pod "nginx-deployment-55fb7cb77f-cn9sm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-cn9sm,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-55fb7cb77f-cn9sm,UID:746c44ae-3cec-46d8-83ed-40af1ebf34ba,ResourceVersion:11518,Generation:0,CreationTimestamp:2019-06-27 17:37:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.177.236/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e2a62e9-5e45-497e-a6da-e384627d16e5 0xc0024ed370 0xc0024ed371}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nzp9p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024ed3e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024ed400}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:23 +0000 UTC  }],Message:,Reason:,HostIP:147.75.70.201,PodIP:10.244.177.236,StartTime:2019-06-27 17:37:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to resolve image "docker.io/library/nginx:404": no available registry endpoint: docker.io/library/nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.064: INFO: Pod "nginx-deployment-55fb7cb77f-cppdh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-cppdh,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-55fb7cb77f-cppdh,UID:59d2d96a-32d4-4501-a65a-70c688d5fd33,ResourceVersion:11480,Generation:0,CreationTimestamp:2019-06-27 17:37:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.20.202/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e2a62e9-5e45-497e-a6da-e384627d16e5 0xc0024ed500 0xc0024ed501}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-mskvs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024ed570} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024ed590}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  }],Message:,Reason:,HostIP:147.75.109.65,PodIP:,StartTime:2019-06-27 17:37:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.064: INFO: Pod "nginx-deployment-55fb7cb77f-f7hhc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-f7hhc,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-55fb7cb77f-f7hhc,UID:5e41559d-961d-4878-a6c8-35bf5b936e78,ResourceVersion:11449,Generation:0,CreationTimestamp:2019-06-27 17:37:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.177.240/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e2a62e9-5e45-497e-a6da-e384627d16e5 0xc0024ed660 0xc0024ed661}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nzp9p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024ed6d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024ed6f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  }],Message:,Reason:,HostIP:147.75.70.201,PodIP:,StartTime:2019-06-27 17:37:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.065: INFO: Pod "nginx-deployment-55fb7cb77f-fl4qf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-fl4qf,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-55fb7cb77f-fl4qf,UID:aaed33b5-dc2f-4bde-b8ac-7553b0673e0a,ResourceVersion:11419,Generation:0,CreationTimestamp:2019-06-27 17:37:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.187.124/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e2a62e9-5e45-497e-a6da-e384627d16e5 0xc0024ed7c0 0xc0024ed7c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nk6xg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024ed830} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024ed850}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  }],Message:,Reason:,HostIP:139.178.70.235,PodIP:,StartTime:2019-06-27 17:37:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.065: INFO: Pod "nginx-deployment-55fb7cb77f-klmh2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-klmh2,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-55fb7cb77f-klmh2,UID:90ef9285-1fec-431c-a5de-9d65cc64b2df,ResourceVersion:11502,Generation:0,CreationTimestamp:2019-06-27 17:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.187.122/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e2a62e9-5e45-497e-a6da-e384627d16e5 0xc0024ed930 0xc0024ed931}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nk6xg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024ed9a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024ed9c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:24 +0000 UTC  }],Message:,Reason:,HostIP:139.178.70.235,PodIP:10.244.187.122,StartTime:2019-06-27 17:37:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to resolve image "docker.io/library/nginx:404": no available registry endpoint: docker.io/library/nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.065: INFO: Pod "nginx-deployment-55fb7cb77f-qw8ls" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-qw8ls,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-55fb7cb77f-qw8ls,UID:894972c6-3833-4b78-b65f-f72c1db74f2a,ResourceVersion:11536,Generation:0,CreationTimestamp:2019-06-27 17:37:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.177.245/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e2a62e9-5e45-497e-a6da-e384627d16e5 0xc0024edab0 0xc0024edab1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nzp9p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024edb20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024edb40}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  }],Message:,Reason:,HostIP:147.75.70.201,PodIP:,StartTime:2019-06-27 17:37:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.066: INFO: Pod "nginx-deployment-55fb7cb77f-rwfqc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-rwfqc,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-55fb7cb77f-rwfqc,UID:989a7df2-3f5b-418d-8d1e-f4b53f66aa36,ResourceVersion:11482,Generation:0,CreationTimestamp:2019-06-27 17:37:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.177.242/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e2a62e9-5e45-497e-a6da-e384627d16e5 0xc0024edc10 0xc0024edc11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nzp9p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024edc80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024edca0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  }],Message:,Reason:,HostIP:147.75.70.201,PodIP:,StartTime:2019-06-27 17:37:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.066: INFO: Pod "nginx-deployment-55fb7cb77f-vp877" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-vp877,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-55fb7cb77f-vp877,UID:baeda849-fc9b-4c46-9b57-50cdf110500d,ResourceVersion:11456,Generation:0,CreationTimestamp:2019-06-27 17:37:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.20.203/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e2a62e9-5e45-497e-a6da-e384627d16e5 0xc0024edd80 0xc0024edd81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-mskvs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024eddf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024ede10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  }],Message:,Reason:,HostIP:147.75.109.65,PodIP:,StartTime:2019-06-27 17:37:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.066: INFO: Pod "nginx-deployment-55fb7cb77f-w2qck" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-w2qck,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-55fb7cb77f-w2qck,UID:515b8a48-9be1-4e81-b0ef-fa8e53ac1ff0,ResourceVersion:11254,Generation:0,CreationTimestamp:2019-06-27 17:37:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.177.237/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e2a62e9-5e45-497e-a6da-e384627d16e5 0xc0024edee0 0xc0024edee1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nzp9p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0024edf50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0024edf70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:24 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:24 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:24 +0000 UTC  }],Message:,Reason:,HostIP:147.75.70.201,PodIP:,StartTime:2019-06-27 17:37:24 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.067: INFO: Pod "nginx-deployment-55fb7cb77f-z2bwx" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-z2bwx,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-55fb7cb77f-z2bwx,UID:7d007598-9729-4ff6-af7c-18dc31d9fe2d,ResourceVersion:11445,Generation:0,CreationTimestamp:2019-06-27 17:37:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.187.125/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e2a62e9-5e45-497e-a6da-e384627d16e5 0xc002944040 0xc002944041}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nk6xg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029440b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029440d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  }],Message:,Reason:,HostIP:139.178.70.235,PodIP:,StartTime:2019-06-27 17:37:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.067: INFO: Pod "nginx-deployment-55fb7cb77f-zqwh2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-zqwh2,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-55fb7cb77f-zqwh2,UID:e3b2fc5d-1fef-4ae7-91ae-ee87e9d34ff7,ResourceVersion:11305,Generation:0,CreationTimestamp:2019-06-27 17:37:23 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.20.197/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 7e2a62e9-5e45-497e-a6da-e384627d16e5 0xc0029441b0 0xc0029441b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-mskvs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002944220} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002944240}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:23 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:23 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:23 +0000 UTC  }],Message:,Reason:,HostIP:147.75.109.65,PodIP:10.244.20.197,StartTime:2019-06-27 17:37:23 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to resolve image "docker.io/library/nginx:404": no available registry endpoint: docker.io/library/nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.068: INFO: Pod "nginx-deployment-7b8c6f4498-2zw5l" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-2zw5l,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-7b8c6f4498-2zw5l,UID:c9aa3134-5573-4213-97d8-fd099bb0aeb9,ResourceVersion:11404,Generation:0,CreationTimestamp:2019-06-27 17:37:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.20.199/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5acfb9a0-3171-4602-abb3-5faf1ffae314 0xc002944340 0xc002944341}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-mskvs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029443a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029443c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  }],Message:,Reason:,HostIP:147.75.109.65,PodIP:,StartTime:2019-06-27 17:37:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.068: INFO: Pod "nginx-deployment-7b8c6f4498-4d2wl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4d2wl,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-7b8c6f4498-4d2wl,UID:b11d9e1b-4f05-4a52-ac2e-c23d48ec3a43,ResourceVersion:11460,Generation:0,CreationTimestamp:2019-06-27 17:37:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.177.241/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5acfb9a0-3171-4602-abb3-5faf1ffae314 0xc002944487 0xc002944488}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nzp9p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029444f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002944510}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  }],Message:,Reason:,HostIP:147.75.70.201,PodIP:,StartTime:2019-06-27 17:37:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.068: INFO: Pod "nginx-deployment-7b8c6f4498-5rsmp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-5rsmp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-7b8c6f4498-5rsmp,UID:b2f1b8cb-a78e-4fb7-b95a-ba998236b2eb,ResourceVersion:11475,Generation:0,CreationTimestamp:2019-06-27 17:37:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.187.126/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5acfb9a0-3171-4602-abb3-5faf1ffae314 0xc0029445d7 0xc0029445d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nk6xg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002944640} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002944660}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  }],Message:,Reason:,HostIP:139.178.70.235,PodIP:,StartTime:2019-06-27 17:37:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.069: INFO: Pod "nginx-deployment-7b8c6f4498-6hx8k" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-6hx8k,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-7b8c6f4498-6hx8k,UID:d53cb153-d4fe-4ed3-a56c-d2141af49f15,ResourceVersion:11495,Generation:0,CreationTimestamp:2019-06-27 17:37:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.187.127/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5acfb9a0-3171-4602-abb3-5faf1ffae314 0xc002944727 0xc002944728}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nk6xg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002944790} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029447b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  }],Message:,Reason:,HostIP:139.178.70.235,PodIP:,StartTime:2019-06-27 17:37:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.069: INFO: Pod "nginx-deployment-7b8c6f4498-8j8h5" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-8j8h5,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-7b8c6f4498-8j8h5,UID:fd637e78-8cd8-4682-9c32-5b2125ce6fe9,ResourceVersion:11121,Generation:0,CreationTimestamp:2019-06-27 17:37:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.187.120/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5acfb9a0-3171-4602-abb3-5faf1ffae314 0xc002944877 0xc002944878}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nk6xg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029448e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002944900}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:19 +0000 UTC  }],Message:,Reason:,HostIP:139.178.70.235,PodIP:10.244.187.120,StartTime:2019-06-27 17:37:20 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-27 17:37:20 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://f79c273807f41c9547af55dab7128e5b8fafaad17d034eb1ee81a5ae26ec6d90}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.069: INFO: Pod "nginx-deployment-7b8c6f4498-9vjld" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-9vjld,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-7b8c6f4498-9vjld,UID:8fe83028-2266-4959-8a96-dd82e1f2c656,ResourceVersion:11400,Generation:0,CreationTimestamp:2019-06-27 17:37:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.187.123/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5acfb9a0-3171-4602-abb3-5faf1ffae314 0xc0029449d7 0xc0029449d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nk6xg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002944a40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002944a60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  }],Message:,Reason:,HostIP:139.178.70.235,PodIP:,StartTime:2019-06-27 17:37:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.070: INFO: Pod "nginx-deployment-7b8c6f4498-bnq4c" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-bnq4c,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-7b8c6f4498-bnq4c,UID:ad122a4a-595f-4555-bb6e-f2cda407c021,ResourceVersion:11125,Generation:0,CreationTimestamp:2019-06-27 17:37:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.20.253/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5acfb9a0-3171-4602-abb3-5faf1ffae314 0xc002944b37 0xc002944b38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-mskvs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002944ba0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002944bc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:19 +0000 UTC  }],Message:,Reason:,HostIP:147.75.109.65,PodIP:10.244.20.253,StartTime:2019-06-27 17:37:19 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-27 17:37:20 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://1341e4d5af454e20ef6da6c0206228c8be5ab4bd808b58ab97d0c10895c6de0b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.070: INFO: Pod "nginx-deployment-7b8c6f4498-c6thd" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-c6thd,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-7b8c6f4498-c6thd,UID:34342ccd-fe29-4e78-a591-d9fce9bb0825,ResourceVersion:11174,Generation:0,CreationTimestamp:2019-06-27 17:37:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.177.235/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5acfb9a0-3171-4602-abb3-5faf1ffae314 0xc002944c97 0xc002944c98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nzp9p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002944d00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002944d20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:19 +0000 UTC  }],Message:,Reason:,HostIP:147.75.70.201,PodIP:10.244.177.235,StartTime:2019-06-27 17:37:19 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-27 17:37:21 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://a01a11a5ae2933afc51e2b7c33447d105ec3266500aca89d113b3d4e347690fa}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.070: INFO: Pod "nginx-deployment-7b8c6f4498-cgwzx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-cgwzx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-7b8c6f4498-cgwzx,UID:56d7079f-b80b-492a-8e1d-5adce2564023,ResourceVersion:11161,Generation:0,CreationTimestamp:2019-06-27 17:37:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.20.255/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5acfb9a0-3171-4602-abb3-5faf1ffae314 0xc002944e07 0xc002944e08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-mskvs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002944e70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002944e90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:21 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:21 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:20 +0000 UTC  }],Message:,Reason:,HostIP:147.75.109.65,PodIP:10.244.20.255,StartTime:2019-06-27 17:37:20 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-27 17:37:20 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://a37558aab161317f8a22e2930e5406d13c51ec779d89b364e0202bb7e60cae2a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.071: INFO: Pod "nginx-deployment-7b8c6f4498-cw897" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-cw897,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-7b8c6f4498-cw897,UID:e6a21154-9f83-4b64-9721-a887fbb6d7b2,ResourceVersion:11171,Generation:0,CreationTimestamp:2019-06-27 17:37:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.177.234/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5acfb9a0-3171-4602-abb3-5faf1ffae314 0xc002944f67 0xc002944f68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nzp9p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002944fd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002944ff0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:19 +0000 UTC  }],Message:,Reason:,HostIP:147.75.70.201,PodIP:10.244.177.234,StartTime:2019-06-27 17:37:19 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-27 17:37:21 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://c3aa0ced425227e2ef10e9bf43d1ee58ee13a5bdd1b5aa3d7405ac6174a3ef27}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.071: INFO: Pod "nginx-deployment-7b8c6f4498-jcgwz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-jcgwz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-7b8c6f4498-jcgwz,UID:49c04e79-590e-425d-94f4-f3fa82224a11,ResourceVersion:11124,Generation:0,CreationTimestamp:2019-06-27 17:37:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.187.119/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5acfb9a0-3171-4602-abb3-5faf1ffae314 0xc0029450c7 0xc0029450c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nk6xg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002945130} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002945150}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:19 +0000 UTC  }],Message:,Reason:,HostIP:139.178.70.235,PodIP:10.244.187.119,StartTime:2019-06-27 17:37:19 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-27 17:37:20 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://d5553a6fd36adfb812f4252629206bed749023a68560384c1131a8e4d01411de}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.071: INFO: Pod "nginx-deployment-7b8c6f4498-kbcs9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-kbcs9,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-7b8c6f4498-kbcs9,UID:ac093d02-38e9-4703-90e1-7538f582edb8,ResourceVersion:11533,Generation:0,CreationTimestamp:2019-06-27 17:37:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.177.244/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5acfb9a0-3171-4602-abb3-5faf1ffae314 0xc002945237 0xc002945238}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nzp9p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029452a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029452c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  }],Message:,Reason:,HostIP:147.75.70.201,PodIP:,StartTime:2019-06-27 17:37:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.072: INFO: Pod "nginx-deployment-7b8c6f4498-kxmnt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-kxmnt,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-7b8c6f4498-kxmnt,UID:a7e188b0-8433-41ed-94eb-201824ea7a6a,ResourceVersion:11418,Generation:0,CreationTimestamp:2019-06-27 17:37:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.177.239/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5acfb9a0-3171-4602-abb3-5faf1ffae314 0xc002945387 0xc002945388}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nzp9p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029453f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002945410}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  }],Message:,Reason:,HostIP:147.75.70.201,PodIP:,StartTime:2019-06-27 17:37:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.072: INFO: Pod "nginx-deployment-7b8c6f4498-r29hz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-r29hz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-7b8c6f4498-r29hz,UID:e62a99b6-833d-46e0-8ca1-ed9c08fecc5e,ResourceVersion:11118,Generation:0,CreationTimestamp:2019-06-27 17:37:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.187.118/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5acfb9a0-3171-4602-abb3-5faf1ffae314 0xc0029454d7 0xc0029454d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nk6xg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002945540} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002945560}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:19 +0000 UTC  }],Message:,Reason:,HostIP:139.178.70.235,PodIP:10.244.187.118,StartTime:2019-06-27 17:37:19 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-27 17:37:20 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://85a491ee9be8d5f30708912d4ca794f4b462efe18a97f2b3097cd3332289abc7}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.073: INFO: Pod "nginx-deployment-7b8c6f4498-spd8v" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-spd8v,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-7b8c6f4498-spd8v,UID:3fe5b6a1-b169-49a1-b6fa-2045630e112c,ResourceVersion:11416,Generation:0,CreationTimestamp:2019-06-27 17:37:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.20.200/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5acfb9a0-3171-4602-abb3-5faf1ffae314 0xc002945657 0xc002945658}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-mskvs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0029456c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029456e0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  }],Message:,Reason:,HostIP:147.75.109.65,PodIP:,StartTime:2019-06-27 17:37:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.073: INFO: Pod "nginx-deployment-7b8c6f4498-t4prm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-t4prm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-7b8c6f4498-t4prm,UID:b68c53da-e225-483a-9e44-a8b13a7eb980,ResourceVersion:11504,Generation:0,CreationTimestamp:2019-06-27 17:37:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.187.66/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5acfb9a0-3171-4602-abb3-5faf1ffae314 0xc0029457b7 0xc0029457b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nk6xg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002945820} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002945840}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  }],Message:,Reason:,HostIP:139.178.70.235,PodIP:,StartTime:2019-06-27 17:37:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.073: INFO: Pod "nginx-deployment-7b8c6f4498-w699p" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-w699p,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-7b8c6f4498-w699p,UID:1ed39820-4440-4117-9810-8c80cb2a2837,ResourceVersion:11130,Generation:0,CreationTimestamp:2019-06-27 17:37:19 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.20.254/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5acfb9a0-3171-4602-abb3-5faf1ffae314 0xc002945917 0xc002945918}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-mskvs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002945980} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0029459a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:19 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:20 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:20 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:19 +0000 UTC  }],Message:,Reason:,HostIP:147.75.109.65,PodIP:10.244.20.254,StartTime:2019-06-27 17:37:19 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-27 17:37:20 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://408f3ebdedb2090e2623a02a6f1cd3419e74778129464857856579907c109256}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.073: INFO: Pod "nginx-deployment-7b8c6f4498-wmvk9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-wmvk9,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-7b8c6f4498-wmvk9,UID:4ca9020d-34d2-4147-bfd0-b4787d9d2a7a,ResourceVersion:11444,Generation:0,CreationTimestamp:2019-06-27 17:37:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.20.201/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5acfb9a0-3171-4602-abb3-5faf1ffae314 0xc002945a87 0xc002945a88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-mskvs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002945af0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002945b10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  }],Message:,Reason:,HostIP:147.75.109.65,PodIP:,StartTime:2019-06-27 17:37:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.074: INFO: Pod "nginx-deployment-7b8c6f4498-xqmph" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xqmph,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-7b8c6f4498-xqmph,UID:d98ac3cf-78ec-4ff8-a171-c2b94e12f137,ResourceVersion:11388,Generation:0,CreationTimestamp:2019-06-27 17:37:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.177.238/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5acfb9a0-3171-4602-abb3-5faf1ffae314 0xc002945be7 0xc002945be8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nzp9p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002945c50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002945c70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:25 +0000 UTC  }],Message:,Reason:,HostIP:147.75.70.201,PodIP:,StartTime:2019-06-27 17:37:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 27 17:37:28.074: INFO: Pod "nginx-deployment-7b8c6f4498-xt768" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xt768,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-4210,SelfLink:/api/v1/namespaces/deployment-4210/pods/nginx-deployment-7b8c6f4498-xt768,UID:30131646-0b66-4ff8-98c0-55eb959f5860,ResourceVersion:11496,Generation:0,CreationTimestamp:2019-06-27 17:37:26 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.177.243/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 5acfb9a0-3171-4602-abb3-5faf1ffae314 0xc002945d37 0xc002945d38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-kkgj6 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-kkgj6,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-kkgj6 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nzp9p,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002945da0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002945dc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:37:26 +0000 UTC  }],Message:,Reason:,HostIP:147.75.70.201,PodIP:,StartTime:2019-06-27 17:37:26 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:37:28.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4210" for this suite.
Jun 27 17:37:36.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:37:36.224: INFO: namespace deployment-4210 deletion completed in 8.143776023s

• [SLOW TEST:16.536 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:37:36.224: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-590
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun 27 17:37:36.396: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:37:44.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-590" for this suite.
Jun 27 17:37:50.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:37:50.535: INFO: namespace pods-590 deletion completed in 6.142946517s

• [SLOW TEST:14.311 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:37:50.536: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-2016
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun 27 17:37:50.703: INFO: Pod name pod-release: Found 0 pods out of 1
Jun 27 17:37:55.707: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:37:56.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2016" for this suite.
Jun 27 17:38:02.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:38:02.884: INFO: namespace replication-controller-2016 deletion completed in 6.148724662s

• [SLOW TEST:12.348 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:38:02.886: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3052
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 27 17:38:03.058: INFO: Waiting up to 5m0s for pod "pod-f472961c-9fc0-42fc-a568-2aa0bb24db72" in namespace "emptydir-3052" to be "success or failure"
Jun 27 17:38:03.065: INFO: Pod "pod-f472961c-9fc0-42fc-a568-2aa0bb24db72": Phase="Pending", Reason="", readiness=false. Elapsed: 6.611035ms
Jun 27 17:38:05.068: INFO: Pod "pod-f472961c-9fc0-42fc-a568-2aa0bb24db72": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010356934s
Jun 27 17:38:07.072: INFO: Pod "pod-f472961c-9fc0-42fc-a568-2aa0bb24db72": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.014304883s
STEP: Saw pod success
Jun 27 17:38:07.072: INFO: Pod "pod-f472961c-9fc0-42fc-a568-2aa0bb24db72" satisfied condition "success or failure"
Jun 27 17:38:07.076: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nzp9p pod pod-f472961c-9fc0-42fc-a568-2aa0bb24db72 container test-container: <nil>
STEP: delete the pod
Jun 27 17:38:07.105: INFO: Waiting for pod pod-f472961c-9fc0-42fc-a568-2aa0bb24db72 to disappear
Jun 27 17:38:07.109: INFO: Pod pod-f472961c-9fc0-42fc-a568-2aa0bb24db72 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:38:07.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3052" for this suite.
Jun 27 17:38:13.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:38:13.259: INFO: namespace emptydir-3052 deletion completed in 6.145446109s

• [SLOW TEST:10.374 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:38:13.259: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-3487
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:38:17.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3487" for this suite.
Jun 27 17:38:23.455: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:38:23.585: INFO: namespace kubelet-test-3487 deletion completed in 6.144341847s

• [SLOW TEST:10.326 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:38:23.587: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-7212
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jun 27 17:38:25.774: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-307330860 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jun 27 17:38:40.939: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:38:40.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7212" for this suite.
Jun 27 17:38:46.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:38:47.084: INFO: namespace pods-7212 deletion completed in 6.136086752s

• [SLOW TEST:23.498 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:38:47.085: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-76
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jun 27 17:38:47.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 create -f - --namespace=kubectl-76'
Jun 27 17:38:47.855: INFO: stderr: ""
Jun 27 17:38:47.855: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 27 17:38:47.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-76'
Jun 27 17:38:48.034: INFO: stderr: ""
Jun 27 17:38:48.034: INFO: stdout: "update-demo-nautilus-mrvtk update-demo-nautilus-z5pcw "
Jun 27 17:38:48.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-nautilus-mrvtk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-76'
Jun 27 17:38:48.212: INFO: stderr: ""
Jun 27 17:38:48.212: INFO: stdout: ""
Jun 27 17:38:48.212: INFO: update-demo-nautilus-mrvtk is created but not running
Jun 27 17:38:53.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-76'
Jun 27 17:38:53.390: INFO: stderr: ""
Jun 27 17:38:53.390: INFO: stdout: "update-demo-nautilus-mrvtk update-demo-nautilus-z5pcw "
Jun 27 17:38:53.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-nautilus-mrvtk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-76'
Jun 27 17:38:53.559: INFO: stderr: ""
Jun 27 17:38:53.559: INFO: stdout: "true"
Jun 27 17:38:53.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-nautilus-mrvtk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-76'
Jun 27 17:38:53.741: INFO: stderr: ""
Jun 27 17:38:53.741: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 27 17:38:53.741: INFO: validating pod update-demo-nautilus-mrvtk
Jun 27 17:38:53.746: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 27 17:38:53.746: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 27 17:38:53.746: INFO: update-demo-nautilus-mrvtk is verified up and running
Jun 27 17:38:53.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-nautilus-z5pcw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-76'
Jun 27 17:38:53.925: INFO: stderr: ""
Jun 27 17:38:53.925: INFO: stdout: "true"
Jun 27 17:38:53.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods update-demo-nautilus-z5pcw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-76'
Jun 27 17:38:54.097: INFO: stderr: ""
Jun 27 17:38:54.097: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 27 17:38:54.097: INFO: validating pod update-demo-nautilus-z5pcw
Jun 27 17:38:54.103: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 27 17:38:54.103: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 27 17:38:54.103: INFO: update-demo-nautilus-z5pcw is verified up and running
STEP: using delete to clean up resources
Jun 27 17:38:54.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 delete --grace-period=0 --force -f - --namespace=kubectl-76'
Jun 27 17:38:54.278: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 27 17:38:54.279: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 27 17:38:54.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-76'
Jun 27 17:38:54.467: INFO: stderr: "No resources found.\n"
Jun 27 17:38:54.467: INFO: stdout: ""
Jun 27 17:38:54.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pods -l name=update-demo --namespace=kubectl-76 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 27 17:38:54.650: INFO: stderr: ""
Jun 27 17:38:54.650: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:38:54.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-76" for this suite.
Jun 27 17:39:16.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:39:16.802: INFO: namespace kubectl-76 deletion completed in 22.145383465s

• [SLOW TEST:29.717 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:39:16.803: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-2340
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 27 17:39:16.970: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6733b45e-1979-46df-9d22-b24b5bbecbed" in namespace "downward-api-2340" to be "success or failure"
Jun 27 17:39:16.975: INFO: Pod "downwardapi-volume-6733b45e-1979-46df-9d22-b24b5bbecbed": Phase="Pending", Reason="", readiness=false. Elapsed: 5.052894ms
Jun 27 17:39:18.979: INFO: Pod "downwardapi-volume-6733b45e-1979-46df-9d22-b24b5bbecbed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009066943s
STEP: Saw pod success
Jun 27 17:39:18.979: INFO: Pod "downwardapi-volume-6733b45e-1979-46df-9d22-b24b5bbecbed" satisfied condition "success or failure"
Jun 27 17:39:18.983: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod downwardapi-volume-6733b45e-1979-46df-9d22-b24b5bbecbed container client-container: <nil>
STEP: delete the pod
Jun 27 17:39:19.006: INFO: Waiting for pod downwardapi-volume-6733b45e-1979-46df-9d22-b24b5bbecbed to disappear
Jun 27 17:39:19.010: INFO: Pod downwardapi-volume-6733b45e-1979-46df-9d22-b24b5bbecbed no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:39:19.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2340" for this suite.
Jun 27 17:39:25.028: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:39:25.149: INFO: namespace downward-api-2340 deletion completed in 6.133566569s

• [SLOW TEST:8.346 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:39:25.149: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5300
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 27 17:39:25.313: INFO: Waiting up to 5m0s for pod "pod-92acf730-6bd8-44fa-93cc-510e4590bf05" in namespace "emptydir-5300" to be "success or failure"
Jun 27 17:39:25.317: INFO: Pod "pod-92acf730-6bd8-44fa-93cc-510e4590bf05": Phase="Pending", Reason="", readiness=false. Elapsed: 3.989823ms
Jun 27 17:39:27.321: INFO: Pod "pod-92acf730-6bd8-44fa-93cc-510e4590bf05": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008238653s
STEP: Saw pod success
Jun 27 17:39:27.321: INFO: Pod "pod-92acf730-6bd8-44fa-93cc-510e4590bf05" satisfied condition "success or failure"
Jun 27 17:39:27.325: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nzp9p pod pod-92acf730-6bd8-44fa-93cc-510e4590bf05 container test-container: <nil>
STEP: delete the pod
Jun 27 17:39:27.352: INFO: Waiting for pod pod-92acf730-6bd8-44fa-93cc-510e4590bf05 to disappear
Jun 27 17:39:27.355: INFO: Pod pod-92acf730-6bd8-44fa-93cc-510e4590bf05 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:39:27.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5300" for this suite.
Jun 27 17:39:33.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:39:33.512: INFO: namespace emptydir-5300 deletion completed in 6.151245953s

• [SLOW TEST:8.363 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:39:33.512: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8010
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:39:33.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8010" for this suite.
Jun 27 17:39:55.712: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:39:55.901: INFO: namespace pods-8010 deletion completed in 22.204573661s

• [SLOW TEST:22.389 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:39:55.903: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-850
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-c26m
STEP: Creating a pod to test atomic-volume-subpath
Jun 27 17:39:56.079: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-c26m" in namespace "subpath-850" to be "success or failure"
Jun 27 17:39:56.084: INFO: Pod "pod-subpath-test-projected-c26m": Phase="Pending", Reason="", readiness=false. Elapsed: 4.650024ms
Jun 27 17:39:58.088: INFO: Pod "pod-subpath-test-projected-c26m": Phase="Running", Reason="", readiness=true. Elapsed: 2.008750543s
Jun 27 17:40:00.091: INFO: Pod "pod-subpath-test-projected-c26m": Phase="Running", Reason="", readiness=true. Elapsed: 4.012380521s
Jun 27 17:40:02.095: INFO: Pod "pod-subpath-test-projected-c26m": Phase="Running", Reason="", readiness=true. Elapsed: 6.01599281s
Jun 27 17:40:04.099: INFO: Pod "pod-subpath-test-projected-c26m": Phase="Running", Reason="", readiness=true. Elapsed: 8.019804689s
Jun 27 17:40:06.103: INFO: Pod "pod-subpath-test-projected-c26m": Phase="Running", Reason="", readiness=true. Elapsed: 10.023779578s
Jun 27 17:40:08.107: INFO: Pod "pod-subpath-test-projected-c26m": Phase="Running", Reason="", readiness=true. Elapsed: 12.027839967s
Jun 27 17:40:10.111: INFO: Pod "pod-subpath-test-projected-c26m": Phase="Running", Reason="", readiness=true. Elapsed: 14.031685046s
Jun 27 17:40:12.114: INFO: Pod "pod-subpath-test-projected-c26m": Phase="Running", Reason="", readiness=true. Elapsed: 16.035285404s
Jun 27 17:40:14.118: INFO: Pod "pod-subpath-test-projected-c26m": Phase="Running", Reason="", readiness=true. Elapsed: 18.039124203s
Jun 27 17:40:16.122: INFO: Pod "pod-subpath-test-projected-c26m": Phase="Running", Reason="", readiness=true. Elapsed: 20.042801642s
Jun 27 17:40:18.126: INFO: Pod "pod-subpath-test-projected-c26m": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.04651845s
STEP: Saw pod success
Jun 27 17:40:18.126: INFO: Pod "pod-subpath-test-projected-c26m" satisfied condition "success or failure"
Jun 27 17:40:18.129: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-mskvs pod pod-subpath-test-projected-c26m container test-container-subpath-projected-c26m: <nil>
STEP: delete the pod
Jun 27 17:40:18.154: INFO: Waiting for pod pod-subpath-test-projected-c26m to disappear
Jun 27 17:40:18.158: INFO: Pod pod-subpath-test-projected-c26m no longer exists
STEP: Deleting pod pod-subpath-test-projected-c26m
Jun 27 17:40:18.158: INFO: Deleting pod "pod-subpath-test-projected-c26m" in namespace "subpath-850"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:40:18.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-850" for this suite.
Jun 27 17:40:24.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:40:24.312: INFO: namespace subpath-850 deletion completed in 6.144213588s

• [SLOW TEST:28.409 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:40:24.312: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9626
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-2b57f316-6fa3-47af-a7c0-a285b3eab934
STEP: Creating secret with name s-test-opt-upd-fc488fde-25b1-407c-a4f9-26340da9aa25
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-2b57f316-6fa3-47af-a7c0-a285b3eab934
STEP: Updating secret s-test-opt-upd-fc488fde-25b1-407c-a4f9-26340da9aa25
STEP: Creating secret with name s-test-opt-create-5db857df-33fe-47af-8648-91601e5e49ce
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:40:28.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9626" for this suite.
Jun 27 17:40:50.602: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:40:50.731: INFO: namespace secrets-9626 deletion completed in 22.142519794s

• [SLOW TEST:26.419 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:40:50.732: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-9864
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-60ee5b83-aa90-4c0c-ace2-8d36c07e5160
STEP: Creating a pod to test consume secrets
Jun 27 17:40:50.901: INFO: Waiting up to 5m0s for pod "pod-secrets-7f80badf-918d-4a0d-8091-23669dca001c" in namespace "secrets-9864" to be "success or failure"
Jun 27 17:40:50.905: INFO: Pod "pod-secrets-7f80badf-918d-4a0d-8091-23669dca001c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.320253ms
Jun 27 17:40:52.909: INFO: Pod "pod-secrets-7f80badf-918d-4a0d-8091-23669dca001c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008079702s
Jun 27 17:40:54.913: INFO: Pod "pod-secrets-7f80badf-918d-4a0d-8091-23669dca001c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011937521s
STEP: Saw pod success
Jun 27 17:40:54.913: INFO: Pod "pod-secrets-7f80badf-918d-4a0d-8091-23669dca001c" satisfied condition "success or failure"
Jun 27 17:40:54.916: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod pod-secrets-7f80badf-918d-4a0d-8091-23669dca001c container secret-volume-test: <nil>
STEP: delete the pod
Jun 27 17:40:54.936: INFO: Waiting for pod pod-secrets-7f80badf-918d-4a0d-8091-23669dca001c to disappear
Jun 27 17:40:54.939: INFO: Pod pod-secrets-7f80badf-918d-4a0d-8091-23669dca001c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:40:54.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9864" for this suite.
Jun 27 17:41:00.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:41:01.085: INFO: namespace secrets-9864 deletion completed in 6.139787024s

• [SLOW TEST:10.353 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:41:01.086: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-7658
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Jun 27 17:41:01.249: INFO: Waiting up to 5m0s for pod "client-containers-80db72a9-4a03-4276-8ddf-e835218f47a0" in namespace "containers-7658" to be "success or failure"
Jun 27 17:41:01.252: INFO: Pod "client-containers-80db72a9-4a03-4276-8ddf-e835218f47a0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.396292ms
Jun 27 17:41:03.256: INFO: Pod "client-containers-80db72a9-4a03-4276-8ddf-e835218f47a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007572011s
Jun 27 17:41:05.260: INFO: Pod "client-containers-80db72a9-4a03-4276-8ddf-e835218f47a0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011737661s
STEP: Saw pod success
Jun 27 17:41:05.261: INFO: Pod "client-containers-80db72a9-4a03-4276-8ddf-e835218f47a0" satisfied condition "success or failure"
Jun 27 17:41:05.265: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-mskvs pod client-containers-80db72a9-4a03-4276-8ddf-e835218f47a0 container test-container: <nil>
STEP: delete the pod
Jun 27 17:41:05.283: INFO: Waiting for pod client-containers-80db72a9-4a03-4276-8ddf-e835218f47a0 to disappear
Jun 27 17:41:05.289: INFO: Pod client-containers-80db72a9-4a03-4276-8ddf-e835218f47a0 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:41:05.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7658" for this suite.
Jun 27 17:41:11.308: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:41:11.446: INFO: namespace containers-7658 deletion completed in 6.151993374s

• [SLOW TEST:10.360 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:41:11.446: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-3110
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 27 17:41:14.138: INFO: Successfully updated pod "pod-update-5a68eff1-1268-4b8f-969f-622d3bda29a9"
STEP: verifying the updated pod is in kubernetes
Jun 27 17:41:14.147: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:41:14.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3110" for this suite.
Jun 27 17:41:36.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:41:36.293: INFO: namespace pods-3110 deletion completed in 22.140911742s

• [SLOW TEST:24.846 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:41:36.293: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-4745
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jun 27 17:41:46.536: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
W0627 17:41:46.536374      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 27 17:41:46.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4745" for this suite.
Jun 27 17:41:52.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:41:52.687: INFO: namespace gc-4745 deletion completed in 6.146253919s

• [SLOW TEST:16.394 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:41:52.687: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8095
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-866c9ed1-93f8-4dd5-8079-b5499f95b49d
STEP: Creating a pod to test consume configMaps
Jun 27 17:41:52.856: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d13cab2c-a22d-4102-867e-82760dbf0057" in namespace "projected-8095" to be "success or failure"
Jun 27 17:41:52.863: INFO: Pod "pod-projected-configmaps-d13cab2c-a22d-4102-867e-82760dbf0057": Phase="Pending", Reason="", readiness=false. Elapsed: 7.071565ms
Jun 27 17:41:54.866: INFO: Pod "pod-projected-configmaps-d13cab2c-a22d-4102-867e-82760dbf0057": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010765124s
STEP: Saw pod success
Jun 27 17:41:54.866: INFO: Pod "pod-projected-configmaps-d13cab2c-a22d-4102-867e-82760dbf0057" satisfied condition "success or failure"
Jun 27 17:41:54.870: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-mskvs pod pod-projected-configmaps-d13cab2c-a22d-4102-867e-82760dbf0057 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 27 17:41:54.889: INFO: Waiting for pod pod-projected-configmaps-d13cab2c-a22d-4102-867e-82760dbf0057 to disappear
Jun 27 17:41:54.894: INFO: Pod pod-projected-configmaps-d13cab2c-a22d-4102-867e-82760dbf0057 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:41:54.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8095" for this suite.
Jun 27 17:42:00.914: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:42:01.043: INFO: namespace projected-8095 deletion completed in 6.140927385s

• [SLOW TEST:8.356 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:42:01.043: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4019
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 27 17:42:01.221: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun 27 17:42:01.236: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:01.239: INFO: Number of nodes with available pods: 0
Jun 27 17:42:01.239: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:42:02.244: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:02.249: INFO: Number of nodes with available pods: 0
Jun 27 17:42:02.249: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:42:03.245: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:03.250: INFO: Number of nodes with available pods: 2
Jun 27 17:42:03.250: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 17:42:04.245: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:04.249: INFO: Number of nodes with available pods: 3
Jun 27 17:42:04.249: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun 27 17:42:04.287: INFO: Wrong image for pod: daemon-set-6zfl4. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:04.287: INFO: Wrong image for pod: daemon-set-rdcr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:04.287: INFO: Wrong image for pod: daemon-set-tf4gb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:04.295: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:05.301: INFO: Wrong image for pod: daemon-set-6zfl4. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:05.301: INFO: Wrong image for pod: daemon-set-rdcr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:05.301: INFO: Wrong image for pod: daemon-set-tf4gb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:05.305: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:06.300: INFO: Wrong image for pod: daemon-set-6zfl4. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:06.300: INFO: Pod daemon-set-6zfl4 is not available
Jun 27 17:42:06.300: INFO: Wrong image for pod: daemon-set-rdcr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:06.300: INFO: Wrong image for pod: daemon-set-tf4gb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:06.305: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:07.300: INFO: Pod daemon-set-bp7p4 is not available
Jun 27 17:42:07.300: INFO: Wrong image for pod: daemon-set-rdcr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:07.300: INFO: Wrong image for pod: daemon-set-tf4gb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:07.305: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:08.300: INFO: Pod daemon-set-bp7p4 is not available
Jun 27 17:42:08.300: INFO: Wrong image for pod: daemon-set-rdcr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:08.300: INFO: Wrong image for pod: daemon-set-tf4gb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:08.305: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:09.300: INFO: Wrong image for pod: daemon-set-rdcr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:09.300: INFO: Wrong image for pod: daemon-set-tf4gb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:09.308: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:10.300: INFO: Wrong image for pod: daemon-set-rdcr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:10.300: INFO: Wrong image for pod: daemon-set-tf4gb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:10.300: INFO: Pod daemon-set-tf4gb is not available
Jun 27 17:42:10.305: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:11.300: INFO: Wrong image for pod: daemon-set-rdcr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:11.300: INFO: Wrong image for pod: daemon-set-tf4gb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:11.300: INFO: Pod daemon-set-tf4gb is not available
Jun 27 17:42:11.305: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:12.300: INFO: Wrong image for pod: daemon-set-rdcr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:12.300: INFO: Wrong image for pod: daemon-set-tf4gb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:12.300: INFO: Pod daemon-set-tf4gb is not available
Jun 27 17:42:12.305: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:13.300: INFO: Wrong image for pod: daemon-set-rdcr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:13.300: INFO: Wrong image for pod: daemon-set-tf4gb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:13.300: INFO: Pod daemon-set-tf4gb is not available
Jun 27 17:42:13.305: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:14.300: INFO: Wrong image for pod: daemon-set-rdcr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:14.300: INFO: Wrong image for pod: daemon-set-tf4gb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:14.300: INFO: Pod daemon-set-tf4gb is not available
Jun 27 17:42:14.305: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:15.300: INFO: Wrong image for pod: daemon-set-rdcr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:15.300: INFO: Wrong image for pod: daemon-set-tf4gb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:15.300: INFO: Pod daemon-set-tf4gb is not available
Jun 27 17:42:15.305: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:16.300: INFO: Wrong image for pod: daemon-set-rdcr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:16.300: INFO: Wrong image for pod: daemon-set-tf4gb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:16.300: INFO: Pod daemon-set-tf4gb is not available
Jun 27 17:42:16.305: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:17.301: INFO: Wrong image for pod: daemon-set-rdcr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:17.301: INFO: Wrong image for pod: daemon-set-tf4gb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:17.301: INFO: Pod daemon-set-tf4gb is not available
Jun 27 17:42:17.306: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:18.300: INFO: Wrong image for pod: daemon-set-rdcr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:18.300: INFO: Wrong image for pod: daemon-set-tf4gb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:18.300: INFO: Pod daemon-set-tf4gb is not available
Jun 27 17:42:18.305: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:19.300: INFO: Wrong image for pod: daemon-set-rdcr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:19.300: INFO: Wrong image for pod: daemon-set-tf4gb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:19.300: INFO: Pod daemon-set-tf4gb is not available
Jun 27 17:42:19.306: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:20.300: INFO: Pod daemon-set-q2t95 is not available
Jun 27 17:42:20.300: INFO: Wrong image for pod: daemon-set-rdcr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:20.305: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:21.304: INFO: Wrong image for pod: daemon-set-rdcr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:21.309: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:22.300: INFO: Wrong image for pod: daemon-set-rdcr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:22.300: INFO: Pod daemon-set-rdcr8 is not available
Jun 27 17:42:22.305: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:23.301: INFO: Wrong image for pod: daemon-set-rdcr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:23.301: INFO: Pod daemon-set-rdcr8 is not available
Jun 27 17:42:23.307: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:24.300: INFO: Wrong image for pod: daemon-set-rdcr8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 27 17:42:24.300: INFO: Pod daemon-set-rdcr8 is not available
Jun 27 17:42:24.305: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:25.300: INFO: Pod daemon-set-jwdm6 is not available
Jun 27 17:42:25.305: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jun 27 17:42:25.310: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:25.315: INFO: Number of nodes with available pods: 2
Jun 27 17:42:25.315: INFO: Node talos-test-cluster-workers-84c9684cd-nk6xg is running more than one daemon pod
Jun 27 17:42:26.320: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 17:42:26.325: INFO: Number of nodes with available pods: 3
Jun 27 17:42:26.325: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4019, will wait for the garbage collector to delete the pods
Jun 27 17:42:26.406: INFO: Deleting DaemonSet.extensions daemon-set took: 7.660966ms
Jun 27 17:42:26.806: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.294355ms
Jun 27 17:42:39.810: INFO: Number of nodes with available pods: 0
Jun 27 17:42:39.810: INFO: Number of running nodes: 0, number of available pods: 0
Jun 27 17:42:39.814: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4019/daemonsets","resourceVersion":"13433"},"items":null}

Jun 27 17:42:39.817: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4019/pods","resourceVersion":"13433"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:42:39.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4019" for this suite.
Jun 27 17:42:45.853: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:42:46.010: INFO: namespace daemonsets-4019 deletion completed in 6.171482598s

• [SLOW TEST:44.967 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:42:46.010: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-108
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 27 17:42:46.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-108'
Jun 27 17:42:46.383: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 27 17:42:46.383: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Jun 27 17:42:46.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 delete jobs e2e-test-nginx-job --namespace=kubectl-108'
Jun 27 17:42:46.575: INFO: stderr: ""
Jun 27 17:42:46.575: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:42:46.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-108" for this suite.
Jun 27 17:43:08.596: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:43:08.725: INFO: namespace kubectl-108 deletion completed in 22.143722264s

• [SLOW TEST:22.714 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:43:08.726: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1508
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-d08f3964-e3cf-47fd-ab52-288904ca5794
STEP: Creating a pod to test consume secrets
Jun 27 17:43:08.896: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7dcb48be-d500-4a9e-8a13-6287c900cd00" in namespace "projected-1508" to be "success or failure"
Jun 27 17:43:08.903: INFO: Pod "pod-projected-secrets-7dcb48be-d500-4a9e-8a13-6287c900cd00": Phase="Pending", Reason="", readiness=false. Elapsed: 6.943956ms
Jun 27 17:43:10.907: INFO: Pod "pod-projected-secrets-7dcb48be-d500-4a9e-8a13-6287c900cd00": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010796334s
STEP: Saw pod success
Jun 27 17:43:10.907: INFO: Pod "pod-projected-secrets-7dcb48be-d500-4a9e-8a13-6287c900cd00" satisfied condition "success or failure"
Jun 27 17:43:10.911: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod pod-projected-secrets-7dcb48be-d500-4a9e-8a13-6287c900cd00 container secret-volume-test: <nil>
STEP: delete the pod
Jun 27 17:43:10.931: INFO: Waiting for pod pod-projected-secrets-7dcb48be-d500-4a9e-8a13-6287c900cd00 to disappear
Jun 27 17:43:10.935: INFO: Pod pod-projected-secrets-7dcb48be-d500-4a9e-8a13-6287c900cd00 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:43:10.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1508" for this suite.
Jun 27 17:43:16.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:43:17.080: INFO: namespace projected-1508 deletion completed in 6.140354325s

• [SLOW TEST:8.354 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:43:17.081: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-126
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 27 17:43:17.234: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-126'
Jun 27 17:43:17.439: INFO: stderr: ""
Jun 27 17:43:17.439: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Jun 27 17:43:17.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 delete pods e2e-test-nginx-pod --namespace=kubectl-126'
Jun 27 17:43:19.718: INFO: stderr: ""
Jun 27 17:43:19.718: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:43:19.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-126" for this suite.
Jun 27 17:43:25.737: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:43:25.917: INFO: namespace kubectl-126 deletion completed in 6.193428475s

• [SLOW TEST:8.836 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:43:25.918: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4058
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jun 27 17:43:28.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 exec pod-sharedvolume-77536f3b-e63e-4f1c-8376-88e203b788d9 -c busybox-main-container --namespace=emptydir-4058 -- cat /usr/share/volumeshare/shareddata.txt'
Jun 27 17:43:28.469: INFO: stderr: ""
Jun 27 17:43:28.469: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:43:28.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4058" for this suite.
Jun 27 17:43:34.488: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:43:34.633: INFO: namespace emptydir-4058 deletion completed in 6.159226369s

• [SLOW TEST:8.716 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:43:34.634: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-2633
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 27 17:43:37.338: INFO: Successfully updated pod "pod-update-activedeadlineseconds-d447a295-7fcb-444d-8720-19a98a1812ad"
Jun 27 17:43:37.338: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-d447a295-7fcb-444d-8720-19a98a1812ad" in namespace "pods-2633" to be "terminated due to deadline exceeded"
Jun 27 17:43:37.345: INFO: Pod "pod-update-activedeadlineseconds-d447a295-7fcb-444d-8720-19a98a1812ad": Phase="Running", Reason="", readiness=true. Elapsed: 7.354426ms
Jun 27 17:43:39.350: INFO: Pod "pod-update-activedeadlineseconds-d447a295-7fcb-444d-8720-19a98a1812ad": Phase="Running", Reason="", readiness=true. Elapsed: 2.011745575s
Jun 27 17:43:41.354: INFO: Pod "pod-update-activedeadlineseconds-d447a295-7fcb-444d-8720-19a98a1812ad": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.015393704s
Jun 27 17:43:41.354: INFO: Pod "pod-update-activedeadlineseconds-d447a295-7fcb-444d-8720-19a98a1812ad" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:43:41.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2633" for this suite.
Jun 27 17:43:47.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:43:47.516: INFO: namespace pods-2633 deletion completed in 6.157747598s

• [SLOW TEST:12.882 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:43:47.517: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9295
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun 27 17:43:47.747: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9295,SelfLink:/api/v1/namespaces/watch-9295/configmaps/e2e-watch-test-watch-closed,UID:0d891de6-f512-43f1-a06a-42c28d3ae42e,ResourceVersion:13741,Generation:0,CreationTimestamp:2019-06-27 17:43:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 27 17:43:47.747: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9295,SelfLink:/api/v1/namespaces/watch-9295/configmaps/e2e-watch-test-watch-closed,UID:0d891de6-f512-43f1-a06a-42c28d3ae42e,ResourceVersion:13742,Generation:0,CreationTimestamp:2019-06-27 17:43:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun 27 17:43:47.770: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9295,SelfLink:/api/v1/namespaces/watch-9295/configmaps/e2e-watch-test-watch-closed,UID:0d891de6-f512-43f1-a06a-42c28d3ae42e,ResourceVersion:13744,Generation:0,CreationTimestamp:2019-06-27 17:43:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 27 17:43:47.770: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-9295,SelfLink:/api/v1/namespaces/watch-9295/configmaps/e2e-watch-test-watch-closed,UID:0d891de6-f512-43f1-a06a-42c28d3ae42e,ResourceVersion:13746,Generation:0,CreationTimestamp:2019-06-27 17:43:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:43:47.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9295" for this suite.
Jun 27 17:43:53.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:43:53.911: INFO: namespace watch-9295 deletion completed in 6.136149352s

• [SLOW TEST:6.395 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:43:53.913: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-7515
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 27 17:43:56.093: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:43:56.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7515" for this suite.
Jun 27 17:44:02.129: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:44:02.260: INFO: namespace container-runtime-7515 deletion completed in 6.143976438s

• [SLOW TEST:8.348 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:44:02.261: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-2119
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 27 17:44:02.478: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8d808e3-e57a-409d-9361-fcda7dfa605b" in namespace "projected-2119" to be "success or failure"
Jun 27 17:44:02.483: INFO: Pod "downwardapi-volume-a8d808e3-e57a-409d-9361-fcda7dfa605b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.981334ms
Jun 27 17:44:04.487: INFO: Pod "downwardapi-volume-a8d808e3-e57a-409d-9361-fcda7dfa605b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009038953s
STEP: Saw pod success
Jun 27 17:44:04.487: INFO: Pod "downwardapi-volume-a8d808e3-e57a-409d-9361-fcda7dfa605b" satisfied condition "success or failure"
Jun 27 17:44:04.492: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nzp9p pod downwardapi-volume-a8d808e3-e57a-409d-9361-fcda7dfa605b container client-container: <nil>
STEP: delete the pod
Jun 27 17:44:04.517: INFO: Waiting for pod downwardapi-volume-a8d808e3-e57a-409d-9361-fcda7dfa605b to disappear
Jun 27 17:44:04.523: INFO: Pod downwardapi-volume-a8d808e3-e57a-409d-9361-fcda7dfa605b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:44:04.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2119" for this suite.
Jun 27 17:44:10.543: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:44:10.674: INFO: namespace projected-2119 deletion completed in 6.145061599s

• [SLOW TEST:8.413 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:44:10.675: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename replication-controller
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replication-controller-1528
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 27 17:44:10.837: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jun 27 17:44:12.873: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:44:13.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1528" for this suite.
Jun 27 17:44:19.900: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:44:20.036: INFO: namespace replication-controller-1528 deletion completed in 6.148906851s

• [SLOW TEST:9.361 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:44:20.037: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename subpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in subpath-9215
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-jg48
STEP: Creating a pod to test atomic-volume-subpath
Jun 27 17:44:20.208: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-jg48" in namespace "subpath-9215" to be "success or failure"
Jun 27 17:44:20.212: INFO: Pod "pod-subpath-test-secret-jg48": Phase="Pending", Reason="", readiness=false. Elapsed: 4.332403ms
Jun 27 17:44:22.216: INFO: Pod "pod-subpath-test-secret-jg48": Phase="Running", Reason="", readiness=true. Elapsed: 2.008132342s
Jun 27 17:44:24.220: INFO: Pod "pod-subpath-test-secret-jg48": Phase="Running", Reason="", readiness=true. Elapsed: 4.012074081s
Jun 27 17:44:26.224: INFO: Pod "pod-subpath-test-secret-jg48": Phase="Running", Reason="", readiness=true. Elapsed: 6.015777429s
Jun 27 17:44:28.227: INFO: Pod "pod-subpath-test-secret-jg48": Phase="Running", Reason="", readiness=true. Elapsed: 8.019479318s
Jun 27 17:44:30.232: INFO: Pod "pod-subpath-test-secret-jg48": Phase="Running", Reason="", readiness=true. Elapsed: 10.023987537s
Jun 27 17:44:32.236: INFO: Pod "pod-subpath-test-secret-jg48": Phase="Running", Reason="", readiness=true. Elapsed: 12.027649796s
Jun 27 17:44:34.239: INFO: Pod "pod-subpath-test-secret-jg48": Phase="Running", Reason="", readiness=true. Elapsed: 14.031458845s
Jun 27 17:44:36.244: INFO: Pod "pod-subpath-test-secret-jg48": Phase="Running", Reason="", readiness=true. Elapsed: 16.035620984s
Jun 27 17:44:38.247: INFO: Pod "pod-subpath-test-secret-jg48": Phase="Running", Reason="", readiness=true. Elapsed: 18.039332332s
Jun 27 17:44:40.251: INFO: Pod "pod-subpath-test-secret-jg48": Phase="Running", Reason="", readiness=true. Elapsed: 20.043206271s
Jun 27 17:44:42.255: INFO: Pod "pod-subpath-test-secret-jg48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.04704957s
STEP: Saw pod success
Jun 27 17:44:42.255: INFO: Pod "pod-subpath-test-secret-jg48" satisfied condition "success or failure"
Jun 27 17:44:42.259: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod pod-subpath-test-secret-jg48 container test-container-subpath-secret-jg48: <nil>
STEP: delete the pod
Jun 27 17:44:42.281: INFO: Waiting for pod pod-subpath-test-secret-jg48 to disappear
Jun 27 17:44:42.286: INFO: Pod pod-subpath-test-secret-jg48 no longer exists
STEP: Deleting pod pod-subpath-test-secret-jg48
Jun 27 17:44:42.286: INFO: Deleting pod "pod-subpath-test-secret-jg48" in namespace "subpath-9215"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:44:42.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9215" for this suite.
Jun 27 17:44:48.310: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:44:48.444: INFO: namespace subpath-9215 deletion completed in 6.14761422s

• [SLOW TEST:28.408 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:44:48.445: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-5113
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Jun 27 17:44:48.621: INFO: Waiting up to 5m0s for pod "var-expansion-a482a64f-260f-4da4-884a-da05b6006d6f" in namespace "var-expansion-5113" to be "success or failure"
Jun 27 17:44:48.630: INFO: Pod "var-expansion-a482a64f-260f-4da4-884a-da05b6006d6f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.160997ms
Jun 27 17:44:50.634: INFO: Pod "var-expansion-a482a64f-260f-4da4-884a-da05b6006d6f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012997366s
STEP: Saw pod success
Jun 27 17:44:50.634: INFO: Pod "var-expansion-a482a64f-260f-4da4-884a-da05b6006d6f" satisfied condition "success or failure"
Jun 27 17:44:50.638: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-mskvs pod var-expansion-a482a64f-260f-4da4-884a-da05b6006d6f container dapi-container: <nil>
STEP: delete the pod
Jun 27 17:44:50.661: INFO: Waiting for pod var-expansion-a482a64f-260f-4da4-884a-da05b6006d6f to disappear
Jun 27 17:44:50.667: INFO: Pod var-expansion-a482a64f-260f-4da4-884a-da05b6006d6f no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:44:50.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5113" for this suite.
Jun 27 17:44:56.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:44:56.826: INFO: namespace var-expansion-5113 deletion completed in 6.151953774s

• [SLOW TEST:8.381 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:44:56.826: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2732
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-b090c9b3-2ad6-4fc2-ac2a-28cae33e0e30
STEP: Creating a pod to test consume configMaps
Jun 27 17:44:57.007: INFO: Waiting up to 5m0s for pod "pod-configmaps-de82c9d7-f31f-4e83-bbde-0cb8e1d7fd70" in namespace "configmap-2732" to be "success or failure"
Jun 27 17:44:57.011: INFO: Pod "pod-configmaps-de82c9d7-f31f-4e83-bbde-0cb8e1d7fd70": Phase="Pending", Reason="", readiness=false. Elapsed: 3.952123ms
Jun 27 17:44:59.015: INFO: Pod "pod-configmaps-de82c9d7-f31f-4e83-bbde-0cb8e1d7fd70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008310822s
STEP: Saw pod success
Jun 27 17:44:59.015: INFO: Pod "pod-configmaps-de82c9d7-f31f-4e83-bbde-0cb8e1d7fd70" satisfied condition "success or failure"
Jun 27 17:44:59.019: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-mskvs pod pod-configmaps-de82c9d7-f31f-4e83-bbde-0cb8e1d7fd70 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 27 17:44:59.041: INFO: Waiting for pod pod-configmaps-de82c9d7-f31f-4e83-bbde-0cb8e1d7fd70 to disappear
Jun 27 17:44:59.045: INFO: Pod pod-configmaps-de82c9d7-f31f-4e83-bbde-0cb8e1d7fd70 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:44:59.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2732" for this suite.
Jun 27 17:45:05.063: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:45:05.190: INFO: namespace configmap-2732 deletion completed in 6.140753625s

• [SLOW TEST:8.364 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:45:05.191: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-3974
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-3974
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-3974
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3974
Jun 27 17:45:05.371: INFO: Found 0 stateful pods, waiting for 1
Jun 27 17:45:15.375: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun 27 17:45:15.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 exec --namespace=statefulset-3974 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 27 17:45:15.747: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 27 17:45:15.747: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 27 17:45:15.747: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 27 17:45:15.752: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 27 17:45:25.758: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 27 17:45:25.758: INFO: Waiting for statefulset status.replicas updated to 0
Jun 27 17:45:25.780: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jun 27 17:45:25.780: INFO: ss-0  talos-test-cluster-workers-84c9684cd-nzp9p  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:16 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:05 +0000 UTC  }]
Jun 27 17:45:25.780: INFO: 
Jun 27 17:45:25.780: INFO: StatefulSet ss has not reached scale 3, at 1
Jun 27 17:45:26.785: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993235712s
Jun 27 17:45:27.790: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988249385s
Jun 27 17:45:28.795: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983662458s
Jun 27 17:45:29.805: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.978692152s
Jun 27 17:45:30.810: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.968983231s
Jun 27 17:45:31.815: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.963955594s
Jun 27 17:45:32.820: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.958871828s
Jun 27 17:45:33.825: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.953510721s
Jun 27 17:45:34.831: INFO: Verifying statefulset ss doesn't scale past 3 for another 948.500284ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3974
Jun 27 17:45:35.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 exec --namespace=statefulset-3974 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 27 17:45:36.198: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 27 17:45:36.198: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 27 17:45:36.198: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 27 17:45:36.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 exec --namespace=statefulset-3974 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 27 17:45:36.585: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 27 17:45:36.585: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 27 17:45:36.585: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 27 17:45:36.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 exec --namespace=statefulset-3974 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 27 17:45:36.952: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 27 17:45:36.952: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 27 17:45:36.952: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 27 17:45:36.957: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 27 17:45:36.957: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 27 17:45:36.957: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun 27 17:45:36.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 exec --namespace=statefulset-3974 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 27 17:45:37.326: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 27 17:45:37.326: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 27 17:45:37.326: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 27 17:45:37.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 exec --namespace=statefulset-3974 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 27 17:45:37.697: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 27 17:45:37.697: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 27 17:45:37.697: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 27 17:45:37.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 exec --namespace=statefulset-3974 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 27 17:45:38.062: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 27 17:45:38.062: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 27 17:45:38.062: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 27 17:45:38.062: INFO: Waiting for statefulset status.replicas updated to 0
Jun 27 17:45:38.066: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun 27 17:45:48.074: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 27 17:45:48.074: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 27 17:45:48.074: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 27 17:45:48.087: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jun 27 17:45:48.087: INFO: ss-0  talos-test-cluster-workers-84c9684cd-nzp9p  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:05 +0000 UTC  }]
Jun 27 17:45:48.088: INFO: ss-1  talos-test-cluster-workers-84c9684cd-mskvs  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:25 +0000 UTC  }]
Jun 27 17:45:48.088: INFO: ss-2  talos-test-cluster-workers-84c9684cd-nk6xg  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:25 +0000 UTC  }]
Jun 27 17:45:48.088: INFO: 
Jun 27 17:45:48.088: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 27 17:45:49.092: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jun 27 17:45:49.092: INFO: ss-0  talos-test-cluster-workers-84c9684cd-nzp9p  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:05 +0000 UTC  }]
Jun 27 17:45:49.092: INFO: ss-1  talos-test-cluster-workers-84c9684cd-mskvs  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:25 +0000 UTC  }]
Jun 27 17:45:49.092: INFO: ss-2  talos-test-cluster-workers-84c9684cd-nk6xg  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:25 +0000 UTC  }]
Jun 27 17:45:49.092: INFO: 
Jun 27 17:45:49.092: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 27 17:45:50.097: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jun 27 17:45:50.097: INFO: ss-0  talos-test-cluster-workers-84c9684cd-nzp9p  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:37 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:05 +0000 UTC  }]
Jun 27 17:45:50.097: INFO: ss-2  talos-test-cluster-workers-84c9684cd-nk6xg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:25 +0000 UTC  }]
Jun 27 17:45:50.097: INFO: 
Jun 27 17:45:50.097: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 27 17:45:51.101: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jun 27 17:45:51.101: INFO: ss-2  talos-test-cluster-workers-84c9684cd-nk6xg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:25 +0000 UTC  }]
Jun 27 17:45:51.101: INFO: 
Jun 27 17:45:51.101: INFO: StatefulSet ss has not reached scale 0, at 1
Jun 27 17:45:52.105: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jun 27 17:45:52.105: INFO: ss-2  talos-test-cluster-workers-84c9684cd-nk6xg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:25 +0000 UTC  }]
Jun 27 17:45:52.106: INFO: 
Jun 27 17:45:52.106: INFO: StatefulSet ss has not reached scale 0, at 1
Jun 27 17:45:53.110: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jun 27 17:45:53.110: INFO: ss-2  talos-test-cluster-workers-84c9684cd-nk6xg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:25 +0000 UTC  }]
Jun 27 17:45:53.110: INFO: 
Jun 27 17:45:53.110: INFO: StatefulSet ss has not reached scale 0, at 1
Jun 27 17:45:54.114: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jun 27 17:45:54.114: INFO: ss-2  talos-test-cluster-workers-84c9684cd-nk6xg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:25 +0000 UTC  }]
Jun 27 17:45:54.114: INFO: 
Jun 27 17:45:54.114: INFO: StatefulSet ss has not reached scale 0, at 1
Jun 27 17:45:55.118: INFO: POD   NODE                                        PHASE    GRACE  CONDITIONS
Jun 27 17:45:55.119: INFO: ss-2  talos-test-cluster-workers-84c9684cd-nk6xg  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:38 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:45:25 +0000 UTC  }]
Jun 27 17:45:55.119: INFO: 
Jun 27 17:45:55.119: INFO: StatefulSet ss has not reached scale 0, at 1
Jun 27 17:45:56.123: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.964570179s
Jun 27 17:45:57.126: INFO: Verifying statefulset ss doesn't scale past 0 for another 960.624223ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3974
Jun 27 17:45:58.131: INFO: Scaling statefulset ss to 0
Jun 27 17:45:58.142: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 27 17:45:58.146: INFO: Deleting all statefulset in ns statefulset-3974
Jun 27 17:45:58.150: INFO: Scaling statefulset ss to 0
Jun 27 17:45:58.161: INFO: Waiting for statefulset status.replicas updated to 0
Jun 27 17:45:58.165: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:45:58.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3974" for this suite.
Jun 27 17:46:04.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:46:04.339: INFO: namespace statefulset-3974 deletion completed in 6.151362993s

• [SLOW TEST:59.148 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:46:04.340: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-5160
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 27 17:46:10.558: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 27 17:46:10.563: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 27 17:46:12.563: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 27 17:46:12.567: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 27 17:46:14.563: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 27 17:46:14.567: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 27 17:46:16.563: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 27 17:46:16.567: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 27 17:46:18.563: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 27 17:46:18.567: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 27 17:46:20.563: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 27 17:46:20.567: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 27 17:46:22.563: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 27 17:46:22.567: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 27 17:46:24.563: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 27 17:46:24.567: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 27 17:46:26.563: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 27 17:46:26.567: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 27 17:46:28.563: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 27 17:46:28.567: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 27 17:46:30.563: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 27 17:46:30.567: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 27 17:46:32.563: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 27 17:46:32.567: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 27 17:46:34.563: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 27 17:46:34.567: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 27 17:46:36.563: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 27 17:46:36.567: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:46:36.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5160" for this suite.
Jun 27 17:46:58.585: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:46:58.718: INFO: namespace container-lifecycle-hook-5160 deletion completed in 22.146167056s

• [SLOW TEST:54.379 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:46:58.718: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5656
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 27 17:46:58.873: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-5656'
Jun 27 17:46:59.088: INFO: stderr: ""
Jun 27 17:46:59.088: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jun 27 17:47:04.138: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 get pod e2e-test-nginx-pod --namespace=kubectl-5656 -o json'
Jun 27 17:47:04.311: INFO: stderr: ""
Jun 27 17:47:04.311: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.244.20.220/32\",\n            \"kubernetes.io/psp\": \"e2e-test-privileged-psp\"\n        },\n        \"creationTimestamp\": \"2019-06-27T17:46:59Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-5656\",\n        \"resourceVersion\": \"14571\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-5656/pods/e2e-test-nginx-pod\",\n        \"uid\": \"578aea48-e14f-40ab-ae5d-b6f244c60f46\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-pph2l\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"talos-test-cluster-workers-84c9684cd-mskvs\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-pph2l\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-pph2l\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-27T17:46:59Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-27T17:47:00Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-27T17:47:00Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-27T17:46:59Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://a5edf568c1302f1c5c83f091eabfa7161f418ddf345c78daadf314a96cac2153\",\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imageID\": \"docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-06-27T17:46:59Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"147.75.109.65\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.20.220\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-06-27T17:46:59Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun 27 17:47:04.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 replace -f - --namespace=kubectl-5656'
Jun 27 17:47:04.696: INFO: stderr: ""
Jun 27 17:47:04.696: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Jun 27 17:47:04.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 delete pods e2e-test-nginx-pod --namespace=kubectl-5656'
Jun 27 17:47:09.719: INFO: stderr: ""
Jun 27 17:47:09.719: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:47:09.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5656" for this suite.
Jun 27 17:47:15.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:47:15.922: INFO: namespace kubectl-5656 deletion completed in 6.197548189s

• [SLOW TEST:17.203 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:47:15.923: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-1925
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-7c81fbb8-1ee1-4492-8d74-4623de1e6481 in namespace container-probe-1925
Jun 27 17:47:18.101: INFO: Started pod busybox-7c81fbb8-1ee1-4492-8d74-4623de1e6481 in namespace container-probe-1925
STEP: checking the pod's current state and verifying that restartCount is present
Jun 27 17:47:18.105: INFO: Initial restart count of pod busybox-7c81fbb8-1ee1-4492-8d74-4623de1e6481 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:51:18.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1925" for this suite.
Jun 27 17:51:24.593: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:51:24.724: INFO: namespace container-probe-1925 deletion completed in 6.145666429s

• [SLOW TEST:248.810 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:51:24.725: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-9924
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 27 17:51:24.894: INFO: Waiting up to 5m0s for pod "pod-222ed37f-3ea0-4975-82cd-e96de8d386e6" in namespace "emptydir-9924" to be "success or failure"
Jun 27 17:51:24.899: INFO: Pod "pod-222ed37f-3ea0-4975-82cd-e96de8d386e6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.346894ms
Jun 27 17:51:26.903: INFO: Pod "pod-222ed37f-3ea0-4975-82cd-e96de8d386e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008945552s
STEP: Saw pod success
Jun 27 17:51:26.903: INFO: Pod "pod-222ed37f-3ea0-4975-82cd-e96de8d386e6" satisfied condition "success or failure"
Jun 27 17:51:26.907: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod pod-222ed37f-3ea0-4975-82cd-e96de8d386e6 container test-container: <nil>
STEP: delete the pod
Jun 27 17:51:26.931: INFO: Waiting for pod pod-222ed37f-3ea0-4975-82cd-e96de8d386e6 to disappear
Jun 27 17:51:26.934: INFO: Pod pod-222ed37f-3ea0-4975-82cd-e96de8d386e6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:51:26.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9924" for this suite.
Jun 27 17:51:32.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:51:33.098: INFO: namespace emptydir-9924 deletion completed in 6.158542919s

• [SLOW TEST:8.373 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:51:33.099: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename services
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in services-9444
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-9444
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9444 to expose endpoints map[]
Jun 27 17:51:33.284: INFO: successfully validated that service multi-endpoint-test in namespace services-9444 exposes endpoints map[] (8.011956ms elapsed)
STEP: Creating pod pod1 in namespace services-9444
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9444 to expose endpoints map[pod1:[100]]
Jun 27 17:51:35.333: INFO: successfully validated that service multi-endpoint-test in namespace services-9444 exposes endpoints map[pod1:[100]] (2.03157129s elapsed)
STEP: Creating pod pod2 in namespace services-9444
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9444 to expose endpoints map[pod1:[100] pod2:[101]]
Jun 27 17:51:37.388: INFO: successfully validated that service multi-endpoint-test in namespace services-9444 exposes endpoints map[pod1:[100] pod2:[101]] (2.049135214s elapsed)
STEP: Deleting pod pod1 in namespace services-9444
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9444 to expose endpoints map[pod2:[101]]
Jun 27 17:51:37.409: INFO: successfully validated that service multi-endpoint-test in namespace services-9444 exposes endpoints map[pod2:[101]] (12.78445ms elapsed)
STEP: Deleting pod pod2 in namespace services-9444
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-9444 to expose endpoints map[]
Jun 27 17:51:38.427: INFO: successfully validated that service multi-endpoint-test in namespace services-9444 exposes endpoints map[] (1.00932849s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:51:38.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9444" for this suite.
Jun 27 17:52:00.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:52:00.611: INFO: namespace services-9444 deletion completed in 22.152736311s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:27.512 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:52:00.613: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-2437
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-abf80416-3bd5-4e4b-ba1e-648aad973f08
STEP: Creating a pod to test consume configMaps
Jun 27 17:52:00.783: INFO: Waiting up to 5m0s for pod "pod-configmaps-8b149e3e-4e3e-429d-ab7b-2fe7b2faabd7" in namespace "configmap-2437" to be "success or failure"
Jun 27 17:52:00.788: INFO: Pod "pod-configmaps-8b149e3e-4e3e-429d-ab7b-2fe7b2faabd7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.818993ms
Jun 27 17:52:02.791: INFO: Pod "pod-configmaps-8b149e3e-4e3e-429d-ab7b-2fe7b2faabd7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008526632s
STEP: Saw pod success
Jun 27 17:52:02.791: INFO: Pod "pod-configmaps-8b149e3e-4e3e-429d-ab7b-2fe7b2faabd7" satisfied condition "success or failure"
Jun 27 17:52:02.795: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod pod-configmaps-8b149e3e-4e3e-429d-ab7b-2fe7b2faabd7 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 27 17:52:02.813: INFO: Waiting for pod pod-configmaps-8b149e3e-4e3e-429d-ab7b-2fe7b2faabd7 to disappear
Jun 27 17:52:02.819: INFO: Pod pod-configmaps-8b149e3e-4e3e-429d-ab7b-2fe7b2faabd7 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:52:02.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2437" for this suite.
Jun 27 17:52:08.838: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:52:08.967: INFO: namespace configmap-2437 deletion completed in 6.142836176s

• [SLOW TEST:8.354 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:52:08.967: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-422
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 27 17:52:09.133: INFO: Waiting up to 5m0s for pod "pod-8c669e65-d0f7-4c6d-8bec-ff943716ad9d" in namespace "emptydir-422" to be "success or failure"
Jun 27 17:52:09.139: INFO: Pod "pod-8c669e65-d0f7-4c6d-8bec-ff943716ad9d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.867594ms
Jun 27 17:52:11.143: INFO: Pod "pod-8c669e65-d0f7-4c6d-8bec-ff943716ad9d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009478003s
STEP: Saw pod success
Jun 27 17:52:11.143: INFO: Pod "pod-8c669e65-d0f7-4c6d-8bec-ff943716ad9d" satisfied condition "success or failure"
Jun 27 17:52:11.147: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-mskvs pod pod-8c669e65-d0f7-4c6d-8bec-ff943716ad9d container test-container: <nil>
STEP: delete the pod
Jun 27 17:52:11.172: INFO: Waiting for pod pod-8c669e65-d0f7-4c6d-8bec-ff943716ad9d to disappear
Jun 27 17:52:11.179: INFO: Pod pod-8c669e65-d0f7-4c6d-8bec-ff943716ad9d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:52:11.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-422" for this suite.
Jun 27 17:52:17.197: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:52:17.330: INFO: namespace emptydir-422 deletion completed in 6.146073099s

• [SLOW TEST:8.363 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:52:17.331: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-9569
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 27 17:52:17.508: INFO: Waiting up to 5m0s for pod "downwardapi-volume-22d07c49-25c8-4ffe-abc3-354119a022b0" in namespace "downward-api-9569" to be "success or failure"
Jun 27 17:52:17.516: INFO: Pod "downwardapi-volume-22d07c49-25c8-4ffe-abc3-354119a022b0": Phase="Pending", Reason="", readiness=false. Elapsed: 7.591206ms
Jun 27 17:52:19.521: INFO: Pod "downwardapi-volume-22d07c49-25c8-4ffe-abc3-354119a022b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012720586s
STEP: Saw pod success
Jun 27 17:52:19.521: INFO: Pod "downwardapi-volume-22d07c49-25c8-4ffe-abc3-354119a022b0" satisfied condition "success or failure"
Jun 27 17:52:19.526: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod downwardapi-volume-22d07c49-25c8-4ffe-abc3-354119a022b0 container client-container: <nil>
STEP: delete the pod
Jun 27 17:52:19.554: INFO: Waiting for pod downwardapi-volume-22d07c49-25c8-4ffe-abc3-354119a022b0 to disappear
Jun 27 17:52:19.561: INFO: Pod downwardapi-volume-22d07c49-25c8-4ffe-abc3-354119a022b0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:52:19.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9569" for this suite.
Jun 27 17:52:25.579: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:52:25.718: INFO: namespace downward-api-9569 deletion completed in 6.152652074s

• [SLOW TEST:8.387 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:52:25.719: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-849
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:53:25.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-849" for this suite.
Jun 27 17:53:47.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:53:48.054: INFO: namespace container-probe-849 deletion completed in 22.150192159s

• [SLOW TEST:82.334 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:53:48.054: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2936
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 27 17:53:48.227: INFO: Waiting up to 5m0s for pod "pod-febc3a39-9edf-4898-b6bb-1f10351e33c0" in namespace "emptydir-2936" to be "success or failure"
Jun 27 17:53:48.232: INFO: Pod "pod-febc3a39-9edf-4898-b6bb-1f10351e33c0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.228544ms
Jun 27 17:53:50.235: INFO: Pod "pod-febc3a39-9edf-4898-b6bb-1f10351e33c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008065582s
Jun 27 17:53:52.217: INFO: Pod "pod-febc3a39-9edf-4898-b6bb-1f10351e33c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011875141s
STEP: Saw pod success
Jun 27 17:53:52.217: INFO: Pod "pod-febc3a39-9edf-4898-b6bb-1f10351e33c0" satisfied condition "success or failure"
Jun 27 17:53:52.220: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-mskvs pod pod-febc3a39-9edf-4898-b6bb-1f10351e33c0 container test-container: <nil>
STEP: delete the pod
Jun 27 17:53:52.244: INFO: Waiting for pod pod-febc3a39-9edf-4898-b6bb-1f10351e33c0 to disappear
Jun 27 17:53:52.248: INFO: Pod pod-febc3a39-9edf-4898-b6bb-1f10351e33c0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:53:52.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2936" for this suite.
Jun 27 17:53:58.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:53:58.418: INFO: namespace emptydir-2936 deletion completed in 6.164975084s

• [SLOW TEST:10.387 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:53:58.418: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-9694
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun 27 17:53:58.598: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9694,SelfLink:/api/v1/namespaces/watch-9694/configmaps/e2e-watch-test-label-changed,UID:16ec2061-8f49-4539-8cc4-a7825312e54c,ResourceVersion:15555,Generation:0,CreationTimestamp:2019-06-27 17:53:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 27 17:53:58.598: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9694,SelfLink:/api/v1/namespaces/watch-9694/configmaps/e2e-watch-test-label-changed,UID:16ec2061-8f49-4539-8cc4-a7825312e54c,ResourceVersion:15556,Generation:0,CreationTimestamp:2019-06-27 17:53:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 27 17:53:58.598: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9694,SelfLink:/api/v1/namespaces/watch-9694/configmaps/e2e-watch-test-label-changed,UID:16ec2061-8f49-4539-8cc4-a7825312e54c,ResourceVersion:15557,Generation:0,CreationTimestamp:2019-06-27 17:53:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun 27 17:54:08.630: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9694,SelfLink:/api/v1/namespaces/watch-9694/configmaps/e2e-watch-test-label-changed,UID:16ec2061-8f49-4539-8cc4-a7825312e54c,ResourceVersion:15575,Generation:0,CreationTimestamp:2019-06-27 17:53:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 27 17:54:08.630: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9694,SelfLink:/api/v1/namespaces/watch-9694/configmaps/e2e-watch-test-label-changed,UID:16ec2061-8f49-4539-8cc4-a7825312e54c,ResourceVersion:15576,Generation:0,CreationTimestamp:2019-06-27 17:53:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jun 27 17:54:08.631: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9694,SelfLink:/api/v1/namespaces/watch-9694/configmaps/e2e-watch-test-label-changed,UID:16ec2061-8f49-4539-8cc4-a7825312e54c,ResourceVersion:15577,Generation:0,CreationTimestamp:2019-06-27 17:53:58 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:54:08.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9694" for this suite.
Jun 27 17:54:14.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:54:14.780: INFO: namespace watch-9694 deletion completed in 6.143749657s

• [SLOW TEST:16.361 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:54:14.780: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename replicaset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in replicaset-1194
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 27 17:54:14.936: INFO: Creating ReplicaSet my-hostname-basic-e626f9d0-e9f8-43b5-b681-436f88124b5b
Jun 27 17:54:14.946: INFO: Pod name my-hostname-basic-e626f9d0-e9f8-43b5-b681-436f88124b5b: Found 0 pods out of 1
Jun 27 17:54:19.951: INFO: Pod name my-hostname-basic-e626f9d0-e9f8-43b5-b681-436f88124b5b: Found 1 pods out of 1
Jun 27 17:54:19.951: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-e626f9d0-e9f8-43b5-b681-436f88124b5b" is running
Jun 27 17:54:19.956: INFO: Pod "my-hostname-basic-e626f9d0-e9f8-43b5-b681-436f88124b5b-pjtm7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-27 17:54:14 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-27 17:54:18 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-27 17:54:18 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-27 17:54:14 +0000 UTC Reason: Message:}])
Jun 27 17:54:19.956: INFO: Trying to dial the pod
Jun 27 17:54:24.968: INFO: Controller my-hostname-basic-e626f9d0-e9f8-43b5-b681-436f88124b5b: Got expected result from replica 1 [my-hostname-basic-e626f9d0-e9f8-43b5-b681-436f88124b5b-pjtm7]: "my-hostname-basic-e626f9d0-e9f8-43b5-b681-436f88124b5b-pjtm7", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:54:24.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1194" for this suite.
Jun 27 17:54:30.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:54:31.117: INFO: namespace replicaset-1194 deletion completed in 6.143542447s

• [SLOW TEST:16.337 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:54:31.118: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7080
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jun 27 17:54:41.304: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0627 17:54:41.304323      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:54:41.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7080" for this suite.
Jun 27 17:54:47.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:54:47.460: INFO: namespace gc-7080 deletion completed in 6.151532783s

• [SLOW TEST:16.342 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:54:47.461: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8543
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-a3955569-7e54-467d-a406-124fda664c1a
STEP: Creating a pod to test consume secrets
Jun 27 17:54:47.636: INFO: Waiting up to 5m0s for pod "pod-secrets-74280b78-d0ad-436a-8958-427036e0d977" in namespace "secrets-8543" to be "success or failure"
Jun 27 17:54:47.640: INFO: Pod "pod-secrets-74280b78-d0ad-436a-8958-427036e0d977": Phase="Pending", Reason="", readiness=false. Elapsed: 3.455492ms
Jun 27 17:54:49.645: INFO: Pod "pod-secrets-74280b78-d0ad-436a-8958-427036e0d977": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008460752s
STEP: Saw pod success
Jun 27 17:54:49.645: INFO: Pod "pod-secrets-74280b78-d0ad-436a-8958-427036e0d977" satisfied condition "success or failure"
Jun 27 17:54:49.649: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nzp9p pod pod-secrets-74280b78-d0ad-436a-8958-427036e0d977 container secret-volume-test: <nil>
STEP: delete the pod
Jun 27 17:54:49.675: INFO: Waiting for pod pod-secrets-74280b78-d0ad-436a-8958-427036e0d977 to disappear
Jun 27 17:54:49.682: INFO: Pod pod-secrets-74280b78-d0ad-436a-8958-427036e0d977 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:54:49.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8543" for this suite.
Jun 27 17:54:55.700: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:54:55.869: INFO: namespace secrets-8543 deletion completed in 6.182630927s

• [SLOW TEST:8.409 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:54:55.871: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename dns
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in dns-4681
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4681.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4681.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 27 17:54:58.095: INFO: DNS probes using dns-4681/dns-test-ba32ae17-2c11-496a-b0b1-06dab209cf7a succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:54:58.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4681" for this suite.
Jun 27 17:55:04.128: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:55:04.258: INFO: namespace dns-4681 deletion completed in 6.145266358s

• [SLOW TEST:8.387 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:55:04.258: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7175
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 27 17:55:04.428: INFO: Waiting up to 5m0s for pod "downwardapi-volume-84045cae-4124-4e42-bf9f-2d7a0abdf6d7" in namespace "projected-7175" to be "success or failure"
Jun 27 17:55:04.438: INFO: Pod "downwardapi-volume-84045cae-4124-4e42-bf9f-2d7a0abdf6d7": Phase="Pending", Reason="", readiness=false. Elapsed: 9.359537ms
Jun 27 17:55:06.442: INFO: Pod "downwardapi-volume-84045cae-4124-4e42-bf9f-2d7a0abdf6d7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013200836s
STEP: Saw pod success
Jun 27 17:55:06.442: INFO: Pod "downwardapi-volume-84045cae-4124-4e42-bf9f-2d7a0abdf6d7" satisfied condition "success or failure"
Jun 27 17:55:06.446: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-mskvs pod downwardapi-volume-84045cae-4124-4e42-bf9f-2d7a0abdf6d7 container client-container: <nil>
STEP: delete the pod
Jun 27 17:55:06.466: INFO: Waiting for pod downwardapi-volume-84045cae-4124-4e42-bf9f-2d7a0abdf6d7 to disappear
Jun 27 17:55:06.471: INFO: Pod downwardapi-volume-84045cae-4124-4e42-bf9f-2d7a0abdf6d7 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:55:06.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7175" for this suite.
Jun 27 17:55:12.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:55:12.636: INFO: namespace projected-7175 deletion completed in 6.158052528s

• [SLOW TEST:8.378 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:55:12.637: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-4092
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 27 17:55:14.816: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:55:14.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4092" for this suite.
Jun 27 17:55:20.852: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:55:20.982: INFO: namespace container-runtime-4092 deletion completed in 6.144767848s

• [SLOW TEST:8.346 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:55:20.984: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-3026
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-0d3a6158-6ea7-4d4c-bdf2-16505d5e780f
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:55:21.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3026" for this suite.
Jun 27 17:55:27.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:55:27.290: INFO: namespace configmap-3026 deletion completed in 6.145069658s

• [SLOW TEST:6.307 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:55:27.291: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-5565
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 27 17:55:27.459: INFO: Waiting up to 5m0s for pod "pod-f9f04c71-9000-477d-b74e-858d788c9caf" in namespace "emptydir-5565" to be "success or failure"
Jun 27 17:55:27.463: INFO: Pod "pod-f9f04c71-9000-477d-b74e-858d788c9caf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.401654ms
Jun 27 17:55:29.467: INFO: Pod "pod-f9f04c71-9000-477d-b74e-858d788c9caf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008490533s
STEP: Saw pod success
Jun 27 17:55:29.467: INFO: Pod "pod-f9f04c71-9000-477d-b74e-858d788c9caf" satisfied condition "success or failure"
Jun 27 17:55:29.471: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod pod-f9f04c71-9000-477d-b74e-858d788c9caf container test-container: <nil>
STEP: delete the pod
Jun 27 17:55:29.492: INFO: Waiting for pod pod-f9f04c71-9000-477d-b74e-858d788c9caf to disappear
Jun 27 17:55:29.496: INFO: Pod pod-f9f04c71-9000-477d-b74e-858d788c9caf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:55:29.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5565" for this suite.
Jun 27 17:55:35.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:55:35.648: INFO: namespace emptydir-5565 deletion completed in 6.146840629s

• [SLOW TEST:8.357 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:55:35.649: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-1085
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 27 17:55:35.840: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1a176a4e-f1d6-4b18-a8ae-0a404aa3645d" in namespace "downward-api-1085" to be "success or failure"
Jun 27 17:55:35.852: INFO: Pod "downwardapi-volume-1a176a4e-f1d6-4b18-a8ae-0a404aa3645d": Phase="Pending", Reason="", readiness=false. Elapsed: 11.617829ms
Jun 27 17:55:37.856: INFO: Pod "downwardapi-volume-1a176a4e-f1d6-4b18-a8ae-0a404aa3645d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015412318s
STEP: Saw pod success
Jun 27 17:55:37.856: INFO: Pod "downwardapi-volume-1a176a4e-f1d6-4b18-a8ae-0a404aa3645d" satisfied condition "success or failure"
Jun 27 17:55:37.860: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-mskvs pod downwardapi-volume-1a176a4e-f1d6-4b18-a8ae-0a404aa3645d container client-container: <nil>
STEP: delete the pod
Jun 27 17:55:37.887: INFO: Waiting for pod downwardapi-volume-1a176a4e-f1d6-4b18-a8ae-0a404aa3645d to disappear
Jun 27 17:55:37.891: INFO: Pod downwardapi-volume-1a176a4e-f1d6-4b18-a8ae-0a404aa3645d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:55:37.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1085" for this suite.
Jun 27 17:55:43.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:55:44.044: INFO: namespace downward-api-1085 deletion completed in 6.14753263s

• [SLOW TEST:8.396 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:55:44.045: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-8785
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 27 17:55:44.212: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun 27 17:55:49.216: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 27 17:55:49.216: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun 27 17:55:51.220: INFO: Creating deployment "test-rollover-deployment"
Jun 27 17:55:51.229: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun 27 17:55:53.238: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun 27 17:55:53.246: INFO: Ensure that both replica sets have 1 created replica
Jun 27 17:55:53.254: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun 27 17:55:53.264: INFO: Updating deployment test-rollover-deployment
Jun 27 17:55:53.264: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun 27 17:55:55.272: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun 27 17:55:55.279: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun 27 17:55:55.287: INFO: all replica sets need to contain the pod-template-hash label
Jun 27 17:55:55.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697254951, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697254951, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697254954, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697254951, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:55:57.296: INFO: all replica sets need to contain the pod-template-hash label
Jun 27 17:55:57.296: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697254951, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697254951, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697254954, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697254951, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:55:59.298: INFO: all replica sets need to contain the pod-template-hash label
Jun 27 17:55:59.298: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697254951, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697254951, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697254954, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697254951, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:56:01.296: INFO: all replica sets need to contain the pod-template-hash label
Jun 27 17:56:01.296: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697254951, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697254951, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697254954, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697254951, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:56:03.295: INFO: all replica sets need to contain the pod-template-hash label
Jun 27 17:56:03.296: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697254951, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697254951, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63697254954, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697254951, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:56:05.295: INFO: 
Jun 27 17:56:05.296: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 27 17:56:05.306: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-8785,SelfLink:/apis/apps/v1/namespaces/deployment-8785/deployments/test-rollover-deployment,UID:317d261a-9a45-480e-b193-32729323c824,ResourceVersion:16171,Generation:2,CreationTimestamp:2019-06-27 17:55:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-06-27 17:55:51 +0000 UTC 2019-06-27 17:55:51 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-06-27 17:56:04 +0000 UTC 2019-06-27 17:55:51 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jun 27 17:56:05.311: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-8785,SelfLink:/apis/apps/v1/namespaces/deployment-8785/replicasets/test-rollover-deployment-854595fc44,UID:f8b1c94e-1a5c-4a0a-8961-4ddfdf1af22e,ResourceVersion:16160,Generation:2,CreationTimestamp:2019-06-27 17:55:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 317d261a-9a45-480e-b193-32729323c824 0xc00321f607 0xc00321f608}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 27 17:56:05.311: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun 27 17:56:05.311: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-8785,SelfLink:/apis/apps/v1/namespaces/deployment-8785/replicasets/test-rollover-controller,UID:1e6b261c-557f-4e61-89f1-eb65bc8451e8,ResourceVersion:16170,Generation:2,CreationTimestamp:2019-06-27 17:55:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 317d261a-9a45-480e-b193-32729323c824 0xc00321f537 0xc00321f538}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 27 17:56:05.312: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-8785,SelfLink:/apis/apps/v1/namespaces/deployment-8785/replicasets/test-rollover-deployment-9b8b997cf,UID:9acab4ad-aaad-4ccc-b731-d30765a0b1e5,ResourceVersion:16118,Generation:2,CreationTimestamp:2019-06-27 17:55:51 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 317d261a-9a45-480e-b193-32729323c824 0xc00321f6e0 0xc00321f6e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 27 17:56:05.316: INFO: Pod "test-rollover-deployment-854595fc44-dq2zm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-dq2zm,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-8785,SelfLink:/api/v1/namespaces/deployment-8785/pods/test-rollover-deployment-854595fc44-dq2zm,UID:f762f893-9ab3-4b1b-bf70-43c5c9fb4102,ResourceVersion:16140,Generation:0,CreationTimestamp:2019-06-27 17:55:53 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.20.227/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 f8b1c94e-1a5c-4a0a-8961-4ddfdf1af22e 0xc0031410a7 0xc0031410a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-vfx6z {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-vfx6z,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-vfx6z true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-mskvs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003141110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003141130}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:55:53 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:55:54 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:55:54 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 17:55:53 +0000 UTC  }],Message:,Reason:,HostIP:147.75.109.65,PodIP:10.244.20.227,StartTime:2019-06-27 17:55:53 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-06-27 17:55:54 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 containerd://9d8c1b97618d28a33285fe0577b4fe8ea8065b200f7ef46d51c6a569aacfb626}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:56:05.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8785" for this suite.
Jun 27 17:56:11.334: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:56:11.471: INFO: namespace deployment-8785 deletion completed in 6.150251963s

• [SLOW TEST:27.427 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:56:11.472: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-2055
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-12ac7ca9-333c-4187-87da-c4b7510b6295
STEP: Creating a pod to test consume secrets
Jun 27 17:56:11.645: INFO: Waiting up to 5m0s for pod "pod-secrets-3422e424-e8bf-4363-bf0e-cbe4eda7074f" in namespace "secrets-2055" to be "success or failure"
Jun 27 17:56:11.653: INFO: Pod "pod-secrets-3422e424-e8bf-4363-bf0e-cbe4eda7074f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.544206ms
Jun 27 17:56:13.657: INFO: Pod "pod-secrets-3422e424-e8bf-4363-bf0e-cbe4eda7074f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011305025s
STEP: Saw pod success
Jun 27 17:56:13.657: INFO: Pod "pod-secrets-3422e424-e8bf-4363-bf0e-cbe4eda7074f" satisfied condition "success or failure"
Jun 27 17:56:13.660: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nzp9p pod pod-secrets-3422e424-e8bf-4363-bf0e-cbe4eda7074f container secret-env-test: <nil>
STEP: delete the pod
Jun 27 17:56:13.679: INFO: Waiting for pod pod-secrets-3422e424-e8bf-4363-bf0e-cbe4eda7074f to disappear
Jun 27 17:56:13.684: INFO: Pod pod-secrets-3422e424-e8bf-4363-bf0e-cbe4eda7074f no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:56:13.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2055" for this suite.
Jun 27 17:56:19.703: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:56:19.832: INFO: namespace secrets-2055 deletion completed in 6.143491207s

• [SLOW TEST:8.361 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:56:19.833: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-400
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:56:22.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-400" for this suite.
Jun 27 17:57:02.040: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:57:02.175: INFO: namespace kubelet-test-400 deletion completed in 40.150350622s

• [SLOW TEST:42.343 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:57:02.176: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7440
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-eda91d2b-e53f-472b-b4f9-470b0e0593c7
STEP: Creating a pod to test consume configMaps
Jun 27 17:57:02.350: INFO: Waiting up to 5m0s for pod "pod-configmaps-c34e71ce-8a6b-4476-8295-c83d26abdbb9" in namespace "configmap-7440" to be "success or failure"
Jun 27 17:57:02.354: INFO: Pod "pod-configmaps-c34e71ce-8a6b-4476-8295-c83d26abdbb9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.897943ms
Jun 27 17:57:04.358: INFO: Pod "pod-configmaps-c34e71ce-8a6b-4476-8295-c83d26abdbb9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007679212s
STEP: Saw pod success
Jun 27 17:57:04.358: INFO: Pod "pod-configmaps-c34e71ce-8a6b-4476-8295-c83d26abdbb9" satisfied condition "success or failure"
Jun 27 17:57:04.361: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-mskvs pod pod-configmaps-c34e71ce-8a6b-4476-8295-c83d26abdbb9 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 27 17:57:04.383: INFO: Waiting for pod pod-configmaps-c34e71ce-8a6b-4476-8295-c83d26abdbb9 to disappear
Jun 27 17:57:04.387: INFO: Pod pod-configmaps-c34e71ce-8a6b-4476-8295-c83d26abdbb9 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:57:04.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7440" for this suite.
Jun 27 17:57:10.405: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:57:10.538: INFO: namespace configmap-7440 deletion completed in 6.145244968s

• [SLOW TEST:8.361 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:57:10.538: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8927
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-997e7c90-24c5-4e94-bf03-ac1f5ecf3163
STEP: Creating a pod to test consume configMaps
Jun 27 17:57:10.711: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-eff8e6e6-e8cd-4fc8-9cb2-99aa99b79fd9" in namespace "projected-8927" to be "success or failure"
Jun 27 17:57:10.716: INFO: Pod "pod-projected-configmaps-eff8e6e6-e8cd-4fc8-9cb2-99aa99b79fd9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.191594ms
Jun 27 17:57:12.720: INFO: Pod "pod-projected-configmaps-eff8e6e6-e8cd-4fc8-9cb2-99aa99b79fd9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009051433s
STEP: Saw pod success
Jun 27 17:57:12.720: INFO: Pod "pod-projected-configmaps-eff8e6e6-e8cd-4fc8-9cb2-99aa99b79fd9" satisfied condition "success or failure"
Jun 27 17:57:12.723: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nzp9p pod pod-projected-configmaps-eff8e6e6-e8cd-4fc8-9cb2-99aa99b79fd9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 27 17:57:12.750: INFO: Waiting for pod pod-projected-configmaps-eff8e6e6-e8cd-4fc8-9cb2-99aa99b79fd9 to disappear
Jun 27 17:57:12.755: INFO: Pod pod-projected-configmaps-eff8e6e6-e8cd-4fc8-9cb2-99aa99b79fd9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:57:12.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8927" for this suite.
Jun 27 17:57:18.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:57:18.905: INFO: namespace projected-8927 deletion completed in 6.145129048s

• [SLOW TEST:8.367 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:57:18.906: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-2381
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:57:21.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2381" for this suite.
Jun 27 17:58:09.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:58:09.254: INFO: namespace kubelet-test-2381 deletion completed in 48.155999261s

• [SLOW TEST:50.349 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:58:09.254: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5902
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-5902
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 27 17:58:09.433: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 27 17:58:33.555: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.177.209:8080/dial?request=hostName&protocol=udp&host=10.244.187.92&port=8081&tries=1'] Namespace:pod-network-test-5902 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 27 17:58:33.555: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
Jun 27 17:58:33.744: INFO: Waiting for endpoints: map[]
Jun 27 17:58:33.747: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.177.209:8080/dial?request=hostName&protocol=udp&host=10.244.20.229&port=8081&tries=1'] Namespace:pod-network-test-5902 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 27 17:58:33.747: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
Jun 27 17:58:33.926: INFO: Waiting for endpoints: map[]
Jun 27 17:58:33.930: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.177.209:8080/dial?request=hostName&protocol=udp&host=10.244.177.208&port=8081&tries=1'] Namespace:pod-network-test-5902 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 27 17:58:33.930: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
Jun 27 17:58:34.115: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:58:34.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5902" for this suite.
Jun 27 17:58:56.142: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:58:56.273: INFO: namespace pod-network-test-5902 deletion completed in 22.147416697s

• [SLOW TEST:47.018 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:58:56.273: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-5515
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-5515
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 27 17:58:56.430: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 27 17:59:18.541: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.177.210:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5515 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 27 17:59:18.541: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
Jun 27 17:59:18.716: INFO: Found all expected endpoints: [netserver-0]
Jun 27 17:59:18.720: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.187.91:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5515 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 27 17:59:18.720: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
Jun 27 17:59:18.881: INFO: Found all expected endpoints: [netserver-1]
Jun 27 17:59:18.884: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.20.230:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-5515 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 27 17:59:18.884: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
Jun 27 17:59:19.047: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:59:19.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-5515" for this suite.
Jun 27 17:59:41.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:59:41.208: INFO: namespace pod-network-test-5515 deletion completed in 22.155406834s

• [SLOW TEST:44.935 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:59:41.208: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3169
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-32b94591-48b8-47fa-8e21-11e045ab94f2
STEP: Creating a pod to test consume secrets
Jun 27 17:59:41.379: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-086ef189-eff6-417a-876c-efff6cb6caf0" in namespace "projected-3169" to be "success or failure"
Jun 27 17:59:41.385: INFO: Pod "pod-projected-secrets-086ef189-eff6-417a-876c-efff6cb6caf0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.674985ms
Jun 27 17:59:43.389: INFO: Pod "pod-projected-secrets-086ef189-eff6-417a-876c-efff6cb6caf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009357013s
STEP: Saw pod success
Jun 27 17:59:43.389: INFO: Pod "pod-projected-secrets-086ef189-eff6-417a-876c-efff6cb6caf0" satisfied condition "success or failure"
Jun 27 17:59:43.393: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nzp9p pod pod-projected-secrets-086ef189-eff6-417a-876c-efff6cb6caf0 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 27 17:59:43.427: INFO: Waiting for pod pod-projected-secrets-086ef189-eff6-417a-876c-efff6cb6caf0 to disappear
Jun 27 17:59:43.432: INFO: Pod pod-projected-secrets-086ef189-eff6-417a-876c-efff6cb6caf0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:59:43.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3169" for this suite.
Jun 27 17:59:49.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:59:49.598: INFO: namespace projected-3169 deletion completed in 6.161357421s

• [SLOW TEST:8.390 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:59:49.600: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-5700
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0627 17:59:50.820533      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 27 17:59:50.820: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:59:50.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5700" for this suite.
Jun 27 17:59:56.839: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 17:59:56.972: INFO: namespace gc-5700 deletion completed in 6.145187699s

• [SLOW TEST:7.373 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 17:59:56.973: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6566
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-81305dd0-21b5-463f-913d-3ccbf68e07da
STEP: Creating a pod to test consume secrets
Jun 27 17:59:57.146: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-58d01ae8-5fe0-459d-930d-bd5fda3b107b" in namespace "projected-6566" to be "success or failure"
Jun 27 17:59:57.156: INFO: Pod "pod-projected-secrets-58d01ae8-5fe0-459d-930d-bd5fda3b107b": Phase="Pending", Reason="", readiness=false. Elapsed: 10.429108ms
Jun 27 17:59:59.160: INFO: Pod "pod-projected-secrets-58d01ae8-5fe0-459d-930d-bd5fda3b107b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014274667s
STEP: Saw pod success
Jun 27 17:59:59.160: INFO: Pod "pod-projected-secrets-58d01ae8-5fe0-459d-930d-bd5fda3b107b" satisfied condition "success or failure"
Jun 27 17:59:59.164: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nzp9p pod pod-projected-secrets-58d01ae8-5fe0-459d-930d-bd5fda3b107b container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 27 17:59:59.194: INFO: Waiting for pod pod-projected-secrets-58d01ae8-5fe0-459d-930d-bd5fda3b107b to disappear
Jun 27 17:59:59.198: INFO: Pod pod-projected-secrets-58d01ae8-5fe0-459d-930d-bd5fda3b107b no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 17:59:59.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6566" for this suite.
Jun 27 18:00:05.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:00:05.348: INFO: namespace projected-6566 deletion completed in 6.144680318s

• [SLOW TEST:8.375 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:00:05.349: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4207
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 27 18:00:05.595: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 18:00:05.600: INFO: Number of nodes with available pods: 0
Jun 27 18:00:05.600: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 18:00:06.605: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 18:00:06.609: INFO: Number of nodes with available pods: 0
Jun 27 18:00:06.609: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 18:00:07.605: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 18:00:07.610: INFO: Number of nodes with available pods: 3
Jun 27 18:00:07.610: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun 27 18:00:07.631: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 18:00:07.636: INFO: Number of nodes with available pods: 2
Jun 27 18:00:07.636: INFO: Node talos-test-cluster-workers-84c9684cd-nk6xg is running more than one daemon pod
Jun 27 18:00:08.641: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 18:00:08.645: INFO: Number of nodes with available pods: 2
Jun 27 18:00:08.645: INFO: Node talos-test-cluster-workers-84c9684cd-nk6xg is running more than one daemon pod
Jun 27 18:00:09.642: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 18:00:09.647: INFO: Number of nodes with available pods: 2
Jun 27 18:00:09.647: INFO: Node talos-test-cluster-workers-84c9684cd-nk6xg is running more than one daemon pod
Jun 27 18:00:10.641: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 18:00:10.645: INFO: Number of nodes with available pods: 2
Jun 27 18:00:10.645: INFO: Node talos-test-cluster-workers-84c9684cd-nk6xg is running more than one daemon pod
Jun 27 18:00:11.641: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 18:00:11.645: INFO: Number of nodes with available pods: 2
Jun 27 18:00:11.646: INFO: Node talos-test-cluster-workers-84c9684cd-nk6xg is running more than one daemon pod
Jun 27 18:00:12.641: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 18:00:12.646: INFO: Number of nodes with available pods: 3
Jun 27 18:00:12.646: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4207, will wait for the garbage collector to delete the pods
Jun 27 18:00:12.711: INFO: Deleting DaemonSet.extensions daemon-set took: 7.783826ms
Jun 27 18:00:12.812: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.332836ms
Jun 27 18:00:25.115: INFO: Number of nodes with available pods: 0
Jun 27 18:00:25.115: INFO: Number of running nodes: 0, number of available pods: 0
Jun 27 18:00:25.119: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4207/daemonsets","resourceVersion":"17234"},"items":null}

Jun 27 18:00:25.122: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4207/pods","resourceVersion":"17234"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:00:25.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4207" for this suite.
Jun 27 18:00:31.157: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:00:31.291: INFO: namespace daemonsets-4207 deletion completed in 6.14616011s

• [SLOW TEST:25.942 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:00:31.291: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-wrapper-6575
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:00:33.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6575" for this suite.
Jun 27 18:00:39.535: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:00:39.671: INFO: namespace emptydir-wrapper-6575 deletion completed in 6.150262322s

• [SLOW TEST:8.380 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:00:39.672: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-8270
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-8270/configmap-test-84d90f77-78b5-4de7-904c-9fd9a73b25b7
STEP: Creating a pod to test consume configMaps
Jun 27 18:00:39.848: INFO: Waiting up to 5m0s for pod "pod-configmaps-9dc4f1dc-d374-4655-b78d-52ff166ca27c" in namespace "configmap-8270" to be "success or failure"
Jun 27 18:00:39.852: INFO: Pod "pod-configmaps-9dc4f1dc-d374-4655-b78d-52ff166ca27c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.479963ms
Jun 27 18:00:41.855: INFO: Pod "pod-configmaps-9dc4f1dc-d374-4655-b78d-52ff166ca27c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007406902s
STEP: Saw pod success
Jun 27 18:00:41.856: INFO: Pod "pod-configmaps-9dc4f1dc-d374-4655-b78d-52ff166ca27c" satisfied condition "success or failure"
Jun 27 18:00:41.859: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod pod-configmaps-9dc4f1dc-d374-4655-b78d-52ff166ca27c container env-test: <nil>
STEP: delete the pod
Jun 27 18:00:41.882: INFO: Waiting for pod pod-configmaps-9dc4f1dc-d374-4655-b78d-52ff166ca27c to disappear
Jun 27 18:00:41.885: INFO: Pod pod-configmaps-9dc4f1dc-d374-4655-b78d-52ff166ca27c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:00:41.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8270" for this suite.
Jun 27 18:00:47.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:00:48.040: INFO: namespace configmap-8270 deletion completed in 6.148948591s

• [SLOW TEST:8.368 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:00:48.040: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3345
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 27 18:00:48.205: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e612573e-6f9a-4f54-9d7f-8913509e10d5" in namespace "downward-api-3345" to be "success or failure"
Jun 27 18:00:48.211: INFO: Pod "downwardapi-volume-e612573e-6f9a-4f54-9d7f-8913509e10d5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.551225ms
Jun 27 18:00:50.215: INFO: Pod "downwardapi-volume-e612573e-6f9a-4f54-9d7f-8913509e10d5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009900694s
STEP: Saw pod success
Jun 27 18:00:50.215: INFO: Pod "downwardapi-volume-e612573e-6f9a-4f54-9d7f-8913509e10d5" satisfied condition "success or failure"
Jun 27 18:00:50.219: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod downwardapi-volume-e612573e-6f9a-4f54-9d7f-8913509e10d5 container client-container: <nil>
STEP: delete the pod
Jun 27 18:00:50.246: INFO: Waiting for pod downwardapi-volume-e612573e-6f9a-4f54-9d7f-8913509e10d5 to disappear
Jun 27 18:00:50.250: INFO: Pod downwardapi-volume-e612573e-6f9a-4f54-9d7f-8913509e10d5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:00:50.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3345" for this suite.
Jun 27 18:00:56.272: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:00:56.406: INFO: namespace downward-api-3345 deletion completed in 6.148874731s

• [SLOW TEST:8.366 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:00:56.406: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4998
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 27 18:00:56.563: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:00:58.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4998" for this suite.
Jun 27 18:01:36.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:01:36.775: INFO: namespace pods-4998 deletion completed in 38.153707259s

• [SLOW TEST:40.369 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:01:36.776: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-3607
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 27 18:01:36.940: INFO: Waiting up to 5m0s for pod "pod-91a81ab6-9a9d-4902-a838-bf9b84bff815" in namespace "emptydir-3607" to be "success or failure"
Jun 27 18:01:36.944: INFO: Pod "pod-91a81ab6-9a9d-4902-a838-bf9b84bff815": Phase="Pending", Reason="", readiness=false. Elapsed: 4.196953ms
Jun 27 18:01:38.948: INFO: Pod "pod-91a81ab6-9a9d-4902-a838-bf9b84bff815": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008079642s
STEP: Saw pod success
Jun 27 18:01:38.948: INFO: Pod "pod-91a81ab6-9a9d-4902-a838-bf9b84bff815" satisfied condition "success or failure"
Jun 27 18:01:38.952: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-mskvs pod pod-91a81ab6-9a9d-4902-a838-bf9b84bff815 container test-container: <nil>
STEP: delete the pod
Jun 27 18:01:38.976: INFO: Waiting for pod pod-91a81ab6-9a9d-4902-a838-bf9b84bff815 to disappear
Jun 27 18:01:38.979: INFO: Pod pod-91a81ab6-9a9d-4902-a838-bf9b84bff815 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:01:38.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3607" for this suite.
Jun 27 18:01:44.999: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:01:45.128: INFO: namespace emptydir-3607 deletion completed in 6.143722907s

• [SLOW TEST:8.352 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:01:45.129: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in custom-resource-definition-8018
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 27 18:01:45.289: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:01:46.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8018" for this suite.
Jun 27 18:01:52.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:01:52.539: INFO: namespace custom-resource-definition-8018 deletion completed in 6.153507745s

• [SLOW TEST:7.409 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:01:52.539: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6368
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 27 18:01:52.706: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e72f8a7c-34ef-474e-8416-88f40e3c93ee" in namespace "downward-api-6368" to be "success or failure"
Jun 27 18:01:52.711: INFO: Pod "downwardapi-volume-e72f8a7c-34ef-474e-8416-88f40e3c93ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046303ms
Jun 27 18:01:54.714: INFO: Pod "downwardapi-volume-e72f8a7c-34ef-474e-8416-88f40e3c93ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007790252s
STEP: Saw pod success
Jun 27 18:01:54.714: INFO: Pod "downwardapi-volume-e72f8a7c-34ef-474e-8416-88f40e3c93ee" satisfied condition "success or failure"
Jun 27 18:01:54.718: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod downwardapi-volume-e72f8a7c-34ef-474e-8416-88f40e3c93ee container client-container: <nil>
STEP: delete the pod
Jun 27 18:01:54.740: INFO: Waiting for pod downwardapi-volume-e72f8a7c-34ef-474e-8416-88f40e3c93ee to disappear
Jun 27 18:01:54.744: INFO: Pod downwardapi-volume-e72f8a7c-34ef-474e-8416-88f40e3c93ee no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:01:54.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6368" for this suite.
Jun 27 18:02:00.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:02:00.905: INFO: namespace downward-api-6368 deletion completed in 6.155264287s

• [SLOW TEST:8.366 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:02:00.905: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-4538
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 27 18:02:01.075: INFO: (0) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 8.422616ms)
Jun 27 18:02:01.080: INFO: (1) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.309774ms)
Jun 27 18:02:01.087: INFO: (2) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 6.372494ms)
Jun 27 18:02:01.092: INFO: (3) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.704434ms)
Jun 27 18:02:01.098: INFO: (4) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.562984ms)
Jun 27 18:02:01.103: INFO: (5) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.478404ms)
Jun 27 18:02:01.109: INFO: (6) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.565745ms)
Jun 27 18:02:01.115: INFO: (7) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.711134ms)
Jun 27 18:02:01.121: INFO: (8) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.644564ms)
Jun 27 18:02:01.126: INFO: (9) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.707955ms)
Jun 27 18:02:01.132: INFO: (10) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.764444ms)
Jun 27 18:02:01.138: INFO: (11) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 6.211285ms)
Jun 27 18:02:01.144: INFO: (12) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 6.012495ms)
Jun 27 18:02:01.150: INFO: (13) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.454274ms)
Jun 27 18:02:01.156: INFO: (14) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.645004ms)
Jun 27 18:02:01.162: INFO: (15) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.805305ms)
Jun 27 18:02:01.167: INFO: (16) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.154514ms)
Jun 27 18:02:01.172: INFO: (17) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.487854ms)
Jun 27 18:02:01.178: INFO: (18) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.406114ms)
Jun 27 18:02:01.183: INFO: (19) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.494334ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:02:01.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4538" for this suite.
Jun 27 18:02:07.202: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:02:07.337: INFO: namespace proxy-4538 deletion completed in 6.147794321s

• [SLOW TEST:6.432 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:02:07.337: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6034
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jun 27 18:02:10.040: INFO: Successfully updated pod "annotationupdate83d53e68-cb18-470e-9809-18fc35549410"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:02:14.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6034" for this suite.
Jun 27 18:02:36.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:02:36.219: INFO: namespace downward-api-6034 deletion completed in 22.146615936s

• [SLOW TEST:28.882 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:02:36.219: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-945
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jun 27 18:02:36.427: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 27 18:02:36.438: INFO: Waiting for terminating namespaces to be deleted...
Jun 27 18:02:36.441: INFO: 
Logging pods the kubelet thinks is on node talos-test-cluster-workers-84c9684cd-mskvs before test
Jun 27 18:02:36.451: INFO: calico-node-8pbvx from kube-system started at 2019-06-27 16:57:06 +0000 UTC (1 container statuses recorded)
Jun 27 18:02:36.451: INFO: 	Container calico-node ready: true, restart count 0
Jun 27 18:02:36.451: INFO: coredns-5c98db65d4-5xpvh from kube-system started at 2019-06-27 16:57:22 +0000 UTC (1 container statuses recorded)
Jun 27 18:02:36.451: INFO: 	Container coredns ready: true, restart count 0
Jun 27 18:02:36.451: INFO: sonobuoy-e2e-job-a038e5b194bb40cd from heptio-sonobuoy started at 2019-06-27 16:59:08 +0000 UTC (2 container statuses recorded)
Jun 27 18:02:36.451: INFO: 	Container e2e ready: true, restart count 0
Jun 27 18:02:36.451: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 27 18:02:36.451: INFO: kube-proxy-94s9p from kube-system started at 2019-06-27 16:57:11 +0000 UTC (1 container statuses recorded)
Jun 27 18:02:36.451: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 27 18:02:36.451: INFO: calico-kube-controllers-65c994fdb-6ss7n from kube-system started at 2019-06-27 16:57:22 +0000 UTC (1 container statuses recorded)
Jun 27 18:02:36.452: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 27 18:02:36.452: INFO: coredns-5c98db65d4-7xgq9 from kube-system started at 2019-06-27 16:57:22 +0000 UTC (1 container statuses recorded)
Jun 27 18:02:36.452: INFO: 	Container coredns ready: true, restart count 0
Jun 27 18:02:36.452: INFO: 
Logging pods the kubelet thinks is on node talos-test-cluster-workers-84c9684cd-nk6xg before test
Jun 27 18:02:36.463: INFO: calico-node-5knrj from kube-system started at 2019-06-27 16:57:02 +0000 UTC (1 container statuses recorded)
Jun 27 18:02:36.463: INFO: 	Container calico-node ready: true, restart count 0
Jun 27 18:02:36.463: INFO: kube-proxy-l4whf from kube-system started at 2019-06-27 16:57:07 +0000 UTC (1 container statuses recorded)
Jun 27 18:02:36.463: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 27 18:02:36.463: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-27 16:58:56 +0000 UTC (1 container statuses recorded)
Jun 27 18:02:36.463: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 27 18:02:36.463: INFO: 
Logging pods the kubelet thinks is on node talos-test-cluster-workers-84c9684cd-nzp9p before test
Jun 27 18:02:36.471: INFO: calico-node-w546x from kube-system started at 2019-06-27 16:57:04 +0000 UTC (1 container statuses recorded)
Jun 27 18:02:36.471: INFO: 	Container calico-node ready: true, restart count 0
Jun 27 18:02:36.471: INFO: kube-proxy-s2qrr from kube-system started at 2019-06-27 16:57:09 +0000 UTC (1 container statuses recorded)
Jun 27 18:02:36.471: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-25b36365-76d6-4735-9ffc-773ad7988e38 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-25b36365-76d6-4735-9ffc-773ad7988e38 off the node talos-test-cluster-workers-84c9684cd-mskvs
STEP: verifying the node doesn't have the label kubernetes.io/e2e-25b36365-76d6-4735-9ffc-773ad7988e38
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:02:40.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-945" for this suite.
Jun 27 18:02:50.584: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:02:50.716: INFO: namespace sched-pred-945 deletion completed in 10.148049813s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:14.497 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:02:50.717: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7940
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W0627 18:03:21.414938      18 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 27 18:03:21.414: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:03:21.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7940" for this suite.
Jun 27 18:03:27.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:03:27.603: INFO: namespace gc-7940 deletion completed in 6.184147619s

• [SLOW TEST:36.887 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:03:27.604: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-5431
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 27 18:03:27.825: INFO: Waiting up to 5m0s for pod "downwardapi-volume-094fea95-4efc-4095-a92b-6671646698ed" in namespace "projected-5431" to be "success or failure"
Jun 27 18:03:27.829: INFO: Pod "downwardapi-volume-094fea95-4efc-4095-a92b-6671646698ed": Phase="Pending", Reason="", readiness=false. Elapsed: 4.160493ms
Jun 27 18:03:29.833: INFO: Pod "downwardapi-volume-094fea95-4efc-4095-a92b-6671646698ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007858181s
STEP: Saw pod success
Jun 27 18:03:29.833: INFO: Pod "downwardapi-volume-094fea95-4efc-4095-a92b-6671646698ed" satisfied condition "success or failure"
Jun 27 18:03:29.837: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nzp9p pod downwardapi-volume-094fea95-4efc-4095-a92b-6671646698ed container client-container: <nil>
STEP: delete the pod
Jun 27 18:03:29.859: INFO: Waiting for pod downwardapi-volume-094fea95-4efc-4095-a92b-6671646698ed to disappear
Jun 27 18:03:29.863: INFO: Pod downwardapi-volume-094fea95-4efc-4095-a92b-6671646698ed no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:03:29.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5431" for this suite.
Jun 27 18:03:35.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:03:36.050: INFO: namespace projected-5431 deletion completed in 6.182311247s

• [SLOW TEST:8.446 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:03:36.051: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-7331
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-7331
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 27 18:03:36.257: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 27 18:03:54.370: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.177.217:8080/dial?request=hostName&protocol=http&host=10.244.177.214&port=8080&tries=1'] Namespace:pod-network-test-7331 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 27 18:03:54.370: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
Jun 27 18:03:54.542: INFO: Waiting for endpoints: map[]
Jun 27 18:03:54.546: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.177.217:8080/dial?request=hostName&protocol=http&host=10.244.187.104&port=8080&tries=1'] Namespace:pod-network-test-7331 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 27 18:03:54.546: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
Jun 27 18:03:54.703: INFO: Waiting for endpoints: map[]
Jun 27 18:03:54.707: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.177.217:8080/dial?request=hostName&protocol=http&host=10.244.20.237&port=8080&tries=1'] Namespace:pod-network-test-7331 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 27 18:03:54.707: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
Jun 27 18:03:54.876: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:03:54.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7331" for this suite.
Jun 27 18:04:16.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:04:17.029: INFO: namespace pod-network-test-7331 deletion completed in 22.148550758s

• [SLOW TEST:40.979 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:04:17.030: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-6758
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 27 18:04:21.236: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 27 18:04:21.241: INFO: Pod pod-with-prestop-http-hook still exists
Jun 27 18:04:23.241: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 27 18:04:23.247: INFO: Pod pod-with-prestop-http-hook still exists
Jun 27 18:04:25.241: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 27 18:04:25.245: INFO: Pod pod-with-prestop-http-hook still exists
Jun 27 18:04:27.242: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 27 18:04:27.245: INFO: Pod pod-with-prestop-http-hook still exists
Jun 27 18:04:29.241: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 27 18:04:29.249: INFO: Pod pod-with-prestop-http-hook still exists
Jun 27 18:04:31.242: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 27 18:04:31.246: INFO: Pod pod-with-prestop-http-hook still exists
Jun 27 18:04:33.241: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 27 18:04:33.246: INFO: Pod pod-with-prestop-http-hook still exists
Jun 27 18:04:35.241: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 27 18:04:35.245: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:04:35.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6758" for this suite.
Jun 27 18:04:57.271: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:04:57.405: INFO: namespace container-lifecycle-hook-6758 deletion completed in 22.146871417s

• [SLOW TEST:40.375 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:04:57.405: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-8528
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 27 18:04:57.589: INFO: Waiting up to 5m0s for pod "downwardapi-volume-259a5e70-bf6c-401e-a00b-235cd1b3893d" in namespace "downward-api-8528" to be "success or failure"
Jun 27 18:04:57.595: INFO: Pod "downwardapi-volume-259a5e70-bf6c-401e-a00b-235cd1b3893d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.452724ms
Jun 27 18:04:59.599: INFO: Pod "downwardapi-volume-259a5e70-bf6c-401e-a00b-235cd1b3893d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009777494s
STEP: Saw pod success
Jun 27 18:04:59.599: INFO: Pod "downwardapi-volume-259a5e70-bf6c-401e-a00b-235cd1b3893d" satisfied condition "success or failure"
Jun 27 18:04:59.603: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod downwardapi-volume-259a5e70-bf6c-401e-a00b-235cd1b3893d container client-container: <nil>
STEP: delete the pod
Jun 27 18:04:59.626: INFO: Waiting for pod downwardapi-volume-259a5e70-bf6c-401e-a00b-235cd1b3893d to disappear
Jun 27 18:04:59.630: INFO: Pod downwardapi-volume-259a5e70-bf6c-401e-a00b-235cd1b3893d no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:04:59.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8528" for this suite.
Jun 27 18:05:05.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:05:05.826: INFO: namespace downward-api-8528 deletion completed in 6.190283713s

• [SLOW TEST:8.420 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:05:05.827: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3255
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 27 18:05:06.007: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3f7d3320-5b87-4c84-bcdf-0c315d8d70c6" in namespace "projected-3255" to be "success or failure"
Jun 27 18:05:06.016: INFO: Pod "downwardapi-volume-3f7d3320-5b87-4c84-bcdf-0c315d8d70c6": Phase="Pending", Reason="", readiness=false. Elapsed: 8.766247ms
Jun 27 18:05:08.019: INFO: Pod "downwardapi-volume-3f7d3320-5b87-4c84-bcdf-0c315d8d70c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012718336s
STEP: Saw pod success
Jun 27 18:05:08.020: INFO: Pod "downwardapi-volume-3f7d3320-5b87-4c84-bcdf-0c315d8d70c6" satisfied condition "success or failure"
Jun 27 18:05:08.023: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod downwardapi-volume-3f7d3320-5b87-4c84-bcdf-0c315d8d70c6 container client-container: <nil>
STEP: delete the pod
Jun 27 18:05:08.045: INFO: Waiting for pod downwardapi-volume-3f7d3320-5b87-4c84-bcdf-0c315d8d70c6 to disappear
Jun 27 18:05:08.049: INFO: Pod downwardapi-volume-3f7d3320-5b87-4c84-bcdf-0c315d8d70c6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:05:08.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3255" for this suite.
Jun 27 18:05:14.070: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:05:14.200: INFO: namespace projected-3255 deletion completed in 6.146335819s

• [SLOW TEST:8.373 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:05:14.202: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename hostpath
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in hostpath-577
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Jun 27 18:05:14.373: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-577" to be "success or failure"
Jun 27 18:05:14.379: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 5.351384ms
Jun 27 18:05:16.382: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009164563s
STEP: Saw pod success
Jun 27 18:05:16.382: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jun 27 18:05:16.386: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nzp9p pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jun 27 18:05:16.413: INFO: Waiting for pod pod-host-path-test to disappear
Jun 27 18:05:16.417: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:05:16.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-577" for this suite.
Jun 27 18:05:22.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:05:22.572: INFO: namespace hostpath-577 deletion completed in 6.149889052s

• [SLOW TEST:8.370 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:05:22.573: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-2670
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 27 18:05:24.757: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:05:24.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2670" for this suite.
Jun 27 18:05:30.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:05:30.927: INFO: namespace container-runtime-2670 deletion completed in 6.144402788s

• [SLOW TEST:8.355 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:05:30.928: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in e2e-kubelet-etc-hosts-1757
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun 27 18:05:37.124: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1757 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 27 18:05:37.124: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
Jun 27 18:05:37.290: INFO: Exec stderr: ""
Jun 27 18:05:37.290: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1757 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 27 18:05:37.290: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
Jun 27 18:05:37.454: INFO: Exec stderr: ""
Jun 27 18:05:37.454: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1757 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 27 18:05:37.454: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
Jun 27 18:05:37.632: INFO: Exec stderr: ""
Jun 27 18:05:37.632: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1757 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 27 18:05:37.632: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
Jun 27 18:05:37.821: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun 27 18:05:37.821: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1757 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 27 18:05:37.821: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
Jun 27 18:05:38.013: INFO: Exec stderr: ""
Jun 27 18:05:38.013: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1757 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 27 18:05:38.013: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
Jun 27 18:05:38.203: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun 27 18:05:38.203: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1757 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 27 18:05:38.203: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
Jun 27 18:05:38.374: INFO: Exec stderr: ""
Jun 27 18:05:38.374: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1757 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 27 18:05:38.374: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
Jun 27 18:05:38.562: INFO: Exec stderr: ""
Jun 27 18:05:38.562: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1757 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 27 18:05:38.562: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
Jun 27 18:05:38.724: INFO: Exec stderr: ""
Jun 27 18:05:38.724: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1757 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 27 18:05:38.724: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
Jun 27 18:05:38.892: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:05:38.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-1757" for this suite.
Jun 27 18:06:22.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:06:23.042: INFO: namespace e2e-kubelet-etc-hosts-1757 deletion completed in 44.144618519s

• [SLOW TEST:52.113 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:06:23.042: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8826
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-065f5871-d354-4704-9bfe-c58dddd88fd4
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:06:23.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8826" for this suite.
Jun 27 18:06:29.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:06:29.407: INFO: namespace secrets-8826 deletion completed in 6.204271904s

• [SLOW TEST:6.365 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:06:29.408: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-7826
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jun 27 18:06:29.569: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 27 18:06:29.579: INFO: Waiting for terminating namespaces to be deleted...
Jun 27 18:06:29.583: INFO: 
Logging pods the kubelet thinks is on node talos-test-cluster-workers-84c9684cd-mskvs before test
Jun 27 18:06:29.592: INFO: calico-node-8pbvx from kube-system started at 2019-06-27 16:57:06 +0000 UTC (1 container statuses recorded)
Jun 27 18:06:29.592: INFO: 	Container calico-node ready: true, restart count 0
Jun 27 18:06:29.592: INFO: coredns-5c98db65d4-5xpvh from kube-system started at 2019-06-27 16:57:22 +0000 UTC (1 container statuses recorded)
Jun 27 18:06:29.593: INFO: 	Container coredns ready: true, restart count 0
Jun 27 18:06:29.593: INFO: sonobuoy-e2e-job-a038e5b194bb40cd from heptio-sonobuoy started at 2019-06-27 16:59:08 +0000 UTC (2 container statuses recorded)
Jun 27 18:06:29.593: INFO: 	Container e2e ready: true, restart count 0
Jun 27 18:06:29.593: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 27 18:06:29.593: INFO: kube-proxy-94s9p from kube-system started at 2019-06-27 16:57:11 +0000 UTC (1 container statuses recorded)
Jun 27 18:06:29.593: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 27 18:06:29.593: INFO: calico-kube-controllers-65c994fdb-6ss7n from kube-system started at 2019-06-27 16:57:22 +0000 UTC (1 container statuses recorded)
Jun 27 18:06:29.593: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 27 18:06:29.593: INFO: coredns-5c98db65d4-7xgq9 from kube-system started at 2019-06-27 16:57:22 +0000 UTC (1 container statuses recorded)
Jun 27 18:06:29.593: INFO: 	Container coredns ready: true, restart count 0
Jun 27 18:06:29.593: INFO: 
Logging pods the kubelet thinks is on node talos-test-cluster-workers-84c9684cd-nk6xg before test
Jun 27 18:06:29.602: INFO: calico-node-5knrj from kube-system started at 2019-06-27 16:57:02 +0000 UTC (1 container statuses recorded)
Jun 27 18:06:29.602: INFO: 	Container calico-node ready: true, restart count 0
Jun 27 18:06:29.602: INFO: kube-proxy-l4whf from kube-system started at 2019-06-27 16:57:07 +0000 UTC (1 container statuses recorded)
Jun 27 18:06:29.602: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 27 18:06:29.602: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-27 16:58:56 +0000 UTC (1 container statuses recorded)
Jun 27 18:06:29.602: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 27 18:06:29.602: INFO: 
Logging pods the kubelet thinks is on node talos-test-cluster-workers-84c9684cd-nzp9p before test
Jun 27 18:06:29.610: INFO: calico-node-w546x from kube-system started at 2019-06-27 16:57:04 +0000 UTC (1 container statuses recorded)
Jun 27 18:06:29.610: INFO: 	Container calico-node ready: true, restart count 0
Jun 27 18:06:29.610: INFO: kube-proxy-s2qrr from kube-system started at 2019-06-27 16:57:09 +0000 UTC (1 container statuses recorded)
Jun 27 18:06:29.610: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node talos-test-cluster-workers-84c9684cd-mskvs
STEP: verifying the node has the label node talos-test-cluster-workers-84c9684cd-nk6xg
STEP: verifying the node has the label node talos-test-cluster-workers-84c9684cd-nzp9p
Jun 27 18:06:29.683: INFO: Pod sonobuoy requesting resource cpu=0m on Node talos-test-cluster-workers-84c9684cd-nk6xg
Jun 27 18:06:29.683: INFO: Pod sonobuoy-e2e-job-a038e5b194bb40cd requesting resource cpu=0m on Node talos-test-cluster-workers-84c9684cd-mskvs
Jun 27 18:06:29.683: INFO: Pod calico-kube-controllers-65c994fdb-6ss7n requesting resource cpu=0m on Node talos-test-cluster-workers-84c9684cd-mskvs
Jun 27 18:06:29.683: INFO: Pod calico-node-5knrj requesting resource cpu=250m on Node talos-test-cluster-workers-84c9684cd-nk6xg
Jun 27 18:06:29.683: INFO: Pod calico-node-8pbvx requesting resource cpu=250m on Node talos-test-cluster-workers-84c9684cd-mskvs
Jun 27 18:06:29.683: INFO: Pod calico-node-w546x requesting resource cpu=250m on Node talos-test-cluster-workers-84c9684cd-nzp9p
Jun 27 18:06:29.683: INFO: Pod coredns-5c98db65d4-5xpvh requesting resource cpu=100m on Node talos-test-cluster-workers-84c9684cd-mskvs
Jun 27 18:06:29.683: INFO: Pod coredns-5c98db65d4-7xgq9 requesting resource cpu=100m on Node talos-test-cluster-workers-84c9684cd-mskvs
Jun 27 18:06:29.684: INFO: Pod kube-proxy-94s9p requesting resource cpu=0m on Node talos-test-cluster-workers-84c9684cd-mskvs
Jun 27 18:06:29.684: INFO: Pod kube-proxy-l4whf requesting resource cpu=0m on Node talos-test-cluster-workers-84c9684cd-nk6xg
Jun 27 18:06:29.684: INFO: Pod kube-proxy-s2qrr requesting resource cpu=0m on Node talos-test-cluster-workers-84c9684cd-nzp9p
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-50142080-5173-4d0b-9914-911dabb46be0.15ac20652c8f9bbd], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7826/filler-pod-50142080-5173-4d0b-9914-911dabb46be0 to talos-test-cluster-workers-84c9684cd-nzp9p]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-50142080-5173-4d0b-9914-911dabb46be0.15ac20658f503a4c], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-50142080-5173-4d0b-9914-911dabb46be0.15ac206591aa657d], Reason = [Created], Message = [Created container filler-pod-50142080-5173-4d0b-9914-911dabb46be0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-50142080-5173-4d0b-9914-911dabb46be0.15ac20659818f9e9], Reason = [Started], Message = [Started container filler-pod-50142080-5173-4d0b-9914-911dabb46be0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c828cbe3-6b29-41d6-91d7-956831f25e38.15ac20652eb7468d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7826/filler-pod-c828cbe3-6b29-41d6-91d7-956831f25e38 to talos-test-cluster-workers-84c9684cd-mskvs]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c828cbe3-6b29-41d6-91d7-956831f25e38.15ac20655aafb6ae], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c828cbe3-6b29-41d6-91d7-956831f25e38.15ac20655d791349], Reason = [Created], Message = [Created container filler-pod-c828cbe3-6b29-41d6-91d7-956831f25e38]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c828cbe3-6b29-41d6-91d7-956831f25e38.15ac2065650d1d28], Reason = [Started], Message = [Started container filler-pod-c828cbe3-6b29-41d6-91d7-956831f25e38]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e653db58-c404-48dc-aede-57cf94e5ee4d.15ac20652f306bdb], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7826/filler-pod-e653db58-c404-48dc-aede-57cf94e5ee4d to talos-test-cluster-workers-84c9684cd-nk6xg]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e653db58-c404-48dc-aede-57cf94e5ee4d.15ac206559f00a74], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e653db58-c404-48dc-aede-57cf94e5ee4d.15ac20655c79e63e], Reason = [Created], Message = [Created container filler-pod-e653db58-c404-48dc-aede-57cf94e5ee4d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e653db58-c404-48dc-aede-57cf94e5ee4d.15ac206563dbea92], Reason = [Started], Message = [Started container filler-pod-e653db58-c404-48dc-aede-57cf94e5ee4d]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15ac2065a8172f52], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 Insufficient cpu.]
STEP: removing the label node off the node talos-test-cluster-workers-84c9684cd-mskvs
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node talos-test-cluster-workers-84c9684cd-nk6xg
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node talos-test-cluster-workers-84c9684cd-nzp9p
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:06:32.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7826" for this suite.
Jun 27 18:06:38.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:06:38.996: INFO: namespace sched-pred-7826 deletion completed in 6.145581959s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:9.588 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:06:38.997: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1644
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jun 27 18:06:39.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 create -f - --namespace=kubectl-1644'
Jun 27 18:06:39.614: INFO: stderr: ""
Jun 27 18:06:39.614: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 27 18:06:40.619: INFO: Selector matched 1 pods for map[app:redis]
Jun 27 18:06:40.619: INFO: Found 0 / 1
Jun 27 18:06:41.619: INFO: Selector matched 1 pods for map[app:redis]
Jun 27 18:06:41.619: INFO: Found 1 / 1
Jun 27 18:06:41.619: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun 27 18:06:41.623: INFO: Selector matched 1 pods for map[app:redis]
Jun 27 18:06:41.623: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 27 18:06:41.623: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 patch pod redis-master-ptx6t --namespace=kubectl-1644 -p {"metadata":{"annotations":{"x":"y"}}}'
Jun 27 18:06:41.820: INFO: stderr: ""
Jun 27 18:06:41.820: INFO: stdout: "pod/redis-master-ptx6t patched\n"
STEP: checking annotations
Jun 27 18:06:41.825: INFO: Selector matched 1 pods for map[app:redis]
Jun 27 18:06:41.825: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:06:41.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1644" for this suite.
Jun 27 18:07:03.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:07:03.971: INFO: namespace kubectl-1644 deletion completed in 22.141153372s

• [SLOW TEST:24.974 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:07:03.971: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename gc
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in gc-7144
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 27 18:07:04.202: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"d368f530-d4c0-44b8-8cf1-ed59b989ef9a", Controller:(*bool)(0xc002ac3896), BlockOwnerDeletion:(*bool)(0xc002ac3897)}}
Jun 27 18:07:04.219: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"ffb7e4e4-af53-4f46-bb27-38e8f607bb8b", Controller:(*bool)(0xc002d929e6), BlockOwnerDeletion:(*bool)(0xc002d929e7)}}
Jun 27 18:07:04.226: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"5b39ea01-a83c-4bb3-83e3-a39ebfac7fcb", Controller:(*bool)(0xc002d92bd6), BlockOwnerDeletion:(*bool)(0xc002d92bd7)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:07:09.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7144" for this suite.
Jun 27 18:07:15.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:07:15.390: INFO: namespace gc-7144 deletion completed in 6.14698499s

• [SLOW TEST:11.419 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:07:15.391: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-8356
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 27 18:07:15.559: INFO: Waiting up to 5m0s for pod "downwardapi-volume-729aa8ad-83d5-407b-a191-ba666bd7130c" in namespace "projected-8356" to be "success or failure"
Jun 27 18:07:15.568: INFO: Pod "downwardapi-volume-729aa8ad-83d5-407b-a191-ba666bd7130c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.090697ms
Jun 27 18:07:17.572: INFO: Pod "downwardapi-volume-729aa8ad-83d5-407b-a191-ba666bd7130c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013233816s
STEP: Saw pod success
Jun 27 18:07:17.572: INFO: Pod "downwardapi-volume-729aa8ad-83d5-407b-a191-ba666bd7130c" satisfied condition "success or failure"
Jun 27 18:07:17.576: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nzp9p pod downwardapi-volume-729aa8ad-83d5-407b-a191-ba666bd7130c container client-container: <nil>
STEP: delete the pod
Jun 27 18:07:17.595: INFO: Waiting for pod downwardapi-volume-729aa8ad-83d5-407b-a191-ba666bd7130c to disappear
Jun 27 18:07:17.601: INFO: Pod downwardapi-volume-729aa8ad-83d5-407b-a191-ba666bd7130c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:07:17.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8356" for this suite.
Jun 27 18:07:23.621: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:07:23.751: INFO: namespace projected-8356 deletion completed in 6.144043268s

• [SLOW TEST:8.360 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:07:23.752: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename svc-latency
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svc-latency-1222
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1222
I0627 18:07:23.967041      18 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1222, replica count: 1
I0627 18:07:25.017579      18 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0627 18:07:26.017796      18 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 27 18:07:26.131: INFO: Created: latency-svc-lmb45
Jun 27 18:07:26.138: INFO: Got endpoints: latency-svc-lmb45 [20.840476ms]
Jun 27 18:07:26.171: INFO: Created: latency-svc-nztll
Jun 27 18:07:26.191: INFO: Got endpoints: latency-svc-nztll [52.24127ms]
Jun 27 18:07:26.199: INFO: Created: latency-svc-nj896
Jun 27 18:07:26.199: INFO: Got endpoints: latency-svc-nj896 [60.155746ms]
Jun 27 18:07:26.214: INFO: Created: latency-svc-tt755
Jun 27 18:07:26.215: INFO: Got endpoints: latency-svc-tt755 [76.578949ms]
Jun 27 18:07:26.241: INFO: Created: latency-svc-fcn7w
Jun 27 18:07:26.242: INFO: Got endpoints: latency-svc-fcn7w [43.205073ms]
Jun 27 18:07:26.264: INFO: Created: latency-svc-hggbd
Jun 27 18:07:26.266: INFO: Got endpoints: latency-svc-hggbd [127.464008ms]
Jun 27 18:07:26.278: INFO: Created: latency-svc-hwrgq
Jun 27 18:07:26.289: INFO: Got endpoints: latency-svc-hwrgq [150.122735ms]
Jun 27 18:07:26.299: INFO: Created: latency-svc-6926j
Jun 27 18:07:26.304: INFO: Got endpoints: latency-svc-6926j [164.501755ms]
Jun 27 18:07:26.326: INFO: Created: latency-svc-6lvc5
Jun 27 18:07:26.335: INFO: Got endpoints: latency-svc-6lvc5 [196.165989ms]
Jun 27 18:07:26.372: INFO: Created: latency-svc-kd7vp
Jun 27 18:07:26.380: INFO: Got endpoints: latency-svc-kd7vp [241.083094ms]
Jun 27 18:07:26.397: INFO: Created: latency-svc-trvm4
Jun 27 18:07:26.404: INFO: Got endpoints: latency-svc-trvm4 [265.123072ms]
Jun 27 18:07:26.430: INFO: Created: latency-svc-7c9cj
Jun 27 18:07:26.439: INFO: Created: latency-svc-2whmk
Jun 27 18:07:26.440: INFO: Got endpoints: latency-svc-7c9cj [300.976149ms]
Jun 27 18:07:26.446: INFO: Got endpoints: latency-svc-2whmk [307.162024ms]
Jun 27 18:07:26.466: INFO: Created: latency-svc-s9rhf
Jun 27 18:07:26.470: INFO: Got endpoints: latency-svc-s9rhf [331.126532ms]
Jun 27 18:07:26.485: INFO: Created: latency-svc-l2ngz
Jun 27 18:07:26.490: INFO: Got endpoints: latency-svc-l2ngz [350.794167ms]
Jun 27 18:07:26.507: INFO: Created: latency-svc-525zk
Jun 27 18:07:26.510: INFO: Got endpoints: latency-svc-525zk [370.883593ms]
Jun 27 18:07:26.529: INFO: Created: latency-svc-6lmtv
Jun 27 18:07:26.533: INFO: Got endpoints: latency-svc-6lmtv [393.99352ms]
Jun 27 18:07:26.555: INFO: Created: latency-svc-qtwgm
Jun 27 18:07:26.559: INFO: Got endpoints: latency-svc-qtwgm [367.936901ms]
Jun 27 18:07:26.568: INFO: Created: latency-svc-24dmq
Jun 27 18:07:26.581: INFO: Got endpoints: latency-svc-24dmq [365.456819ms]
Jun 27 18:07:26.589: INFO: Created: latency-svc-vcz7v
Jun 27 18:07:26.592: INFO: Got endpoints: latency-svc-vcz7v [349.813147ms]
Jun 27 18:07:26.645: INFO: Created: latency-svc-c66gl
Jun 27 18:07:26.645: INFO: Got endpoints: latency-svc-c66gl [378.923769ms]
Jun 27 18:07:26.660: INFO: Created: latency-svc-7pp7c
Jun 27 18:07:26.705: INFO: Got endpoints: latency-svc-7pp7c [415.864607ms]
Jun 27 18:07:26.748: INFO: Created: latency-svc-22cfw
Jun 27 18:07:26.759: INFO: Got endpoints: latency-svc-22cfw [455.215168ms]
Jun 27 18:07:26.772: INFO: Created: latency-svc-vwzvd
Jun 27 18:07:26.792: INFO: Got endpoints: latency-svc-vwzvd [457.254729ms]
Jun 27 18:07:26.818: INFO: Created: latency-svc-ph6q5
Jun 27 18:07:26.830: INFO: Got endpoints: latency-svc-ph6q5 [449.727003ms]
Jun 27 18:07:26.849: INFO: Created: latency-svc-vz8hv
Jun 27 18:07:26.863: INFO: Got endpoints: latency-svc-vz8hv [459.13431ms]
Jun 27 18:07:26.957: INFO: Created: latency-svc-pqlq7
Jun 27 18:07:26.957: INFO: Got endpoints: latency-svc-pqlq7 [516.739825ms]
Jun 27 18:07:26.967: INFO: Created: latency-svc-t64xz
Jun 27 18:07:26.980: INFO: Got endpoints: latency-svc-t64xz [533.599737ms]
Jun 27 18:07:26.992: INFO: Created: latency-svc-bgrtn
Jun 27 18:07:26.992: INFO: Got endpoints: latency-svc-bgrtn [521.822878ms]
Jun 27 18:07:27.006: INFO: Created: latency-svc-vkk5j
Jun 27 18:07:27.032: INFO: Created: latency-svc-zpg6m
Jun 27 18:07:27.032: INFO: Got endpoints: latency-svc-vkk5j [541.938734ms]
Jun 27 18:07:27.036: INFO: Got endpoints: latency-svc-zpg6m [525.716651ms]
Jun 27 18:07:27.060: INFO: Created: latency-svc-5z8cg
Jun 27 18:07:27.063: INFO: Got endpoints: latency-svc-5z8cg [529.706005ms]
Jun 27 18:07:27.087: INFO: Created: latency-svc-hf75r
Jun 27 18:07:27.096: INFO: Got endpoints: latency-svc-hf75r [537.20857ms]
Jun 27 18:07:27.111: INFO: Created: latency-svc-qv9c6
Jun 27 18:07:27.113: INFO: Got endpoints: latency-svc-qv9c6 [531.871665ms]
Jun 27 18:07:27.124: INFO: Created: latency-svc-j2wqh
Jun 27 18:07:27.133: INFO: Got endpoints: latency-svc-j2wqh [540.718343ms]
Jun 27 18:07:27.154: INFO: Created: latency-svc-86xgp
Jun 27 18:07:27.161: INFO: Got endpoints: latency-svc-86xgp [515.888233ms]
Jun 27 18:07:27.175: INFO: Created: latency-svc-fc8jt
Jun 27 18:07:27.179: INFO: Got endpoints: latency-svc-fc8jt [474.821772ms]
Jun 27 18:07:27.192: INFO: Created: latency-svc-p85gn
Jun 27 18:07:27.195: INFO: Got endpoints: latency-svc-p85gn [435.901472ms]
Jun 27 18:07:27.213: INFO: Created: latency-svc-66tgj
Jun 27 18:07:27.223: INFO: Got endpoints: latency-svc-66tgj [430.683249ms]
Jun 27 18:07:27.257: INFO: Created: latency-svc-swbgb
Jun 27 18:07:27.259: INFO: Got endpoints: latency-svc-swbgb [429.353507ms]
Jun 27 18:07:27.282: INFO: Created: latency-svc-kf7zq
Jun 27 18:07:27.293: INFO: Got endpoints: latency-svc-kf7zq [429.811458ms]
Jun 27 18:07:27.316: INFO: Created: latency-svc-42c6h
Jun 27 18:07:27.319: INFO: Got endpoints: latency-svc-42c6h [362.531076ms]
Jun 27 18:07:27.341: INFO: Created: latency-svc-vlstj
Jun 27 18:07:27.344: INFO: Got endpoints: latency-svc-vlstj [364.351578ms]
Jun 27 18:07:27.356: INFO: Created: latency-svc-wh5kq
Jun 27 18:07:27.360: INFO: Got endpoints: latency-svc-wh5kq [368.333772ms]
Jun 27 18:07:27.397: INFO: Created: latency-svc-sdf46
Jun 27 18:07:27.405: INFO: Got endpoints: latency-svc-sdf46 [373.167234ms]
Jun 27 18:07:27.420: INFO: Created: latency-svc-bn256
Jun 27 18:07:27.426: INFO: Got endpoints: latency-svc-bn256 [390.544188ms]
Jun 27 18:07:27.448: INFO: Created: latency-svc-djczr
Jun 27 18:07:27.455: INFO: Got endpoints: latency-svc-djczr [391.800138ms]
Jun 27 18:07:27.463: INFO: Created: latency-svc-4kk77
Jun 27 18:07:27.467: INFO: Got endpoints: latency-svc-4kk77 [370.910773ms]
Jun 27 18:07:27.483: INFO: Created: latency-svc-p2w26
Jun 27 18:07:27.493: INFO: Got endpoints: latency-svc-p2w26 [380.356161ms]
Jun 27 18:07:27.519: INFO: Created: latency-svc-8zh2z
Jun 27 18:07:27.530: INFO: Got endpoints: latency-svc-8zh2z [397.096123ms]
Jun 27 18:07:27.548: INFO: Created: latency-svc-knls4
Jun 27 18:07:27.556: INFO: Got endpoints: latency-svc-knls4 [394.645941ms]
Jun 27 18:07:27.587: INFO: Created: latency-svc-r9j9d
Jun 27 18:07:27.587: INFO: Got endpoints: latency-svc-r9j9d [407.218661ms]
Jun 27 18:07:27.604: INFO: Created: latency-svc-rbz89
Jun 27 18:07:27.607: INFO: Got endpoints: latency-svc-rbz89 [411.922865ms]
Jun 27 18:07:27.631: INFO: Created: latency-svc-hss5k
Jun 27 18:07:27.636: INFO: Got endpoints: latency-svc-hss5k [412.631805ms]
Jun 27 18:07:27.659: INFO: Created: latency-svc-djxt6
Jun 27 18:07:27.670: INFO: Got endpoints: latency-svc-djxt6 [410.680934ms]
Jun 27 18:07:27.688: INFO: Created: latency-svc-xvt25
Jun 27 18:07:27.692: INFO: Got endpoints: latency-svc-xvt25 [398.452204ms]
Jun 27 18:07:27.708: INFO: Created: latency-svc-gzrfr
Jun 27 18:07:27.716: INFO: Got endpoints: latency-svc-gzrfr [396.168943ms]
Jun 27 18:07:27.740: INFO: Created: latency-svc-kzmtb
Jun 27 18:07:27.751: INFO: Got endpoints: latency-svc-kzmtb [406.703491ms]
Jun 27 18:07:27.781: INFO: Created: latency-svc-j7x8h
Jun 27 18:07:27.781: INFO: Got endpoints: latency-svc-j7x8h [420.54456ms]
Jun 27 18:07:27.829: INFO: Created: latency-svc-rhdj2
Jun 27 18:07:27.829: INFO: Got endpoints: latency-svc-rhdj2 [423.981374ms]
Jun 27 18:07:27.837: INFO: Created: latency-svc-75b22
Jun 27 18:07:27.852: INFO: Created: latency-svc-nsrwq
Jun 27 18:07:27.853: INFO: Got endpoints: latency-svc-75b22 [426.871806ms]
Jun 27 18:07:27.863: INFO: Got endpoints: latency-svc-nsrwq [408.198801ms]
Jun 27 18:07:27.875: INFO: Created: latency-svc-zhzc8
Jun 27 18:07:27.888: INFO: Got endpoints: latency-svc-zhzc8 [420.462561ms]
Jun 27 18:07:27.897: INFO: Created: latency-svc-kmkl6
Jun 27 18:07:27.899: INFO: Got endpoints: latency-svc-kmkl6 [405.671199ms]
Jun 27 18:07:27.921: INFO: Created: latency-svc-n2lkb
Jun 27 18:07:27.926: INFO: Got endpoints: latency-svc-n2lkb [395.639092ms]
Jun 27 18:07:27.944: INFO: Created: latency-svc-gn9zn
Jun 27 18:07:27.960: INFO: Got endpoints: latency-svc-gn9zn [404.496698ms]
Jun 27 18:07:27.970: INFO: Created: latency-svc-btpqb
Jun 27 18:07:27.982: INFO: Created: latency-svc-bktqt
Jun 27 18:07:27.988: INFO: Got endpoints: latency-svc-btpqb [401.195356ms]
Jun 27 18:07:28.005: INFO: Created: latency-svc-g9m5p
Jun 27 18:07:28.021: INFO: Created: latency-svc-7gdqc
Jun 27 18:07:28.043: INFO: Created: latency-svc-ttkmc
Jun 27 18:07:28.047: INFO: Got endpoints: latency-svc-bktqt [439.830385ms]
Jun 27 18:07:28.059: INFO: Created: latency-svc-bzm7v
Jun 27 18:07:28.089: INFO: Got endpoints: latency-svc-g9m5p [452.893015ms]
Jun 27 18:07:28.096: INFO: Created: latency-svc-hd844
Jun 27 18:07:28.130: INFO: Created: latency-svc-4z4fm
Jun 27 18:07:28.141: INFO: Got endpoints: latency-svc-7gdqc [470.453969ms]
Jun 27 18:07:28.151: INFO: Created: latency-svc-gwgp4
Jun 27 18:07:28.168: INFO: Created: latency-svc-jdfjk
Jun 27 18:07:28.189: INFO: Got endpoints: latency-svc-ttkmc [497.40449ms]
Jun 27 18:07:28.197: INFO: Created: latency-svc-hrzl7
Jun 27 18:07:28.210: INFO: Created: latency-svc-pscpz
Jun 27 18:07:28.230: INFO: Created: latency-svc-xtpds
Jun 27 18:07:28.239: INFO: Got endpoints: latency-svc-bzm7v [523.164229ms]
Jun 27 18:07:28.258: INFO: Created: latency-svc-qpb5f
Jun 27 18:07:28.275: INFO: Created: latency-svc-zflxs
Jun 27 18:07:28.298: INFO: Got endpoints: latency-svc-hd844 [547.172197ms]
Jun 27 18:07:28.302: INFO: Created: latency-svc-dnmc8
Jun 27 18:07:28.323: INFO: Created: latency-svc-fcdrc
Jun 27 18:07:28.340: INFO: Got endpoints: latency-svc-4z4fm [558.952117ms]
Jun 27 18:07:28.342: INFO: Created: latency-svc-g5mlh
Jun 27 18:07:28.369: INFO: Created: latency-svc-2tq6d
Jun 27 18:07:28.388: INFO: Created: latency-svc-sckfr
Jun 27 18:07:28.391: INFO: Got endpoints: latency-svc-gwgp4 [561.627098ms]
Jun 27 18:07:28.415: INFO: Created: latency-svc-j87tr
Jun 27 18:07:28.438: INFO: Created: latency-svc-khlgc
Jun 27 18:07:28.441: INFO: Got endpoints: latency-svc-jdfjk [587.801858ms]
Jun 27 18:07:28.461: INFO: Created: latency-svc-smzzq
Jun 27 18:07:28.491: INFO: Got endpoints: latency-svc-hrzl7 [628.157039ms]
Jun 27 18:07:28.504: INFO: Created: latency-svc-6p4xq
Jun 27 18:07:28.533: INFO: Created: latency-svc-npqml
Jun 27 18:07:28.540: INFO: Got endpoints: latency-svc-pscpz [652.574138ms]
Jun 27 18:07:28.555: INFO: Created: latency-svc-fn6qb
Jun 27 18:07:28.563: INFO: Created: latency-svc-clbc7
Jun 27 18:07:28.590: INFO: Got endpoints: latency-svc-xtpds [690.545247ms]
Jun 27 18:07:28.611: INFO: Created: latency-svc-nwz5h
Jun 27 18:07:28.638: INFO: Got endpoints: latency-svc-qpb5f [712.531703ms]
Jun 27 18:07:28.664: INFO: Created: latency-svc-smt24
Jun 27 18:07:28.688: INFO: Got endpoints: latency-svc-zflxs [727.235035ms]
Jun 27 18:07:28.714: INFO: Created: latency-svc-c6zkb
Jun 27 18:07:28.739: INFO: Got endpoints: latency-svc-dnmc8 [751.306573ms]
Jun 27 18:07:28.761: INFO: Created: latency-svc-j22r5
Jun 27 18:07:28.790: INFO: Got endpoints: latency-svc-fcdrc [742.642227ms]
Jun 27 18:07:28.815: INFO: Created: latency-svc-cv84q
Jun 27 18:07:28.837: INFO: Got endpoints: latency-svc-g5mlh [747.817701ms]
Jun 27 18:07:28.860: INFO: Created: latency-svc-br9gm
Jun 27 18:07:28.888: INFO: Got endpoints: latency-svc-2tq6d [747.11807ms]
Jun 27 18:07:28.908: INFO: Created: latency-svc-wnp7l
Jun 27 18:07:28.937: INFO: Got endpoints: latency-svc-sckfr [747.9249ms]
Jun 27 18:07:28.960: INFO: Created: latency-svc-876q9
Jun 27 18:07:28.988: INFO: Got endpoints: latency-svc-j87tr [748.687601ms]
Jun 27 18:07:29.011: INFO: Created: latency-svc-95vss
Jun 27 18:07:29.039: INFO: Got endpoints: latency-svc-khlgc [740.918585ms]
Jun 27 18:07:29.060: INFO: Created: latency-svc-7lb74
Jun 27 18:07:29.087: INFO: Got endpoints: latency-svc-smzzq [746.62684ms]
Jun 27 18:07:29.111: INFO: Created: latency-svc-dsbpg
Jun 27 18:07:29.138: INFO: Got endpoints: latency-svc-6p4xq [746.96408ms]
Jun 27 18:07:29.158: INFO: Created: latency-svc-fj7rj
Jun 27 18:07:29.188: INFO: Got endpoints: latency-svc-npqml [746.063399ms]
Jun 27 18:07:29.215: INFO: Created: latency-svc-7dl78
Jun 27 18:07:29.243: INFO: Got endpoints: latency-svc-fn6qb [751.632074ms]
Jun 27 18:07:29.275: INFO: Created: latency-svc-mngnb
Jun 27 18:07:29.292: INFO: Got endpoints: latency-svc-clbc7 [751.062353ms]
Jun 27 18:07:29.344: INFO: Got endpoints: latency-svc-nwz5h [754.851526ms]
Jun 27 18:07:29.362: INFO: Created: latency-svc-mlhn6
Jun 27 18:07:29.389: INFO: Got endpoints: latency-svc-smt24 [750.605693ms]
Jun 27 18:07:29.410: INFO: Created: latency-svc-cl95j
Jun 27 18:07:29.446: INFO: Got endpoints: latency-svc-c6zkb [758.815589ms]
Jun 27 18:07:29.452: INFO: Created: latency-svc-txrms
Jun 27 18:07:29.474: INFO: Created: latency-svc-r8lnt
Jun 27 18:07:29.488: INFO: Got endpoints: latency-svc-j22r5 [749.160192ms]
Jun 27 18:07:29.510: INFO: Created: latency-svc-vwl8g
Jun 27 18:07:29.538: INFO: Got endpoints: latency-svc-cv84q [748.003571ms]
Jun 27 18:07:29.560: INFO: Created: latency-svc-znzsj
Jun 27 18:07:29.588: INFO: Got endpoints: latency-svc-br9gm [751.203043ms]
Jun 27 18:07:29.610: INFO: Created: latency-svc-cdkkp
Jun 27 18:07:29.638: INFO: Got endpoints: latency-svc-wnp7l [749.394112ms]
Jun 27 18:07:29.659: INFO: Created: latency-svc-ts6c7
Jun 27 18:07:29.688: INFO: Got endpoints: latency-svc-876q9 [750.397492ms]
Jun 27 18:07:29.750: INFO: Got endpoints: latency-svc-95vss [762.121532ms]
Jun 27 18:07:29.755: INFO: Created: latency-svc-k78wr
Jun 27 18:07:29.780: INFO: Created: latency-svc-89vdk
Jun 27 18:07:29.791: INFO: Got endpoints: latency-svc-7lb74 [751.183414ms]
Jun 27 18:07:29.832: INFO: Created: latency-svc-nxsqg
Jun 27 18:07:29.838: INFO: Got endpoints: latency-svc-dsbpg [751.301813ms]
Jun 27 18:07:29.896: INFO: Got endpoints: latency-svc-fj7rj [757.519768ms]
Jun 27 18:07:29.962: INFO: Got endpoints: latency-svc-7dl78 [773.77552ms]
Jun 27 18:07:29.993: INFO: Created: latency-svc-8zpfh
Jun 27 18:07:29.995: INFO: Got endpoints: latency-svc-mngnb [751.509263ms]
Jun 27 18:07:30.011: INFO: Created: latency-svc-9fb6t
Jun 27 18:07:30.033: INFO: Created: latency-svc-7zg7r
Jun 27 18:07:30.042: INFO: Got endpoints: latency-svc-mlhn6 [750.227622ms]
Jun 27 18:07:30.054: INFO: Created: latency-svc-jwz57
Jun 27 18:07:30.071: INFO: Created: latency-svc-vbktj
Jun 27 18:07:30.088: INFO: Got endpoints: latency-svc-cl95j [743.497588ms]
Jun 27 18:07:30.110: INFO: Created: latency-svc-m5gk5
Jun 27 18:07:30.137: INFO: Got endpoints: latency-svc-txrms [748.394321ms]
Jun 27 18:07:30.169: INFO: Created: latency-svc-qbtff
Jun 27 18:07:30.189: INFO: Got endpoints: latency-svc-r8lnt [742.499986ms]
Jun 27 18:07:30.211: INFO: Created: latency-svc-d96wh
Jun 27 18:07:30.237: INFO: Got endpoints: latency-svc-vwl8g [748.818681ms]
Jun 27 18:07:30.256: INFO: Created: latency-svc-59ltt
Jun 27 18:07:30.288: INFO: Got endpoints: latency-svc-znzsj [750.158192ms]
Jun 27 18:07:30.309: INFO: Created: latency-svc-8vqp7
Jun 27 18:07:30.340: INFO: Got endpoints: latency-svc-cdkkp [752.044984ms]
Jun 27 18:07:30.369: INFO: Created: latency-svc-zmv4p
Jun 27 18:07:30.389: INFO: Got endpoints: latency-svc-ts6c7 [751.562623ms]
Jun 27 18:07:30.409: INFO: Created: latency-svc-vxsrs
Jun 27 18:07:30.438: INFO: Got endpoints: latency-svc-k78wr [750.158732ms]
Jun 27 18:07:30.462: INFO: Created: latency-svc-fqpn9
Jun 27 18:07:30.488: INFO: Got endpoints: latency-svc-89vdk [738.106473ms]
Jun 27 18:07:30.508: INFO: Created: latency-svc-s8jdp
Jun 27 18:07:30.538: INFO: Got endpoints: latency-svc-nxsqg [747.00463ms]
Jun 27 18:07:30.562: INFO: Created: latency-svc-7h8hv
Jun 27 18:07:30.589: INFO: Got endpoints: latency-svc-8zpfh [750.811793ms]
Jun 27 18:07:30.610: INFO: Created: latency-svc-8ldkv
Jun 27 18:07:30.638: INFO: Got endpoints: latency-svc-9fb6t [742.371146ms]
Jun 27 18:07:30.664: INFO: Created: latency-svc-v7txn
Jun 27 18:07:30.688: INFO: Got endpoints: latency-svc-7zg7r [725.520964ms]
Jun 27 18:07:30.709: INFO: Created: latency-svc-7l4bs
Jun 27 18:07:30.738: INFO: Got endpoints: latency-svc-jwz57 [743.431957ms]
Jun 27 18:07:30.761: INFO: Created: latency-svc-7fzd9
Jun 27 18:07:30.787: INFO: Got endpoints: latency-svc-vbktj [745.060529ms]
Jun 27 18:07:30.812: INFO: Created: latency-svc-x4z8m
Jun 27 18:07:30.839: INFO: Got endpoints: latency-svc-m5gk5 [750.998843ms]
Jun 27 18:07:30.863: INFO: Created: latency-svc-rvpw6
Jun 27 18:07:30.887: INFO: Got endpoints: latency-svc-qbtff [749.737952ms]
Jun 27 18:07:30.910: INFO: Created: latency-svc-kk2sx
Jun 27 18:07:30.939: INFO: Got endpoints: latency-svc-d96wh [750.240182ms]
Jun 27 18:07:30.959: INFO: Created: latency-svc-pzcmt
Jun 27 18:07:30.988: INFO: Got endpoints: latency-svc-59ltt [750.498883ms]
Jun 27 18:07:31.010: INFO: Created: latency-svc-4q9ln
Jun 27 18:07:31.038: INFO: Got endpoints: latency-svc-8vqp7 [750.212032ms]
Jun 27 18:07:31.059: INFO: Created: latency-svc-xx5nw
Jun 27 18:07:31.087: INFO: Got endpoints: latency-svc-zmv4p [747.21775ms]
Jun 27 18:07:31.120: INFO: Created: latency-svc-qrxp2
Jun 27 18:07:31.139: INFO: Got endpoints: latency-svc-vxsrs [749.796282ms]
Jun 27 18:07:31.157: INFO: Created: latency-svc-d66w5
Jun 27 18:07:31.187: INFO: Got endpoints: latency-svc-fqpn9 [748.967072ms]
Jun 27 18:07:31.205: INFO: Created: latency-svc-gvp5f
Jun 27 18:07:31.238: INFO: Got endpoints: latency-svc-s8jdp [749.904882ms]
Jun 27 18:07:31.260: INFO: Created: latency-svc-hzngj
Jun 27 18:07:31.289: INFO: Got endpoints: latency-svc-7h8hv [751.063813ms]
Jun 27 18:07:31.315: INFO: Created: latency-svc-lgv46
Jun 27 18:07:31.339: INFO: Got endpoints: latency-svc-8ldkv [749.785002ms]
Jun 27 18:07:31.362: INFO: Created: latency-svc-p8pfg
Jun 27 18:07:31.388: INFO: Got endpoints: latency-svc-v7txn [749.572822ms]
Jun 27 18:07:31.410: INFO: Created: latency-svc-ndch5
Jun 27 18:07:31.439: INFO: Got endpoints: latency-svc-7l4bs [751.335903ms]
Jun 27 18:07:31.460: INFO: Created: latency-svc-wzks9
Jun 27 18:07:31.489: INFO: Got endpoints: latency-svc-7fzd9 [751.043873ms]
Jun 27 18:07:31.510: INFO: Created: latency-svc-n6fr2
Jun 27 18:07:31.538: INFO: Got endpoints: latency-svc-x4z8m [750.746933ms]
Jun 27 18:07:31.556: INFO: Created: latency-svc-sxv4m
Jun 27 18:07:31.590: INFO: Got endpoints: latency-svc-rvpw6 [750.651452ms]
Jun 27 18:07:31.609: INFO: Created: latency-svc-x5sjl
Jun 27 18:07:31.638: INFO: Got endpoints: latency-svc-kk2sx [751.288093ms]
Jun 27 18:07:31.658: INFO: Created: latency-svc-sltwd
Jun 27 18:07:31.688: INFO: Got endpoints: latency-svc-pzcmt [748.970331ms]
Jun 27 18:07:31.709: INFO: Created: latency-svc-d2vxk
Jun 27 18:07:31.739: INFO: Got endpoints: latency-svc-4q9ln [751.158763ms]
Jun 27 18:07:31.757: INFO: Created: latency-svc-x5qdh
Jun 27 18:07:31.788: INFO: Got endpoints: latency-svc-xx5nw [749.700152ms]
Jun 27 18:07:31.810: INFO: Created: latency-svc-gsds2
Jun 27 18:07:31.837: INFO: Got endpoints: latency-svc-qrxp2 [749.126601ms]
Jun 27 18:07:31.858: INFO: Created: latency-svc-xck75
Jun 27 18:07:31.888: INFO: Got endpoints: latency-svc-d66w5 [748.827022ms]
Jun 27 18:07:31.912: INFO: Created: latency-svc-c85tp
Jun 27 18:07:31.938: INFO: Got endpoints: latency-svc-gvp5f [750.626513ms]
Jun 27 18:07:31.985: INFO: Created: latency-svc-2gtwz
Jun 27 18:07:31.993: INFO: Got endpoints: latency-svc-hzngj [755.047726ms]
Jun 27 18:07:32.022: INFO: Created: latency-svc-7n86j
Jun 27 18:07:32.039: INFO: Got endpoints: latency-svc-lgv46 [749.991152ms]
Jun 27 18:07:32.064: INFO: Created: latency-svc-bxd2q
Jun 27 18:07:32.089: INFO: Got endpoints: latency-svc-p8pfg [749.782492ms]
Jun 27 18:07:32.105: INFO: Created: latency-svc-slwn7
Jun 27 18:07:32.138: INFO: Got endpoints: latency-svc-ndch5 [750.411733ms]
Jun 27 18:07:32.163: INFO: Created: latency-svc-f9786
Jun 27 18:07:32.187: INFO: Got endpoints: latency-svc-wzks9 [747.657621ms]
Jun 27 18:07:32.206: INFO: Created: latency-svc-rqwf7
Jun 27 18:07:32.239: INFO: Got endpoints: latency-svc-n6fr2 [749.271071ms]
Jun 27 18:07:32.263: INFO: Created: latency-svc-6mdxf
Jun 27 18:07:32.288: INFO: Got endpoints: latency-svc-sxv4m [750.325022ms]
Jun 27 18:07:32.311: INFO: Created: latency-svc-gw6mn
Jun 27 18:07:32.338: INFO: Got endpoints: latency-svc-x5sjl [748.198781ms]
Jun 27 18:07:32.359: INFO: Created: latency-svc-5nzxc
Jun 27 18:07:32.390: INFO: Got endpoints: latency-svc-sltwd [751.099633ms]
Jun 27 18:07:32.415: INFO: Created: latency-svc-4h47h
Jun 27 18:07:32.439: INFO: Got endpoints: latency-svc-d2vxk [750.660363ms]
Jun 27 18:07:32.460: INFO: Created: latency-svc-wbpqt
Jun 27 18:07:32.488: INFO: Got endpoints: latency-svc-x5qdh [748.360011ms]
Jun 27 18:07:32.505: INFO: Created: latency-svc-b9m5k
Jun 27 18:07:32.537: INFO: Got endpoints: latency-svc-gsds2 [749.368282ms]
Jun 27 18:07:32.560: INFO: Created: latency-svc-hzz88
Jun 27 18:07:32.588: INFO: Got endpoints: latency-svc-xck75 [750.238143ms]
Jun 27 18:07:32.617: INFO: Created: latency-svc-mphkw
Jun 27 18:07:32.639: INFO: Got endpoints: latency-svc-c85tp [750.573712ms]
Jun 27 18:07:32.661: INFO: Created: latency-svc-sb4hm
Jun 27 18:07:32.690: INFO: Got endpoints: latency-svc-2gtwz [752.342444ms]
Jun 27 18:07:32.710: INFO: Created: latency-svc-wspnt
Jun 27 18:07:32.737: INFO: Got endpoints: latency-svc-7n86j [743.945718ms]
Jun 27 18:07:32.779: INFO: Created: latency-svc-b89v8
Jun 27 18:07:32.788: INFO: Got endpoints: latency-svc-bxd2q [748.430941ms]
Jun 27 18:07:32.810: INFO: Created: latency-svc-wk7h8
Jun 27 18:07:32.839: INFO: Got endpoints: latency-svc-slwn7 [749.825172ms]
Jun 27 18:07:32.873: INFO: Created: latency-svc-pjfnz
Jun 27 18:07:32.888: INFO: Got endpoints: latency-svc-f9786 [749.414072ms]
Jun 27 18:07:32.906: INFO: Created: latency-svc-cjm9d
Jun 27 18:07:32.938: INFO: Got endpoints: latency-svc-rqwf7 [751.242553ms]
Jun 27 18:07:32.958: INFO: Created: latency-svc-qkmzx
Jun 27 18:07:32.987: INFO: Got endpoints: latency-svc-6mdxf [748.154731ms]
Jun 27 18:07:33.006: INFO: Created: latency-svc-sbs7q
Jun 27 18:07:33.038: INFO: Got endpoints: latency-svc-gw6mn [749.421392ms]
Jun 27 18:07:33.060: INFO: Created: latency-svc-lpzsr
Jun 27 18:07:33.089: INFO: Got endpoints: latency-svc-5nzxc [750.441813ms]
Jun 27 18:07:33.121: INFO: Created: latency-svc-vx8h5
Jun 27 18:07:33.137: INFO: Got endpoints: latency-svc-4h47h [747.288641ms]
Jun 27 18:07:33.160: INFO: Created: latency-svc-lgxd9
Jun 27 18:07:33.187: INFO: Got endpoints: latency-svc-wbpqt [748.228421ms]
Jun 27 18:07:33.205: INFO: Created: latency-svc-dn84x
Jun 27 18:07:33.237: INFO: Got endpoints: latency-svc-b9m5k [749.570352ms]
Jun 27 18:07:33.255: INFO: Created: latency-svc-sb8zt
Jun 27 18:07:33.288: INFO: Got endpoints: latency-svc-hzz88 [750.312883ms]
Jun 27 18:07:33.314: INFO: Created: latency-svc-wfnbg
Jun 27 18:07:33.339: INFO: Got endpoints: latency-svc-mphkw [751.150903ms]
Jun 27 18:07:33.361: INFO: Created: latency-svc-gsz2j
Jun 27 18:07:33.387: INFO: Got endpoints: latency-svc-sb4hm [748.695032ms]
Jun 27 18:07:33.420: INFO: Created: latency-svc-gbxh8
Jun 27 18:07:33.438: INFO: Got endpoints: latency-svc-wspnt [747.88722ms]
Jun 27 18:07:33.458: INFO: Created: latency-svc-7zfrr
Jun 27 18:07:33.488: INFO: Got endpoints: latency-svc-b89v8 [750.902873ms]
Jun 27 18:07:33.508: INFO: Created: latency-svc-hs498
Jun 27 18:07:33.537: INFO: Got endpoints: latency-svc-wk7h8 [749.568982ms]
Jun 27 18:07:33.558: INFO: Created: latency-svc-jnx66
Jun 27 18:07:33.587: INFO: Got endpoints: latency-svc-pjfnz [748.217321ms]
Jun 27 18:07:33.606: INFO: Created: latency-svc-tgphv
Jun 27 18:07:33.638: INFO: Got endpoints: latency-svc-cjm9d [750.350442ms]
Jun 27 18:07:33.658: INFO: Created: latency-svc-fq5kq
Jun 27 18:07:33.691: INFO: Got endpoints: latency-svc-qkmzx [752.528704ms]
Jun 27 18:07:33.713: INFO: Created: latency-svc-7xp66
Jun 27 18:07:33.738: INFO: Got endpoints: latency-svc-sbs7q [751.028223ms]
Jun 27 18:07:33.760: INFO: Created: latency-svc-cbl6d
Jun 27 18:07:33.791: INFO: Got endpoints: latency-svc-lpzsr [753.135405ms]
Jun 27 18:07:33.814: INFO: Created: latency-svc-pfrf7
Jun 27 18:07:33.839: INFO: Got endpoints: latency-svc-vx8h5 [750.263132ms]
Jun 27 18:07:33.859: INFO: Created: latency-svc-zmq9d
Jun 27 18:07:33.888: INFO: Got endpoints: latency-svc-lgxd9 [751.096703ms]
Jun 27 18:07:33.915: INFO: Created: latency-svc-mwt8s
Jun 27 18:07:33.937: INFO: Got endpoints: latency-svc-dn84x [749.704292ms]
Jun 27 18:07:33.962: INFO: Created: latency-svc-7vzv8
Jun 27 18:07:33.991: INFO: Got endpoints: latency-svc-sb8zt [753.445895ms]
Jun 27 18:07:34.039: INFO: Got endpoints: latency-svc-wfnbg [750.608312ms]
Jun 27 18:07:34.089: INFO: Got endpoints: latency-svc-gsz2j [749.460532ms]
Jun 27 18:07:34.138: INFO: Got endpoints: latency-svc-gbxh8 [751.014873ms]
Jun 27 18:07:34.188: INFO: Got endpoints: latency-svc-7zfrr [750.086853ms]
Jun 27 18:07:34.239: INFO: Got endpoints: latency-svc-hs498 [750.643652ms]
Jun 27 18:07:34.290: INFO: Got endpoints: latency-svc-jnx66 [752.487484ms]
Jun 27 18:07:34.337: INFO: Got endpoints: latency-svc-tgphv [750.171872ms]
Jun 27 18:07:34.387: INFO: Got endpoints: latency-svc-fq5kq [748.725521ms]
Jun 27 18:07:34.438: INFO: Got endpoints: latency-svc-7xp66 [747.09239ms]
Jun 27 18:07:34.489: INFO: Got endpoints: latency-svc-cbl6d [750.931203ms]
Jun 27 18:07:34.538: INFO: Got endpoints: latency-svc-pfrf7 [746.85758ms]
Jun 27 18:07:34.588: INFO: Got endpoints: latency-svc-zmq9d [748.318621ms]
Jun 27 18:07:34.638: INFO: Got endpoints: latency-svc-mwt8s [749.675122ms]
Jun 27 18:07:34.687: INFO: Got endpoints: latency-svc-7vzv8 [749.801542ms]
Jun 27 18:07:34.687: INFO: Latencies: [43.205073ms 52.24127ms 60.155746ms 76.578949ms 127.464008ms 150.122735ms 164.501755ms 196.165989ms 241.083094ms 265.123072ms 300.976149ms 307.162024ms 331.126532ms 349.813147ms 350.794167ms 362.531076ms 364.351578ms 365.456819ms 367.936901ms 368.333772ms 370.883593ms 370.910773ms 373.167234ms 378.923769ms 380.356161ms 390.544188ms 391.800138ms 393.99352ms 394.645941ms 395.639092ms 396.168943ms 397.096123ms 398.452204ms 401.195356ms 404.496698ms 405.671199ms 406.703491ms 407.218661ms 408.198801ms 410.680934ms 411.922865ms 412.631805ms 415.864607ms 420.462561ms 420.54456ms 423.981374ms 426.871806ms 429.353507ms 429.811458ms 430.683249ms 435.901472ms 439.830385ms 449.727003ms 452.893015ms 455.215168ms 457.254729ms 459.13431ms 470.453969ms 474.821772ms 497.40449ms 515.888233ms 516.739825ms 521.822878ms 523.164229ms 525.716651ms 529.706005ms 531.871665ms 533.599737ms 537.20857ms 540.718343ms 541.938734ms 547.172197ms 558.952117ms 561.627098ms 587.801858ms 628.157039ms 652.574138ms 690.545247ms 712.531703ms 725.520964ms 727.235035ms 738.106473ms 740.918585ms 742.371146ms 742.499986ms 742.642227ms 743.431957ms 743.497588ms 743.945718ms 745.060529ms 746.063399ms 746.62684ms 746.85758ms 746.96408ms 747.00463ms 747.09239ms 747.11807ms 747.21775ms 747.288641ms 747.657621ms 747.817701ms 747.88722ms 747.9249ms 748.003571ms 748.154731ms 748.198781ms 748.217321ms 748.228421ms 748.318621ms 748.360011ms 748.394321ms 748.430941ms 748.687601ms 748.695032ms 748.725521ms 748.818681ms 748.827022ms 748.967072ms 748.970331ms 749.126601ms 749.160192ms 749.271071ms 749.368282ms 749.394112ms 749.414072ms 749.421392ms 749.460532ms 749.568982ms 749.570352ms 749.572822ms 749.675122ms 749.700152ms 749.704292ms 749.737952ms 749.782492ms 749.785002ms 749.796282ms 749.801542ms 749.825172ms 749.904882ms 749.991152ms 750.086853ms 750.158192ms 750.158732ms 750.171872ms 750.212032ms 750.227622ms 750.238143ms 750.240182ms 750.263132ms 750.312883ms 750.325022ms 750.350442ms 750.397492ms 750.411733ms 750.441813ms 750.498883ms 750.573712ms 750.605693ms 750.608312ms 750.626513ms 750.643652ms 750.651452ms 750.660363ms 750.746933ms 750.811793ms 750.902873ms 750.931203ms 750.998843ms 751.014873ms 751.028223ms 751.043873ms 751.062353ms 751.063813ms 751.096703ms 751.099633ms 751.150903ms 751.158763ms 751.183414ms 751.203043ms 751.242553ms 751.288093ms 751.301813ms 751.306573ms 751.335903ms 751.509263ms 751.562623ms 751.632074ms 752.044984ms 752.342444ms 752.487484ms 752.528704ms 753.135405ms 753.445895ms 754.851526ms 755.047726ms 757.519768ms 758.815589ms 762.121532ms 773.77552ms]
Jun 27 18:07:34.688: INFO: 50 %ile: 747.817701ms
Jun 27 18:07:34.688: INFO: 90 %ile: 751.242553ms
Jun 27 18:07:34.688: INFO: 99 %ile: 762.121532ms
Jun 27 18:07:34.688: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:07:34.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1222" for this suite.
Jun 27 18:07:50.715: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:07:50.848: INFO: namespace svc-latency-1222 deletion completed in 16.151039122s

• [SLOW TEST:27.096 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:07:50.849: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-3217
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jun 27 18:07:53.553: INFO: Successfully updated pod "labelsupdateea4682f3-52ea-4124-a167-eee4834ddeb2"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:07:57.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3217" for this suite.
Jun 27 18:08:19.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:08:19.733: INFO: namespace projected-3217 deletion completed in 22.149030859s

• [SLOW TEST:28.885 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:08:19.734: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9454
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-7b68ccca-8810-470c-856c-4bde8697aec9
STEP: Creating configMap with name cm-test-opt-upd-bf9024ae-66ef-4db0-a7bd-607139c1a4cc
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-7b68ccca-8810-470c-856c-4bde8697aec9
STEP: Updating configmap cm-test-opt-upd-bf9024ae-66ef-4db0-a7bd-607139c1a4cc
STEP: Creating configMap with name cm-test-opt-create-f1fdf8fa-59f2-41bd-8417-57579d6be923
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:09:54.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9454" for this suite.
Jun 27 18:10:16.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:10:16.548: INFO: namespace configmap-9454 deletion completed in 22.143646275s

• [SLOW TEST:116.821 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:10:16.549: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-5984
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 27 18:10:16.722: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun 27 18:10:21.726: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 27 18:10:21.726: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 27 18:10:21.751: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-5984,SelfLink:/apis/apps/v1/namespaces/deployment-5984/deployments/test-cleanup-deployment,UID:34bd9441-bcb8-4392-86c4-dc2ee2c37704,ResourceVersion:20680,Generation:1,CreationTimestamp:2019-06-27 18:10:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[],ReadyReplicas:0,CollisionCount:nil,},}

Jun 27 18:10:21.756: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
Jun 27 18:10:21.756: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
Jun 27 18:10:21.756: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller,GenerateName:,Namespace:deployment-5984,SelfLink:/apis/apps/v1/namespaces/deployment-5984/replicasets/test-cleanup-controller,UID:ae16d85c-53e2-47d9-a8e0-475c0bf1aafc,ResourceVersion:20681,Generation:1,CreationTimestamp:2019-06-27 18:10:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 34bd9441-bcb8-4392-86c4-dc2ee2c37704 0xc002d93e87 0xc002d93e88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 27 18:10:21.762: INFO: Pod "test-cleanup-controller-7fzn6" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-controller-7fzn6,GenerateName:test-cleanup-controller-,Namespace:deployment-5984,SelfLink:/api/v1/namespaces/deployment-5984/pods/test-cleanup-controller-7fzn6,UID:a871d5f9-ea92-4a59-a493-4d5b466a5462,ResourceVersion:20672,Generation:0,CreationTimestamp:2019-06-27 18:10:16 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod: nginx,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.187.113/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-controller ae16d85c-53e2-47d9-a8e0-475c0bf1aafc 0xc0031be627 0xc0031be628}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-9b7sg {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-9b7sg,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-9b7sg true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nk6xg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031be690} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031be6b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 18:10:16 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 18:10:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 18:10:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 18:10:16 +0000 UTC  }],Message:,Reason:,HostIP:139.178.70.235,PodIP:10.244.187.113,StartTime:2019-06-27 18:10:16 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-27 18:10:17 +0000 UTC,} nil} {nil nil nil} true 0 docker.io/library/nginx:1.14-alpine docker.io/library/nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 containerd://e61a6ae81d82055b6e125044bf6dc8d0cb44c6f8b17b552734b2c9059d40a136}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:10:21.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5984" for this suite.
Jun 27 18:10:27.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:10:27.929: INFO: namespace deployment-5984 deletion completed in 6.158440918s

• [SLOW TEST:11.379 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:10:27.930: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-7573
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jun 27 18:10:28.094: INFO: PodSpec: initContainers in spec.initContainers
Jun 27 18:11:08.405: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-4b622ba1-fbb5-4cc3-87d1-1284165d99a3", GenerateName:"", Namespace:"init-container-7573", SelfLink:"/api/v1/namespaces/init-container-7573/pods/pod-init-4b622ba1-fbb5-4cc3-87d1-1284165d99a3", UID:"3b1da0f7-d6db-4d1d-911b-614d91642d36", ResourceVersion:"20824", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63697255828, loc:(*time.Location)(0x80bb5c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"94532222"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.244.187.114/32", "kubernetes.io/psp":"e2e-test-privileged-psp"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-44g5b", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002a49300), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-44g5b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-44g5b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-44g5b", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0032c8178), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"talos-test-cluster-workers-84c9684cd-nk6xg", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002e244e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0032c81f0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0032c8210)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0032c8218), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0032c821c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697255828, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697255828, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697255828, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63697255828, loc:(*time.Location)(0x80bb5c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"139.178.70.235", PodIP:"10.244.187.114", StartTime:(*v1.Time)(0xc0033a5260), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc002830460)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0028304d0)}, Ready:false, RestartCount:3, Image:"docker.io/library/busybox:1.29", ImageID:"docker.io/library/busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"containerd://04cd1d945a7f5583adaaf0883b981180f4fb3ca4db6ae95c095466f19648baeb"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0033a52a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0033a5280), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:11:08.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7573" for this suite.
Jun 27 18:11:30.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:11:30.561: INFO: namespace init-container-7573 deletion completed in 22.150495479s

• [SLOW TEST:62.632 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:11:30.563: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-6482
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 27 18:11:30.762: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 18:11:30.767: INFO: Number of nodes with available pods: 0
Jun 27 18:11:30.767: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 18:11:31.772: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 18:11:31.777: INFO: Number of nodes with available pods: 0
Jun 27 18:11:31.777: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 18:11:32.772: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 18:11:32.777: INFO: Number of nodes with available pods: 3
Jun 27 18:11:32.777: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun 27 18:11:32.800: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 18:11:32.811: INFO: Number of nodes with available pods: 2
Jun 27 18:11:32.811: INFO: Node talos-test-cluster-workers-84c9684cd-nk6xg is running more than one daemon pod
Jun 27 18:11:33.816: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 18:11:33.821: INFO: Number of nodes with available pods: 2
Jun 27 18:11:33.821: INFO: Node talos-test-cluster-workers-84c9684cd-nk6xg is running more than one daemon pod
Jun 27 18:11:34.816: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 18:11:34.820: INFO: Number of nodes with available pods: 3
Jun 27 18:11:34.820: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6482, will wait for the garbage collector to delete the pods
Jun 27 18:11:34.888: INFO: Deleting DaemonSet.extensions daemon-set took: 7.124095ms
Jun 27 18:11:35.289: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.241986ms
Jun 27 18:11:45.192: INFO: Number of nodes with available pods: 0
Jun 27 18:11:45.192: INFO: Number of running nodes: 0, number of available pods: 0
Jun 27 18:11:45.196: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6482/daemonsets","resourceVersion":"20998"},"items":null}

Jun 27 18:11:45.199: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6482/pods","resourceVersion":"20998"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:11:45.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6482" for this suite.
Jun 27 18:11:51.235: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:11:51.374: INFO: namespace daemonsets-6482 deletion completed in 6.153457145s

• [SLOW TEST:20.811 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:11:51.374: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-5997
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-drs5d in namespace proxy-5997
I0627 18:11:51.574599      18 runners.go:180] Created replication controller with name: proxy-service-drs5d, namespace: proxy-5997, replica count: 1
I0627 18:11:52.625118      18 runners.go:180] proxy-service-drs5d Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0627 18:11:53.625363      18 runners.go:180] proxy-service-drs5d Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0627 18:11:54.625604      18 runners.go:180] proxy-service-drs5d Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0627 18:11:55.625788      18 runners.go:180] proxy-service-drs5d Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0627 18:11:56.625992      18 runners.go:180] proxy-service-drs5d Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0627 18:11:57.626196      18 runners.go:180] proxy-service-drs5d Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0627 18:11:58.626449      18 runners.go:180] proxy-service-drs5d Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0627 18:11:59.626723      18 runners.go:180] proxy-service-drs5d Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0627 18:12:00.626989      18 runners.go:180] proxy-service-drs5d Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0627 18:12:01.627205      18 runners.go:180] proxy-service-drs5d Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0627 18:12:02.627505      18 runners.go:180] proxy-service-drs5d Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0627 18:12:03.627696      18 runners.go:180] proxy-service-drs5d Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 27 18:12:03.631: INFO: setup took 12.095597318s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun 27 18:12:03.659: INFO: (0) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 26.99404ms)
Jun 27 18:12:03.659: INFO: (0) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname2/proxy/: bar (200; 27.15524ms)
Jun 27 18:12:03.659: INFO: (0) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 27.996172ms)
Jun 27 18:12:03.660: INFO: (0) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 28.127522ms)
Jun 27 18:12:03.660: INFO: (0) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">test<... (200; 28.090621ms)
Jun 27 18:12:03.660: INFO: (0) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname2/proxy/: bar (200; 28.293892ms)
Jun 27 18:12:03.660: INFO: (0) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">... (200; 28.166061ms)
Jun 27 18:12:03.661: INFO: (0) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname1/proxy/: foo (200; 29.497143ms)
Jun 27 18:12:03.661: INFO: (0) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/rewriteme">test</a> (200; 29.630873ms)
Jun 27 18:12:03.661: INFO: (0) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname1/proxy/: foo (200; 29.502632ms)
Jun 27 18:12:03.660: INFO: (0) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 28.696772ms)
Jun 27 18:12:03.662: INFO: (0) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:462/proxy/: tls qux (200; 30.384103ms)
Jun 27 18:12:03.676: INFO: (0) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:460/proxy/: tls baz (200; 44.505504ms)
Jun 27 18:12:03.676: INFO: (0) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/tlsrewritem... (200; 44.552084ms)
Jun 27 18:12:03.685: INFO: (0) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname2/proxy/: tls qux (200; 53.550971ms)
Jun 27 18:12:03.686: INFO: (0) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname1/proxy/: tls baz (200; 54.418032ms)
Jun 27 18:12:03.695: INFO: (1) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:462/proxy/: tls qux (200; 8.836377ms)
Jun 27 18:12:03.696: INFO: (1) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">test<... (200; 10.535368ms)
Jun 27 18:12:03.697: INFO: (1) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 10.580438ms)
Jun 27 18:12:03.698: INFO: (1) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 12.19579ms)
Jun 27 18:12:03.702: INFO: (1) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:460/proxy/: tls baz (200; 15.351352ms)
Jun 27 18:12:03.703: INFO: (1) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/tlsrewritem... (200; 16.323993ms)
Jun 27 18:12:03.704: INFO: (1) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 17.626934ms)
Jun 27 18:12:03.705: INFO: (1) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 18.109924ms)
Jun 27 18:12:03.705: INFO: (1) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname2/proxy/: bar (200; 18.568954ms)
Jun 27 18:12:03.705: INFO: (1) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/rewriteme">test</a> (200; 18.318434ms)
Jun 27 18:12:03.705: INFO: (1) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">... (200; 18.639525ms)
Jun 27 18:12:03.706: INFO: (1) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname1/proxy/: tls baz (200; 19.714295ms)
Jun 27 18:12:03.707: INFO: (1) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname2/proxy/: bar (200; 20.875096ms)
Jun 27 18:12:03.707: INFO: (1) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname1/proxy/: foo (200; 20.887546ms)
Jun 27 18:12:03.708: INFO: (1) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname2/proxy/: tls qux (200; 22.020067ms)
Jun 27 18:12:03.709: INFO: (1) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname1/proxy/: foo (200; 22.730888ms)
Jun 27 18:12:03.724: INFO: (2) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:462/proxy/: tls qux (200; 14.232581ms)
Jun 27 18:12:03.724: INFO: (2) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">... (200; 14.306821ms)
Jun 27 18:12:03.725: INFO: (2) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">test<... (200; 15.664082ms)
Jun 27 18:12:03.725: INFO: (2) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 15.278852ms)
Jun 27 18:12:03.726: INFO: (2) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 16.370682ms)
Jun 27 18:12:03.726: INFO: (2) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 16.813833ms)
Jun 27 18:12:03.727: INFO: (2) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 17.407223ms)
Jun 27 18:12:03.731: INFO: (2) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/tlsrewritem... (200; 21.228886ms)
Jun 27 18:12:03.731: INFO: (2) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname1/proxy/: tls baz (200; 21.649196ms)
Jun 27 18:12:03.731: INFO: (2) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname2/proxy/: tls qux (200; 22.017317ms)
Jun 27 18:12:03.732: INFO: (2) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname1/proxy/: foo (200; 22.354957ms)
Jun 27 18:12:03.732: INFO: (2) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:460/proxy/: tls baz (200; 22.157187ms)
Jun 27 18:12:03.732: INFO: (2) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/rewriteme">test</a> (200; 22.417497ms)
Jun 27 18:12:03.732: INFO: (2) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname1/proxy/: foo (200; 22.167637ms)
Jun 27 18:12:03.732: INFO: (2) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname2/proxy/: bar (200; 22.465057ms)
Jun 27 18:12:03.732: INFO: (2) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname2/proxy/: bar (200; 21.935717ms)
Jun 27 18:12:03.741: INFO: (3) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/tlsrewritem... (200; 8.724247ms)
Jun 27 18:12:03.742: INFO: (3) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:462/proxy/: tls qux (200; 9.484038ms)
Jun 27 18:12:03.745: INFO: (3) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:460/proxy/: tls baz (200; 12.611119ms)
Jun 27 18:12:03.745: INFO: (3) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/rewriteme">test</a> (200; 11.775198ms)
Jun 27 18:12:03.752: INFO: (3) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 16.856663ms)
Jun 27 18:12:03.752: INFO: (3) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 16.720193ms)
Jun 27 18:12:03.752: INFO: (3) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">... (200; 17.685724ms)
Jun 27 18:12:03.753: INFO: (3) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 19.140055ms)
Jun 27 18:12:03.754: INFO: (3) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">test<... (200; 19.334945ms)
Jun 27 18:12:03.755: INFO: (3) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 20.635135ms)
Jun 27 18:12:03.755: INFO: (3) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname2/proxy/: tls qux (200; 19.965795ms)
Jun 27 18:12:03.756: INFO: (3) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname1/proxy/: foo (200; 20.605945ms)
Jun 27 18:12:03.757: INFO: (3) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname1/proxy/: foo (200; 24.816279ms)
Jun 27 18:12:03.758: INFO: (3) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname2/proxy/: bar (200; 23.121698ms)
Jun 27 18:12:03.758: INFO: (3) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname2/proxy/: bar (200; 23.660108ms)
Jun 27 18:12:03.759: INFO: (3) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname1/proxy/: tls baz (200; 25.282199ms)
Jun 27 18:12:03.773: INFO: (4) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:460/proxy/: tls baz (200; 14.02506ms)
Jun 27 18:12:03.773: INFO: (4) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 14.739671ms)
Jun 27 18:12:03.773: INFO: (4) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 14.148301ms)
Jun 27 18:12:03.774: INFO: (4) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/tlsrewritem... (200; 14.657171ms)
Jun 27 18:12:03.774: INFO: (4) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:462/proxy/: tls qux (200; 15.316771ms)
Jun 27 18:12:03.777: INFO: (4) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 18.363494ms)
Jun 27 18:12:03.778: INFO: (4) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">test<... (200; 18.403494ms)
Jun 27 18:12:03.778: INFO: (4) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 18.899364ms)
Jun 27 18:12:03.779: INFO: (4) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname2/proxy/: bar (200; 19.776995ms)
Jun 27 18:12:03.779: INFO: (4) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/rewriteme">test</a> (200; 20.788895ms)
Jun 27 18:12:03.780: INFO: (4) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname2/proxy/: tls qux (200; 20.654796ms)
Jun 27 18:12:03.780: INFO: (4) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname2/proxy/: bar (200; 21.337806ms)
Jun 27 18:12:03.781: INFO: (4) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname1/proxy/: foo (200; 21.898597ms)
Jun 27 18:12:03.781: INFO: (4) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname1/proxy/: tls baz (200; 22.123267ms)
Jun 27 18:12:03.782: INFO: (4) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">... (200; 22.402107ms)
Jun 27 18:12:03.782: INFO: (4) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname1/proxy/: foo (200; 23.432727ms)
Jun 27 18:12:03.791: INFO: (5) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:462/proxy/: tls qux (200; 9.216117ms)
Jun 27 18:12:03.792: INFO: (5) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/rewriteme">test</a> (200; 8.611377ms)
Jun 27 18:12:03.796: INFO: (5) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 12.90016ms)
Jun 27 18:12:03.800: INFO: (5) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/tlsrewritem... (200; 15.304902ms)
Jun 27 18:12:03.800: INFO: (5) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 15.763012ms)
Jun 27 18:12:03.800: INFO: (5) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:460/proxy/: tls baz (200; 15.067342ms)
Jun 27 18:12:03.800: INFO: (5) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">test<... (200; 16.086312ms)
Jun 27 18:12:03.800: INFO: (5) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname2/proxy/: bar (200; 17.411303ms)
Jun 27 18:12:03.800: INFO: (5) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname1/proxy/: foo (200; 16.489673ms)
Jun 27 18:12:03.800: INFO: (5) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">... (200; 16.091902ms)
Jun 27 18:12:03.800: INFO: (5) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 16.024802ms)
Jun 27 18:12:03.800: INFO: (5) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 16.944493ms)
Jun 27 18:12:03.800: INFO: (5) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname2/proxy/: tls qux (200; 16.102912ms)
Jun 27 18:12:03.800: INFO: (5) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname2/proxy/: bar (200; 17.675193ms)
Jun 27 18:12:03.801: INFO: (5) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname1/proxy/: foo (200; 16.526753ms)
Jun 27 18:12:03.802: INFO: (5) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname1/proxy/: tls baz (200; 19.377315ms)
Jun 27 18:12:03.817: INFO: (6) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">... (200; 14.630621ms)
Jun 27 18:12:03.818: INFO: (6) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname1/proxy/: foo (200; 14.953781ms)
Jun 27 18:12:03.818: INFO: (6) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 15.171162ms)
Jun 27 18:12:03.818: INFO: (6) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname1/proxy/: foo (200; 15.424732ms)
Jun 27 18:12:03.819: INFO: (6) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname2/proxy/: tls qux (200; 16.403292ms)
Jun 27 18:12:03.819: INFO: (6) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/rewriteme">test</a> (200; 15.915232ms)
Jun 27 18:12:03.819: INFO: (6) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 16.809223ms)
Jun 27 18:12:03.819: INFO: (6) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname1/proxy/: tls baz (200; 16.744963ms)
Jun 27 18:12:03.819: INFO: (6) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname2/proxy/: bar (200; 17.001963ms)
Jun 27 18:12:03.821: INFO: (6) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname2/proxy/: bar (200; 18.349154ms)
Jun 27 18:12:03.821: INFO: (6) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/tlsrewritem... (200; 18.127414ms)
Jun 27 18:12:03.822: INFO: (6) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 19.038595ms)
Jun 27 18:12:03.822: INFO: (6) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:460/proxy/: tls baz (200; 19.150905ms)
Jun 27 18:12:03.822: INFO: (6) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">test<... (200; 19.346825ms)
Jun 27 18:12:03.822: INFO: (6) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:462/proxy/: tls qux (200; 19.386465ms)
Jun 27 18:12:03.822: INFO: (6) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 19.389085ms)
Jun 27 18:12:03.829: INFO: (7) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 7.100915ms)
Jun 27 18:12:03.835: INFO: (7) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">... (200; 12.23287ms)
Jun 27 18:12:03.836: INFO: (7) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 13.53685ms)
Jun 27 18:12:03.836: INFO: (7) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 13.947371ms)
Jun 27 18:12:03.837: INFO: (7) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/rewriteme">test</a> (200; 14.207121ms)
Jun 27 18:12:03.837: INFO: (7) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/tlsrewritem... (200; 14.581141ms)
Jun 27 18:12:03.838: INFO: (7) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">test<... (200; 15.326312ms)
Jun 27 18:12:03.838: INFO: (7) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 15.496462ms)
Jun 27 18:12:03.838: INFO: (7) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname2/proxy/: bar (200; 15.908382ms)
Jun 27 18:12:03.839: INFO: (7) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:462/proxy/: tls qux (200; 16.864273ms)
Jun 27 18:12:03.839: INFO: (7) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname1/proxy/: tls baz (200; 17.119213ms)
Jun 27 18:12:03.840: INFO: (7) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:460/proxy/: tls baz (200; 18.151424ms)
Jun 27 18:12:03.843: INFO: (7) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname1/proxy/: foo (200; 20.841146ms)
Jun 27 18:12:03.843: INFO: (7) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname2/proxy/: tls qux (200; 20.865816ms)
Jun 27 18:12:03.843: INFO: (7) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname2/proxy/: bar (200; 20.913916ms)
Jun 27 18:12:03.843: INFO: (7) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname1/proxy/: foo (200; 21.252336ms)
Jun 27 18:12:03.850: INFO: (8) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/tlsrewritem... (200; 6.784405ms)
Jun 27 18:12:03.859: INFO: (8) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname1/proxy/: tls baz (200; 14.884431ms)
Jun 27 18:12:03.859: INFO: (8) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname2/proxy/: tls qux (200; 14.930561ms)
Jun 27 18:12:03.859: INFO: (8) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname1/proxy/: foo (200; 15.542882ms)
Jun 27 18:12:03.860: INFO: (8) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname2/proxy/: bar (200; 15.735492ms)
Jun 27 18:12:03.860: INFO: (8) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 15.637972ms)
Jun 27 18:12:03.860: INFO: (8) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">test<... (200; 16.189033ms)
Jun 27 18:12:03.861: INFO: (8) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:460/proxy/: tls baz (200; 17.659024ms)
Jun 27 18:12:03.861: INFO: (8) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 17.386863ms)
Jun 27 18:12:03.862: INFO: (8) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname1/proxy/: foo (200; 18.078614ms)
Jun 27 18:12:03.863: INFO: (8) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">... (200; 19.227935ms)
Jun 27 18:12:03.863: INFO: (8) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:462/proxy/: tls qux (200; 19.379394ms)
Jun 27 18:12:03.864: INFO: (8) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 19.619205ms)
Jun 27 18:12:03.864: INFO: (8) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/rewriteme">test</a> (200; 20.069025ms)
Jun 27 18:12:03.864: INFO: (8) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 20.984696ms)
Jun 27 18:12:03.865: INFO: (8) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname2/proxy/: bar (200; 20.860786ms)
Jun 27 18:12:03.875: INFO: (9) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/tlsrewritem... (200; 9.754727ms)
Jun 27 18:12:03.876: INFO: (9) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:462/proxy/: tls qux (200; 10.534128ms)
Jun 27 18:12:03.878: INFO: (9) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/rewriteme">test</a> (200; 11.617408ms)
Jun 27 18:12:03.878: INFO: (9) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:460/proxy/: tls baz (200; 12.79129ms)
Jun 27 18:12:03.879: INFO: (9) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">... (200; 12.073249ms)
Jun 27 18:12:03.879: INFO: (9) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 12.691359ms)
Jun 27 18:12:03.879: INFO: (9) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname2/proxy/: tls qux (200; 14.21763ms)
Jun 27 18:12:03.880: INFO: (9) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 14.798202ms)
Jun 27 18:12:03.880: INFO: (9) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">test<... (200; 13.14676ms)
Jun 27 18:12:03.884: INFO: (9) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname1/proxy/: foo (200; 18.736514ms)
Jun 27 18:12:03.885: INFO: (9) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname1/proxy/: tls baz (200; 19.307565ms)
Jun 27 18:12:03.886: INFO: (9) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname2/proxy/: bar (200; 19.782555ms)
Jun 27 18:12:03.886: INFO: (9) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname2/proxy/: bar (200; 20.131276ms)
Jun 27 18:12:03.887: INFO: (9) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 21.156886ms)
Jun 27 18:12:03.888: INFO: (9) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 20.834626ms)
Jun 27 18:12:03.888: INFO: (9) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname1/proxy/: foo (200; 21.786806ms)
Jun 27 18:12:03.901: INFO: (10) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">test<... (200; 12.899289ms)
Jun 27 18:12:03.901: INFO: (10) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:462/proxy/: tls qux (200; 13.005189ms)
Jun 27 18:12:03.902: INFO: (10) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:460/proxy/: tls baz (200; 13.13863ms)
Jun 27 18:12:03.902: INFO: (10) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/tlsrewritem... (200; 13.27349ms)
Jun 27 18:12:03.902: INFO: (10) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 13.66868ms)
Jun 27 18:12:03.902: INFO: (10) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 13.4574ms)
Jun 27 18:12:03.902: INFO: (10) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/rewriteme">test</a> (200; 13.4152ms)
Jun 27 18:12:03.902: INFO: (10) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 13.60305ms)
Jun 27 18:12:03.902: INFO: (10) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">... (200; 13.59386ms)
Jun 27 18:12:03.905: INFO: (10) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname1/proxy/: tls baz (200; 15.798122ms)
Jun 27 18:12:03.906: INFO: (10) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname2/proxy/: bar (200; 17.141533ms)
Jun 27 18:12:03.907: INFO: (10) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 17.828273ms)
Jun 27 18:12:03.908: INFO: (10) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname2/proxy/: tls qux (200; 19.140644ms)
Jun 27 18:12:03.908: INFO: (10) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname1/proxy/: foo (200; 19.282125ms)
Jun 27 18:12:03.909: INFO: (10) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname2/proxy/: bar (200; 20.180335ms)
Jun 27 18:12:03.909: INFO: (10) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname1/proxy/: foo (200; 20.472406ms)
Jun 27 18:12:03.921: INFO: (11) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:462/proxy/: tls qux (200; 10.957458ms)
Jun 27 18:12:03.923: INFO: (11) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">test<... (200; 13.1891ms)
Jun 27 18:12:03.923: INFO: (11) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 13.11293ms)
Jun 27 18:12:03.924: INFO: (11) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/rewriteme">test</a> (200; 14.063051ms)
Jun 27 18:12:03.924: INFO: (11) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 14.853511ms)
Jun 27 18:12:03.926: INFO: (11) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/tlsrewritem... (200; 15.495262ms)
Jun 27 18:12:03.926: INFO: (11) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 15.614922ms)
Jun 27 18:12:03.927: INFO: (11) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:460/proxy/: tls baz (200; 16.866683ms)
Jun 27 18:12:03.927: INFO: (11) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 17.061913ms)
Jun 27 18:12:03.927: INFO: (11) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">... (200; 16.734403ms)
Jun 27 18:12:03.928: INFO: (11) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname1/proxy/: foo (200; 17.825264ms)
Jun 27 18:12:03.928: INFO: (11) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname1/proxy/: tls baz (200; 18.032564ms)
Jun 27 18:12:03.928: INFO: (11) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname1/proxy/: foo (200; 18.190604ms)
Jun 27 18:12:03.929: INFO: (11) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname2/proxy/: tls qux (200; 18.932324ms)
Jun 27 18:12:03.931: INFO: (11) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname2/proxy/: bar (200; 20.933846ms)
Jun 27 18:12:03.931: INFO: (11) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname2/proxy/: bar (200; 21.030746ms)
Jun 27 18:12:03.943: INFO: (12) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:460/proxy/: tls baz (200; 11.151599ms)
Jun 27 18:12:03.943: INFO: (12) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 11.504948ms)
Jun 27 18:12:03.943: INFO: (12) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 11.488208ms)
Jun 27 18:12:03.947: INFO: (12) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">test<... (200; 15.308632ms)
Jun 27 18:12:03.952: INFO: (12) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 19.921935ms)
Jun 27 18:12:03.952: INFO: (12) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/rewriteme">test</a> (200; 19.928585ms)
Jun 27 18:12:03.952: INFO: (12) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/tlsrewritem... (200; 20.542615ms)
Jun 27 18:12:03.954: INFO: (12) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 22.677838ms)
Jun 27 18:12:03.954: INFO: (12) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:462/proxy/: tls qux (200; 22.464608ms)
Jun 27 18:12:03.955: INFO: (12) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">... (200; 23.052518ms)
Jun 27 18:12:03.959: INFO: (12) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname1/proxy/: foo (200; 27.250531ms)
Jun 27 18:12:03.959: INFO: (12) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname1/proxy/: tls baz (200; 26.78089ms)
Jun 27 18:12:03.959: INFO: (12) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname2/proxy/: tls qux (200; 26.8776ms)
Jun 27 18:12:03.959: INFO: (12) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname1/proxy/: foo (200; 27.451211ms)
Jun 27 18:12:03.959: INFO: (12) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname2/proxy/: bar (200; 27.456991ms)
Jun 27 18:12:03.959: INFO: (12) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname2/proxy/: bar (200; 27.579061ms)
Jun 27 18:12:03.965: INFO: (13) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 6.087844ms)
Jun 27 18:12:03.976: INFO: (13) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">... (200; 15.009042ms)
Jun 27 18:12:03.977: INFO: (13) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname2/proxy/: bar (200; 16.313093ms)
Jun 27 18:12:03.979: INFO: (13) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 17.353263ms)
Jun 27 18:12:03.979: INFO: (13) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname2/proxy/: bar (200; 18.790095ms)
Jun 27 18:12:03.980: INFO: (13) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname2/proxy/: tls qux (200; 20.182705ms)
Jun 27 18:12:03.980: INFO: (13) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 19.292195ms)
Jun 27 18:12:03.981: INFO: (13) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">test<... (200; 19.522245ms)
Jun 27 18:12:03.981: INFO: (13) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 20.404755ms)
Jun 27 18:12:03.981: INFO: (13) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/rewriteme">test</a> (200; 20.771156ms)
Jun 27 18:12:03.982: INFO: (13) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/tlsrewritem... (200; 22.266877ms)
Jun 27 18:12:03.982: INFO: (13) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname1/proxy/: foo (200; 22.602347ms)
Jun 27 18:12:03.984: INFO: (13) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname1/proxy/: tls baz (200; 23.547298ms)
Jun 27 18:12:03.984: INFO: (13) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:460/proxy/: tls baz (200; 24.496629ms)
Jun 27 18:12:03.985: INFO: (13) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname1/proxy/: foo (200; 23.192728ms)
Jun 27 18:12:03.985: INFO: (13) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:462/proxy/: tls qux (200; 24.526189ms)
Jun 27 18:12:04.000: INFO: (14) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname2/proxy/: bar (200; 15.024942ms)
Jun 27 18:12:04.000: INFO: (14) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname1/proxy/: tls baz (200; 15.195912ms)
Jun 27 18:12:04.000: INFO: (14) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 14.964372ms)
Jun 27 18:12:04.001: INFO: (14) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname1/proxy/: foo (200; 15.029421ms)
Jun 27 18:12:04.001: INFO: (14) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/rewriteme">test</a> (200; 15.513562ms)
Jun 27 18:12:04.001: INFO: (14) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname1/proxy/: foo (200; 15.350422ms)
Jun 27 18:12:04.003: INFO: (14) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/tlsrewritem... (200; 17.210693ms)
Jun 27 18:12:04.003: INFO: (14) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:460/proxy/: tls baz (200; 18.096244ms)
Jun 27 18:12:04.003: INFO: (14) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 17.608003ms)
Jun 27 18:12:04.004: INFO: (14) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">... (200; 17.919103ms)
Jun 27 18:12:04.004: INFO: (14) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 18.475064ms)
Jun 27 18:12:04.004: INFO: (14) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">test<... (200; 18.390664ms)
Jun 27 18:12:04.005: INFO: (14) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 20.099025ms)
Jun 27 18:12:04.006: INFO: (14) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname2/proxy/: bar (200; 20.794296ms)
Jun 27 18:12:04.006: INFO: (14) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname2/proxy/: tls qux (200; 19.993455ms)
Jun 27 18:12:04.006: INFO: (14) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:462/proxy/: tls qux (200; 21.126876ms)
Jun 27 18:12:04.011: INFO: (15) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:462/proxy/: tls qux (200; 5.368354ms)
Jun 27 18:12:04.020: INFO: (15) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/tlsrewritem... (200; 13.58343ms)
Jun 27 18:12:04.021: INFO: (15) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 14.662471ms)
Jun 27 18:12:04.022: INFO: (15) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 14.931191ms)
Jun 27 18:12:04.023: INFO: (15) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname1/proxy/: foo (200; 15.889472ms)
Jun 27 18:12:04.023: INFO: (15) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:460/proxy/: tls baz (200; 16.258592ms)
Jun 27 18:12:04.023: INFO: (15) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 16.793822ms)
Jun 27 18:12:04.024: INFO: (15) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 17.332464ms)
Jun 27 18:12:04.024: INFO: (15) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">... (200; 17.866413ms)
Jun 27 18:12:04.024: INFO: (15) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">test<... (200; 18.375194ms)
Jun 27 18:12:04.026: INFO: (15) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/rewriteme">test</a> (200; 19.749695ms)
Jun 27 18:12:04.026: INFO: (15) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname1/proxy/: foo (200; 19.498885ms)
Jun 27 18:12:04.027: INFO: (15) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname2/proxy/: bar (200; 20.235875ms)
Jun 27 18:12:04.027: INFO: (15) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname2/proxy/: bar (200; 19.917146ms)
Jun 27 18:12:04.028: INFO: (15) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname2/proxy/: tls qux (200; 21.226266ms)
Jun 27 18:12:04.028: INFO: (15) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname1/proxy/: tls baz (200; 22.244217ms)
Jun 27 18:12:04.043: INFO: (16) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 13.930481ms)
Jun 27 18:12:04.044: INFO: (16) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/rewriteme">test</a> (200; 13.55267ms)
Jun 27 18:12:04.044: INFO: (16) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:460/proxy/: tls baz (200; 14.295621ms)
Jun 27 18:12:04.044: INFO: (16) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 15.118151ms)
Jun 27 18:12:04.045: INFO: (16) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">test<... (200; 15.778812ms)
Jun 27 18:12:04.045: INFO: (16) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 16.234503ms)
Jun 27 18:12:04.045: INFO: (16) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">... (200; 15.748292ms)
Jun 27 18:12:04.045: INFO: (16) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/tlsrewritem... (200; 15.127202ms)
Jun 27 18:12:04.045: INFO: (16) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 14.769551ms)
Jun 27 18:12:04.050: INFO: (16) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname1/proxy/: foo (200; 21.120246ms)
Jun 27 18:12:04.050: INFO: (16) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname2/proxy/: tls qux (200; 20.630676ms)
Jun 27 18:12:04.050: INFO: (16) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname1/proxy/: foo (200; 20.361106ms)
Jun 27 18:12:04.050: INFO: (16) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:462/proxy/: tls qux (200; 20.076805ms)
Jun 27 18:12:04.050: INFO: (16) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname1/proxy/: tls baz (200; 19.949705ms)
Jun 27 18:12:04.050: INFO: (16) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname2/proxy/: bar (200; 19.806945ms)
Jun 27 18:12:04.050: INFO: (16) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname2/proxy/: bar (200; 20.152345ms)
Jun 27 18:12:04.065: INFO: (17) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/rewriteme">test</a> (200; 12.084309ms)
Jun 27 18:12:04.065: INFO: (17) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname1/proxy/: foo (200; 13.53105ms)
Jun 27 18:12:04.065: INFO: (17) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 12.334049ms)
Jun 27 18:12:04.066: INFO: (17) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:462/proxy/: tls qux (200; 13.587461ms)
Jun 27 18:12:04.069: INFO: (17) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">... (200; 17.831074ms)
Jun 27 18:12:04.070: INFO: (17) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname1/proxy/: tls baz (200; 17.005043ms)
Jun 27 18:12:04.070: INFO: (17) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">test<... (200; 18.999745ms)
Jun 27 18:12:04.070: INFO: (17) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:460/proxy/: tls baz (200; 17.788004ms)
Jun 27 18:12:04.070: INFO: (17) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 16.791593ms)
Jun 27 18:12:04.070: INFO: (17) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/tlsrewritem... (200; 18.095304ms)
Jun 27 18:12:04.070: INFO: (17) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 19.126624ms)
Jun 27 18:12:04.070: INFO: (17) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname2/proxy/: tls qux (200; 18.543574ms)
Jun 27 18:12:04.070: INFO: (17) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname2/proxy/: bar (200; 17.870373ms)
Jun 27 18:12:04.071: INFO: (17) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 19.509585ms)
Jun 27 18:12:04.071: INFO: (17) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname1/proxy/: foo (200; 18.424134ms)
Jun 27 18:12:04.071: INFO: (17) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname2/proxy/: bar (200; 20.866206ms)
Jun 27 18:12:04.080: INFO: (18) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 9.107237ms)
Jun 27 18:12:04.081: INFO: (18) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:462/proxy/: tls qux (200; 9.981048ms)
Jun 27 18:12:04.083: INFO: (18) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/rewriteme">test</a> (200; 11.448969ms)
Jun 27 18:12:04.083: INFO: (18) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:460/proxy/: tls baz (200; 11.822769ms)
Jun 27 18:12:04.085: INFO: (18) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 13.19058ms)
Jun 27 18:12:04.085: INFO: (18) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/tlsrewritem... (200; 13.07849ms)
Jun 27 18:12:04.085: INFO: (18) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">test<... (200; 12.98117ms)
Jun 27 18:12:04.085: INFO: (18) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 13.421281ms)
Jun 27 18:12:04.085: INFO: (18) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 13.430951ms)
Jun 27 18:12:04.089: INFO: (18) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname2/proxy/: bar (200; 17.418063ms)
Jun 27 18:12:04.089: INFO: (18) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname1/proxy/: tls baz (200; 17.826244ms)
Jun 27 18:12:04.090: INFO: (18) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname1/proxy/: foo (200; 18.121484ms)
Jun 27 18:12:04.090: INFO: (18) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname1/proxy/: foo (200; 17.947994ms)
Jun 27 18:12:04.090: INFO: (18) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname2/proxy/: bar (200; 18.720204ms)
Jun 27 18:12:04.091: INFO: (18) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">... (200; 19.703905ms)
Jun 27 18:12:04.091: INFO: (18) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname2/proxy/: tls qux (200; 19.797935ms)
Jun 27 18:12:04.100: INFO: (19) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 8.485456ms)
Jun 27 18:12:04.100: INFO: (19) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">... (200; 8.617936ms)
Jun 27 18:12:04.105: INFO: (19) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 13.36269ms)
Jun 27 18:12:04.109: INFO: (19) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:462/proxy/: tls qux (200; 17.333574ms)
Jun 27 18:12:04.109: INFO: (19) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:160/proxy/: foo (200; 17.038503ms)
Jun 27 18:12:04.109: INFO: (19) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb/proxy/rewriteme">test</a> (200; 17.149653ms)
Jun 27 18:12:04.110: INFO: (19) /api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/proxy-service-drs5d-vnwqb:1080/proxy/rewriteme">test<... (200; 17.932903ms)
Jun 27 18:12:04.110: INFO: (19) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:460/proxy/: tls baz (200; 18.238224ms)
Jun 27 18:12:04.111: INFO: (19) /api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/: <a href="/api/v1/namespaces/proxy-5997/pods/https:proxy-service-drs5d-vnwqb:443/proxy/tlsrewritem... (200; 19.761726ms)
Jun 27 18:12:04.111: INFO: (19) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname1/proxy/: foo (200; 19.727245ms)
Jun 27 18:12:04.113: INFO: (19) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname1/proxy/: tls baz (200; 21.448056ms)
Jun 27 18:12:04.113: INFO: (19) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname1/proxy/: foo (200; 21.861207ms)
Jun 27 18:12:04.113: INFO: (19) /api/v1/namespaces/proxy-5997/services/https:proxy-service-drs5d:tlsportname2/proxy/: tls qux (200; 21.881757ms)
Jun 27 18:12:04.113: INFO: (19) /api/v1/namespaces/proxy-5997/services/http:proxy-service-drs5d:portname2/proxy/: bar (200; 21.313506ms)
Jun 27 18:12:04.113: INFO: (19) /api/v1/namespaces/proxy-5997/pods/http:proxy-service-drs5d-vnwqb:162/proxy/: bar (200; 21.502056ms)
Jun 27 18:12:04.113: INFO: (19) /api/v1/namespaces/proxy-5997/services/proxy-service-drs5d:portname2/proxy/: bar (200; 21.509996ms)
STEP: deleting ReplicationController proxy-service-drs5d in namespace proxy-5997, will wait for the garbage collector to delete the pods
Jun 27 18:12:04.175: INFO: Deleting ReplicationController proxy-service-drs5d took: 7.576045ms
Jun 27 18:12:04.575: INFO: Terminating ReplicationController proxy-service-drs5d pods took: 400.270626ms
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:12:15.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5997" for this suite.
Jun 27 18:12:21.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:12:21.325: INFO: namespace proxy-5997 deletion completed in 6.144304658s

• [SLOW TEST:29.951 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:12:21.326: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-4493
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 27 18:12:21.494: INFO: Waiting up to 5m0s for pod "pod-ef94ed37-5d18-4eb8-aba9-7e57830161ac" in namespace "emptydir-4493" to be "success or failure"
Jun 27 18:12:21.499: INFO: Pod "pod-ef94ed37-5d18-4eb8-aba9-7e57830161ac": Phase="Pending", Reason="", readiness=false. Elapsed: 5.315214ms
Jun 27 18:12:23.503: INFO: Pod "pod-ef94ed37-5d18-4eb8-aba9-7e57830161ac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009345623s
STEP: Saw pod success
Jun 27 18:12:23.503: INFO: Pod "pod-ef94ed37-5d18-4eb8-aba9-7e57830161ac" satisfied condition "success or failure"
Jun 27 18:12:23.507: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nzp9p pod pod-ef94ed37-5d18-4eb8-aba9-7e57830161ac container test-container: <nil>
STEP: delete the pod
Jun 27 18:12:23.529: INFO: Waiting for pod pod-ef94ed37-5d18-4eb8-aba9-7e57830161ac to disappear
Jun 27 18:12:23.534: INFO: Pod pod-ef94ed37-5d18-4eb8-aba9-7e57830161ac no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:12:23.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4493" for this suite.
Jun 27 18:12:29.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:12:29.701: INFO: namespace emptydir-4493 deletion completed in 6.16053422s

• [SLOW TEST:8.375 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:12:29.701: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename sched-pred
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in sched-pred-5735
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jun 27 18:12:29.863: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 27 18:12:29.872: INFO: Waiting for terminating namespaces to be deleted...
Jun 27 18:12:29.876: INFO: 
Logging pods the kubelet thinks is on node talos-test-cluster-workers-84c9684cd-mskvs before test
Jun 27 18:12:29.889: INFO: calico-node-8pbvx from kube-system started at 2019-06-27 16:57:06 +0000 UTC (1 container statuses recorded)
Jun 27 18:12:29.889: INFO: 	Container calico-node ready: true, restart count 0
Jun 27 18:12:29.889: INFO: coredns-5c98db65d4-5xpvh from kube-system started at 2019-06-27 16:57:22 +0000 UTC (1 container statuses recorded)
Jun 27 18:12:29.889: INFO: 	Container coredns ready: true, restart count 0
Jun 27 18:12:29.889: INFO: sonobuoy-e2e-job-a038e5b194bb40cd from heptio-sonobuoy started at 2019-06-27 16:59:08 +0000 UTC (2 container statuses recorded)
Jun 27 18:12:29.889: INFO: 	Container e2e ready: true, restart count 0
Jun 27 18:12:29.889: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 27 18:12:29.889: INFO: kube-proxy-94s9p from kube-system started at 2019-06-27 16:57:11 +0000 UTC (1 container statuses recorded)
Jun 27 18:12:29.889: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 27 18:12:29.889: INFO: calico-kube-controllers-65c994fdb-6ss7n from kube-system started at 2019-06-27 16:57:22 +0000 UTC (1 container statuses recorded)
Jun 27 18:12:29.889: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 27 18:12:29.889: INFO: coredns-5c98db65d4-7xgq9 from kube-system started at 2019-06-27 16:57:22 +0000 UTC (1 container statuses recorded)
Jun 27 18:12:29.889: INFO: 	Container coredns ready: true, restart count 0
Jun 27 18:12:29.889: INFO: 
Logging pods the kubelet thinks is on node talos-test-cluster-workers-84c9684cd-nk6xg before test
Jun 27 18:12:29.897: INFO: calico-node-5knrj from kube-system started at 2019-06-27 16:57:02 +0000 UTC (1 container statuses recorded)
Jun 27 18:12:29.897: INFO: 	Container calico-node ready: true, restart count 0
Jun 27 18:12:29.897: INFO: kube-proxy-l4whf from kube-system started at 2019-06-27 16:57:07 +0000 UTC (1 container statuses recorded)
Jun 27 18:12:29.897: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 27 18:12:29.897: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-27 16:58:56 +0000 UTC (1 container statuses recorded)
Jun 27 18:12:29.897: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 27 18:12:29.897: INFO: 
Logging pods the kubelet thinks is on node talos-test-cluster-workers-84c9684cd-nzp9p before test
Jun 27 18:12:29.905: INFO: calico-node-w546x from kube-system started at 2019-06-27 16:57:04 +0000 UTC (1 container statuses recorded)
Jun 27 18:12:29.905: INFO: 	Container calico-node ready: true, restart count 0
Jun 27 18:12:29.905: INFO: kube-proxy-s2qrr from kube-system started at 2019-06-27 16:57:09 +0000 UTC (1 container statuses recorded)
Jun 27 18:12:29.905: INFO: 	Container kube-proxy ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15ac20b90dc77bb6], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:12:30.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5735" for this suite.
Jun 27 18:12:36.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:12:37.080: INFO: namespace sched-pred-5735 deletion completed in 6.140895045s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.379 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:12:37.080: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-7983
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 27 18:12:37.248: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e9a03413-3fec-48d5-bda3-969af2d0e25b" in namespace "projected-7983" to be "success or failure"
Jun 27 18:12:37.254: INFO: Pod "downwardapi-volume-e9a03413-3fec-48d5-bda3-969af2d0e25b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.683704ms
Jun 27 18:12:39.258: INFO: Pod "downwardapi-volume-e9a03413-3fec-48d5-bda3-969af2d0e25b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009852593s
STEP: Saw pod success
Jun 27 18:12:39.258: INFO: Pod "downwardapi-volume-e9a03413-3fec-48d5-bda3-969af2d0e25b" satisfied condition "success or failure"
Jun 27 18:12:39.262: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod downwardapi-volume-e9a03413-3fec-48d5-bda3-969af2d0e25b container client-container: <nil>
STEP: delete the pod
Jun 27 18:12:39.291: INFO: Waiting for pod downwardapi-volume-e9a03413-3fec-48d5-bda3-969af2d0e25b to disappear
Jun 27 18:12:39.296: INFO: Pod downwardapi-volume-e9a03413-3fec-48d5-bda3-969af2d0e25b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:12:39.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7983" for this suite.
Jun 27 18:12:45.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:12:45.461: INFO: namespace projected-7983 deletion completed in 6.155262876s

• [SLOW TEST:8.381 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:12:45.462: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename containers
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in containers-792
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Jun 27 18:12:45.629: INFO: Waiting up to 5m0s for pod "client-containers-fbe74129-e92c-4e59-bed9-0edb0ad2319f" in namespace "containers-792" to be "success or failure"
Jun 27 18:12:45.635: INFO: Pod "client-containers-fbe74129-e92c-4e59-bed9-0edb0ad2319f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.849524ms
Jun 27 18:12:47.639: INFO: Pod "client-containers-fbe74129-e92c-4e59-bed9-0edb0ad2319f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010527124s
STEP: Saw pod success
Jun 27 18:12:47.639: INFO: Pod "client-containers-fbe74129-e92c-4e59-bed9-0edb0ad2319f" satisfied condition "success or failure"
Jun 27 18:12:47.644: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-mskvs pod client-containers-fbe74129-e92c-4e59-bed9-0edb0ad2319f container test-container: <nil>
STEP: delete the pod
Jun 27 18:12:47.665: INFO: Waiting for pod client-containers-fbe74129-e92c-4e59-bed9-0edb0ad2319f to disappear
Jun 27 18:12:47.668: INFO: Pod client-containers-fbe74129-e92c-4e59-bed9-0edb0ad2319f no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:12:47.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-792" for this suite.
Jun 27 18:12:53.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:12:53.822: INFO: namespace containers-792 deletion completed in 6.148384691s

• [SLOW TEST:8.360 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:12:53.823: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-6256
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:12:54.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6256" for this suite.
Jun 27 18:13:00.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:13:00.158: INFO: namespace kubelet-test-6256 deletion completed in 6.14659705s

• [SLOW TEST:6.335 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:13:00.158: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7189
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-b0bbcf11-c70b-4062-a050-73cad7c5e29a in namespace container-probe-7189
Jun 27 18:13:04.344: INFO: Started pod liveness-b0bbcf11-c70b-4062-a050-73cad7c5e29a in namespace container-probe-7189
STEP: checking the pod's current state and verifying that restartCount is present
Jun 27 18:13:04.347: INFO: Initial restart count of pod liveness-b0bbcf11-c70b-4062-a050-73cad7c5e29a is 0
Jun 27 18:13:18.377: INFO: Restart count of pod container-probe-7189/liveness-b0bbcf11-c70b-4062-a050-73cad7c5e29a is now 1 (14.029204963s elapsed)
Jun 27 18:13:38.418: INFO: Restart count of pod container-probe-7189/liveness-b0bbcf11-c70b-4062-a050-73cad7c5e29a is now 2 (34.070489914s elapsed)
Jun 27 18:13:58.456: INFO: Restart count of pod container-probe-7189/liveness-b0bbcf11-c70b-4062-a050-73cad7c5e29a is now 3 (54.108627612s elapsed)
Jun 27 18:14:18.488: INFO: Restart count of pod container-probe-7189/liveness-b0bbcf11-c70b-4062-a050-73cad7c5e29a is now 4 (1m14.145679428s elapsed)
Jun 27 18:15:28.624: INFO: Restart count of pod container-probe-7189/liveness-b0bbcf11-c70b-4062-a050-73cad7c5e29a is now 5 (2m24.281652517s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:15:28.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7189" for this suite.
Jun 27 18:15:34.659: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:15:34.795: INFO: namespace container-probe-7189 deletion completed in 6.151550073s

• [SLOW TEST:154.642 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:15:34.795: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename container-probe
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-probe-7116
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 27 18:15:56.971: INFO: Container started at 2019-06-27 18:15:36 +0000 UTC, pod became ready at 2019-06-27 18:15:56 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:15:56.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7116" for this suite.
Jun 27 18:16:08.990: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:16:09.124: INFO: namespace container-probe-7116 deletion completed in 12.148253879s

• [SLOW TEST:34.329 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:16:09.125: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-5611
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-25d7b928-c462-4568-af43-5049632d2e4c
STEP: Creating a pod to test consume secrets
Jun 27 18:16:09.310: INFO: Waiting up to 5m0s for pod "pod-secrets-6692e252-d346-4b28-9e4f-a1345d6370db" in namespace "secrets-5611" to be "success or failure"
Jun 27 18:16:09.316: INFO: Pod "pod-secrets-6692e252-d346-4b28-9e4f-a1345d6370db": Phase="Pending", Reason="", readiness=false. Elapsed: 5.101444ms
Jun 27 18:16:11.319: INFO: Pod "pod-secrets-6692e252-d346-4b28-9e4f-a1345d6370db": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008831572s
Jun 27 18:16:13.323: INFO: Pod "pod-secrets-6692e252-d346-4b28-9e4f-a1345d6370db": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012831661s
STEP: Saw pod success
Jun 27 18:16:13.323: INFO: Pod "pod-secrets-6692e252-d346-4b28-9e4f-a1345d6370db" satisfied condition "success or failure"
Jun 27 18:16:13.327: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nzp9p pod pod-secrets-6692e252-d346-4b28-9e4f-a1345d6370db container secret-volume-test: <nil>
STEP: delete the pod
Jun 27 18:16:13.351: INFO: Waiting for pod pod-secrets-6692e252-d346-4b28-9e4f-a1345d6370db to disappear
Jun 27 18:16:13.356: INFO: Pod pod-secrets-6692e252-d346-4b28-9e4f-a1345d6370db no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:16:13.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5611" for this suite.
Jun 27 18:16:19.376: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:16:19.521: INFO: namespace secrets-5611 deletion completed in 6.158463358s

• [SLOW TEST:10.397 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:16:19.522: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-4022
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7557
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-2761
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:16:46.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4022" for this suite.
Jun 27 18:16:52.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:16:52.169: INFO: namespace namespaces-4022 deletion completed in 6.14659553s
STEP: Destroying namespace "nsdeletetest-7557" for this suite.
Jun 27 18:16:52.172: INFO: Namespace nsdeletetest-7557 was already deleted
STEP: Destroying namespace "nsdeletetest-2761" for this suite.
Jun 27 18:16:58.187: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:16:58.320: INFO: namespace nsdeletetest-2761 deletion completed in 6.14754432s

• [SLOW TEST:38.798 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:16:58.320: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename proxy
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in proxy-6886
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 27 18:16:58.494: INFO: (0) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 9.248877ms)
Jun 27 18:16:58.500: INFO: (1) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.959325ms)
Jun 27 18:16:58.506: INFO: (2) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.633594ms)
Jun 27 18:16:58.512: INFO: (3) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.841954ms)
Jun 27 18:16:58.517: INFO: (4) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.368555ms)
Jun 27 18:16:58.523: INFO: (5) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 6.115694ms)
Jun 27 18:16:58.530: INFO: (6) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 6.367575ms)
Jun 27 18:16:58.535: INFO: (7) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.495014ms)
Jun 27 18:16:58.541: INFO: (8) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.972115ms)
Jun 27 18:16:58.547: INFO: (9) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.769664ms)
Jun 27 18:16:58.553: INFO: (10) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.965175ms)
Jun 27 18:16:58.559: INFO: (11) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.636564ms)
Jun 27 18:16:58.565: INFO: (12) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 6.272845ms)
Jun 27 18:16:58.571: INFO: (13) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 6.047275ms)
Jun 27 18:16:58.577: INFO: (14) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.568364ms)
Jun 27 18:16:58.583: INFO: (15) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.902725ms)
Jun 27 18:16:58.589: INFO: (16) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.982524ms)
Jun 27 18:16:58.595: INFO: (17) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 6.122635ms)
Jun 27 18:16:58.601: INFO: (18) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.876744ms)
Jun 27 18:16:58.607: INFO: (19) /api/v1/nodes/talos-test-cluster-workers-84c9684cd-mskvs:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.952954ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:16:58.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6886" for this suite.
Jun 27 18:17:04.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:17:04.762: INFO: namespace proxy-6886 deletion completed in 6.149863512s

• [SLOW TEST:6.442 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:17:04.763: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename events
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in events-9286
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jun 27 18:17:06.947: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-96597355-133f-4237-b83d-b66423eaef0b,GenerateName:,Namespace:events-9286,SelfLink:/api/v1/namespaces/events-9286/pods/send-events-96597355-133f-4237-b83d-b66423eaef0b,UID:b7e2f75c-909a-436c-94cc-1c7cf123896b,ResourceVersion:21918,Generation:0,CreationTimestamp:2019-06-27 18:17:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 919432811,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.244.20.242/32,kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-hd4f8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-hd4f8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-hd4f8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-mskvs,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0031401b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0031401d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 18:17:04 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 18:17:06 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 18:17:06 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 18:17:04 +0000 UTC  }],Message:,Reason:,HostIP:147.75.109.65,PodIP:10.244.20.242,StartTime:2019-06-27 18:17:04 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-06-27 18:17:05 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 containerd://c13410beb4301a86e7773efd81f147419ed3fe60b35f2bdffed15e510d7068cd}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jun 27 18:17:08.951: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jun 27 18:17:10.956: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:17:10.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9286" for this suite.
Jun 27 18:17:50.984: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:17:51.121: INFO: namespace events-9286 deletion completed in 40.153176984s

• [SLOW TEST:46.358 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:17:51.121: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5183
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 27 18:17:51.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 create -f - --namespace=kubectl-5183'
Jun 27 18:17:51.765: INFO: stderr: ""
Jun 27 18:17:51.765: INFO: stdout: "replicationcontroller/redis-master created\n"
Jun 27 18:17:51.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 create -f - --namespace=kubectl-5183'
Jun 27 18:17:52.193: INFO: stderr: ""
Jun 27 18:17:52.193: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 27 18:17:53.197: INFO: Selector matched 1 pods for map[app:redis]
Jun 27 18:17:53.197: INFO: Found 1 / 1
Jun 27 18:17:53.198: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 27 18:17:53.202: INFO: Selector matched 1 pods for map[app:redis]
Jun 27 18:17:53.202: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 27 18:17:53.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 describe pod redis-master-mj667 --namespace=kubectl-5183'
Jun 27 18:17:53.411: INFO: stderr: ""
Jun 27 18:17:53.411: INFO: stdout: "Name:           redis-master-mj667\nNamespace:      kubectl-5183\nPriority:       0\nNode:           talos-test-cluster-workers-84c9684cd-nzp9p/147.75.70.201\nStart Time:     Thu, 27 Jun 2019 18:17:51 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    cni.projectcalico.org/podIP: 10.244.177.229/32\n                kubernetes.io/psp: e2e-test-privileged-psp\nStatus:         Running\nIP:             10.244.177.229\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   containerd://e30ae3601e2d8afeb71b676582f2d7bfdc5a7108a1f7e4fa5fc3a79cf849f370\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 27 Jun 2019 18:17:52 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-7sq8w (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-7sq8w:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-7sq8w\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                                 Message\n  ----    ------     ----  ----                                                 -------\n  Normal  Scheduled  2s    default-scheduler                                    Successfully assigned kubectl-5183/redis-master-mj667 to talos-test-cluster-workers-84c9684cd-nzp9p\n  Normal  Pulled     1s    kubelet, talos-test-cluster-workers-84c9684cd-nzp9p  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, talos-test-cluster-workers-84c9684cd-nzp9p  Created container redis-master\n  Normal  Started    1s    kubelet, talos-test-cluster-workers-84c9684cd-nzp9p  Started container redis-master\n"
Jun 27 18:17:53.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 describe rc redis-master --namespace=kubectl-5183'
Jun 27 18:17:53.641: INFO: stderr: ""
Jun 27 18:17:53.641: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-5183\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-mj667\n"
Jun 27 18:17:53.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 describe service redis-master --namespace=kubectl-5183'
Jun 27 18:17:53.839: INFO: stderr: ""
Jun 27 18:17:53.839: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-5183\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.100.76.217\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.244.177.229:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun 27 18:17:53.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 describe node talos-test-cluster-master-0'
Jun 27 18:17:54.073: INFO: stderr: ""
Jun 27 18:17:54.073: INFO: stdout: "Name:               talos-test-cluster-master-0\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=talos-test-cluster-master-0\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: /run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 139.178.70.165/31\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.244.142.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 27 Jun 2019 16:50:45 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 27 Jun 2019 16:57:27 +0000   Thu, 27 Jun 2019 16:57:27 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 27 Jun 2019 18:17:39 +0000   Thu, 27 Jun 2019 16:50:45 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 27 Jun 2019 18:17:39 +0000   Thu, 27 Jun 2019 16:50:45 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 27 Jun 2019 18:17:39 +0000   Thu, 27 Jun 2019 16:50:45 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 27 Jun 2019 18:17:39 +0000   Thu, 27 Jun 2019 16:57:25 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  139.178.70.168\n  Hostname:    talos-test-cluster-master-0\nCapacity:\n cpu:                4\n ephemeral-storage:  141513108Ki\n hugepages-2Mi:      0\n memory:             8146132Ki\n pods:               110\nAllocatable:\n cpu:                4\n ephemeral-storage:  130418480117\n hugepages-2Mi:      0\n memory:             8043732Ki\n pods:               110\nSystem Info:\n Machine ID:                 311936e2303b034fe7ef70182235b8cb\n System UUID:                311936e2303b034fe7ef70182235b8cb\n Boot ID:                    c99a940b-79fe-4448-af49-ecddf1728f54\n Kernel Version:             4.19.40-talos\n OS Image:                   Talos (v0.1.0-beta.0)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  containerd://1.2.6\n Kubelet Version:            v1.15.0\n Kube-Proxy Version:         v1.15.0\nPodCIDR:                     10.244.0.0/24\nNon-terminated Pods:         (6 in total)\n  Namespace                  Name                                                   CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                   ------------  ----------  ---------------  -------------  ---\n  kube-system                calico-node-l9xn7                                      250m (6%)     0 (0%)      0 (0%)           0 (0%)         80m\n  kube-system                etcd-talos-test-cluster-master-0                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         84m\n  kube-system                kube-apiserver-talos-test-cluster-master-0             250m (6%)     0 (0%)      0 (0%)           0 (0%)         84m\n  kube-system                kube-controller-manager-talos-test-cluster-master-0    200m (5%)     0 (0%)      0 (0%)           0 (0%)         84m\n  kube-system                kube-proxy-ccswj                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         80m\n  kube-system                kube-scheduler-talos-test-cluster-master-0             100m (2%)     0 (0%)      0 (0%)           0 (0%)         84m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                800m (20%)  0 (0%)\n  memory             0 (0%)      0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
Jun 27 18:17:54.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 describe namespace kubectl-5183'
Jun 27 18:17:54.271: INFO: stderr: ""
Jun 27 18:17:54.271: INFO: stdout: "Name:         kubectl-5183\nLabels:       e2e-framework=kubectl\n              e2e-run=29bcf2a6-66cb-4dd4-ad10-778e2965a5e8\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:17:54.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5183" for this suite.
Jun 27 18:18:16.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:18:16.421: INFO: namespace kubectl-5183 deletion completed in 22.144860255s

• [SLOW TEST:25.300 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:18:16.422: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2467
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 27 18:18:16.592: INFO: Waiting up to 5m0s for pod "pod-74cf7222-a011-4d09-83be-36e3583e50d6" in namespace "emptydir-2467" to be "success or failure"
Jun 27 18:18:16.597: INFO: Pod "pod-74cf7222-a011-4d09-83be-36e3583e50d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.890414ms
Jun 27 18:18:18.601: INFO: Pod "pod-74cf7222-a011-4d09-83be-36e3583e50d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008786563s
STEP: Saw pod success
Jun 27 18:18:18.601: INFO: Pod "pod-74cf7222-a011-4d09-83be-36e3583e50d6" satisfied condition "success or failure"
Jun 27 18:18:18.605: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod pod-74cf7222-a011-4d09-83be-36e3583e50d6 container test-container: <nil>
STEP: delete the pod
Jun 27 18:18:18.636: INFO: Waiting for pod pod-74cf7222-a011-4d09-83be-36e3583e50d6 to disappear
Jun 27 18:18:18.644: INFO: Pod pod-74cf7222-a011-4d09-83be-36e3583e50d6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:18:18.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2467" for this suite.
Jun 27 18:18:24.666: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:18:24.800: INFO: namespace emptydir-2467 deletion completed in 6.150349472s

• [SLOW TEST:8.378 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:18:24.801: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5402
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Jun 27 18:18:24.959: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-307330860 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:18:25.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5402" for this suite.
Jun 27 18:18:31.135: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:18:31.287: INFO: namespace kubectl-5402 deletion completed in 6.164675783s

• [SLOW TEST:6.486 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:18:31.287: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename namespaces
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in namespaces-9341
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-7604
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in nsdeletetest-8279
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:18:37.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9341" for this suite.
Jun 27 18:18:43.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:18:43.926: INFO: namespace namespaces-9341 deletion completed in 6.142272505s
STEP: Destroying namespace "nsdeletetest-7604" for this suite.
Jun 27 18:18:43.930: INFO: Namespace nsdeletetest-7604 was already deleted
STEP: Destroying namespace "nsdeletetest-8279" for this suite.
Jun 27 18:18:49.946: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:18:50.077: INFO: namespace nsdeletetest-8279 deletion completed in 6.146798729s

• [SLOW TEST:18.793 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:18:50.077: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-4310
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Jun 27 18:18:52.260: INFO: Pod pod-hostip-11c47f9a-82cc-4a62-a9b2-8b0b7b48f3d2 has hostIP: 147.75.109.65
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:18:52.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4310" for this suite.
Jun 27 18:19:14.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:19:14.411: INFO: namespace pods-4310 deletion completed in 22.146397577s

• [SLOW TEST:24.334 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:19:14.411: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-9750
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-9750
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-9750
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9750
Jun 27 18:19:14.601: INFO: Found 0 stateful pods, waiting for 1
Jun 27 18:19:24.605: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun 27 18:19:24.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 exec --namespace=statefulset-9750 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 27 18:19:24.984: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 27 18:19:24.984: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 27 18:19:24.984: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 27 18:19:24.988: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 27 18:19:34.992: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 27 18:19:34.992: INFO: Waiting for statefulset status.replicas updated to 0
Jun 27 18:19:35.009: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999937s
Jun 27 18:19:36.013: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995781724s
Jun 27 18:19:37.018: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.991361228s
Jun 27 18:19:38.022: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987031762s
Jun 27 18:19:39.027: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.982671855s
Jun 27 18:19:40.031: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.978042689s
Jun 27 18:19:41.037: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.973597163s
Jun 27 18:19:42.041: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.968075395s
Jun 27 18:19:43.046: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.963236339s
Jun 27 18:19:44.050: INFO: Verifying statefulset ss doesn't scale past 1 for another 958.975393ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9750
Jun 27 18:19:45.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 exec --namespace=statefulset-9750 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 27 18:19:45.436: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 27 18:19:45.436: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 27 18:19:45.436: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 27 18:19:45.440: INFO: Found 1 stateful pods, waiting for 3
Jun 27 18:19:55.445: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 27 18:19:55.445: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 27 18:19:55.445: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun 27 18:19:55.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 exec --namespace=statefulset-9750 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 27 18:19:55.828: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 27 18:19:55.828: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 27 18:19:55.828: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 27 18:19:55.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 exec --namespace=statefulset-9750 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 27 18:19:56.214: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 27 18:19:56.214: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 27 18:19:56.214: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 27 18:19:56.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 exec --namespace=statefulset-9750 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 27 18:19:56.614: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 27 18:19:56.614: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 27 18:19:56.614: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 27 18:19:56.614: INFO: Waiting for statefulset status.replicas updated to 0
Jun 27 18:19:56.618: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Jun 27 18:20:06.627: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 27 18:20:06.627: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 27 18:20:06.627: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 27 18:20:06.645: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999935s
Jun 27 18:20:07.650: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.991598541s
Jun 27 18:20:08.654: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986764994s
Jun 27 18:20:09.660: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.982125358s
Jun 27 18:20:10.665: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.977037891s
Jun 27 18:20:11.670: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.971778074s
Jun 27 18:20:12.675: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.966678827s
Jun 27 18:20:13.680: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.96180757s
Jun 27 18:20:14.684: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.957132024s
Jun 27 18:20:15.689: INFO: Verifying statefulset ss doesn't scale past 3 for another 952.527667ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9750
Jun 27 18:20:16.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 exec --namespace=statefulset-9750 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 27 18:20:17.070: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 27 18:20:17.070: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 27 18:20:17.070: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 27 18:20:17.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 exec --namespace=statefulset-9750 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 27 18:20:17.436: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 27 18:20:17.436: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 27 18:20:17.436: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 27 18:20:17.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 exec --namespace=statefulset-9750 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 27 18:20:17.817: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 27 18:20:17.817: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 27 18:20:17.817: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 27 18:20:17.817: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 27 18:20:47.840: INFO: Deleting all statefulset in ns statefulset-9750
Jun 27 18:20:47.844: INFO: Scaling statefulset ss to 0
Jun 27 18:20:47.855: INFO: Waiting for statefulset status.replicas updated to 0
Jun 27 18:20:47.859: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:20:47.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9750" for this suite.
Jun 27 18:20:53.896: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:20:54.030: INFO: namespace statefulset-9750 deletion completed in 6.150284322s

• [SLOW TEST:99.619 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:20:54.031: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-1432
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-0125c88a-1bdb-4fcc-8935-824319c67af2
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-0125c88a-1bdb-4fcc-8935-824319c67af2
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:20:58.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1432" for this suite.
Jun 27 18:21:20.281: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:21:20.412: INFO: namespace projected-1432 deletion completed in 22.143890675s

• [SLOW TEST:26.381 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:21:20.412: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-2046
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Jun 27 18:21:20.581: INFO: Waiting up to 5m0s for pod "pod-6ce91c37-d4da-4046-bb3b-b83f5c155a19" in namespace "emptydir-2046" to be "success or failure"
Jun 27 18:21:20.589: INFO: Pod "pod-6ce91c37-d4da-4046-bb3b-b83f5c155a19": Phase="Pending", Reason="", readiness=false. Elapsed: 7.434156ms
Jun 27 18:21:22.593: INFO: Pod "pod-6ce91c37-d4da-4046-bb3b-b83f5c155a19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011384365s
STEP: Saw pod success
Jun 27 18:21:22.593: INFO: Pod "pod-6ce91c37-d4da-4046-bb3b-b83f5c155a19" satisfied condition "success or failure"
Jun 27 18:21:22.596: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod pod-6ce91c37-d4da-4046-bb3b-b83f5c155a19 container test-container: <nil>
STEP: delete the pod
Jun 27 18:21:22.618: INFO: Waiting for pod pod-6ce91c37-d4da-4046-bb3b-b83f5c155a19 to disappear
Jun 27 18:21:22.621: INFO: Pod pod-6ce91c37-d4da-4046-bb3b-b83f5c155a19 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:21:22.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2046" for this suite.
Jun 27 18:21:28.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:21:28.773: INFO: namespace emptydir-2046 deletion completed in 6.14671805s

• [SLOW TEST:8.361 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:21:28.774: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-7884
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun 27 18:21:28.942: INFO: Waiting up to 5m0s for pod "pod-66cae435-1bf5-49f7-bcf8-3b51e764f977" in namespace "emptydir-7884" to be "success or failure"
Jun 27 18:21:28.946: INFO: Pod "pod-66cae435-1bf5-49f7-bcf8-3b51e764f977": Phase="Pending", Reason="", readiness=false. Elapsed: 3.728683ms
Jun 27 18:21:30.950: INFO: Pod "pod-66cae435-1bf5-49f7-bcf8-3b51e764f977": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007531201s
STEP: Saw pod success
Jun 27 18:21:30.950: INFO: Pod "pod-66cae435-1bf5-49f7-bcf8-3b51e764f977" satisfied condition "success or failure"
Jun 27 18:21:30.953: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nzp9p pod pod-66cae435-1bf5-49f7-bcf8-3b51e764f977 container test-container: <nil>
STEP: delete the pod
Jun 27 18:21:30.976: INFO: Waiting for pod pod-66cae435-1bf5-49f7-bcf8-3b51e764f977 to disappear
Jun 27 18:21:30.979: INFO: Pod pod-66cae435-1bf5-49f7-bcf8-3b51e764f977 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:21:30.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7884" for this suite.
Jun 27 18:21:36.998: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:21:37.127: INFO: namespace emptydir-7884 deletion completed in 6.142442796s

• [SLOW TEST:8.353 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:21:37.128: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-lifecycle-hook-4935
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 27 18:21:41.351: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 27 18:21:41.356: INFO: Pod pod-with-poststart-http-hook still exists
Jun 27 18:21:43.356: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 27 18:21:43.360: INFO: Pod pod-with-poststart-http-hook still exists
Jun 27 18:21:45.356: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 27 18:21:45.360: INFO: Pod pod-with-poststart-http-hook still exists
Jun 27 18:21:47.356: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 27 18:21:47.360: INFO: Pod pod-with-poststart-http-hook still exists
Jun 27 18:21:49.356: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 27 18:21:49.360: INFO: Pod pod-with-poststart-http-hook still exists
Jun 27 18:21:51.356: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 27 18:21:51.360: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:21:51.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4935" for this suite.
Jun 27 18:22:13.379: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:22:13.515: INFO: namespace container-lifecycle-hook-4935 deletion completed in 22.149913989s

• [SLOW TEST:36.387 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:22:13.515: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename pod-network-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pod-network-test-3317
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-3317
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 27 18:22:13.673: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 27 18:22:33.801: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.187.123 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3317 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 27 18:22:33.801: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
Jun 27 18:22:34.974: INFO: Found all expected endpoints: [netserver-0]
Jun 27 18:22:34.978: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.177.232 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3317 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 27 18:22:34.978: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
Jun 27 18:22:36.141: INFO: Found all expected endpoints: [netserver-1]
Jun 27 18:22:36.145: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.20.247 8081 | grep -v '^\s*$'] Namespace:pod-network-test-3317 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 27 18:22:36.145: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
Jun 27 18:22:37.305: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:22:37.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3317" for this suite.
Jun 27 18:22:59.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:22:59.492: INFO: namespace pod-network-test-3317 deletion completed in 22.181110563s

• [SLOW TEST:45.977 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:22:59.492: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-6818
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-4b3a93a1-f7d8-4374-8133-edff9ea56d69
STEP: Creating a pod to test consume configMaps
Jun 27 18:22:59.664: INFO: Waiting up to 5m0s for pod "pod-configmaps-401a5802-7d84-44e7-8b12-f68610f12860" in namespace "configmap-6818" to be "success or failure"
Jun 27 18:22:59.670: INFO: Pod "pod-configmaps-401a5802-7d84-44e7-8b12-f68610f12860": Phase="Pending", Reason="", readiness=false. Elapsed: 5.288584ms
Jun 27 18:23:01.674: INFO: Pod "pod-configmaps-401a5802-7d84-44e7-8b12-f68610f12860": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009316903s
STEP: Saw pod success
Jun 27 18:23:01.674: INFO: Pod "pod-configmaps-401a5802-7d84-44e7-8b12-f68610f12860" satisfied condition "success or failure"
Jun 27 18:23:01.677: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod pod-configmaps-401a5802-7d84-44e7-8b12-f68610f12860 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 27 18:23:01.700: INFO: Waiting for pod pod-configmaps-401a5802-7d84-44e7-8b12-f68610f12860 to disappear
Jun 27 18:23:01.706: INFO: Pod pod-configmaps-401a5802-7d84-44e7-8b12-f68610f12860 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:23:01.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6818" for this suite.
Jun 27 18:23:07.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:23:07.873: INFO: namespace configmap-6818 deletion completed in 6.161632571s

• [SLOW TEST:8.380 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:23:07.873: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename init-container
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in init-container-2797
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jun 27 18:23:08.032: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:23:11.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2797" for this suite.
Jun 27 18:23:18.005: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:23:18.143: INFO: namespace init-container-2797 deletion completed in 6.150567552s

• [SLOW TEST:10.270 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:23:18.144: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename secrets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in secrets-8049
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-628e11b8-cf4e-4cfd-9076-2d45af0a16d4
STEP: Creating a pod to test consume secrets
Jun 27 18:23:18.315: INFO: Waiting up to 5m0s for pod "pod-secrets-5ce65328-ce19-4e3f-854c-c8b1ffad11f4" in namespace "secrets-8049" to be "success or failure"
Jun 27 18:23:18.323: INFO: Pod "pod-secrets-5ce65328-ce19-4e3f-854c-c8b1ffad11f4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.582937ms
Jun 27 18:23:20.337: INFO: Pod "pod-secrets-5ce65328-ce19-4e3f-854c-c8b1ffad11f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.022079053s
STEP: Saw pod success
Jun 27 18:23:20.337: INFO: Pod "pod-secrets-5ce65328-ce19-4e3f-854c-c8b1ffad11f4" satisfied condition "success or failure"
Jun 27 18:23:20.341: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nzp9p pod pod-secrets-5ce65328-ce19-4e3f-854c-c8b1ffad11f4 container secret-volume-test: <nil>
STEP: delete the pod
Jun 27 18:23:20.370: INFO: Waiting for pod pod-secrets-5ce65328-ce19-4e3f-854c-c8b1ffad11f4 to disappear
Jun 27 18:23:20.375: INFO: Pod pod-secrets-5ce65328-ce19-4e3f-854c-c8b1ffad11f4 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:23:20.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8049" for this suite.
Jun 27 18:23:26.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:23:26.527: INFO: namespace secrets-8049 deletion completed in 6.14685114s

• [SLOW TEST:8.383 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:23:26.527: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-7797
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-f1cb83c3-1440-476b-8064-e3c799e5c253
STEP: Creating a pod to test consume configMaps
Jun 27 18:23:26.698: INFO: Waiting up to 5m0s for pod "pod-configmaps-7fcaa50e-9c7f-4559-9b0c-5be3cff42e4e" in namespace "configmap-7797" to be "success or failure"
Jun 27 18:23:26.703: INFO: Pod "pod-configmaps-7fcaa50e-9c7f-4559-9b0c-5be3cff42e4e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.808084ms
Jun 27 18:23:28.707: INFO: Pod "pod-configmaps-7fcaa50e-9c7f-4559-9b0c-5be3cff42e4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008848883s
STEP: Saw pod success
Jun 27 18:23:28.707: INFO: Pod "pod-configmaps-7fcaa50e-9c7f-4559-9b0c-5be3cff42e4e" satisfied condition "success or failure"
Jun 27 18:23:28.711: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod pod-configmaps-7fcaa50e-9c7f-4559-9b0c-5be3cff42e4e container configmap-volume-test: <nil>
STEP: delete the pod
Jun 27 18:23:28.734: INFO: Waiting for pod pod-configmaps-7fcaa50e-9c7f-4559-9b0c-5be3cff42e4e to disappear
Jun 27 18:23:28.738: INFO: Pod pod-configmaps-7fcaa50e-9c7f-4559-9b0c-5be3cff42e4e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:23:28.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7797" for this suite.
Jun 27 18:23:34.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:23:34.890: INFO: namespace configmap-7797 deletion completed in 6.146437409s

• [SLOW TEST:8.363 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:23:34.892: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8841
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 27 18:23:35.106: INFO: Waiting up to 5m0s for pod "pod-6562f6e8-1428-41bb-9662-15a39c0a64e4" in namespace "emptydir-8841" to be "success or failure"
Jun 27 18:23:35.111: INFO: Pod "pod-6562f6e8-1428-41bb-9662-15a39c0a64e4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.192664ms
Jun 27 18:23:37.115: INFO: Pod "pod-6562f6e8-1428-41bb-9662-15a39c0a64e4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008930493s
STEP: Saw pod success
Jun 27 18:23:37.115: INFO: Pod "pod-6562f6e8-1428-41bb-9662-15a39c0a64e4" satisfied condition "success or failure"
Jun 27 18:23:37.119: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-mskvs pod pod-6562f6e8-1428-41bb-9662-15a39c0a64e4 container test-container: <nil>
STEP: delete the pod
Jun 27 18:23:37.145: INFO: Waiting for pod pod-6562f6e8-1428-41bb-9662-15a39c0a64e4 to disappear
Jun 27 18:23:37.148: INFO: Pod pod-6562f6e8-1428-41bb-9662-15a39c0a64e4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:23:37.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8841" for this suite.
Jun 27 18:23:43.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:23:43.302: INFO: namespace emptydir-8841 deletion completed in 6.1479413s

• [SLOW TEST:8.411 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:23:43.303: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename statefulset
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in statefulset-7192
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-7192
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7192
STEP: Creating statefulset with conflicting port in namespace statefulset-7192
STEP: Waiting until pod test-pod will start running in namespace statefulset-7192
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7192
Jun 27 18:23:45.516: INFO: Observed stateful pod in namespace: statefulset-7192, name: ss-0, uid: ffc3bc48-f964-4690-9ec4-3a72b8baa692, status phase: Pending. Waiting for statefulset controller to delete.
Jun 27 18:23:46.096: INFO: Observed stateful pod in namespace: statefulset-7192, name: ss-0, uid: ffc3bc48-f964-4690-9ec4-3a72b8baa692, status phase: Failed. Waiting for statefulset controller to delete.
Jun 27 18:23:46.106: INFO: Observed stateful pod in namespace: statefulset-7192, name: ss-0, uid: ffc3bc48-f964-4690-9ec4-3a72b8baa692, status phase: Failed. Waiting for statefulset controller to delete.
Jun 27 18:23:46.112: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7192
STEP: Removing pod with conflicting port in namespace statefulset-7192
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7192 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 27 18:23:48.146: INFO: Deleting all statefulset in ns statefulset-7192
Jun 27 18:23:48.150: INFO: Scaling statefulset ss to 0
Jun 27 18:23:58.173: INFO: Waiting for statefulset status.replicas updated to 0
Jun 27 18:23:58.177: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:23:58.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7192" for this suite.
Jun 27 18:24:04.215: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:24:04.355: INFO: namespace statefulset-7192 deletion completed in 6.156368547s

• [SLOW TEST:21.052 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:24:04.357: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename svcaccounts
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in svcaccounts-8376
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Jun 27 18:24:07.044: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8376 pod-service-account-acdeab10-4b27-4299-ba94-5c4b9bedd90e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun 27 18:24:07.406: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8376 pod-service-account-acdeab10-4b27-4299-ba94-5c4b9bedd90e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun 27 18:24:07.761: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8376 pod-service-account-acdeab10-4b27-4299-ba94-5c4b9bedd90e -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:24:08.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8376" for this suite.
Jun 27 18:24:14.144: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:24:14.281: INFO: namespace svcaccounts-8376 deletion completed in 6.150918393s

• [SLOW TEST:9.924 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:24:14.282: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename pods
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in pods-8688
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 27 18:24:14.438: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:24:16.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8688" for this suite.
Jun 27 18:25:08.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:25:08.780: INFO: namespace pods-8688 deletion completed in 52.1534104s

• [SLOW TEST:54.498 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:25:08.780: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubelet-test
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubelet-test-9656
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:25:10.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9656" for this suite.
Jun 27 18:25:48.986: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:25:49.118: INFO: namespace kubelet-test-9656 deletion completed in 38.143751912s

• [SLOW TEST:40.337 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:25:49.118: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-5561
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Jun 27 18:25:49.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-307330860 api-versions'
Jun 27 18:25:49.447: INFO: stderr: ""
Jun 27 18:25:49.447: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nsettings.k8s.io/v1alpha1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:25:49.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5561" for this suite.
Jun 27 18:25:55.466: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:25:55.600: INFO: namespace kubectl-5561 deletion completed in 6.147853s

• [SLOW TEST:6.482 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:25:55.601: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename kubectl
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in kubectl-1777
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Jun 27 18:25:55.759: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-307330860 proxy --unix-socket=/tmp/kubectl-proxy-unix344996863/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:25:55.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1777" for this suite.
Jun 27 18:26:01.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:26:02.051: INFO: namespace kubectl-1777 deletion completed in 6.147836151s

• [SLOW TEST:6.450 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:26:02.052: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename container-runtime
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in container-runtime-8923
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 27 18:26:04.240: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:26:04.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8923" for this suite.
Jun 27 18:26:10.294: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:26:10.428: INFO: namespace container-runtime-8923 deletion completed in 6.147397401s

• [SLOW TEST:8.377 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:26:10.429: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-1063
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-4eb4ffe9-d1f0-46a6-a135-9ae8ec94200f
STEP: Creating a pod to test consume configMaps
Jun 27 18:26:10.609: INFO: Waiting up to 5m0s for pod "pod-configmaps-336b2239-6bd3-4f7b-83c3-49a098e7ae84" in namespace "configmap-1063" to be "success or failure"
Jun 27 18:26:10.614: INFO: Pod "pod-configmaps-336b2239-6bd3-4f7b-83c3-49a098e7ae84": Phase="Pending", Reason="", readiness=false. Elapsed: 5.458294ms
Jun 27 18:26:12.618: INFO: Pod "pod-configmaps-336b2239-6bd3-4f7b-83c3-49a098e7ae84": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009274693s
STEP: Saw pod success
Jun 27 18:26:12.618: INFO: Pod "pod-configmaps-336b2239-6bd3-4f7b-83c3-49a098e7ae84" satisfied condition "success or failure"
Jun 27 18:26:12.622: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod pod-configmaps-336b2239-6bd3-4f7b-83c3-49a098e7ae84 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 27 18:26:12.647: INFO: Waiting for pod pod-configmaps-336b2239-6bd3-4f7b-83c3-49a098e7ae84 to disappear
Jun 27 18:26:12.651: INFO: Pod pod-configmaps-336b2239-6bd3-4f7b-83c3-49a098e7ae84 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:26:12.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1063" for this suite.
Jun 27 18:26:18.670: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:26:18.801: INFO: namespace configmap-1063 deletion completed in 6.144375307s

• [SLOW TEST:8.372 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:26:18.802: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename daemonsets
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in daemonsets-4771
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 27 18:26:18.990: INFO: Create a RollingUpdate DaemonSet
Jun 27 18:26:18.995: INFO: Check that daemon pods launch on every node of the cluster
Jun 27 18:26:19.000: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 18:26:19.005: INFO: Number of nodes with available pods: 0
Jun 27 18:26:19.005: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 18:26:20.011: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 18:26:20.015: INFO: Number of nodes with available pods: 0
Jun 27 18:26:20.015: INFO: Node talos-test-cluster-workers-84c9684cd-mskvs is running more than one daemon pod
Jun 27 18:26:21.011: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 18:26:21.015: INFO: Number of nodes with available pods: 3
Jun 27 18:26:21.015: INFO: Number of running nodes: 3, number of available pods: 3
Jun 27 18:26:21.015: INFO: Update the DaemonSet to trigger a rollout
Jun 27 18:26:21.023: INFO: Updating DaemonSet daemon-set
Jun 27 18:26:25.047: INFO: Roll back the DaemonSet before rollout is complete
Jun 27 18:26:25.055: INFO: Updating DaemonSet daemon-set
Jun 27 18:26:25.055: INFO: Make sure DaemonSet rollback is complete
Jun 27 18:26:25.060: INFO: Wrong image for pod: daemon-set-h6c4v. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 27 18:26:25.060: INFO: Pod daemon-set-h6c4v is not available
Jun 27 18:26:25.067: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 18:26:26.072: INFO: Wrong image for pod: daemon-set-h6c4v. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 27 18:26:26.072: INFO: Pod daemon-set-h6c4v is not available
Jun 27 18:26:26.077: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 18:26:27.072: INFO: Wrong image for pod: daemon-set-h6c4v. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 27 18:26:27.072: INFO: Pod daemon-set-h6c4v is not available
Jun 27 18:26:27.077: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 18:26:28.072: INFO: Wrong image for pod: daemon-set-h6c4v. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 27 18:26:28.072: INFO: Pod daemon-set-h6c4v is not available
Jun 27 18:26:28.077: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 18:26:29.072: INFO: Wrong image for pod: daemon-set-h6c4v. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 27 18:26:29.072: INFO: Pod daemon-set-h6c4v is not available
Jun 27 18:26:29.077: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 27 18:26:30.080: INFO: Pod daemon-set-mcrds is not available
Jun 27 18:26:30.084: INFO: DaemonSet pods can't tolerate node talos-test-cluster-master-0 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4771, will wait for the garbage collector to delete the pods
Jun 27 18:26:30.161: INFO: Deleting DaemonSet.extensions daemon-set took: 8.532817ms
Jun 27 18:26:30.561: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.206345ms
Jun 27 18:27:34.768: INFO: Number of nodes with available pods: 0
Jun 27 18:27:34.768: INFO: Number of running nodes: 0, number of available pods: 0
Jun 27 18:27:34.772: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4771/daemonsets","resourceVersion":"24176"},"items":null}

Jun 27 18:27:34.776: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4771/pods","resourceVersion":"24176"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:27:34.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4771" for this suite.
Jun 27 18:27:40.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:27:40.947: INFO: namespace daemonsets-4771 deletion completed in 6.148941412s

• [SLOW TEST:82.145 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:27:40.948: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-9204
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-2e02a146-a29d-4a20-bc3f-4ebe391fda40
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:27:43.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9204" for this suite.
Jun 27 18:28:05.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:28:05.316: INFO: namespace configmap-9204 deletion completed in 22.148758849s

• [SLOW TEST:24.368 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:28:05.317: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename emptydir
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in emptydir-8176
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 27 18:28:05.486: INFO: Waiting up to 5m0s for pod "pod-5e807847-6a87-443f-8a9b-3037d49b282a" in namespace "emptydir-8176" to be "success or failure"
Jun 27 18:28:05.492: INFO: Pod "pod-5e807847-6a87-443f-8a9b-3037d49b282a": Phase="Pending", Reason="", readiness=false. Elapsed: 6.228894ms
Jun 27 18:28:07.496: INFO: Pod "pod-5e807847-6a87-443f-8a9b-3037d49b282a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010216903s
STEP: Saw pod success
Jun 27 18:28:07.496: INFO: Pod "pod-5e807847-6a87-443f-8a9b-3037d49b282a" satisfied condition "success or failure"
Jun 27 18:28:07.500: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod pod-5e807847-6a87-443f-8a9b-3037d49b282a container test-container: <nil>
STEP: delete the pod
Jun 27 18:28:07.527: INFO: Waiting for pod pod-5e807847-6a87-443f-8a9b-3037d49b282a to disappear
Jun 27 18:28:07.531: INFO: Pod pod-5e807847-6a87-443f-8a9b-3037d49b282a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:28:07.531: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8176" for this suite.
Jun 27 18:28:13.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:28:13.685: INFO: namespace emptydir-8176 deletion completed in 6.148578171s

• [SLOW TEST:8.369 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:28:13.687: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename configmap
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in configmap-412
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-f25ec139-7aa4-42fa-bd79-627253b11bce
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-f25ec139-7aa4-42fa-bd79-627253b11bce
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:28:19.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-412" for this suite.
Jun 27 18:28:42.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:28:42.148: INFO: namespace configmap-412 deletion completed in 22.148958387s

• [SLOW TEST:28.437 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:28:42.148: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-9709
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jun 27 18:28:44.852: INFO: Successfully updated pod "annotationupdate0ddd4825-7b62-4a07-b31d-fc752b89d7de"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:28:48.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9709" for this suite.
Jun 27 18:29:10.901: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:29:11.038: INFO: namespace projected-9709 deletion completed in 22.150420589s

• [SLOW TEST:28.891 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:29:11.039: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename deployment
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in deployment-7633
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 27 18:29:11.194: INFO: Creating deployment "test-recreate-deployment"
Jun 27 18:29:11.200: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun 27 18:29:11.212: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jun 27 18:29:13.220: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun 27 18:29:13.224: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun 27 18:29:13.234: INFO: Updating deployment test-recreate-deployment
Jun 27 18:29:13.234: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 27 18:29:13.352: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-7633,SelfLink:/apis/apps/v1/namespaces/deployment-7633/deployments/test-recreate-deployment,UID:74841d91-87de-4ecc-b042-970c188dadcc,ResourceVersion:24557,Generation:2,CreationTimestamp:2019-06-27 18:29:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-06-27 18:29:13 +0000 UTC 2019-06-27 18:29:13 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-06-27 18:29:13 +0000 UTC 2019-06-27 18:29:11 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jun 27 18:29:13.357: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-7633,SelfLink:/apis/apps/v1/namespaces/deployment-7633/replicasets/test-recreate-deployment-5c8c9cc69d,UID:09b68b97-a426-4dd5-8936-df55d7926ef4,ResourceVersion:24554,Generation:1,CreationTimestamp:2019-06-27 18:29:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 74841d91-87de-4ecc-b042-970c188dadcc 0xc002fcfd77 0xc002fcfd78}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 27 18:29:13.357: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun 27 18:29:13.357: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-7633,SelfLink:/apis/apps/v1/namespaces/deployment-7633/replicasets/test-recreate-deployment-6df85df6b9,UID:0212fbe1-e71e-4031-ab1d-0bcc59fa983f,ResourceVersion:24545,Generation:2,CreationTimestamp:2019-06-27 18:29:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 74841d91-87de-4ecc-b042-970c188dadcc 0xc002fcfe47 0xc002fcfe48}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 27 18:29:13.362: INFO: Pod "test-recreate-deployment-5c8c9cc69d-mvxdf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-mvxdf,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-7633,SelfLink:/api/v1/namespaces/deployment-7633/pods/test-recreate-deployment-5c8c9cc69d-mvxdf,UID:e6c23ae0-c30e-43a4-b1e8-76b9516a8f72,ResourceVersion:24555,Generation:0,CreationTimestamp:2019-06-27 18:29:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{kubernetes.io/psp: e2e-test-privileged-psp,},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 09b68b97-a426-4dd5-8936-df55d7926ef4 0xc001ad6a07 0xc001ad6a08}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-4tlh9 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-4tlh9,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-4tlh9 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:talos-test-cluster-workers-84c9684cd-nk6xg,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc001ad6bf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc001ad6c10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 18:29:13 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 18:29:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-27 18:29:13 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-27 18:29:13 +0000 UTC  }],Message:,Reason:,HostIP:139.178.70.235,PodIP:,StartTime:2019-06-27 18:29:13 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:29:13.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7633" for this suite.
Jun 27 18:29:19.381: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:29:19.527: INFO: namespace deployment-7633 deletion completed in 6.158972839s

• [SLOW TEST:8.488 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:29:19.527: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-3372
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 27 18:29:19.700: INFO: Waiting up to 5m0s for pod "downward-api-eacd8aab-eac9-4e23-8cdc-980665040e0b" in namespace "downward-api-3372" to be "success or failure"
Jun 27 18:29:19.704: INFO: Pod "downward-api-eacd8aab-eac9-4e23-8cdc-980665040e0b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.263093ms
Jun 27 18:29:21.708: INFO: Pod "downward-api-eacd8aab-eac9-4e23-8cdc-980665040e0b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008024192s
STEP: Saw pod success
Jun 27 18:29:21.708: INFO: Pod "downward-api-eacd8aab-eac9-4e23-8cdc-980665040e0b" satisfied condition "success or failure"
Jun 27 18:29:21.711: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod downward-api-eacd8aab-eac9-4e23-8cdc-980665040e0b container dapi-container: <nil>
STEP: delete the pod
Jun 27 18:29:21.740: INFO: Waiting for pod downward-api-eacd8aab-eac9-4e23-8cdc-980665040e0b to disappear
Jun 27 18:29:21.745: INFO: Pod downward-api-eacd8aab-eac9-4e23-8cdc-980665040e0b no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:29:21.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3372" for this suite.
Jun 27 18:29:27.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:29:27.899: INFO: namespace downward-api-3372 deletion completed in 6.148708931s

• [SLOW TEST:8.372 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:29:27.900: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-6306
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-28f07feb-5bc8-427f-929f-17bd229de71e
STEP: Creating a pod to test consume configMaps
Jun 27 18:29:28.074: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bc5c3d78-36f7-4684-ac7f-aca6a233bb4e" in namespace "projected-6306" to be "success or failure"
Jun 27 18:29:28.078: INFO: Pod "pod-projected-configmaps-bc5c3d78-36f7-4684-ac7f-aca6a233bb4e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037283ms
Jun 27 18:29:30.082: INFO: Pod "pod-projected-configmaps-bc5c3d78-36f7-4684-ac7f-aca6a233bb4e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008186543s
STEP: Saw pod success
Jun 27 18:29:30.082: INFO: Pod "pod-projected-configmaps-bc5c3d78-36f7-4684-ac7f-aca6a233bb4e" satisfied condition "success or failure"
Jun 27 18:29:30.086: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nzp9p pod pod-projected-configmaps-bc5c3d78-36f7-4684-ac7f-aca6a233bb4e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 27 18:29:30.110: INFO: Waiting for pod pod-projected-configmaps-bc5c3d78-36f7-4684-ac7f-aca6a233bb4e to disappear
Jun 27 18:29:30.116: INFO: Pod pod-projected-configmaps-bc5c3d78-36f7-4684-ac7f-aca6a233bb4e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:29:30.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6306" for this suite.
Jun 27 18:29:36.134: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:29:36.276: INFO: namespace projected-6306 deletion completed in 6.155498246s

• [SLOW TEST:8.377 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:29:36.277: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename projected
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in projected-631
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 27 18:29:36.445: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f131b831-992b-46d2-bd1b-b305fe0a2d8d" in namespace "projected-631" to be "success or failure"
Jun 27 18:29:36.451: INFO: Pod "downwardapi-volume-f131b831-992b-46d2-bd1b-b305fe0a2d8d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.493485ms
Jun 27 18:29:38.456: INFO: Pod "downwardapi-volume-f131b831-992b-46d2-bd1b-b305fe0a2d8d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011121185s
STEP: Saw pod success
Jun 27 18:29:38.456: INFO: Pod "downwardapi-volume-f131b831-992b-46d2-bd1b-b305fe0a2d8d" satisfied condition "success or failure"
Jun 27 18:29:38.459: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nk6xg pod downwardapi-volume-f131b831-992b-46d2-bd1b-b305fe0a2d8d container client-container: <nil>
STEP: delete the pod
Jun 27 18:29:38.485: INFO: Waiting for pod downwardapi-volume-f131b831-992b-46d2-bd1b-b305fe0a2d8d to disappear
Jun 27 18:29:38.489: INFO: Pod downwardapi-volume-f131b831-992b-46d2-bd1b-b305fe0a2d8d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:29:38.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-631" for this suite.
Jun 27 18:29:44.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:29:44.647: INFO: namespace projected-631 deletion completed in 6.153051075s

• [SLOW TEST:8.370 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:29:44.649: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename watch
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in watch-7202
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun 27 18:29:44.870: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7202,SelfLink:/api/v1/namespaces/watch-7202/configmaps/e2e-watch-test-resource-version,UID:2b45cab7-8d91-4cae-b720-ddaa70a4f972,ResourceVersion:24745,Generation:0,CreationTimestamp:2019-06-27 18:29:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 27 18:29:44.870: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-7202,SelfLink:/api/v1/namespaces/watch-7202/configmaps/e2e-watch-test-resource-version,UID:2b45cab7-8d91-4cae-b720-ddaa70a4f972,ResourceVersion:24746,Generation:0,CreationTimestamp:2019-06-27 18:29:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:29:44.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7202" for this suite.
Jun 27 18:29:50.892: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:29:51.024: INFO: namespace watch-7202 deletion completed in 6.14675707s

• [SLOW TEST:6.375 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:29:51.025: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename downward-api
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in downward-api-6368
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 27 18:29:51.190: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aba25b85-9d02-4a5b-9e4f-1f036aa8ba2f" in namespace "downward-api-6368" to be "success or failure"
Jun 27 18:29:51.194: INFO: Pod "downwardapi-volume-aba25b85-9d02-4a5b-9e4f-1f036aa8ba2f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.480723ms
Jun 27 18:29:53.198: INFO: Pod "downwardapi-volume-aba25b85-9d02-4a5b-9e4f-1f036aa8ba2f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008901543s
STEP: Saw pod success
Jun 27 18:29:53.199: INFO: Pod "downwardapi-volume-aba25b85-9d02-4a5b-9e4f-1f036aa8ba2f" satisfied condition "success or failure"
Jun 27 18:29:53.205: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-mskvs pod downwardapi-volume-aba25b85-9d02-4a5b-9e4f-1f036aa8ba2f container client-container: <nil>
STEP: delete the pod
Jun 27 18:29:53.228: INFO: Waiting for pod downwardapi-volume-aba25b85-9d02-4a5b-9e4f-1f036aa8ba2f to disappear
Jun 27 18:29:53.233: INFO: Pod downwardapi-volume-aba25b85-9d02-4a5b-9e4f-1f036aa8ba2f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:29:53.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6368" for this suite.
Jun 27 18:29:59.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:29:59.423: INFO: namespace downward-api-6368 deletion completed in 6.183813028s

• [SLOW TEST:8.398 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 27 18:29:59.423: INFO: >>> kubeConfig: /tmp/kubeconfig-307330860
STEP: Building a namespace api object, basename var-expansion
STEP: Binding the e2e-test-privileged-psp PodSecurityPolicy to the default service account in var-expansion-9422
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Jun 27 18:29:59.594: INFO: Waiting up to 5m0s for pod "var-expansion-ca12254a-1afd-46c1-9400-5d8bcf380c33" in namespace "var-expansion-9422" to be "success or failure"
Jun 27 18:29:59.599: INFO: Pod "var-expansion-ca12254a-1afd-46c1-9400-5d8bcf380c33": Phase="Pending", Reason="", readiness=false. Elapsed: 4.478284ms
Jun 27 18:30:01.603: INFO: Pod "var-expansion-ca12254a-1afd-46c1-9400-5d8bcf380c33": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008483593s
STEP: Saw pod success
Jun 27 18:30:01.603: INFO: Pod "var-expansion-ca12254a-1afd-46c1-9400-5d8bcf380c33" satisfied condition "success or failure"
Jun 27 18:30:01.606: INFO: Trying to get logs from node talos-test-cluster-workers-84c9684cd-nzp9p pod var-expansion-ca12254a-1afd-46c1-9400-5d8bcf380c33 container dapi-container: <nil>
STEP: delete the pod
Jun 27 18:30:01.627: INFO: Waiting for pod var-expansion-ca12254a-1afd-46c1-9400-5d8bcf380c33 to disappear
Jun 27 18:30:01.631: INFO: Pod var-expansion-ca12254a-1afd-46c1-9400-5d8bcf380c33 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 27 18:30:01.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-9422" for this suite.
Jun 27 18:30:07.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 27 18:30:07.785: INFO: namespace var-expansion-9422 deletion completed in 6.147877541s

• [SLOW TEST:8.362 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSJun 27 18:30:07.785: INFO: Running AfterSuite actions on all nodes
Jun 27 18:30:07.785: INFO: Running AfterSuite actions on node 1
Jun 27 18:30:07.785: INFO: Skipping dumping logs from cluster

Ran 215 of 4411 Specs in 5443.440 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4196 Skipped
PASS

Ginkgo ran 1 suite in 1h30m47.165003288s
Test Suite Passed
