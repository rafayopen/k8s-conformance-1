I1223 17:51:33.311234      14 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-424598258
I1223 17:51:33.311804      14 e2e.go:243] Starting e2e run "b3f6ae40-7c58-4c32-ade4-3ef9573f8b96" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1577123490 - Will randomize all specs
Will run 215 of 4413 specs

Dec 23 17:51:33.485: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 17:51:33.491: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec 23 17:51:33.516: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec 23 17:51:33.560: INFO: The status of Pod rke-coredns-addon-deploy-job-lppwq is Succeeded, skipping waiting
Dec 23 17:51:33.560: INFO: The status of Pod rke-ingress-controller-deploy-job-bz9f6 is Succeeded, skipping waiting
Dec 23 17:51:33.560: INFO: The status of Pod rke-metrics-addon-deploy-job-glj7v is Succeeded, skipping waiting
Dec 23 17:51:33.560: INFO: The status of Pod rke-network-plugin-deploy-job-xvn4m is Succeeded, skipping waiting
Dec 23 17:51:33.560: INFO: 15 / 19 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 23 17:51:33.560: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Dec 23 17:51:33.561: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 23 17:51:33.569: INFO: 9 / 9 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Dec 23 17:51:33.569: INFO: e2e test version: v1.15.6
Dec 23 17:51:33.570: INFO: kube-apiserver version: v1.15.6
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 17:51:33.571: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename sched-pred
Dec 23 17:51:33.668: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Dec 23 17:51:33.672: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 23 17:51:33.689: INFO: Waiting for terminating namespaces to be deleted...
Dec 23 17:51:33.694: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-control-1 before test
Dec 23 17:51:33.702: INFO: calico-node-drwck from kube-system started at 2019-12-23 16:49:25 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.703: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 17:51:33.703: INFO: cattle-node-agent-5hgvg from cattle-system started at 2019-12-23 16:51:44 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.704: INFO: 	Container agent ready: true, restart count 0
Dec 23 17:51:33.704: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-vcnpn from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 17:51:33.705: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 17:51:33.705: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 17:51:33.705: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-control-2 before test
Dec 23 17:51:33.722: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-s25j6 from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 17:51:33.722: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 17:51:33.722: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 17:51:33.722: INFO: rke-network-plugin-deploy-job-xvn4m from kube-system started at 2019-12-23 16:48:57 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.722: INFO: 	Container rke-network-plugin-pod ready: false, restart count 0
Dec 23 17:51:33.722: INFO: calico-node-v9rvr from kube-system started at 2019-12-23 16:49:25 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.722: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 17:51:33.722: INFO: rke-coredns-addon-deploy-job-lppwq from kube-system started at 2019-12-23 16:51:09 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.722: INFO: 	Container rke-coredns-addon-pod ready: false, restart count 0
Dec 23 17:51:33.722: INFO: rke-metrics-addon-deploy-job-glj7v from kube-system started at 2019-12-23 16:51:14 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.722: INFO: 	Container rke-metrics-addon-pod ready: false, restart count 0
Dec 23 17:51:33.722: INFO: rke-ingress-controller-deploy-job-bz9f6 from kube-system started at 2019-12-23 16:51:19 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.722: INFO: 	Container rke-ingress-controller-pod ready: false, restart count 0
Dec 23 17:51:33.722: INFO: cattle-node-agent-9jbrb from cattle-system started at 2019-12-23 16:51:44 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.722: INFO: 	Container agent ready: true, restart count 0
Dec 23 17:51:33.723: INFO: sonobuoy from sonobuoy started at 2019-12-23 17:51:06 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.723: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 23 17:51:33.723: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-control-3 before test
Dec 23 17:51:33.733: INFO: cattle-node-agent-whghm from cattle-system started at 2019-12-23 16:51:44 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.733: INFO: 	Container agent ready: true, restart count 0
Dec 23 17:51:33.733: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-97q92 from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 17:51:33.733: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 17:51:33.733: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 17:51:33.733: INFO: calico-node-ptzn9 from kube-system started at 2019-12-23 16:49:25 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.733: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 17:51:33.733: INFO: cattle-cluster-agent-6dfc4fd4fc-gsf4h from cattle-system started at 2019-12-23 16:51:44 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.733: INFO: 	Container cluster-register ready: true, restart count 0
Dec 23 17:51:33.733: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-etcd-1 before test
Dec 23 17:51:33.742: INFO: calico-node-qv786 from kube-system started at 2019-12-23 16:49:25 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.742: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 17:51:33.742: INFO: cattle-node-agent-bckfv from cattle-system started at 2019-12-23 16:51:44 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.743: INFO: 	Container agent ready: true, restart count 0
Dec 23 17:51:33.743: INFO: sonobuoy-e2e-job-7470178446ee4019 from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 17:51:33.743: INFO: 	Container e2e ready: true, restart count 0
Dec 23 17:51:33.743: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 17:51:33.743: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-sxmpv from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 17:51:33.743: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 17:51:33.743: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 17:51:33.743: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-etcd-2 before test
Dec 23 17:51:33.752: INFO: calico-node-648gc from kube-system started at 2019-12-23 16:49:25 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.752: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 17:51:33.752: INFO: calico-kube-controllers-77cd95cb44-b9g9b from kube-system started at 2019-12-23 16:49:25 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.752: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 23 17:51:33.752: INFO: cattle-node-agent-7jfhx from cattle-system started at 2019-12-23 16:51:44 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.752: INFO: 	Container agent ready: true, restart count 0
Dec 23 17:51:33.752: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-2cg6j from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 17:51:33.752: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 17:51:33.752: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 17:51:33.752: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-etcd-3 before test
Dec 23 17:51:33.767: INFO: calico-node-vk4b5 from kube-system started at 2019-12-23 16:49:25 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.768: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 17:51:33.768: INFO: cattle-node-agent-28j5k from cattle-system started at 2019-12-23 16:51:44 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.768: INFO: 	Container agent ready: true, restart count 0
Dec 23 17:51:33.768: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-xm8d7 from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 17:51:33.768: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 17:51:33.768: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 17:51:33.768: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-worker-1 before test
Dec 23 17:51:33.781: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-mjhc2 from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 17:51:33.781: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 17:51:33.781: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 17:51:33.781: INFO: calico-node-7c2k8 from kube-system started at 2019-12-23 16:53:55 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.781: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 17:51:33.781: INFO: cattle-node-agent-9brqm from cattle-system started at 2019-12-23 16:53:55 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.781: INFO: 	Container agent ready: true, restart count 0
Dec 23 17:51:33.781: INFO: coredns-799dffd9c4-2459g from kube-system started at 2019-12-23 16:53:56 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.781: INFO: 	Container coredns ready: true, restart count 0
Dec 23 17:51:33.781: INFO: nginx-ingress-controller-ddhqm from ingress-nginx started at 2019-12-23 16:53:56 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.782: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec 23 17:51:33.782: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-worker-2 before test
Dec 23 17:51:33.795: INFO: default-http-backend-5bcc9fd598-pz56l from ingress-nginx started at 2019-12-23 16:52:49 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.795: INFO: 	Container default-http-backend ready: true, restart count 0
Dec 23 17:51:33.795: INFO: metrics-server-59c6fd6767-4nvkr from kube-system started at 2019-12-23 16:52:49 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.795: INFO: 	Container metrics-server ready: true, restart count 0
Dec 23 17:51:33.795: INFO: coredns-799dffd9c4-2swvh from kube-system started at 2019-12-23 16:53:56 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.795: INFO: 	Container coredns ready: true, restart count 0
Dec 23 17:51:33.795: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-zgfft from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 17:51:33.795: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 17:51:33.795: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 17:51:33.795: INFO: calico-node-cvld6 from kube-system started at 2019-12-23 16:52:48 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.795: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 17:51:33.795: INFO: cattle-node-agent-l6k7z from cattle-system started at 2019-12-23 16:52:48 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.795: INFO: 	Container agent ready: true, restart count 0
Dec 23 17:51:33.795: INFO: nginx-ingress-controller-nczlg from ingress-nginx started at 2019-12-23 16:52:48 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.795: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec 23 17:51:33.795: INFO: coredns-autoscaler-84766fbb4-2s6b6 from kube-system started at 2019-12-23 16:52:49 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.795: INFO: 	Container autoscaler ready: true, restart count 0
Dec 23 17:51:33.796: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-worker-3 before test
Dec 23 17:51:33.804: INFO: cattle-node-agent-22s8h from cattle-system started at 2019-12-23 16:52:46 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.804: INFO: 	Container agent ready: true, restart count 0
Dec 23 17:51:33.805: INFO: nginx-ingress-controller-sq5vm from ingress-nginx started at 2019-12-23 16:52:46 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.805: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec 23 17:51:33.805: INFO: coredns-799dffd9c4-gc76w from kube-system started at 2019-12-23 16:52:49 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.805: INFO: 	Container coredns ready: true, restart count 0
Dec 23 17:51:33.805: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-ptdwt from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 17:51:33.805: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 17:51:33.805: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 17:51:33.805: INFO: calico-node-pgzp6 from kube-system started at 2019-12-23 16:52:46 +0000 UTC (1 container statuses recorded)
Dec 23 17:51:33.805: INFO: 	Container calico-node ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15e3117630e669f8], Reason = [FailedScheduling], Message = [0/9 nodes are available: 9 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 17:51:34.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2213" for this suite.
Dec 23 17:51:40.885: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 17:51:41.091: INFO: namespace sched-pred-2213 deletion completed in 6.226113089s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.520 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 17:51:41.100: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 23 17:51:53.295: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 23 17:51:53.303: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 23 17:51:55.304: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 23 17:51:55.309: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 23 17:51:57.304: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 23 17:51:57.311: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 23 17:51:59.304: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 23 17:51:59.311: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 23 17:52:01.304: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 23 17:52:01.310: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 23 17:52:03.304: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 23 17:52:03.310: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 23 17:52:05.304: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 23 17:52:05.312: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 23 17:52:07.304: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 23 17:52:07.309: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 23 17:52:09.304: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 23 17:52:09.309: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 23 17:52:11.304: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 23 17:52:11.309: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 23 17:52:13.304: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 23 17:52:13.309: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 17:52:13.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-1576" for this suite.
Dec 23 17:52:35.342: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 17:52:35.555: INFO: namespace container-lifecycle-hook-1576 deletion completed in 22.236392334s

• [SLOW TEST:54.455 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 17:52:35.562: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 23 17:52:35.680: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0a5f820c-4502-4f4c-8820-58f78b89f8b6" in namespace "downward-api-577" to be "success or failure"
Dec 23 17:52:35.688: INFO: Pod "downwardapi-volume-0a5f820c-4502-4f4c-8820-58f78b89f8b6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.178304ms
Dec 23 17:52:37.695: INFO: Pod "downwardapi-volume-0a5f820c-4502-4f4c-8820-58f78b89f8b6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.014249452s
Dec 23 17:52:39.701: INFO: Pod "downwardapi-volume-0a5f820c-4502-4f4c-8820-58f78b89f8b6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020538338s
Dec 23 17:52:41.707: INFO: Pod "downwardapi-volume-0a5f820c-4502-4f4c-8820-58f78b89f8b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026720861s
STEP: Saw pod success
Dec 23 17:52:41.707: INFO: Pod "downwardapi-volume-0a5f820c-4502-4f4c-8820-58f78b89f8b6" satisfied condition "success or failure"
Dec 23 17:52:41.712: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod downwardapi-volume-0a5f820c-4502-4f4c-8820-58f78b89f8b6 container client-container: <nil>
STEP: delete the pod
Dec 23 17:52:41.766: INFO: Waiting for pod downwardapi-volume-0a5f820c-4502-4f4c-8820-58f78b89f8b6 to disappear
Dec 23 17:52:41.775: INFO: Pod downwardapi-volume-0a5f820c-4502-4f4c-8820-58f78b89f8b6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 17:52:41.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-577" for this suite.
Dec 23 17:52:47.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 17:52:48.029: INFO: namespace downward-api-577 deletion completed in 6.244016642s

• [SLOW TEST:12.468 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 17:52:48.035: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-0091c94f-bbf2-47c0-91b7-178a1e59bb8e
STEP: Creating a pod to test consume secrets
Dec 23 17:52:48.160: INFO: Waiting up to 5m0s for pod "pod-secrets-558c5f27-3b89-471a-88b6-7273f5fa0fd5" in namespace "secrets-6601" to be "success or failure"
Dec 23 17:52:48.168: INFO: Pod "pod-secrets-558c5f27-3b89-471a-88b6-7273f5fa0fd5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.322681ms
Dec 23 17:52:50.175: INFO: Pod "pod-secrets-558c5f27-3b89-471a-88b6-7273f5fa0fd5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014715078s
STEP: Saw pod success
Dec 23 17:52:50.176: INFO: Pod "pod-secrets-558c5f27-3b89-471a-88b6-7273f5fa0fd5" satisfied condition "success or failure"
Dec 23 17:52:50.182: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-secrets-558c5f27-3b89-471a-88b6-7273f5fa0fd5 container secret-env-test: <nil>
STEP: delete the pod
Dec 23 17:52:50.217: INFO: Waiting for pod pod-secrets-558c5f27-3b89-471a-88b6-7273f5fa0fd5 to disappear
Dec 23 17:52:50.224: INFO: Pod pod-secrets-558c5f27-3b89-471a-88b6-7273f5fa0fd5 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 17:52:50.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6601" for this suite.
Dec 23 17:52:56.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 17:52:56.473: INFO: namespace secrets-6601 deletion completed in 6.240002361s

• [SLOW TEST:8.439 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 17:52:56.487: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1223 17:53:02.611307      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 23 17:53:02.611: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 17:53:02.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3477" for this suite.
Dec 23 17:53:08.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 17:53:08.893: INFO: namespace gc-3477 deletion completed in 6.249261958s

• [SLOW TEST:12.408 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 17:53:08.902: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Dec 23 17:53:11.044: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec pod-sharedvolume-eb912974-3dc4-401f-b3f3-65f65593f634 -c busybox-main-container --namespace=emptydir-5857 -- cat /usr/share/volumeshare/shareddata.txt'
Dec 23 17:53:11.753: INFO: stderr: ""
Dec 23 17:53:11.753: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 17:53:11.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5857" for this suite.
Dec 23 17:53:17.787: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 17:53:18.009: INFO: namespace emptydir-5857 deletion completed in 6.248233154s

• [SLOW TEST:9.108 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 17:53:18.012: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Dec 23 17:53:18.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 create -f - --namespace=kubectl-6963'
Dec 23 17:53:18.924: INFO: stderr: ""
Dec 23 17:53:18.924: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 23 17:53:18.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6963'
Dec 23 17:53:19.260: INFO: stderr: ""
Dec 23 17:53:19.260: INFO: stdout: "update-demo-nautilus-dmwh9 update-demo-nautilus-mlq4l "
Dec 23 17:53:19.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-nautilus-dmwh9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6963'
Dec 23 17:53:19.522: INFO: stderr: ""
Dec 23 17:53:19.522: INFO: stdout: ""
Dec 23 17:53:19.522: INFO: update-demo-nautilus-dmwh9 is created but not running
Dec 23 17:53:24.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6963'
Dec 23 17:53:24.787: INFO: stderr: ""
Dec 23 17:53:24.787: INFO: stdout: "update-demo-nautilus-dmwh9 update-demo-nautilus-mlq4l "
Dec 23 17:53:24.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-nautilus-dmwh9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6963'
Dec 23 17:53:25.024: INFO: stderr: ""
Dec 23 17:53:25.025: INFO: stdout: "true"
Dec 23 17:53:25.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-nautilus-dmwh9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6963'
Dec 23 17:53:25.190: INFO: stderr: ""
Dec 23 17:53:25.190: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 23 17:53:25.190: INFO: validating pod update-demo-nautilus-dmwh9
Dec 23 17:53:25.197: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 23 17:53:25.197: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 23 17:53:25.197: INFO: update-demo-nautilus-dmwh9 is verified up and running
Dec 23 17:53:25.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-nautilus-mlq4l -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6963'
Dec 23 17:53:25.368: INFO: stderr: ""
Dec 23 17:53:25.368: INFO: stdout: "true"
Dec 23 17:53:25.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-nautilus-mlq4l -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6963'
Dec 23 17:53:25.541: INFO: stderr: ""
Dec 23 17:53:25.541: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 23 17:53:25.541: INFO: validating pod update-demo-nautilus-mlq4l
Dec 23 17:53:25.547: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 23 17:53:25.547: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 23 17:53:25.547: INFO: update-demo-nautilus-mlq4l is verified up and running
STEP: using delete to clean up resources
Dec 23 17:53:25.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 delete --grace-period=0 --force -f - --namespace=kubectl-6963'
Dec 23 17:53:25.669: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 23 17:53:25.669: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 23 17:53:25.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6963'
Dec 23 17:53:25.793: INFO: stderr: "No resources found.\n"
Dec 23 17:53:25.793: INFO: stdout: ""
Dec 23 17:53:25.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods -l name=update-demo --namespace=kubectl-6963 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 23 17:53:26.040: INFO: stderr: ""
Dec 23 17:53:26.040: INFO: stdout: "update-demo-nautilus-dmwh9\nupdate-demo-nautilus-mlq4l\n"
Dec 23 17:53:26.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6963'
Dec 23 17:53:26.695: INFO: stderr: "No resources found.\n"
Dec 23 17:53:26.695: INFO: stdout: ""
Dec 23 17:53:26.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods -l name=update-demo --namespace=kubectl-6963 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 23 17:53:26.915: INFO: stderr: ""
Dec 23 17:53:26.915: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 17:53:26.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6963" for this suite.
Dec 23 17:53:48.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 17:53:49.270: INFO: namespace kubectl-6963 deletion completed in 22.347913635s

• [SLOW TEST:31.259 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 17:53:49.275: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1516
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 23 17:53:49.376: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-8089'
Dec 23 17:53:49.677: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 23 17:53:49.677: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Dec 23 17:53:49.705: INFO: scanned /root for discovery docs: <nil>
Dec 23 17:53:49.705: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-8089'
Dec 23 17:54:03.933: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 23 17:54:03.933: INFO: stdout: "Created e2e-test-nginx-rc-f17829e58b881fba1840048a03ef4a16\nScaling up e2e-test-nginx-rc-f17829e58b881fba1840048a03ef4a16 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-f17829e58b881fba1840048a03ef4a16 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-f17829e58b881fba1840048a03ef4a16 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Dec 23 17:54:03.933: INFO: stdout: "Created e2e-test-nginx-rc-f17829e58b881fba1840048a03ef4a16\nScaling up e2e-test-nginx-rc-f17829e58b881fba1840048a03ef4a16 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-f17829e58b881fba1840048a03ef4a16 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-f17829e58b881fba1840048a03ef4a16 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Dec 23 17:54:03.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-8089'
Dec 23 17:54:04.166: INFO: stderr: ""
Dec 23 17:54:04.166: INFO: stdout: "e2e-test-nginx-rc-f17829e58b881fba1840048a03ef4a16-6gjrv "
Dec 23 17:54:04.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods e2e-test-nginx-rc-f17829e58b881fba1840048a03ef4a16-6gjrv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8089'
Dec 23 17:54:04.385: INFO: stderr: ""
Dec 23 17:54:04.385: INFO: stdout: "true"
Dec 23 17:54:04.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods e2e-test-nginx-rc-f17829e58b881fba1840048a03ef4a16-6gjrv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8089'
Dec 23 17:54:04.608: INFO: stderr: ""
Dec 23 17:54:04.608: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Dec 23 17:54:04.608: INFO: e2e-test-nginx-rc-f17829e58b881fba1840048a03ef4a16-6gjrv is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1522
Dec 23 17:54:04.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 delete rc e2e-test-nginx-rc --namespace=kubectl-8089'
Dec 23 17:54:04.911: INFO: stderr: ""
Dec 23 17:54:04.911: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 17:54:04.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8089" for this suite.
Dec 23 17:54:26.966: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 17:54:27.198: INFO: namespace kubectl-8089 deletion completed in 22.276845898s

• [SLOW TEST:37.924 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 17:54:27.206: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-a48e2543-1a36-400b-af83-5230bde42718
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 17:54:27.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-715" for this suite.
Dec 23 17:54:33.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 17:54:33.588: INFO: namespace configmap-715 deletion completed in 6.254917311s

• [SLOW TEST:6.382 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 17:54:33.589: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Dec 23 17:54:33.686: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-424598258 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 17:54:33.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-129" for this suite.
Dec 23 17:54:39.910: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 17:54:40.125: INFO: namespace kubectl-129 deletion completed in 6.25196688s

• [SLOW TEST:6.536 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 17:54:40.127: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 23 17:54:40.252: INFO: Waiting up to 5m0s for pod "pod-97436985-55bf-4917-aafe-9807065b6ac4" in namespace "emptydir-1016" to be "success or failure"
Dec 23 17:54:40.259: INFO: Pod "pod-97436985-55bf-4917-aafe-9807065b6ac4": Phase="Pending", Reason="", readiness=false. Elapsed: 6.849269ms
Dec 23 17:54:42.266: INFO: Pod "pod-97436985-55bf-4917-aafe-9807065b6ac4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013892134s
Dec 23 17:54:44.272: INFO: Pod "pod-97436985-55bf-4917-aafe-9807065b6ac4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019940822s
Dec 23 17:54:46.279: INFO: Pod "pod-97436985-55bf-4917-aafe-9807065b6ac4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026370568s
STEP: Saw pod success
Dec 23 17:54:46.279: INFO: Pod "pod-97436985-55bf-4917-aafe-9807065b6ac4" satisfied condition "success or failure"
Dec 23 17:54:46.286: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-97436985-55bf-4917-aafe-9807065b6ac4 container test-container: <nil>
STEP: delete the pod
Dec 23 17:54:46.319: INFO: Waiting for pod pod-97436985-55bf-4917-aafe-9807065b6ac4 to disappear
Dec 23 17:54:46.325: INFO: Pod pod-97436985-55bf-4917-aafe-9807065b6ac4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 17:54:46.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1016" for this suite.
Dec 23 17:54:52.357: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 17:54:52.581: INFO: namespace emptydir-1016 deletion completed in 6.246643384s

• [SLOW TEST:12.454 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 17:54:52.587: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Dec 23 17:54:52.652: INFO: Waiting up to 5m0s for pod "var-expansion-e16235bf-1c89-4867-a42d-bfa04db16ebe" in namespace "var-expansion-5736" to be "success or failure"
Dec 23 17:54:52.670: INFO: Pod "var-expansion-e16235bf-1c89-4867-a42d-bfa04db16ebe": Phase="Pending", Reason="", readiness=false. Elapsed: 15.820182ms
Dec 23 17:54:54.675: INFO: Pod "var-expansion-e16235bf-1c89-4867-a42d-bfa04db16ebe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021507961s
STEP: Saw pod success
Dec 23 17:54:54.676: INFO: Pod "var-expansion-e16235bf-1c89-4867-a42d-bfa04db16ebe" satisfied condition "success or failure"
Dec 23 17:54:54.681: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod var-expansion-e16235bf-1c89-4867-a42d-bfa04db16ebe container dapi-container: <nil>
STEP: delete the pod
Dec 23 17:54:54.720: INFO: Waiting for pod var-expansion-e16235bf-1c89-4867-a42d-bfa04db16ebe to disappear
Dec 23 17:54:54.725: INFO: Pod var-expansion-e16235bf-1c89-4867-a42d-bfa04db16ebe no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 17:54:54.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5736" for this suite.
Dec 23 17:55:00.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 17:55:00.947: INFO: namespace var-expansion-5736 deletion completed in 6.211892337s

• [SLOW TEST:8.361 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 17:55:00.950: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 23 17:55:01.086: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4017579c-7e54-41b9-8400-23e3fd7308a9" in namespace "downward-api-1471" to be "success or failure"
Dec 23 17:55:01.093: INFO: Pod "downwardapi-volume-4017579c-7e54-41b9-8400-23e3fd7308a9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.079961ms
Dec 23 17:55:03.100: INFO: Pod "downwardapi-volume-4017579c-7e54-41b9-8400-23e3fd7308a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01276045s
STEP: Saw pod success
Dec 23 17:55:03.100: INFO: Pod "downwardapi-volume-4017579c-7e54-41b9-8400-23e3fd7308a9" satisfied condition "success or failure"
Dec 23 17:55:03.105: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod downwardapi-volume-4017579c-7e54-41b9-8400-23e3fd7308a9 container client-container: <nil>
STEP: delete the pod
Dec 23 17:55:03.136: INFO: Waiting for pod downwardapi-volume-4017579c-7e54-41b9-8400-23e3fd7308a9 to disappear
Dec 23 17:55:03.144: INFO: Pod downwardapi-volume-4017579c-7e54-41b9-8400-23e3fd7308a9 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 17:55:03.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1471" for this suite.
Dec 23 17:55:09.196: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 17:55:09.427: INFO: namespace downward-api-1471 deletion completed in 6.269785634s

• [SLOW TEST:8.478 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 17:55:09.430: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Dec 23 17:55:09.535: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Dec 23 17:55:09.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 create -f - --namespace=kubectl-1031'
Dec 23 17:55:10.112: INFO: stderr: ""
Dec 23 17:55:10.112: INFO: stdout: "service/redis-slave created\n"
Dec 23 17:55:10.112: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Dec 23 17:55:10.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 create -f - --namespace=kubectl-1031'
Dec 23 17:55:11.051: INFO: stderr: ""
Dec 23 17:55:11.051: INFO: stdout: "service/redis-master created\n"
Dec 23 17:55:11.051: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec 23 17:55:11.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 create -f - --namespace=kubectl-1031'
Dec 23 17:55:12.023: INFO: stderr: ""
Dec 23 17:55:12.023: INFO: stdout: "service/frontend created\n"
Dec 23 17:55:12.024: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Dec 23 17:55:12.024: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 create -f - --namespace=kubectl-1031'
Dec 23 17:55:12.786: INFO: stderr: ""
Dec 23 17:55:12.786: INFO: stdout: "deployment.apps/frontend created\n"
Dec 23 17:55:12.787: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 23 17:55:12.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 create -f - --namespace=kubectl-1031'
Dec 23 17:55:13.666: INFO: stderr: ""
Dec 23 17:55:13.666: INFO: stdout: "deployment.apps/redis-master created\n"
Dec 23 17:55:13.669: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Dec 23 17:55:13.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 create -f - --namespace=kubectl-1031'
Dec 23 17:55:14.244: INFO: stderr: ""
Dec 23 17:55:14.244: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Dec 23 17:55:14.244: INFO: Waiting for all frontend pods to be Running.
Dec 23 17:55:54.297: INFO: Waiting for frontend to serve content.
Dec 23 17:55:54.317: INFO: Trying to add a new entry to the guestbook.
Dec 23 17:55:54.336: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Dec 23 17:55:54.352: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 delete --grace-period=0 --force -f - --namespace=kubectl-1031'
Dec 23 17:55:54.684: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 23 17:55:54.684: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Dec 23 17:55:54.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 delete --grace-period=0 --force -f - --namespace=kubectl-1031'
Dec 23 17:55:55.113: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 23 17:55:55.113: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 23 17:55:55.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 delete --grace-period=0 --force -f - --namespace=kubectl-1031'
Dec 23 17:55:55.514: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 23 17:55:55.514: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 23 17:55:55.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 delete --grace-period=0 --force -f - --namespace=kubectl-1031'
Dec 23 17:55:55.906: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 23 17:55:55.906: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 23 17:55:55.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 delete --grace-period=0 --force -f - --namespace=kubectl-1031'
Dec 23 17:55:56.119: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 23 17:55:56.120: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 23 17:55:56.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 delete --grace-period=0 --force -f - --namespace=kubectl-1031'
Dec 23 17:55:56.459: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 23 17:55:56.459: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 17:55:56.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1031" for this suite.
Dec 23 17:56:36.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 17:56:36.732: INFO: namespace kubectl-1031 deletion completed in 40.261787947s

• [SLOW TEST:87.302 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 17:56:36.740: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 17:56:38.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3007" for this suite.
Dec 23 17:57:16.927: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 17:57:17.138: INFO: namespace kubelet-test-3007 deletion completed in 38.233850896s

• [SLOW TEST:40.399 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 17:57:17.148: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 23 17:57:17.272: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1467a331-3fac-405b-8916-f8c896e422f8" in namespace "projected-6394" to be "success or failure"
Dec 23 17:57:17.282: INFO: Pod "downwardapi-volume-1467a331-3fac-405b-8916-f8c896e422f8": Phase="Pending", Reason="", readiness=false. Elapsed: 8.830316ms
Dec 23 17:57:19.291: INFO: Pod "downwardapi-volume-1467a331-3fac-405b-8916-f8c896e422f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01822822s
STEP: Saw pod success
Dec 23 17:57:19.291: INFO: Pod "downwardapi-volume-1467a331-3fac-405b-8916-f8c896e422f8" satisfied condition "success or failure"
Dec 23 17:57:19.298: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod downwardapi-volume-1467a331-3fac-405b-8916-f8c896e422f8 container client-container: <nil>
STEP: delete the pod
Dec 23 17:57:19.341: INFO: Waiting for pod downwardapi-volume-1467a331-3fac-405b-8916-f8c896e422f8 to disappear
Dec 23 17:57:19.348: INFO: Pod downwardapi-volume-1467a331-3fac-405b-8916-f8c896e422f8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 17:57:19.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6394" for this suite.
Dec 23 17:57:25.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 17:57:25.632: INFO: namespace projected-6394 deletion completed in 6.275255418s

• [SLOW TEST:8.484 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 17:57:25.635: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-2db45feb-2fb7-4aef-aeac-88162309b3e0 in namespace container-probe-6306
Dec 23 17:57:29.714: INFO: Started pod test-webserver-2db45feb-2fb7-4aef-aeac-88162309b3e0 in namespace container-probe-6306
STEP: checking the pod's current state and verifying that restartCount is present
Dec 23 17:57:29.720: INFO: Initial restart count of pod test-webserver-2db45feb-2fb7-4aef-aeac-88162309b3e0 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:01:30.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6306" for this suite.
Dec 23 18:01:36.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:01:36.775: INFO: namespace container-probe-6306 deletion completed in 6.23042962s

• [SLOW TEST:251.141 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:01:36.779: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Dec 23 18:01:36.908: INFO: Waiting up to 5m0s for pod "client-containers-55d4436b-e0ae-44f7-aa3e-a3dc5574e44e" in namespace "containers-6930" to be "success or failure"
Dec 23 18:01:36.916: INFO: Pod "client-containers-55d4436b-e0ae-44f7-aa3e-a3dc5574e44e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.189923ms
Dec 23 18:01:38.922: INFO: Pod "client-containers-55d4436b-e0ae-44f7-aa3e-a3dc5574e44e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012214571s
Dec 23 18:01:40.928: INFO: Pod "client-containers-55d4436b-e0ae-44f7-aa3e-a3dc5574e44e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018088184s
Dec 23 18:01:42.933: INFO: Pod "client-containers-55d4436b-e0ae-44f7-aa3e-a3dc5574e44e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02392399s
STEP: Saw pod success
Dec 23 18:01:42.934: INFO: Pod "client-containers-55d4436b-e0ae-44f7-aa3e-a3dc5574e44e" satisfied condition "success or failure"
Dec 23 18:01:42.938: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod client-containers-55d4436b-e0ae-44f7-aa3e-a3dc5574e44e container test-container: <nil>
STEP: delete the pod
Dec 23 18:01:42.969: INFO: Waiting for pod client-containers-55d4436b-e0ae-44f7-aa3e-a3dc5574e44e to disappear
Dec 23 18:01:42.973: INFO: Pod client-containers-55d4436b-e0ae-44f7-aa3e-a3dc5574e44e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:01:42.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6930" for this suite.
Dec 23 18:01:49.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:01:49.218: INFO: namespace containers-6930 deletion completed in 6.234479782s

• [SLOW TEST:12.440 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:01:49.222: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 23 18:01:49.358: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70037c09-4ca7-46cf-92ce-8f8d2eda08d6" in namespace "downward-api-9219" to be "success or failure"
Dec 23 18:01:49.377: INFO: Pod "downwardapi-volume-70037c09-4ca7-46cf-92ce-8f8d2eda08d6": Phase="Pending", Reason="", readiness=false. Elapsed: 11.091693ms
Dec 23 18:01:51.385: INFO: Pod "downwardapi-volume-70037c09-4ca7-46cf-92ce-8f8d2eda08d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019300957s
STEP: Saw pod success
Dec 23 18:01:51.385: INFO: Pod "downwardapi-volume-70037c09-4ca7-46cf-92ce-8f8d2eda08d6" satisfied condition "success or failure"
Dec 23 18:01:51.393: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod downwardapi-volume-70037c09-4ca7-46cf-92ce-8f8d2eda08d6 container client-container: <nil>
STEP: delete the pod
Dec 23 18:01:51.435: INFO: Waiting for pod downwardapi-volume-70037c09-4ca7-46cf-92ce-8f8d2eda08d6 to disappear
Dec 23 18:01:51.440: INFO: Pod downwardapi-volume-70037c09-4ca7-46cf-92ce-8f8d2eda08d6 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:01:51.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9219" for this suite.
Dec 23 18:01:57.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:01:57.785: INFO: namespace downward-api-9219 deletion completed in 6.336350815s

• [SLOW TEST:8.563 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:01:57.790: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Dec 23 18:01:57.940: INFO: Waiting up to 5m0s for pod "client-containers-40a4871f-aad6-4066-95ad-ba8561323731" in namespace "containers-2796" to be "success or failure"
Dec 23 18:01:57.947: INFO: Pod "client-containers-40a4871f-aad6-4066-95ad-ba8561323731": Phase="Pending", Reason="", readiness=false. Elapsed: 6.037589ms
Dec 23 18:01:59.952: INFO: Pod "client-containers-40a4871f-aad6-4066-95ad-ba8561323731": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011219325s
STEP: Saw pod success
Dec 23 18:01:59.952: INFO: Pod "client-containers-40a4871f-aad6-4066-95ad-ba8561323731" satisfied condition "success or failure"
Dec 23 18:01:59.957: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod client-containers-40a4871f-aad6-4066-95ad-ba8561323731 container test-container: <nil>
STEP: delete the pod
Dec 23 18:01:59.988: INFO: Waiting for pod client-containers-40a4871f-aad6-4066-95ad-ba8561323731 to disappear
Dec 23 18:01:59.994: INFO: Pod client-containers-40a4871f-aad6-4066-95ad-ba8561323731 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:01:59.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2796" for this suite.
Dec 23 18:02:06.027: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:02:06.246: INFO: namespace containers-2796 deletion completed in 6.243016468s

• [SLOW TEST:8.456 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:02:06.249: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-d866w in namespace proxy-9965
I1223 18:02:06.418201      14 runners.go:180] Created replication controller with name: proxy-service-d866w, namespace: proxy-9965, replica count: 1
I1223 18:02:07.469733      14 runners.go:180] proxy-service-d866w Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1223 18:02:08.470327      14 runners.go:180] proxy-service-d866w Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1223 18:02:09.470936      14 runners.go:180] proxy-service-d866w Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1223 18:02:10.471294      14 runners.go:180] proxy-service-d866w Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1223 18:02:11.525672      14 runners.go:180] proxy-service-d866w Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1223 18:02:12.526072      14 runners.go:180] proxy-service-d866w Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1223 18:02:13.526481      14 runners.go:180] proxy-service-d866w Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1223 18:02:14.526844      14 runners.go:180] proxy-service-d866w Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1223 18:02:15.527514      14 runners.go:180] proxy-service-d866w Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 23 18:02:15.533: INFO: setup took 9.179650006s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec 23 18:02:15.586: INFO: (0) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:460/proxy/: tls baz (200; 51.275066ms)
Dec 23 18:02:15.587: INFO: (0) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:160/proxy/: foo (200; 48.095416ms)
Dec 23 18:02:15.588: INFO: (0) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname1/proxy/: foo (200; 52.655838ms)
Dec 23 18:02:15.589: INFO: (0) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname2/proxy/: bar (200; 55.764307ms)
Dec 23 18:02:15.590: INFO: (0) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/rewriteme">test</a> (200; 52.502534ms)
Dec 23 18:02:15.592: INFO: (0) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname1/proxy/: foo (200; 53.281643ms)
Dec 23 18:02:15.592: INFO: (0) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/tlsrewritem... (200; 58.564866ms)
Dec 23 18:02:15.592: INFO: (0) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/rewriteme">test<... (200; 53.218587ms)
Dec 23 18:02:15.593: INFO: (0) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:160/proxy/: foo (200; 53.534204ms)
Dec 23 18:02:15.593: INFO: (0) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:162/proxy/: bar (200; 54.912319ms)
Dec 23 18:02:15.593: INFO: (0) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/rewriteme">... (200; 53.95112ms)
Dec 23 18:02:15.593: INFO: (0) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname1/proxy/: tls baz (200; 55.435511ms)
Dec 23 18:02:15.594: INFO: (0) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname2/proxy/: tls qux (200; 55.239502ms)
Dec 23 18:02:15.594: INFO: (0) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:462/proxy/: tls qux (200; 59.44943ms)
Dec 23 18:02:15.594: INFO: (0) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:162/proxy/: bar (200; 56.293469ms)
Dec 23 18:02:15.594: INFO: (0) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname2/proxy/: bar (200; 56.893719ms)
Dec 23 18:02:15.608: INFO: (1) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname1/proxy/: foo (200; 12.152844ms)
Dec 23 18:02:15.609: INFO: (1) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/rewriteme">test<... (200; 13.66937ms)
Dec 23 18:02:15.609: INFO: (1) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/rewriteme">test</a> (200; 10.764925ms)
Dec 23 18:02:15.613: INFO: (1) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/tlsrewritem... (200; 13.193406ms)
Dec 23 18:02:15.616: INFO: (1) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname1/proxy/: tls baz (200; 17.866316ms)
Dec 23 18:02:15.616: INFO: (1) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:460/proxy/: tls baz (200; 14.708619ms)
Dec 23 18:02:15.616: INFO: (1) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:462/proxy/: tls qux (200; 14.215502ms)
Dec 23 18:02:15.616: INFO: (1) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname2/proxy/: bar (200; 20.40812ms)
Dec 23 18:02:15.616: INFO: (1) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:162/proxy/: bar (200; 16.414671ms)
Dec 23 18:02:15.616: INFO: (1) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname2/proxy/: tls qux (200; 12.821701ms)
Dec 23 18:02:15.616: INFO: (1) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname2/proxy/: bar (200; 19.5593ms)
Dec 23 18:02:15.616: INFO: (1) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:160/proxy/: foo (200; 11.686671ms)
Dec 23 18:02:15.616: INFO: (1) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:160/proxy/: foo (200; 11.254632ms)
Dec 23 18:02:15.616: INFO: (1) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/rewriteme">... (200; 12.643494ms)
Dec 23 18:02:15.616: INFO: (1) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:162/proxy/: bar (200; 10.921036ms)
Dec 23 18:02:15.618: INFO: (1) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname1/proxy/: foo (200; 15.929984ms)
Dec 23 18:02:15.634: INFO: (2) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/tlsrewritem... (200; 10.173929ms)
Dec 23 18:02:15.636: INFO: (2) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/rewriteme">test<... (200; 13.027482ms)
Dec 23 18:02:15.637: INFO: (2) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/rewriteme">test</a> (200; 13.875965ms)
Dec 23 18:02:15.637: INFO: (2) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:162/proxy/: bar (200; 13.867348ms)
Dec 23 18:02:15.638: INFO: (2) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:160/proxy/: foo (200; 13.864631ms)
Dec 23 18:02:15.638: INFO: (2) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:460/proxy/: tls baz (200; 14.192409ms)
Dec 23 18:02:15.638: INFO: (2) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:462/proxy/: tls qux (200; 14.127602ms)
Dec 23 18:02:15.644: INFO: (2) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname1/proxy/: foo (200; 20.189512ms)
Dec 23 18:02:15.645: INFO: (2) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/rewriteme">... (200; 20.202335ms)
Dec 23 18:02:15.645: INFO: (2) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:160/proxy/: foo (200; 20.497712ms)
Dec 23 18:02:15.645: INFO: (2) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:162/proxy/: bar (200; 20.446172ms)
Dec 23 18:02:15.645: INFO: (2) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname2/proxy/: bar (200; 21.230677ms)
Dec 23 18:02:15.645: INFO: (2) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname2/proxy/: bar (200; 20.879744ms)
Dec 23 18:02:15.645: INFO: (2) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname1/proxy/: tls baz (200; 20.911804ms)
Dec 23 18:02:15.645: INFO: (2) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname1/proxy/: foo (200; 21.029925ms)
Dec 23 18:02:15.645: INFO: (2) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname2/proxy/: tls qux (200; 20.890172ms)
Dec 23 18:02:15.663: INFO: (3) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:460/proxy/: tls baz (200; 12.12974ms)
Dec 23 18:02:15.664: INFO: (3) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/rewriteme">test<... (200; 14.148094ms)
Dec 23 18:02:15.664: INFO: (3) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/rewriteme">test</a> (200; 13.221313ms)
Dec 23 18:02:15.664: INFO: (3) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:162/proxy/: bar (200; 13.105532ms)
Dec 23 18:02:15.667: INFO: (3) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:160/proxy/: foo (200; 16.209076ms)
Dec 23 18:02:15.667: INFO: (3) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname2/proxy/: bar (200; 17.565462ms)
Dec 23 18:02:15.667: INFO: (3) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:462/proxy/: tls qux (200; 16.642119ms)
Dec 23 18:02:15.668: INFO: (3) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:160/proxy/: foo (200; 16.386959ms)
Dec 23 18:02:15.668: INFO: (3) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/tlsrewritem... (200; 16.908984ms)
Dec 23 18:02:15.670: INFO: (3) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:162/proxy/: bar (200; 18.975666ms)
Dec 23 18:02:15.674: INFO: (3) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname1/proxy/: foo (200; 22.848419ms)
Dec 23 18:02:15.674: INFO: (3) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/rewriteme">... (200; 22.954352ms)
Dec 23 18:02:15.675: INFO: (3) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname1/proxy/: foo (200; 23.733602ms)
Dec 23 18:02:15.675: INFO: (3) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname1/proxy/: tls baz (200; 23.809597ms)
Dec 23 18:02:15.676: INFO: (3) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname2/proxy/: bar (200; 24.676679ms)
Dec 23 18:02:15.676: INFO: (3) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname2/proxy/: tls qux (200; 24.960299ms)
Dec 23 18:02:15.684: INFO: (4) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/rewriteme">... (200; 6.517647ms)
Dec 23 18:02:15.687: INFO: (4) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:160/proxy/: foo (200; 9.169317ms)
Dec 23 18:02:15.689: INFO: (4) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:162/proxy/: bar (200; 8.95798ms)
Dec 23 18:02:15.689: INFO: (4) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:160/proxy/: foo (200; 11.510885ms)
Dec 23 18:02:15.690: INFO: (4) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:162/proxy/: bar (200; 12.332433ms)
Dec 23 18:02:15.692: INFO: (4) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:460/proxy/: tls baz (200; 10.448155ms)
Dec 23 18:02:15.693: INFO: (4) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/rewriteme">test</a> (200; 13.231417ms)
Dec 23 18:02:15.693: INFO: (4) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/rewriteme">test<... (200; 14.105405ms)
Dec 23 18:02:15.693: INFO: (4) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:462/proxy/: tls qux (200; 10.13007ms)
Dec 23 18:02:15.694: INFO: (4) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname1/proxy/: foo (200; 17.534348ms)
Dec 23 18:02:15.694: INFO: (4) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname2/proxy/: tls qux (200; 17.279348ms)
Dec 23 18:02:15.694: INFO: (4) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/tlsrewritem... (200; 14.299008ms)
Dec 23 18:02:15.696: INFO: (4) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname2/proxy/: bar (200; 17.399712ms)
Dec 23 18:02:15.697: INFO: (4) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname1/proxy/: foo (200; 18.550906ms)
Dec 23 18:02:15.698: INFO: (4) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname1/proxy/: tls baz (200; 18.20186ms)
Dec 23 18:02:15.698: INFO: (4) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname2/proxy/: bar (200; 18.667124ms)
Dec 23 18:02:15.711: INFO: (5) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:462/proxy/: tls qux (200; 9.315546ms)
Dec 23 18:02:15.711: INFO: (5) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:160/proxy/: foo (200; 13.058706ms)
Dec 23 18:02:15.712: INFO: (5) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/rewriteme">test<... (200; 13.677217ms)
Dec 23 18:02:15.712: INFO: (5) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:162/proxy/: bar (200; 12.952063ms)
Dec 23 18:02:15.713: INFO: (5) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/tlsrewritem... (200; 12.509842ms)
Dec 23 18:02:15.713: INFO: (5) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:460/proxy/: tls baz (200; 12.173666ms)
Dec 23 18:02:15.713: INFO: (5) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname2/proxy/: bar (200; 13.864999ms)
Dec 23 18:02:15.716: INFO: (5) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/rewriteme">test</a> (200; 10.502951ms)
Dec 23 18:02:15.719: INFO: (5) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/rewriteme">... (200; 9.385193ms)
Dec 23 18:02:15.721: INFO: (5) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname2/proxy/: bar (200; 17.212164ms)
Dec 23 18:02:15.721: INFO: (5) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname1/proxy/: tls baz (200; 16.843076ms)
Dec 23 18:02:15.721: INFO: (5) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:160/proxy/: foo (200; 14.862383ms)
Dec 23 18:02:15.721: INFO: (5) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:162/proxy/: bar (200; 14.596454ms)
Dec 23 18:02:15.721: INFO: (5) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname1/proxy/: foo (200; 19.076743ms)
Dec 23 18:02:15.723: INFO: (5) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname1/proxy/: foo (200; 15.784682ms)
Dec 23 18:02:15.725: INFO: (5) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname2/proxy/: tls qux (200; 16.006156ms)
Dec 23 18:02:15.738: INFO: (6) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname2/proxy/: bar (200; 12.858819ms)
Dec 23 18:02:15.738: INFO: (6) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:462/proxy/: tls qux (200; 11.917345ms)
Dec 23 18:02:15.740: INFO: (6) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:460/proxy/: tls baz (200; 13.856435ms)
Dec 23 18:02:15.740: INFO: (6) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/tlsrewritem... (200; 14.559297ms)
Dec 23 18:02:15.740: INFO: (6) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/rewriteme">test</a> (200; 11.474065ms)
Dec 23 18:02:15.741: INFO: (6) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:162/proxy/: bar (200; 11.98833ms)
Dec 23 18:02:15.741: INFO: (6) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:162/proxy/: bar (200; 10.784498ms)
Dec 23 18:02:15.741: INFO: (6) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname1/proxy/: foo (200; 14.838684ms)
Dec 23 18:02:15.744: INFO: (6) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname1/proxy/: tls baz (200; 16.337139ms)
Dec 23 18:02:15.745: INFO: (6) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname2/proxy/: bar (200; 18.765113ms)
Dec 23 18:02:15.746: INFO: (6) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/rewriteme">... (200; 12.443097ms)
Dec 23 18:02:15.746: INFO: (6) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:160/proxy/: foo (200; 11.070351ms)
Dec 23 18:02:15.748: INFO: (6) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname1/proxy/: foo (200; 17.156988ms)
Dec 23 18:02:15.748: INFO: (6) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/rewriteme">test<... (200; 11.360997ms)
Dec 23 18:02:15.748: INFO: (6) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:160/proxy/: foo (200; 12.514794ms)
Dec 23 18:02:15.749: INFO: (6) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname2/proxy/: tls qux (200; 17.486537ms)
Dec 23 18:02:15.762: INFO: (7) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/tlsrewritem... (200; 13.614881ms)
Dec 23 18:02:15.763: INFO: (7) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:160/proxy/: foo (200; 10.921142ms)
Dec 23 18:02:15.763: INFO: (7) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname1/proxy/: foo (200; 13.635622ms)
Dec 23 18:02:15.763: INFO: (7) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/rewriteme">... (200; 12.916007ms)
Dec 23 18:02:15.764: INFO: (7) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:160/proxy/: foo (200; 10.315393ms)
Dec 23 18:02:15.765: INFO: (7) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/rewriteme">test<... (200; 9.724707ms)
Dec 23 18:02:15.765: INFO: (7) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname2/proxy/: tls qux (200; 14.633851ms)
Dec 23 18:02:15.765: INFO: (7) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:162/proxy/: bar (200; 10.297542ms)
Dec 23 18:02:15.767: INFO: (7) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:460/proxy/: tls baz (200; 10.946945ms)
Dec 23 18:02:15.768: INFO: (7) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:462/proxy/: tls qux (200; 10.902766ms)
Dec 23 18:02:15.770: INFO: (7) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:162/proxy/: bar (200; 11.500338ms)
Dec 23 18:02:15.773: INFO: (7) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname1/proxy/: tls baz (200; 14.858822ms)
Dec 23 18:02:15.774: INFO: (7) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname2/proxy/: bar (200; 18.126641ms)
Dec 23 18:02:15.774: INFO: (7) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/rewriteme">test</a> (200; 15.264868ms)
Dec 23 18:02:15.774: INFO: (7) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname1/proxy/: foo (200; 15.992393ms)
Dec 23 18:02:15.775: INFO: (7) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname2/proxy/: bar (200; 16.579502ms)
Dec 23 18:02:15.786: INFO: (8) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:462/proxy/: tls qux (200; 10.070019ms)
Dec 23 18:02:15.787: INFO: (8) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:460/proxy/: tls baz (200; 11.178011ms)
Dec 23 18:02:15.788: INFO: (8) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname2/proxy/: bar (200; 13.24918ms)
Dec 23 18:02:15.790: INFO: (8) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname1/proxy/: foo (200; 13.783424ms)
Dec 23 18:02:15.791: INFO: (8) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:162/proxy/: bar (200; 10.534865ms)
Dec 23 18:02:15.791: INFO: (8) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/rewriteme">test</a> (200; 12.2667ms)
Dec 23 18:02:15.794: INFO: (8) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/tlsrewritem... (200; 13.012969ms)
Dec 23 18:02:15.794: INFO: (8) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname2/proxy/: bar (200; 16.817855ms)
Dec 23 18:02:15.794: INFO: (8) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/rewriteme">... (200; 11.503311ms)
Dec 23 18:02:15.794: INFO: (8) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:162/proxy/: bar (200; 9.958756ms)
Dec 23 18:02:15.797: INFO: (8) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname1/proxy/: foo (200; 14.670794ms)
Dec 23 18:02:15.797: INFO: (8) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/rewriteme">test<... (200; 12.38628ms)
Dec 23 18:02:15.798: INFO: (8) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:160/proxy/: foo (200; 14.858296ms)
Dec 23 18:02:15.798: INFO: (8) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:160/proxy/: foo (200; 14.502441ms)
Dec 23 18:02:15.798: INFO: (8) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname1/proxy/: tls baz (200; 19.543828ms)
Dec 23 18:02:15.799: INFO: (8) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname2/proxy/: tls qux (200; 16.434044ms)
Dec 23 18:02:15.810: INFO: (9) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/rewriteme">test<... (200; 10.835433ms)
Dec 23 18:02:15.812: INFO: (9) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:162/proxy/: bar (200; 13.331488ms)
Dec 23 18:02:15.814: INFO: (9) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:462/proxy/: tls qux (200; 13.249291ms)
Dec 23 18:02:15.814: INFO: (9) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:460/proxy/: tls baz (200; 14.368022ms)
Dec 23 18:02:15.819: INFO: (9) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname2/proxy/: tls qux (200; 13.292152ms)
Dec 23 18:02:15.820: INFO: (9) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/rewriteme">test</a> (200; 16.913765ms)
Dec 23 18:02:15.820: INFO: (9) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:162/proxy/: bar (200; 16.446738ms)
Dec 23 18:02:15.821: INFO: (9) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname2/proxy/: bar (200; 21.072763ms)
Dec 23 18:02:15.821: INFO: (9) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/tlsrewritem... (200; 16.935194ms)
Dec 23 18:02:15.822: INFO: (9) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/rewriteme">... (200; 15.636929ms)
Dec 23 18:02:15.822: INFO: (9) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname1/proxy/: foo (200; 21.276123ms)
Dec 23 18:02:15.822: INFO: (9) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:160/proxy/: foo (200; 14.570424ms)
Dec 23 18:02:15.823: INFO: (9) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname2/proxy/: bar (200; 20.621871ms)
Dec 23 18:02:15.823: INFO: (9) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:160/proxy/: foo (200; 15.902505ms)
Dec 23 18:02:15.824: INFO: (9) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname1/proxy/: tls baz (200; 21.38922ms)
Dec 23 18:02:15.824: INFO: (9) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname1/proxy/: foo (200; 19.001528ms)
Dec 23 18:02:15.837: INFO: (10) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:162/proxy/: bar (200; 12.151805ms)
Dec 23 18:02:15.839: INFO: (10) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/tlsrewritem... (200; 13.014133ms)
Dec 23 18:02:15.839: INFO: (10) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:460/proxy/: tls baz (200; 13.714935ms)
Dec 23 18:02:15.840: INFO: (10) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:462/proxy/: tls qux (200; 14.056497ms)
Dec 23 18:02:15.841: INFO: (10) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname1/proxy/: foo (200; 14.560523ms)
Dec 23 18:02:15.841: INFO: (10) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname2/proxy/: bar (200; 16.290415ms)
Dec 23 18:02:15.842: INFO: (10) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:160/proxy/: foo (200; 14.103564ms)
Dec 23 18:02:15.842: INFO: (10) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:162/proxy/: bar (200; 13.840465ms)
Dec 23 18:02:15.843: INFO: (10) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/rewriteme">test</a> (200; 14.372933ms)
Dec 23 18:02:15.844: INFO: (10) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname1/proxy/: tls baz (200; 16.541111ms)
Dec 23 18:02:15.845: INFO: (10) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname2/proxy/: bar (200; 18.476239ms)
Dec 23 18:02:15.845: INFO: (10) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:160/proxy/: foo (200; 14.275365ms)
Dec 23 18:02:15.848: INFO: (10) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname2/proxy/: tls qux (200; 18.645526ms)
Dec 23 18:02:15.848: INFO: (10) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/rewriteme">test<... (200; 15.816793ms)
Dec 23 18:02:15.848: INFO: (10) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/rewriteme">... (200; 17.491621ms)
Dec 23 18:02:15.848: INFO: (10) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname1/proxy/: foo (200; 19.285275ms)
Dec 23 18:02:15.865: INFO: (11) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:462/proxy/: tls qux (200; 11.368047ms)
Dec 23 18:02:15.867: INFO: (11) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/tlsrewritem... (200; 13.313059ms)
Dec 23 18:02:15.868: INFO: (11) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:162/proxy/: bar (200; 14.843698ms)
Dec 23 18:02:15.868: INFO: (11) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:460/proxy/: tls baz (200; 14.701541ms)
Dec 23 18:02:15.869: INFO: (11) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/rewriteme">test</a> (200; 15.657421ms)
Dec 23 18:02:15.872: INFO: (11) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/rewriteme">... (200; 17.876567ms)
Dec 23 18:02:15.875: INFO: (11) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:160/proxy/: foo (200; 21.197103ms)
Dec 23 18:02:15.877: INFO: (11) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname2/proxy/: bar (200; 25.185895ms)
Dec 23 18:02:15.877: INFO: (11) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/rewriteme">test<... (200; 23.002881ms)
Dec 23 18:02:15.877: INFO: (11) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:162/proxy/: bar (200; 23.370927ms)
Dec 23 18:02:15.878: INFO: (11) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname1/proxy/: tls baz (200; 25.885082ms)
Dec 23 18:02:15.879: INFO: (11) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname2/proxy/: bar (200; 27.160161ms)
Dec 23 18:02:15.879: INFO: (11) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:160/proxy/: foo (200; 24.89555ms)
Dec 23 18:02:15.879: INFO: (11) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname1/proxy/: foo (200; 25.315227ms)
Dec 23 18:02:15.881: INFO: (11) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname2/proxy/: tls qux (200; 26.988275ms)
Dec 23 18:02:15.884: INFO: (11) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname1/proxy/: foo (200; 29.337204ms)
Dec 23 18:02:15.894: INFO: (12) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:160/proxy/: foo (200; 9.903889ms)
Dec 23 18:02:15.894: INFO: (12) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/rewriteme">test<... (200; 10.094032ms)
Dec 23 18:02:15.895: INFO: (12) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:162/proxy/: bar (200; 9.405188ms)
Dec 23 18:02:15.895: INFO: (12) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:460/proxy/: tls baz (200; 8.94397ms)
Dec 23 18:02:15.896: INFO: (12) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/tlsrewritem... (200; 10.046822ms)
Dec 23 18:02:15.897: INFO: (12) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname2/proxy/: bar (200; 12.407689ms)
Dec 23 18:02:15.897: INFO: (12) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:462/proxy/: tls qux (200; 10.188627ms)
Dec 23 18:02:15.903: INFO: (12) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname1/proxy/: tls baz (200; 14.490686ms)
Dec 23 18:02:15.904: INFO: (12) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/rewriteme">... (200; 13.749134ms)
Dec 23 18:02:15.907: INFO: (12) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname1/proxy/: foo (200; 19.83059ms)
Dec 23 18:02:15.908: INFO: (12) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname2/proxy/: bar (200; 19.508271ms)
Dec 23 18:02:15.908: INFO: (12) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/rewriteme">test</a> (200; 18.498711ms)
Dec 23 18:02:15.909: INFO: (12) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname2/proxy/: tls qux (200; 18.846955ms)
Dec 23 18:02:15.909: INFO: (12) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:160/proxy/: foo (200; 19.268775ms)
Dec 23 18:02:15.909: INFO: (12) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:162/proxy/: bar (200; 19.380231ms)
Dec 23 18:02:15.909: INFO: (12) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname1/proxy/: foo (200; 19.496574ms)
Dec 23 18:02:15.919: INFO: (13) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/rewriteme">test<... (200; 9.639912ms)
Dec 23 18:02:15.923: INFO: (13) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:162/proxy/: bar (200; 10.501398ms)
Dec 23 18:02:15.923: INFO: (13) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/tlsrewritem... (200; 9.66532ms)
Dec 23 18:02:15.923: INFO: (13) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:462/proxy/: tls qux (200; 8.833253ms)
Dec 23 18:02:15.926: INFO: (13) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/rewriteme">... (200; 9.534141ms)
Dec 23 18:02:15.927: INFO: (13) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:160/proxy/: foo (200; 10.016906ms)
Dec 23 18:02:15.928: INFO: (13) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:460/proxy/: tls baz (200; 13.638846ms)
Dec 23 18:02:15.930: INFO: (13) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/rewriteme">test</a> (200; 18.778291ms)
Dec 23 18:02:15.932: INFO: (13) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname1/proxy/: foo (200; 12.969247ms)
Dec 23 18:02:15.932: INFO: (13) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname2/proxy/: bar (200; 21.550297ms)
Dec 23 18:02:15.932: INFO: (13) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:160/proxy/: foo (200; 15.288435ms)
Dec 23 18:02:15.933: INFO: (13) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname2/proxy/: bar (200; 22.569077ms)
Dec 23 18:02:15.933: INFO: (13) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname1/proxy/: foo (200; 17.616096ms)
Dec 23 18:02:15.933: INFO: (13) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:162/proxy/: bar (200; 15.045068ms)
Dec 23 18:02:15.933: INFO: (13) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname2/proxy/: tls qux (200; 17.75717ms)
Dec 23 18:02:15.933: INFO: (13) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname1/proxy/: tls baz (200; 22.433593ms)
Dec 23 18:02:15.944: INFO: (14) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/rewriteme">test<... (200; 9.220518ms)
Dec 23 18:02:15.944: INFO: (14) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:160/proxy/: foo (200; 10.563028ms)
Dec 23 18:02:15.945: INFO: (14) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/tlsrewritem... (200; 9.184126ms)
Dec 23 18:02:15.945: INFO: (14) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:162/proxy/: bar (200; 9.849242ms)
Dec 23 18:02:15.948: INFO: (14) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:462/proxy/: tls qux (200; 10.353525ms)
Dec 23 18:02:15.948: INFO: (14) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:460/proxy/: tls baz (200; 11.157544ms)
Dec 23 18:02:15.954: INFO: (14) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/rewriteme">test</a> (200; 13.397956ms)
Dec 23 18:02:15.954: INFO: (14) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname2/proxy/: bar (200; 19.241832ms)
Dec 23 18:02:15.957: INFO: (14) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname1/proxy/: foo (200; 18.794811ms)
Dec 23 18:02:15.959: INFO: (14) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname2/proxy/: bar (200; 19.770868ms)
Dec 23 18:02:15.960: INFO: (14) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/rewriteme">... (200; 18.996222ms)
Dec 23 18:02:15.960: INFO: (14) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:162/proxy/: bar (200; 19.838092ms)
Dec 23 18:02:15.960: INFO: (14) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:160/proxy/: foo (200; 19.940304ms)
Dec 23 18:02:15.961: INFO: (14) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname1/proxy/: foo (200; 20.023935ms)
Dec 23 18:02:15.963: INFO: (14) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname1/proxy/: tls baz (200; 22.866679ms)
Dec 23 18:02:15.966: INFO: (14) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname2/proxy/: tls qux (200; 25.278666ms)
Dec 23 18:02:15.979: INFO: (15) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:460/proxy/: tls baz (200; 12.732841ms)
Dec 23 18:02:15.979: INFO: (15) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:462/proxy/: tls qux (200; 11.996523ms)
Dec 23 18:02:15.987: INFO: (15) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/rewriteme">test<... (200; 7.820309ms)
Dec 23 18:02:15.987: INFO: (15) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname2/proxy/: bar (200; 20.865704ms)
Dec 23 18:02:15.987: INFO: (15) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/rewriteme">test</a> (200; 17.392921ms)
Dec 23 18:02:15.988: INFO: (15) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:162/proxy/: bar (200; 17.158117ms)
Dec 23 18:02:15.988: INFO: (15) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/tlsrewritem... (200; 16.771502ms)
Dec 23 18:02:15.988: INFO: (15) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname1/proxy/: tls baz (200; 18.823715ms)
Dec 23 18:02:15.988: INFO: (15) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname2/proxy/: bar (200; 19.705538ms)
Dec 23 18:02:15.988: INFO: (15) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname1/proxy/: foo (200; 20.264555ms)
Dec 23 18:02:15.988: INFO: (15) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:160/proxy/: foo (200; 14.62862ms)
Dec 23 18:02:15.988: INFO: (15) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:162/proxy/: bar (200; 13.566975ms)
Dec 23 18:02:15.988: INFO: (15) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/rewriteme">... (200; 15.479972ms)
Dec 23 18:02:15.988: INFO: (15) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:160/proxy/: foo (200; 15.198776ms)
Dec 23 18:02:15.989: INFO: (15) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname1/proxy/: foo (200; 17.29587ms)
Dec 23 18:02:15.989: INFO: (15) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname2/proxy/: tls qux (200; 16.864182ms)
Dec 23 18:02:16.002: INFO: (16) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/rewriteme">... (200; 9.424832ms)
Dec 23 18:02:16.003: INFO: (16) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/rewriteme">test</a> (200; 12.6413ms)
Dec 23 18:02:16.004: INFO: (16) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:160/proxy/: foo (200; 13.703954ms)
Dec 23 18:02:16.004: INFO: (16) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:162/proxy/: bar (200; 13.333871ms)
Dec 23 18:02:16.005: INFO: (16) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:160/proxy/: foo (200; 11.896898ms)
Dec 23 18:02:16.007: INFO: (16) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/rewriteme">test<... (200; 12.382664ms)
Dec 23 18:02:16.007: INFO: (16) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/tlsrewritem... (200; 10.709842ms)
Dec 23 18:02:16.007: INFO: (16) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname2/proxy/: tls qux (200; 15.660667ms)
Dec 23 18:02:16.009: INFO: (16) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:462/proxy/: tls qux (200; 10.760057ms)
Dec 23 18:02:16.009: INFO: (16) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname1/proxy/: foo (200; 17.553959ms)
Dec 23 18:02:16.009: INFO: (16) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:162/proxy/: bar (200; 13.105404ms)
Dec 23 18:02:16.009: INFO: (16) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:460/proxy/: tls baz (200; 11.342183ms)
Dec 23 18:02:16.010: INFO: (16) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname2/proxy/: bar (200; 14.387395ms)
Dec 23 18:02:16.014: INFO: (16) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname1/proxy/: tls baz (200; 15.615645ms)
Dec 23 18:02:16.015: INFO: (16) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname1/proxy/: foo (200; 17.43461ms)
Dec 23 18:02:16.016: INFO: (16) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname2/proxy/: bar (200; 17.57725ms)
Dec 23 18:02:16.029: INFO: (17) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/rewriteme">test<... (200; 13.020459ms)
Dec 23 18:02:16.030: INFO: (17) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:162/proxy/: bar (200; 10.536698ms)
Dec 23 18:02:16.030: INFO: (17) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/rewriteme">test</a> (200; 12.024133ms)
Dec 23 18:02:16.032: INFO: (17) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname1/proxy/: foo (200; 16.547892ms)
Dec 23 18:02:16.035: INFO: (17) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:460/proxy/: tls baz (200; 15.125464ms)
Dec 23 18:02:16.035: INFO: (17) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/tlsrewritem... (200; 15.822543ms)
Dec 23 18:02:16.040: INFO: (17) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname2/proxy/: tls qux (200; 18.178169ms)
Dec 23 18:02:16.040: INFO: (17) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname2/proxy/: bar (200; 23.079532ms)
Dec 23 18:02:16.040: INFO: (17) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:462/proxy/: tls qux (200; 18.618121ms)
Dec 23 18:02:16.040: INFO: (17) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname2/proxy/: bar (200; 22.939219ms)
Dec 23 18:02:16.040: INFO: (17) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname1/proxy/: tls baz (200; 22.481646ms)
Dec 23 18:02:16.040: INFO: (17) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/rewriteme">... (200; 18.299942ms)
Dec 23 18:02:16.040: INFO: (17) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:160/proxy/: foo (200; 18.350609ms)
Dec 23 18:02:16.040: INFO: (17) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:160/proxy/: foo (200; 18.329501ms)
Dec 23 18:02:16.040: INFO: (17) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:162/proxy/: bar (200; 18.260375ms)
Dec 23 18:02:16.040: INFO: (17) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname1/proxy/: foo (200; 18.723631ms)
Dec 23 18:02:16.052: INFO: (18) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/rewriteme">test</a> (200; 10.038259ms)
Dec 23 18:02:16.053: INFO: (18) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:160/proxy/: foo (200; 9.815407ms)
Dec 23 18:02:16.055: INFO: (18) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:162/proxy/: bar (200; 12.11458ms)
Dec 23 18:02:16.057: INFO: (18) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:162/proxy/: bar (200; 10.858729ms)
Dec 23 18:02:16.058: INFO: (18) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/rewriteme">... (200; 14.851322ms)
Dec 23 18:02:16.059: INFO: (18) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:160/proxy/: foo (200; 16.501807ms)
Dec 23 18:02:16.061: INFO: (18) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/rewriteme">test<... (200; 15.580488ms)
Dec 23 18:02:16.061: INFO: (18) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname1/proxy/: foo (200; 17.982803ms)
Dec 23 18:02:16.062: INFO: (18) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname2/proxy/: tls qux (200; 18.7936ms)
Dec 23 18:02:16.062: INFO: (18) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/tlsrewritem... (200; 15.978791ms)
Dec 23 18:02:16.062: INFO: (18) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:460/proxy/: tls baz (200; 15.930482ms)
Dec 23 18:02:16.062: INFO: (18) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:462/proxy/: tls qux (200; 15.954605ms)
Dec 23 18:02:16.066: INFO: (18) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname2/proxy/: bar (200; 18.864074ms)
Dec 23 18:02:16.068: INFO: (18) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname2/proxy/: bar (200; 22.066858ms)
Dec 23 18:02:16.068: INFO: (18) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname1/proxy/: foo (200; 20.545142ms)
Dec 23 18:02:16.069: INFO: (18) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname1/proxy/: tls baz (200; 21.604037ms)
Dec 23 18:02:16.093: INFO: (19) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:443/proxy/tlsrewritem... (200; 9.07347ms)
Dec 23 18:02:16.095: INFO: (19) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:162/proxy/: bar (200; 26.052376ms)
Dec 23 18:02:16.096: INFO: (19) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:162/proxy/: bar (200; 25.992703ms)
Dec 23 18:02:16.096: INFO: (19) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname1/proxy/: foo (200; 25.770905ms)
Dec 23 18:02:16.096: INFO: (19) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:160/proxy/: foo (200; 21.908753ms)
Dec 23 18:02:16.097: INFO: (19) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:1080/proxy/rewriteme">... (200; 22.522965ms)
Dec 23 18:02:16.098: INFO: (19) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname2/proxy/: tls qux (200; 25.08167ms)
Dec 23 18:02:16.098: INFO: (19) /api/v1/namespaces/proxy-9965/pods/http:proxy-service-d866w-jtl6w:160/proxy/: foo (200; 22.991992ms)
Dec 23 18:02:16.098: INFO: (19) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w:1080/proxy/rewriteme">test<... (200; 14.200422ms)
Dec 23 18:02:16.098: INFO: (19) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname2/proxy/: bar (200; 14.431234ms)
Dec 23 18:02:16.098: INFO: (19) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:460/proxy/: tls baz (200; 11.41907ms)
Dec 23 18:02:16.098: INFO: (19) /api/v1/namespaces/proxy-9965/pods/https:proxy-service-d866w-jtl6w:462/proxy/: tls qux (200; 10.561845ms)
Dec 23 18:02:16.101: INFO: (19) /api/v1/namespaces/proxy-9965/services/proxy-service-d866w:portname1/proxy/: foo (200; 12.768962ms)
Dec 23 18:02:16.101: INFO: (19) /api/v1/namespaces/proxy-9965/services/http:proxy-service-d866w:portname2/proxy/: bar (200; 12.402634ms)
Dec 23 18:02:16.101: INFO: (19) /api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/: <a href="/api/v1/namespaces/proxy-9965/pods/proxy-service-d866w-jtl6w/proxy/rewriteme">test</a> (200; 8.810054ms)
Dec 23 18:02:16.103: INFO: (19) /api/v1/namespaces/proxy-9965/services/https:proxy-service-d866w:tlsportname1/proxy/: tls baz (200; 11.665203ms)
STEP: deleting ReplicationController proxy-service-d866w in namespace proxy-9965, will wait for the garbage collector to delete the pods
Dec 23 18:02:16.178: INFO: Deleting ReplicationController proxy-service-d866w took: 18.434861ms
Dec 23 18:02:16.578: INFO: Terminating ReplicationController proxy-service-d866w pods took: 400.61845ms
[AfterEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:02:23.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9965" for this suite.
Dec 23 18:02:29.312: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:02:29.536: INFO: namespace proxy-9965 deletion completed in 6.246127823s

• [SLOW TEST:23.288 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:02:29.542: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-hzh9
STEP: Creating a pod to test atomic-volume-subpath
Dec 23 18:02:29.682: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-hzh9" in namespace "subpath-5133" to be "success or failure"
Dec 23 18:02:29.688: INFO: Pod "pod-subpath-test-configmap-hzh9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.436305ms
Dec 23 18:02:31.694: INFO: Pod "pod-subpath-test-configmap-hzh9": Phase="Running", Reason="", readiness=true. Elapsed: 2.011872029s
Dec 23 18:02:33.700: INFO: Pod "pod-subpath-test-configmap-hzh9": Phase="Running", Reason="", readiness=true. Elapsed: 4.017357547s
Dec 23 18:02:35.706: INFO: Pod "pod-subpath-test-configmap-hzh9": Phase="Running", Reason="", readiness=true. Elapsed: 6.023181903s
Dec 23 18:02:37.712: INFO: Pod "pod-subpath-test-configmap-hzh9": Phase="Running", Reason="", readiness=true. Elapsed: 8.030041036s
Dec 23 18:02:39.718: INFO: Pod "pod-subpath-test-configmap-hzh9": Phase="Running", Reason="", readiness=true. Elapsed: 10.03584639s
Dec 23 18:02:41.725: INFO: Pod "pod-subpath-test-configmap-hzh9": Phase="Running", Reason="", readiness=true. Elapsed: 12.04234974s
Dec 23 18:02:43.731: INFO: Pod "pod-subpath-test-configmap-hzh9": Phase="Running", Reason="", readiness=true. Elapsed: 14.048960739s
Dec 23 18:02:45.737: INFO: Pod "pod-subpath-test-configmap-hzh9": Phase="Running", Reason="", readiness=true. Elapsed: 16.054191427s
Dec 23 18:02:47.743: INFO: Pod "pod-subpath-test-configmap-hzh9": Phase="Running", Reason="", readiness=true. Elapsed: 18.060728735s
Dec 23 18:02:49.750: INFO: Pod "pod-subpath-test-configmap-hzh9": Phase="Running", Reason="", readiness=true. Elapsed: 20.06716361s
Dec 23 18:02:51.758: INFO: Pod "pod-subpath-test-configmap-hzh9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.075956454s
STEP: Saw pod success
Dec 23 18:02:51.759: INFO: Pod "pod-subpath-test-configmap-hzh9" satisfied condition "success or failure"
Dec 23 18:02:51.764: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-subpath-test-configmap-hzh9 container test-container-subpath-configmap-hzh9: <nil>
STEP: delete the pod
Dec 23 18:02:51.809: INFO: Waiting for pod pod-subpath-test-configmap-hzh9 to disappear
Dec 23 18:02:51.818: INFO: Pod pod-subpath-test-configmap-hzh9 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-hzh9
Dec 23 18:02:51.818: INFO: Deleting pod "pod-subpath-test-configmap-hzh9" in namespace "subpath-5133"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:02:51.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5133" for this suite.
Dec 23 18:02:57.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:02:58.086: INFO: namespace subpath-5133 deletion completed in 6.249790962s

• [SLOW TEST:28.545 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:02:58.088: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:03:24.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2897" for this suite.
Dec 23 18:03:30.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:03:30.645: INFO: namespace namespaces-2897 deletion completed in 6.236428756s
STEP: Destroying namespace "nsdeletetest-6605" for this suite.
Dec 23 18:03:30.650: INFO: Namespace nsdeletetest-6605 was already deleted
STEP: Destroying namespace "nsdeletetest-8138" for this suite.
Dec 23 18:03:36.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:03:36.873: INFO: namespace nsdeletetest-8138 deletion completed in 6.222431668s

• [SLOW TEST:38.786 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:03:36.880: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 23 18:03:37.001: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec 23 18:03:42.007: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 23 18:03:42.007: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec 23 18:03:48.083: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-8910,SelfLink:/apis/apps/v1/namespaces/deployment-8910/deployments/test-cleanup-deployment,UID:d8b29a16-235f-408f-a075-9587a7175849,ResourceVersion:14766,Generation:1,CreationTimestamp:2019-12-23 18:03:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-23 18:03:42 +0000 UTC 2019-12-23 18:03:42 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-23 18:03:46 +0000 UTC 2019-12-23 18:03:42 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec 23 18:03:48.088: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-8910,SelfLink:/apis/apps/v1/namespaces/deployment-8910/replicasets/test-cleanup-deployment-55bbcbc84c,UID:4b823166-2523-4b8a-9488-420f72643df8,ResourceVersion:14757,Generation:1,CreationTimestamp:2019-12-23 18:03:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment d8b29a16-235f-408f-a075-9587a7175849 0xc000e38047 0xc000e38048}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec 23 18:03:48.094: INFO: Pod "test-cleanup-deployment-55bbcbc84c-h7dvl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-h7dvl,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-8910,SelfLink:/api/v1/namespaces/deployment-8910/pods/test-cleanup-deployment-55bbcbc84c-h7dvl,UID:19baee09-dbaf-4906-a28c-88f34cdd4f1d,ResourceVersion:14756,Generation:0,CreationTimestamp:2019-12-23 18:03:42 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.206.79/32,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 4b823166-2523-4b8a-9488-420f72643df8 0xc000e386f7 0xc000e386f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-lsk47 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-lsk47,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-lsk47 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-control-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000e38760} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000e38780}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:03:42 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:03:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:03:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:03:42 +0000 UTC  }],Message:,Reason:,HostIP:72.2.119.160,PodIP:10.42.206.79,StartTime:2019-12-23 18:03:42 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-23 18:03:46 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://42ff40758517527e520b63abf396834aa042ff0a96ac9e2b2e817aace6a99e32}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:03:48.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8910" for this suite.
Dec 23 18:03:54.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:03:54.324: INFO: namespace deployment-8910 deletion completed in 6.219798402s

• [SLOW TEST:17.444 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:03:54.329: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
W1223 18:04:24.497757      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 23 18:04:24.498: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:04:24.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6348" for this suite.
Dec 23 18:04:30.529: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:04:30.751: INFO: namespace gc-6348 deletion completed in 6.244415203s

• [SLOW TEST:36.423 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:04:30.759: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 23 18:04:36.889: INFO: Waiting up to 5m0s for pod "client-envvars-20deafe8-2412-43d7-8284-ad66190085b6" in namespace "pods-2939" to be "success or failure"
Dec 23 18:04:36.899: INFO: Pod "client-envvars-20deafe8-2412-43d7-8284-ad66190085b6": Phase="Pending", Reason="", readiness=false. Elapsed: 9.779885ms
Dec 23 18:04:38.906: INFO: Pod "client-envvars-20deafe8-2412-43d7-8284-ad66190085b6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017083576s
STEP: Saw pod success
Dec 23 18:04:38.906: INFO: Pod "client-envvars-20deafe8-2412-43d7-8284-ad66190085b6" satisfied condition "success or failure"
Dec 23 18:04:38.911: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod client-envvars-20deafe8-2412-43d7-8284-ad66190085b6 container env3cont: <nil>
STEP: delete the pod
Dec 23 18:04:38.970: INFO: Waiting for pod client-envvars-20deafe8-2412-43d7-8284-ad66190085b6 to disappear
Dec 23 18:04:38.975: INFO: Pod client-envvars-20deafe8-2412-43d7-8284-ad66190085b6 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:04:38.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2939" for this suite.
Dec 23 18:05:27.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:05:27.232: INFO: namespace pods-2939 deletion completed in 48.24343317s

• [SLOW TEST:56.473 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:05:27.235: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Dec 23 18:05:27.349: INFO: Waiting up to 5m0s for pod "var-expansion-23aa8c30-5134-4308-af37-f4a1e42be48e" in namespace "var-expansion-6832" to be "success or failure"
Dec 23 18:05:27.355: INFO: Pod "var-expansion-23aa8c30-5134-4308-af37-f4a1e42be48e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.588725ms
Dec 23 18:05:29.360: INFO: Pod "var-expansion-23aa8c30-5134-4308-af37-f4a1e42be48e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011460508s
STEP: Saw pod success
Dec 23 18:05:29.360: INFO: Pod "var-expansion-23aa8c30-5134-4308-af37-f4a1e42be48e" satisfied condition "success or failure"
Dec 23 18:05:29.366: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod var-expansion-23aa8c30-5134-4308-af37-f4a1e42be48e container dapi-container: <nil>
STEP: delete the pod
Dec 23 18:05:29.398: INFO: Waiting for pod var-expansion-23aa8c30-5134-4308-af37-f4a1e42be48e to disappear
Dec 23 18:05:29.417: INFO: Pod var-expansion-23aa8c30-5134-4308-af37-f4a1e42be48e no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:05:29.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6832" for this suite.
Dec 23 18:05:35.451: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:05:35.640: INFO: namespace var-expansion-6832 deletion completed in 6.213271524s

• [SLOW TEST:8.405 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:05:35.648: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 23 18:05:35.711: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9976817b-58c1-4504-aa8e-f148864bd163" in namespace "downward-api-6416" to be "success or failure"
Dec 23 18:05:35.719: INFO: Pod "downwardapi-volume-9976817b-58c1-4504-aa8e-f148864bd163": Phase="Pending", Reason="", readiness=false. Elapsed: 7.452702ms
Dec 23 18:05:37.726: INFO: Pod "downwardapi-volume-9976817b-58c1-4504-aa8e-f148864bd163": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014878603s
STEP: Saw pod success
Dec 23 18:05:37.727: INFO: Pod "downwardapi-volume-9976817b-58c1-4504-aa8e-f148864bd163" satisfied condition "success or failure"
Dec 23 18:05:37.732: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod downwardapi-volume-9976817b-58c1-4504-aa8e-f148864bd163 container client-container: <nil>
STEP: delete the pod
Dec 23 18:05:37.777: INFO: Waiting for pod downwardapi-volume-9976817b-58c1-4504-aa8e-f148864bd163 to disappear
Dec 23 18:05:37.782: INFO: Pod downwardapi-volume-9976817b-58c1-4504-aa8e-f148864bd163 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:05:37.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6416" for this suite.
Dec 23 18:05:43.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:05:44.053: INFO: namespace downward-api-6416 deletion completed in 6.258761444s

• [SLOW TEST:8.405 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:05:44.055: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 23 18:05:44.127: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7fb621ad-cdfa-4188-9fc2-c489d18778ad" in namespace "projected-7906" to be "success or failure"
Dec 23 18:05:44.135: INFO: Pod "downwardapi-volume-7fb621ad-cdfa-4188-9fc2-c489d18778ad": Phase="Pending", Reason="", readiness=false. Elapsed: 6.995685ms
Dec 23 18:05:46.142: INFO: Pod "downwardapi-volume-7fb621ad-cdfa-4188-9fc2-c489d18778ad": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014561129s
STEP: Saw pod success
Dec 23 18:05:46.143: INFO: Pod "downwardapi-volume-7fb621ad-cdfa-4188-9fc2-c489d18778ad" satisfied condition "success or failure"
Dec 23 18:05:46.148: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod downwardapi-volume-7fb621ad-cdfa-4188-9fc2-c489d18778ad container client-container: <nil>
STEP: delete the pod
Dec 23 18:05:46.180: INFO: Waiting for pod downwardapi-volume-7fb621ad-cdfa-4188-9fc2-c489d18778ad to disappear
Dec 23 18:05:46.192: INFO: Pod downwardapi-volume-7fb621ad-cdfa-4188-9fc2-c489d18778ad no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:05:46.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7906" for this suite.
Dec 23 18:05:52.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:05:52.423: INFO: namespace projected-7906 deletion completed in 6.218143444s

• [SLOW TEST:8.368 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:05:52.425: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec 23 18:05:52.544: INFO: Waiting up to 5m0s for pod "downward-api-e9515553-732c-49cf-90ed-fbc448e6f21e" in namespace "downward-api-4651" to be "success or failure"
Dec 23 18:05:52.556: INFO: Pod "downward-api-e9515553-732c-49cf-90ed-fbc448e6f21e": Phase="Pending", Reason="", readiness=false. Elapsed: 11.334858ms
Dec 23 18:05:54.562: INFO: Pod "downward-api-e9515553-732c-49cf-90ed-fbc448e6f21e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017703247s
STEP: Saw pod success
Dec 23 18:05:54.563: INFO: Pod "downward-api-e9515553-732c-49cf-90ed-fbc448e6f21e" satisfied condition "success or failure"
Dec 23 18:05:54.568: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod downward-api-e9515553-732c-49cf-90ed-fbc448e6f21e container dapi-container: <nil>
STEP: delete the pod
Dec 23 18:05:54.615: INFO: Waiting for pod downward-api-e9515553-732c-49cf-90ed-fbc448e6f21e to disappear
Dec 23 18:05:54.627: INFO: Pod downward-api-e9515553-732c-49cf-90ed-fbc448e6f21e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:05:54.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4651" for this suite.
Dec 23 18:06:00.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:06:00.868: INFO: namespace downward-api-4651 deletion completed in 6.230563528s

• [SLOW TEST:8.443 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:06:00.873: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-4e7e7c61-710b-4af8-9e16-cf7e4a09eb72
STEP: Creating a pod to test consume configMaps
Dec 23 18:06:01.004: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-af2f7bf9-fe37-4c7e-a0c9-641845c9071e" in namespace "projected-2301" to be "success or failure"
Dec 23 18:06:01.011: INFO: Pod "pod-projected-configmaps-af2f7bf9-fe37-4c7e-a0c9-641845c9071e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.80285ms
Dec 23 18:06:03.017: INFO: Pod "pod-projected-configmaps-af2f7bf9-fe37-4c7e-a0c9-641845c9071e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012595847s
STEP: Saw pod success
Dec 23 18:06:03.017: INFO: Pod "pod-projected-configmaps-af2f7bf9-fe37-4c7e-a0c9-641845c9071e" satisfied condition "success or failure"
Dec 23 18:06:03.022: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-projected-configmaps-af2f7bf9-fe37-4c7e-a0c9-641845c9071e container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 23 18:06:03.082: INFO: Waiting for pod pod-projected-configmaps-af2f7bf9-fe37-4c7e-a0c9-641845c9071e to disappear
Dec 23 18:06:03.114: INFO: Pod pod-projected-configmaps-af2f7bf9-fe37-4c7e-a0c9-641845c9071e no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:06:03.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2301" for this suite.
Dec 23 18:06:09.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:06:09.425: INFO: namespace projected-2301 deletion completed in 6.295474942s

• [SLOW TEST:8.552 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:06:09.427: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 23 18:06:09.530: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec 23 18:06:09.545: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 23 18:06:14.552: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 23 18:06:14.553: INFO: Creating deployment "test-rolling-update-deployment"
Dec 23 18:06:14.562: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec 23 18:06:14.573: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec 23 18:06:16.585: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec 23 18:06:16.596: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721174, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721174, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721174, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721174, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 18:06:18.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721174, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721174, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721174, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721174, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rolling-update-deployment-79f6b9d75c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 18:06:20.603: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec 23 18:06:20.618: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-6621,SelfLink:/apis/apps/v1/namespaces/deployment-6621/deployments/test-rolling-update-deployment,UID:ba8427e7-8c2e-4d7c-9625-34cb877f5a97,ResourceVersion:15535,Generation:1,CreationTimestamp:2019-12-23 18:06:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-23 18:06:14 +0000 UTC 2019-12-23 18:06:14 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-23 18:06:19 +0000 UTC 2019-12-23 18:06:14 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec 23 18:06:20.623: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-6621,SelfLink:/apis/apps/v1/namespaces/deployment-6621/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:3e73e110-c2e7-4f9f-aa21-eeb3bedc4cb5,ResourceVersion:15524,Generation:1,CreationTimestamp:2019-12-23 18:06:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment ba8427e7-8c2e-4d7c-9625-34cb877f5a97 0xc001e41937 0xc001e41938}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec 23 18:06:20.623: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec 23 18:06:20.624: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-6621,SelfLink:/apis/apps/v1/namespaces/deployment-6621/replicasets/test-rolling-update-controller,UID:91aecebd-c7ac-4c71-ab20-b68179034a28,ResourceVersion:15534,Generation:2,CreationTimestamp:2019-12-23 18:06:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment ba8427e7-8c2e-4d7c-9625-34cb877f5a97 0xc001e41867 0xc001e41868}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 23 18:06:20.629: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-sr46z" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-sr46z,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-6621,SelfLink:/api/v1/namespaces/deployment-6621/pods/test-rolling-update-deployment-79f6b9d75c-sr46z,UID:3105bf1c-faa5-4c5a-b3aa-c8ede5300f69,ResourceVersion:15523,Generation:0,CreationTimestamp:2019-12-23 18:06:14 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.209.84/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 3e73e110-c2e7-4f9f-aa21-eeb3bedc4cb5 0xc0020c42b7 0xc0020c42b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-l4zfv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-l4zfv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-l4zfv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-etcd-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0020c4320} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0020c4340}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:06:14 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:06:18 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:06:18 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:06:14 +0000 UTC  }],Message:,Reason:,HostIP:72.2.119.135,PodIP:10.42.209.84,StartTime:2019-12-23 18:06:14 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-23 18:06:18 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://c92c1a0985fc4d645fbb6edcf980c8459e7deec9c59db522cd09a1c04d09425b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:06:20.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-6621" for this suite.
Dec 23 18:06:26.662: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:06:26.863: INFO: namespace deployment-6621 deletion completed in 6.225450636s

• [SLOW TEST:17.437 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:06:26.868: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:06:27.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2066" for this suite.
Dec 23 18:06:49.079: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:06:49.299: INFO: namespace pods-2066 deletion completed in 22.243621053s

• [SLOW TEST:22.431 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:06:49.302: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 23 18:06:49.467: INFO: Waiting up to 5m0s for pod "pod-0f656e33-4aa2-490b-8453-5d3c3123648a" in namespace "emptydir-5338" to be "success or failure"
Dec 23 18:06:49.482: INFO: Pod "pod-0f656e33-4aa2-490b-8453-5d3c3123648a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.40803ms
Dec 23 18:06:51.527: INFO: Pod "pod-0f656e33-4aa2-490b-8453-5d3c3123648a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058575202s
Dec 23 18:06:53.533: INFO: Pod "pod-0f656e33-4aa2-490b-8453-5d3c3123648a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.064603525s
Dec 23 18:06:55.540: INFO: Pod "pod-0f656e33-4aa2-490b-8453-5d3c3123648a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.071628178s
STEP: Saw pod success
Dec 23 18:06:55.540: INFO: Pod "pod-0f656e33-4aa2-490b-8453-5d3c3123648a" satisfied condition "success or failure"
Dec 23 18:06:55.545: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-0f656e33-4aa2-490b-8453-5d3c3123648a container test-container: <nil>
STEP: delete the pod
Dec 23 18:06:55.578: INFO: Waiting for pod pod-0f656e33-4aa2-490b-8453-5d3c3123648a to disappear
Dec 23 18:06:55.584: INFO: Pod pod-0f656e33-4aa2-490b-8453-5d3c3123648a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:06:55.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5338" for this suite.
Dec 23 18:07:01.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:07:01.836: INFO: namespace emptydir-5338 deletion completed in 6.23697449s

• [SLOW TEST:12.535 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:07:01.843: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 23 18:07:01.940: INFO: Creating ReplicaSet my-hostname-basic-21eb8664-437c-4608-a0d9-130cc610ff5a
Dec 23 18:07:01.954: INFO: Pod name my-hostname-basic-21eb8664-437c-4608-a0d9-130cc610ff5a: Found 0 pods out of 1
Dec 23 18:07:06.970: INFO: Pod name my-hostname-basic-21eb8664-437c-4608-a0d9-130cc610ff5a: Found 1 pods out of 1
Dec 23 18:07:06.970: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-21eb8664-437c-4608-a0d9-130cc610ff5a" is running
Dec 23 18:07:08.985: INFO: Pod "my-hostname-basic-21eb8664-437c-4608-a0d9-130cc610ff5a-mdl4r" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-23 18:07:01 +0000 UTC Reason: Message:} {Type:Ready Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-23 18:07:01 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-21eb8664-437c-4608-a0d9-130cc610ff5a]} {Type:ContainersReady Status:False LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-23 18:07:01 +0000 UTC Reason:ContainersNotReady Message:containers with unready status: [my-hostname-basic-21eb8664-437c-4608-a0d9-130cc610ff5a]} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-23 18:07:01 +0000 UTC Reason: Message:}])
Dec 23 18:07:08.985: INFO: Trying to dial the pod
Dec 23 18:07:14.000: INFO: Controller my-hostname-basic-21eb8664-437c-4608-a0d9-130cc610ff5a: Got expected result from replica 1 [my-hostname-basic-21eb8664-437c-4608-a0d9-130cc610ff5a-mdl4r]: "my-hostname-basic-21eb8664-437c-4608-a0d9-130cc610ff5a-mdl4r", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:07:14.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3174" for this suite.
Dec 23 18:07:20.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:07:20.249: INFO: namespace replicaset-3174 deletion completed in 6.24004821s

• [SLOW TEST:18.407 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:07:20.254: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 23 18:07:20.371: INFO: Waiting up to 5m0s for pod "pod-85c8c18b-106f-40f0-acb0-223b40e61bb6" in namespace "emptydir-4816" to be "success or failure"
Dec 23 18:07:20.378: INFO: Pod "pod-85c8c18b-106f-40f0-acb0-223b40e61bb6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.506878ms
Dec 23 18:07:22.389: INFO: Pod "pod-85c8c18b-106f-40f0-acb0-223b40e61bb6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017792403s
STEP: Saw pod success
Dec 23 18:07:22.390: INFO: Pod "pod-85c8c18b-106f-40f0-acb0-223b40e61bb6" satisfied condition "success or failure"
Dec 23 18:07:22.397: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-85c8c18b-106f-40f0-acb0-223b40e61bb6 container test-container: <nil>
STEP: delete the pod
Dec 23 18:07:22.453: INFO: Waiting for pod pod-85c8c18b-106f-40f0-acb0-223b40e61bb6 to disappear
Dec 23 18:07:22.465: INFO: Pod pod-85c8c18b-106f-40f0-acb0-223b40e61bb6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:07:22.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4816" for this suite.
Dec 23 18:07:28.509: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:07:28.711: INFO: namespace emptydir-4816 deletion completed in 6.234336585s

• [SLOW TEST:8.457 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:07:28.713: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Dec 23 18:07:28.812: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Dec 23 18:07:30.084: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec 23 18:07:32.203: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 18:07:34.210: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 18:07:36.213: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 18:07:38.209: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 18:07:40.211: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 18:07:42.209: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 18:07:44.209: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 18:07:46.210: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 18:07:48.208: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721266, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721250, loc:(*time.Location)(0x7ed1a20)}}, Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" has successfully progressed."}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721267, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712721267, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Dec 23 18:07:51.716: INFO: Waited 1.477535931s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:07:53.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-4860" for this suite.
Dec 23 18:07:59.252: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:07:59.454: INFO: namespace aggregator-4860 deletion completed in 6.226431807s

• [SLOW TEST:30.742 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:07:59.464: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 23 18:08:01.617: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:08:01.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1942" for this suite.
Dec 23 18:08:07.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:08:07.891: INFO: namespace container-runtime-1942 deletion completed in 6.230342763s

• [SLOW TEST:8.429 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:08:07.901: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 23 18:08:08.019: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6e9b0c3d-c382-4cd4-8fa7-66f70c1c4e81" in namespace "projected-1970" to be "success or failure"
Dec 23 18:08:08.027: INFO: Pod "downwardapi-volume-6e9b0c3d-c382-4cd4-8fa7-66f70c1c4e81": Phase="Pending", Reason="", readiness=false. Elapsed: 7.42686ms
Dec 23 18:08:10.034: INFO: Pod "downwardapi-volume-6e9b0c3d-c382-4cd4-8fa7-66f70c1c4e81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014957317s
STEP: Saw pod success
Dec 23 18:08:10.035: INFO: Pod "downwardapi-volume-6e9b0c3d-c382-4cd4-8fa7-66f70c1c4e81" satisfied condition "success or failure"
Dec 23 18:08:10.041: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod downwardapi-volume-6e9b0c3d-c382-4cd4-8fa7-66f70c1c4e81 container client-container: <nil>
STEP: delete the pod
Dec 23 18:08:10.078: INFO: Waiting for pod downwardapi-volume-6e9b0c3d-c382-4cd4-8fa7-66f70c1c4e81 to disappear
Dec 23 18:08:10.085: INFO: Pod downwardapi-volume-6e9b0c3d-c382-4cd4-8fa7-66f70c1c4e81 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:08:10.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1970" for this suite.
Dec 23 18:08:16.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:08:16.321: INFO: namespace projected-1970 deletion completed in 6.226616536s

• [SLOW TEST:8.421 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:08:16.329: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-9a081f6e-5e50-4cf3-ba76-767e15c24d95
STEP: Creating a pod to test consume configMaps
Dec 23 18:08:16.469: INFO: Waiting up to 5m0s for pod "pod-configmaps-c2ba707a-f75a-4776-8d3e-fa25ea55e970" in namespace "configmap-8284" to be "success or failure"
Dec 23 18:08:16.484: INFO: Pod "pod-configmaps-c2ba707a-f75a-4776-8d3e-fa25ea55e970": Phase="Pending", Reason="", readiness=false. Elapsed: 15.353088ms
Dec 23 18:08:18.490: INFO: Pod "pod-configmaps-c2ba707a-f75a-4776-8d3e-fa25ea55e970": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02119525s
STEP: Saw pod success
Dec 23 18:08:18.490: INFO: Pod "pod-configmaps-c2ba707a-f75a-4776-8d3e-fa25ea55e970" satisfied condition "success or failure"
Dec 23 18:08:18.498: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-configmaps-c2ba707a-f75a-4776-8d3e-fa25ea55e970 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 23 18:08:18.533: INFO: Waiting for pod pod-configmaps-c2ba707a-f75a-4776-8d3e-fa25ea55e970 to disappear
Dec 23 18:08:18.539: INFO: Pod pod-configmaps-c2ba707a-f75a-4776-8d3e-fa25ea55e970 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:08:18.539: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8284" for this suite.
Dec 23 18:08:24.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:08:24.792: INFO: namespace configmap-8284 deletion completed in 6.243521102s

• [SLOW TEST:8.464 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:08:24.794: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec 23 18:08:26.935: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-dbcde416-a80e-4edb-b778-e86e5e0affe3,GenerateName:,Namespace:events-5549,SelfLink:/api/v1/namespaces/events-5549/pods/send-events-dbcde416-a80e-4edb-b778-e86e5e0affe3,UID:00604ea4-ad83-41d3-b4c3-a6be66b27a08,ResourceVersion:16246,Generation:0,CreationTimestamp:2019-12-23 18:08:24 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 893754882,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.206.88/32,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-fs9db {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-fs9db,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-fs9db true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-control-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002579bf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002579c10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:08:24 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:08:25 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:08:25 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:08:24 +0000 UTC  }],Message:,Reason:,HostIP:72.2.119.160,PodIP:10.42.206.88,StartTime:2019-12-23 18:08:24 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-12-23 18:08:25 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://6be4ca8c9728157e87e79dfd1d029e0863e006ae68f37a3cce9357cff8b3da30}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Dec 23 18:08:28.941: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec 23 18:08:30.948: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:08:30.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5549" for this suite.
Dec 23 18:09:15.010: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:09:15.226: INFO: namespace events-5549 deletion completed in 44.246593826s

• [SLOW TEST:50.432 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:09:15.233: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W1223 18:09:55.393239      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 23 18:09:55.393: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:09:55.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7879" for this suite.
Dec 23 18:10:03.424: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:10:03.624: INFO: namespace gc-7879 deletion completed in 8.221187784s

• [SLOW TEST:48.392 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:10:03.629: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-917ef72b-2620-4ec0-8d98-245ee6b1db11
STEP: Creating secret with name secret-projected-all-test-volume-70a5c79c-694e-422d-af73-4f8c3ebaf12b
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec 23 18:10:03.763: INFO: Waiting up to 5m0s for pod "projected-volume-abe5e49f-2564-4e5a-891d-66860ba1ca60" in namespace "projected-8083" to be "success or failure"
Dec 23 18:10:03.769: INFO: Pod "projected-volume-abe5e49f-2564-4e5a-891d-66860ba1ca60": Phase="Pending", Reason="", readiness=false. Elapsed: 5.934311ms
Dec 23 18:10:05.774: INFO: Pod "projected-volume-abe5e49f-2564-4e5a-891d-66860ba1ca60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011650571s
STEP: Saw pod success
Dec 23 18:10:05.775: INFO: Pod "projected-volume-abe5e49f-2564-4e5a-891d-66860ba1ca60" satisfied condition "success or failure"
Dec 23 18:10:05.784: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod projected-volume-abe5e49f-2564-4e5a-891d-66860ba1ca60 container projected-all-volume-test: <nil>
STEP: delete the pod
Dec 23 18:10:05.820: INFO: Waiting for pod projected-volume-abe5e49f-2564-4e5a-891d-66860ba1ca60 to disappear
Dec 23 18:10:05.826: INFO: Pod projected-volume-abe5e49f-2564-4e5a-891d-66860ba1ca60 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:10:05.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8083" for this suite.
Dec 23 18:10:11.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:10:12.057: INFO: namespace projected-8083 deletion completed in 6.221408036s

• [SLOW TEST:8.429 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:10:12.059: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec 23 18:10:14.692: INFO: Successfully updated pod "labelsupdatef1f2d5c1-6dcc-426e-b4c5-1fc01d6cf62a"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:10:18.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-413" for this suite.
Dec 23 18:10:40.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:10:40.996: INFO: namespace downward-api-413 deletion completed in 22.247871133s

• [SLOW TEST:28.937 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:10:40.999: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 23 18:10:43.141: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:10:43.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2188" for this suite.
Dec 23 18:10:49.228: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:10:49.444: INFO: namespace container-runtime-2188 deletion completed in 6.23827015s

• [SLOW TEST:8.446 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:10:49.455: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-2580/configmap-test-99294b37-9e35-4919-b02a-715517a50ded
STEP: Creating a pod to test consume configMaps
Dec 23 18:10:49.582: INFO: Waiting up to 5m0s for pod "pod-configmaps-0dd91f71-558c-4de3-be18-04e997075190" in namespace "configmap-2580" to be "success or failure"
Dec 23 18:10:49.597: INFO: Pod "pod-configmaps-0dd91f71-558c-4de3-be18-04e997075190": Phase="Pending", Reason="", readiness=false. Elapsed: 14.745208ms
Dec 23 18:10:51.604: INFO: Pod "pod-configmaps-0dd91f71-558c-4de3-be18-04e997075190": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021413314s
STEP: Saw pod success
Dec 23 18:10:51.604: INFO: Pod "pod-configmaps-0dd91f71-558c-4de3-be18-04e997075190" satisfied condition "success or failure"
Dec 23 18:10:51.610: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod pod-configmaps-0dd91f71-558c-4de3-be18-04e997075190 container env-test: <nil>
STEP: delete the pod
Dec 23 18:10:51.649: INFO: Waiting for pod pod-configmaps-0dd91f71-558c-4de3-be18-04e997075190 to disappear
Dec 23 18:10:51.655: INFO: Pod pod-configmaps-0dd91f71-558c-4de3-be18-04e997075190 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:10:51.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2580" for this suite.
Dec 23 18:10:57.697: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:10:57.900: INFO: namespace configmap-2580 deletion completed in 6.235884378s

• [SLOW TEST:8.446 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:10:57.904: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 23 18:10:58.041: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"5274b2ed-9ba6-4b67-bc4f-b354da8a98c7", Controller:(*bool)(0xc0025788ca), BlockOwnerDeletion:(*bool)(0xc0025788cb)}}
Dec 23 18:10:58.078: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"190ffd10-3cc2-4632-829a-d084059759d9", Controller:(*bool)(0xc002578bfa), BlockOwnerDeletion:(*bool)(0xc002578bfb)}}
Dec 23 18:10:58.088: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"8f04ff0f-1851-455e-a04c-faa0a1c0f63a", Controller:(*bool)(0xc002578f1a), BlockOwnerDeletion:(*bool)(0xc002578f1b)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:11:03.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-628" for this suite.
Dec 23 18:11:09.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:11:09.352: INFO: namespace gc-628 deletion completed in 6.234210128s

• [SLOW TEST:11.448 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:11:09.359: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Dec 23 18:11:09.973: INFO: created pod pod-service-account-defaultsa
Dec 23 18:11:09.973: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec 23 18:11:09.995: INFO: created pod pod-service-account-mountsa
Dec 23 18:11:09.996: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec 23 18:11:10.010: INFO: created pod pod-service-account-nomountsa
Dec 23 18:11:10.010: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec 23 18:11:10.028: INFO: created pod pod-service-account-defaultsa-mountspec
Dec 23 18:11:10.028: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec 23 18:11:10.039: INFO: created pod pod-service-account-mountsa-mountspec
Dec 23 18:11:10.040: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec 23 18:11:10.090: INFO: created pod pod-service-account-nomountsa-mountspec
Dec 23 18:11:10.093: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec 23 18:11:10.126: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec 23 18:11:10.126: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec 23 18:11:10.145: INFO: created pod pod-service-account-mountsa-nomountspec
Dec 23 18:11:10.145: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec 23 18:11:10.166: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec 23 18:11:10.166: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:11:10.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6179" for this suite.
Dec 23 18:11:32.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:11:32.407: INFO: namespace svcaccounts-6179 deletion completed in 22.223078472s

• [SLOW TEST:23.048 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:11:32.409: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1456
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 23 18:11:32.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-1138'
Dec 23 18:11:33.026: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 23 18:11:33.026: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Dec 23 18:11:33.038: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-x6mnn]
Dec 23 18:11:33.038: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-x6mnn" in namespace "kubectl-1138" to be "running and ready"
Dec 23 18:11:33.046: INFO: Pod "e2e-test-nginx-rc-x6mnn": Phase="Pending", Reason="", readiness=false. Elapsed: 6.939214ms
Dec 23 18:11:35.053: INFO: Pod "e2e-test-nginx-rc-x6mnn": Phase="Running", Reason="", readiness=true. Elapsed: 2.013741891s
Dec 23 18:11:35.053: INFO: Pod "e2e-test-nginx-rc-x6mnn" satisfied condition "running and ready"
Dec 23 18:11:35.053: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-x6mnn]
Dec 23 18:11:35.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 logs rc/e2e-test-nginx-rc --namespace=kubectl-1138'
Dec 23 18:11:35.345: INFO: stderr: ""
Dec 23 18:11:35.345: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1461
Dec 23 18:11:35.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 delete rc e2e-test-nginx-rc --namespace=kubectl-1138'
Dec 23 18:11:35.640: INFO: stderr: ""
Dec 23 18:11:35.640: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:11:35.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1138" for this suite.
Dec 23 18:11:41.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:11:41.896: INFO: namespace kubectl-1138 deletion completed in 6.241231113s

• [SLOW TEST:9.488 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:11:41.899: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Dec 23 18:11:41.999: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 23 18:11:42.017: INFO: Waiting for terminating namespaces to be deleted...
Dec 23 18:11:42.024: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-control-1 before test
Dec 23 18:11:42.037: INFO: calico-node-drwck from kube-system started at 2019-12-23 16:49:25 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.038: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 18:11:42.038: INFO: cattle-node-agent-5hgvg from cattle-system started at 2019-12-23 16:51:44 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.038: INFO: 	Container agent ready: true, restart count 0
Dec 23 18:11:42.039: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-vcnpn from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 18:11:42.039: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 18:11:42.039: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 18:11:42.039: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-control-2 before test
Dec 23 18:11:42.056: INFO: rke-ingress-controller-deploy-job-bz9f6 from kube-system started at 2019-12-23 16:51:19 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.056: INFO: 	Container rke-ingress-controller-pod ready: false, restart count 0
Dec 23 18:11:42.057: INFO: cattle-node-agent-9jbrb from cattle-system started at 2019-12-23 16:51:44 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.057: INFO: 	Container agent ready: true, restart count 0
Dec 23 18:11:42.057: INFO: sonobuoy from sonobuoy started at 2019-12-23 17:51:06 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.057: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 23 18:11:42.057: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-s25j6 from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 18:11:42.058: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 18:11:42.058: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 18:11:42.058: INFO: calico-node-v9rvr from kube-system started at 2019-12-23 16:49:25 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.058: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 18:11:42.058: INFO: rke-metrics-addon-deploy-job-glj7v from kube-system started at 2019-12-23 16:51:14 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.058: INFO: 	Container rke-metrics-addon-pod ready: false, restart count 0
Dec 23 18:11:42.058: INFO: rke-network-plugin-deploy-job-xvn4m from kube-system started at 2019-12-23 16:48:57 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.058: INFO: 	Container rke-network-plugin-pod ready: false, restart count 0
Dec 23 18:11:42.059: INFO: rke-coredns-addon-deploy-job-lppwq from kube-system started at 2019-12-23 16:51:09 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.059: INFO: 	Container rke-coredns-addon-pod ready: false, restart count 0
Dec 23 18:11:42.059: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-control-3 before test
Dec 23 18:11:42.074: INFO: calico-node-ptzn9 from kube-system started at 2019-12-23 16:49:25 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.074: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 18:11:42.074: INFO: cattle-cluster-agent-6dfc4fd4fc-gsf4h from cattle-system started at 2019-12-23 16:51:44 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.075: INFO: 	Container cluster-register ready: true, restart count 0
Dec 23 18:11:42.075: INFO: cattle-node-agent-whghm from cattle-system started at 2019-12-23 16:51:44 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.075: INFO: 	Container agent ready: true, restart count 0
Dec 23 18:11:42.075: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-97q92 from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 18:11:42.075: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 18:11:42.075: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 18:11:42.075: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-etcd-1 before test
Dec 23 18:11:42.091: INFO: calico-node-qv786 from kube-system started at 2019-12-23 16:49:25 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.091: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 18:11:42.091: INFO: cattle-node-agent-bckfv from cattle-system started at 2019-12-23 16:51:44 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.092: INFO: 	Container agent ready: true, restart count 0
Dec 23 18:11:42.092: INFO: sonobuoy-e2e-job-7470178446ee4019 from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 18:11:42.092: INFO: 	Container e2e ready: true, restart count 0
Dec 23 18:11:42.092: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 18:11:42.092: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-sxmpv from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 18:11:42.092: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 18:11:42.092: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 18:11:42.093: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-etcd-2 before test
Dec 23 18:11:42.104: INFO: calico-node-648gc from kube-system started at 2019-12-23 16:49:25 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.104: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 18:11:42.104: INFO: calico-kube-controllers-77cd95cb44-b9g9b from kube-system started at 2019-12-23 16:49:25 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.104: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 23 18:11:42.104: INFO: cattle-node-agent-7jfhx from cattle-system started at 2019-12-23 16:51:44 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.105: INFO: 	Container agent ready: true, restart count 0
Dec 23 18:11:42.105: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-2cg6j from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 18:11:42.105: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 18:11:42.105: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 18:11:42.105: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-etcd-3 before test
Dec 23 18:11:42.115: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-xm8d7 from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 18:11:42.115: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 18:11:42.116: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 18:11:42.116: INFO: calico-node-vk4b5 from kube-system started at 2019-12-23 16:49:25 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.116: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 18:11:42.116: INFO: cattle-node-agent-28j5k from cattle-system started at 2019-12-23 16:51:44 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.116: INFO: 	Container agent ready: true, restart count 0
Dec 23 18:11:42.116: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-worker-1 before test
Dec 23 18:11:42.130: INFO: cattle-node-agent-9brqm from cattle-system started at 2019-12-23 16:53:55 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.131: INFO: 	Container agent ready: true, restart count 0
Dec 23 18:11:42.131: INFO: coredns-799dffd9c4-2459g from kube-system started at 2019-12-23 16:53:56 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.131: INFO: 	Container coredns ready: true, restart count 0
Dec 23 18:11:42.131: INFO: nginx-ingress-controller-ddhqm from ingress-nginx started at 2019-12-23 16:53:56 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.131: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec 23 18:11:42.131: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-mjhc2 from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 18:11:42.131: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 18:11:42.131: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 18:11:42.131: INFO: calico-node-7c2k8 from kube-system started at 2019-12-23 16:53:55 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.132: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 18:11:42.132: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-worker-2 before test
Dec 23 18:11:42.148: INFO: cattle-node-agent-l6k7z from cattle-system started at 2019-12-23 16:52:48 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.148: INFO: 	Container agent ready: true, restart count 0
Dec 23 18:11:42.148: INFO: coredns-autoscaler-84766fbb4-2s6b6 from kube-system started at 2019-12-23 16:52:49 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.149: INFO: 	Container autoscaler ready: true, restart count 0
Dec 23 18:11:42.149: INFO: metrics-server-59c6fd6767-4nvkr from kube-system started at 2019-12-23 16:52:49 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.149: INFO: 	Container metrics-server ready: true, restart count 0
Dec 23 18:11:42.149: INFO: coredns-799dffd9c4-2swvh from kube-system started at 2019-12-23 16:53:56 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.149: INFO: 	Container coredns ready: true, restart count 0
Dec 23 18:11:42.149: INFO: calico-node-cvld6 from kube-system started at 2019-12-23 16:52:48 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.149: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 18:11:42.150: INFO: nginx-ingress-controller-nczlg from ingress-nginx started at 2019-12-23 16:52:48 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.150: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec 23 18:11:42.150: INFO: default-http-backend-5bcc9fd598-pz56l from ingress-nginx started at 2019-12-23 16:52:49 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.150: INFO: 	Container default-http-backend ready: true, restart count 0
Dec 23 18:11:42.150: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-zgfft from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 18:11:42.150: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 18:11:42.150: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 18:11:42.151: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-worker-3 before test
Dec 23 18:11:42.163: INFO: cattle-node-agent-22s8h from cattle-system started at 2019-12-23 16:52:46 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.164: INFO: 	Container agent ready: true, restart count 0
Dec 23 18:11:42.164: INFO: nginx-ingress-controller-sq5vm from ingress-nginx started at 2019-12-23 16:52:46 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.164: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec 23 18:11:42.164: INFO: coredns-799dffd9c4-gc76w from kube-system started at 2019-12-23 16:52:49 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.164: INFO: 	Container coredns ready: true, restart count 0
Dec 23 18:11:42.164: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-ptdwt from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 18:11:42.164: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 18:11:42.165: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 18:11:42.165: INFO: calico-node-pgzp6 from kube-system started at 2019-12-23 16:52:46 +0000 UTC (1 container statuses recorded)
Dec 23 18:11:42.165: INFO: 	Container calico-node ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node k8s-conformance-1-15-control-1
STEP: verifying the node has the label node k8s-conformance-1-15-control-2
STEP: verifying the node has the label node k8s-conformance-1-15-control-3
STEP: verifying the node has the label node k8s-conformance-1-15-etcd-1
STEP: verifying the node has the label node k8s-conformance-1-15-etcd-2
STEP: verifying the node has the label node k8s-conformance-1-15-etcd-3
STEP: verifying the node has the label node k8s-conformance-1-15-worker-1
STEP: verifying the node has the label node k8s-conformance-1-15-worker-2
STEP: verifying the node has the label node k8s-conformance-1-15-worker-3
Dec 23 18:11:42.487: INFO: Pod cattle-cluster-agent-6dfc4fd4fc-gsf4h requesting resource cpu=0m on Node k8s-conformance-1-15-control-3
Dec 23 18:11:42.487: INFO: Pod cattle-node-agent-22s8h requesting resource cpu=0m on Node k8s-conformance-1-15-worker-3
Dec 23 18:11:42.487: INFO: Pod cattle-node-agent-28j5k requesting resource cpu=0m on Node k8s-conformance-1-15-etcd-3
Dec 23 18:11:42.487: INFO: Pod cattle-node-agent-5hgvg requesting resource cpu=0m on Node k8s-conformance-1-15-control-1
Dec 23 18:11:42.487: INFO: Pod cattle-node-agent-7jfhx requesting resource cpu=0m on Node k8s-conformance-1-15-etcd-2
Dec 23 18:11:42.488: INFO: Pod cattle-node-agent-9brqm requesting resource cpu=0m on Node k8s-conformance-1-15-worker-1
Dec 23 18:11:42.488: INFO: Pod cattle-node-agent-9jbrb requesting resource cpu=0m on Node k8s-conformance-1-15-control-2
Dec 23 18:11:42.488: INFO: Pod cattle-node-agent-bckfv requesting resource cpu=0m on Node k8s-conformance-1-15-etcd-1
Dec 23 18:11:42.488: INFO: Pod cattle-node-agent-l6k7z requesting resource cpu=0m on Node k8s-conformance-1-15-worker-2
Dec 23 18:11:42.488: INFO: Pod cattle-node-agent-whghm requesting resource cpu=0m on Node k8s-conformance-1-15-control-3
Dec 23 18:11:42.488: INFO: Pod default-http-backend-5bcc9fd598-pz56l requesting resource cpu=10m on Node k8s-conformance-1-15-worker-2
Dec 23 18:11:42.488: INFO: Pod nginx-ingress-controller-ddhqm requesting resource cpu=0m on Node k8s-conformance-1-15-worker-1
Dec 23 18:11:42.488: INFO: Pod nginx-ingress-controller-nczlg requesting resource cpu=0m on Node k8s-conformance-1-15-worker-2
Dec 23 18:11:42.488: INFO: Pod nginx-ingress-controller-sq5vm requesting resource cpu=0m on Node k8s-conformance-1-15-worker-3
Dec 23 18:11:42.488: INFO: Pod calico-kube-controllers-77cd95cb44-b9g9b requesting resource cpu=0m on Node k8s-conformance-1-15-etcd-2
Dec 23 18:11:42.488: INFO: Pod calico-node-648gc requesting resource cpu=250m on Node k8s-conformance-1-15-etcd-2
Dec 23 18:11:42.489: INFO: Pod calico-node-7c2k8 requesting resource cpu=250m on Node k8s-conformance-1-15-worker-1
Dec 23 18:11:42.489: INFO: Pod calico-node-cvld6 requesting resource cpu=250m on Node k8s-conformance-1-15-worker-2
Dec 23 18:11:42.489: INFO: Pod calico-node-drwck requesting resource cpu=250m on Node k8s-conformance-1-15-control-1
Dec 23 18:11:42.489: INFO: Pod calico-node-pgzp6 requesting resource cpu=250m on Node k8s-conformance-1-15-worker-3
Dec 23 18:11:42.489: INFO: Pod calico-node-ptzn9 requesting resource cpu=250m on Node k8s-conformance-1-15-control-3
Dec 23 18:11:42.489: INFO: Pod calico-node-qv786 requesting resource cpu=250m on Node k8s-conformance-1-15-etcd-1
Dec 23 18:11:42.490: INFO: Pod calico-node-v9rvr requesting resource cpu=250m on Node k8s-conformance-1-15-control-2
Dec 23 18:11:42.490: INFO: Pod calico-node-vk4b5 requesting resource cpu=250m on Node k8s-conformance-1-15-etcd-3
Dec 23 18:11:42.491: INFO: Pod coredns-799dffd9c4-2459g requesting resource cpu=100m on Node k8s-conformance-1-15-worker-1
Dec 23 18:11:42.491: INFO: Pod coredns-799dffd9c4-2swvh requesting resource cpu=100m on Node k8s-conformance-1-15-worker-2
Dec 23 18:11:42.491: INFO: Pod coredns-799dffd9c4-gc76w requesting resource cpu=100m on Node k8s-conformance-1-15-worker-3
Dec 23 18:11:42.491: INFO: Pod coredns-autoscaler-84766fbb4-2s6b6 requesting resource cpu=20m on Node k8s-conformance-1-15-worker-2
Dec 23 18:11:42.491: INFO: Pod metrics-server-59c6fd6767-4nvkr requesting resource cpu=0m on Node k8s-conformance-1-15-worker-2
Dec 23 18:11:42.491: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8s-conformance-1-15-control-2
Dec 23 18:11:42.492: INFO: Pod sonobuoy-e2e-job-7470178446ee4019 requesting resource cpu=0m on Node k8s-conformance-1-15-etcd-1
Dec 23 18:11:42.492: INFO: Pod sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-2cg6j requesting resource cpu=0m on Node k8s-conformance-1-15-etcd-2
Dec 23 18:11:42.492: INFO: Pod sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-97q92 requesting resource cpu=0m on Node k8s-conformance-1-15-control-3
Dec 23 18:11:42.492: INFO: Pod sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-mjhc2 requesting resource cpu=0m on Node k8s-conformance-1-15-worker-1
Dec 23 18:11:42.492: INFO: Pod sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-ptdwt requesting resource cpu=0m on Node k8s-conformance-1-15-worker-3
Dec 23 18:11:42.492: INFO: Pod sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-s25j6 requesting resource cpu=0m on Node k8s-conformance-1-15-control-2
Dec 23 18:11:42.492: INFO: Pod sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-sxmpv requesting resource cpu=0m on Node k8s-conformance-1-15-etcd-1
Dec 23 18:11:42.493: INFO: Pod sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-vcnpn requesting resource cpu=0m on Node k8s-conformance-1-15-control-1
Dec 23 18:11:42.493: INFO: Pod sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-xm8d7 requesting resource cpu=0m on Node k8s-conformance-1-15-etcd-3
Dec 23 18:11:42.493: INFO: Pod sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-zgfft requesting resource cpu=0m on Node k8s-conformance-1-15-worker-2
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2a81f146-c813-4dca-b5ce-aecfb62cfb0b.15e3128fa149c8b3], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7019/filler-pod-2a81f146-c813-4dca-b5ce-aecfb62cfb0b to k8s-conformance-1-15-control-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2a81f146-c813-4dca-b5ce-aecfb62cfb0b.15e3128ff53690da], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2a81f146-c813-4dca-b5ce-aecfb62cfb0b.15e312905f3df5b1], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2a81f146-c813-4dca-b5ce-aecfb62cfb0b.15e31290657d6244], Reason = [Created], Message = [Created container filler-pod-2a81f146-c813-4dca-b5ce-aecfb62cfb0b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-2a81f146-c813-4dca-b5ce-aecfb62cfb0b.15e312906e5ca464], Reason = [Started], Message = [Started container filler-pod-2a81f146-c813-4dca-b5ce-aecfb62cfb0b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4bea92c8-a6d6-47bf-b84f-5fba8279165f.15e3128fa2454d34], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7019/filler-pod-4bea92c8-a6d6-47bf-b84f-5fba8279165f to k8s-conformance-1-15-etcd-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4bea92c8-a6d6-47bf-b84f-5fba8279165f.15e3128fee4e18c7], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4bea92c8-a6d6-47bf-b84f-5fba8279165f.15e312905a284abd], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4bea92c8-a6d6-47bf-b84f-5fba8279165f.15e312905b723754], Reason = [Created], Message = [Created container filler-pod-4bea92c8-a6d6-47bf-b84f-5fba8279165f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-4bea92c8-a6d6-47bf-b84f-5fba8279165f.15e3129062ad6f54], Reason = [Started], Message = [Started container filler-pod-4bea92c8-a6d6-47bf-b84f-5fba8279165f]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-55e1bd79-aae0-484f-9cec-86188209ee79.15e3128f9cc79f77], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7019/filler-pod-55e1bd79-aae0-484f-9cec-86188209ee79 to k8s-conformance-1-15-worker-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-55e1bd79-aae0-484f-9cec-86188209ee79.15e3128ff7fa0f0e], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-55e1bd79-aae0-484f-9cec-86188209ee79.15e3129066ed615d], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-55e1bd79-aae0-484f-9cec-86188209ee79.15e312906ab4a32d], Reason = [Created], Message = [Created container filler-pod-55e1bd79-aae0-484f-9cec-86188209ee79]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-55e1bd79-aae0-484f-9cec-86188209ee79.15e3129079cef12c], Reason = [Started], Message = [Started container filler-pod-55e1bd79-aae0-484f-9cec-86188209ee79]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-605c62a4-958a-45a7-9ac0-758c97ffadfb.15e3128fa1abb6a8], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7019/filler-pod-605c62a4-958a-45a7-9ac0-758c97ffadfb to k8s-conformance-1-15-etcd-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-605c62a4-958a-45a7-9ac0-758c97ffadfb.15e3128fd7eb52e2], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-605c62a4-958a-45a7-9ac0-758c97ffadfb.15e312904552ae11], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-605c62a4-958a-45a7-9ac0-758c97ffadfb.15e3129046b1c325], Reason = [Created], Message = [Created container filler-pod-605c62a4-958a-45a7-9ac0-758c97ffadfb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-605c62a4-958a-45a7-9ac0-758c97ffadfb.15e312904ddbd46c], Reason = [Started], Message = [Started container filler-pod-605c62a4-958a-45a7-9ac0-758c97ffadfb]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-87a29b78-cdbb-4a73-849a-9185f063d71d.15e3128f9d45e42c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7019/filler-pod-87a29b78-cdbb-4a73-849a-9185f063d71d to k8s-conformance-1-15-worker-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-87a29b78-cdbb-4a73-849a-9185f063d71d.15e3128fd3f27d55], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-87a29b78-cdbb-4a73-849a-9185f063d71d.15e312903e75ebc0], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-87a29b78-cdbb-4a73-849a-9185f063d71d.15e312903ff55bad], Reason = [Created], Message = [Created container filler-pod-87a29b78-cdbb-4a73-849a-9185f063d71d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-87a29b78-cdbb-4a73-849a-9185f063d71d.15e3129048743ecb], Reason = [Started], Message = [Started container filler-pod-87a29b78-cdbb-4a73-849a-9185f063d71d]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8942ce45-4d48-47eb-b7ac-0e421ab916de.15e3128f9f6f39bd], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7019/filler-pod-8942ce45-4d48-47eb-b7ac-0e421ab916de to k8s-conformance-1-15-control-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8942ce45-4d48-47eb-b7ac-0e421ab916de.15e3128fcc29ce45], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8942ce45-4d48-47eb-b7ac-0e421ab916de.15e3128fcda24c80], Reason = [Created], Message = [Created container filler-pod-8942ce45-4d48-47eb-b7ac-0e421ab916de]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-8942ce45-4d48-47eb-b7ac-0e421ab916de.15e3128fd2dccb1d], Reason = [Started], Message = [Started container filler-pod-8942ce45-4d48-47eb-b7ac-0e421ab916de]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a8eec068-f6c4-4f3a-9319-1513224140b0.15e3128f9d205a6e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7019/filler-pod-a8eec068-f6c4-4f3a-9319-1513224140b0 to k8s-conformance-1-15-etcd-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a8eec068-f6c4-4f3a-9319-1513224140b0.15e3129003e5ae8b], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a8eec068-f6c4-4f3a-9319-1513224140b0.15e31290737d070a], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a8eec068-f6c4-4f3a-9319-1513224140b0.15e312907742ab49], Reason = [Created], Message = [Created container filler-pod-a8eec068-f6c4-4f3a-9319-1513224140b0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-a8eec068-f6c4-4f3a-9319-1513224140b0.15e3129083f7fe96], Reason = [Started], Message = [Started container filler-pod-a8eec068-f6c4-4f3a-9319-1513224140b0]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cd16cf46-786d-4eca-8ac9-25c7dcccf070.15e3128f9e08fa56], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7019/filler-pod-cd16cf46-786d-4eca-8ac9-25c7dcccf070 to k8s-conformance-1-15-worker-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cd16cf46-786d-4eca-8ac9-25c7dcccf070.15e3128ff780b32d], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cd16cf46-786d-4eca-8ac9-25c7dcccf070.15e3129062b7c550], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cd16cf46-786d-4eca-8ac9-25c7dcccf070.15e3129063d5b820], Reason = [Created], Message = [Created container filler-pod-cd16cf46-786d-4eca-8ac9-25c7dcccf070]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-cd16cf46-786d-4eca-8ac9-25c7dcccf070.15e312906719e13e], Reason = [Started], Message = [Started container filler-pod-cd16cf46-786d-4eca-8ac9-25c7dcccf070]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d9802375-c4a0-4378-b997-508ebbafa587.15e3128f9cbdb317], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7019/filler-pod-d9802375-c4a0-4378-b997-508ebbafa587 to k8s-conformance-1-15-control-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d9802375-c4a0-4378-b997-508ebbafa587.15e312901199cb0e], Reason = [Pulling], Message = [Pulling image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d9802375-c4a0-4378-b997-508ebbafa587.15e312907e327673], Reason = [Pulled], Message = [Successfully pulled image "k8s.gcr.io/pause:3.1"]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d9802375-c4a0-4378-b997-508ebbafa587.15e3129081cc5de4], Reason = [Created], Message = [Created container filler-pod-d9802375-c4a0-4378-b997-508ebbafa587]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-d9802375-c4a0-4378-b997-508ebbafa587.15e312908dc90811], Reason = [Started], Message = [Started container filler-pod-d9802375-c4a0-4378-b997-508ebbafa587]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15e312910c0b440a], Reason = [FailedScheduling], Message = [0/9 nodes are available: 9 Insufficient cpu.]
STEP: removing the label node off the node k8s-conformance-1-15-worker-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-conformance-1-15-worker-3
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-conformance-1-15-control-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-conformance-1-15-control-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-conformance-1-15-control-3
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-conformance-1-15-etcd-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-conformance-1-15-worker-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-conformance-1-15-etcd-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node k8s-conformance-1-15-etcd-3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:11:50.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7019" for this suite.
Dec 23 18:11:56.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:11:56.657: INFO: namespace sched-pred-7019 deletion completed in 6.504271665s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:14.759 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:11:56.673: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-0536a756-cb22-4684-89cd-46ded1914d7a
STEP: Creating a pod to test consume configMaps
Dec 23 18:11:56.824: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-22736552-6f4b-4b88-b082-710741aed6af" in namespace "projected-3882" to be "success or failure"
Dec 23 18:11:56.830: INFO: Pod "pod-projected-configmaps-22736552-6f4b-4b88-b082-710741aed6af": Phase="Pending", Reason="", readiness=false. Elapsed: 6.204195ms
Dec 23 18:11:58.837: INFO: Pod "pod-projected-configmaps-22736552-6f4b-4b88-b082-710741aed6af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01255922s
STEP: Saw pod success
Dec 23 18:11:58.837: INFO: Pod "pod-projected-configmaps-22736552-6f4b-4b88-b082-710741aed6af" satisfied condition "success or failure"
Dec 23 18:11:58.842: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod pod-projected-configmaps-22736552-6f4b-4b88-b082-710741aed6af container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 23 18:11:58.877: INFO: Waiting for pod pod-projected-configmaps-22736552-6f4b-4b88-b082-710741aed6af to disappear
Dec 23 18:11:58.884: INFO: Pod pod-projected-configmaps-22736552-6f4b-4b88-b082-710741aed6af no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:11:58.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3882" for this suite.
Dec 23 18:12:04.915: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:12:05.113: INFO: namespace projected-3882 deletion completed in 6.220235978s

• [SLOW TEST:8.441 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:12:05.118: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-f8e5a02d-8221-4153-953e-7bcca678555d
STEP: Creating configMap with name cm-test-opt-upd-1511e0c6-aee3-40ae-9486-c67d0cfeb1d2
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-f8e5a02d-8221-4153-953e-7bcca678555d
STEP: Updating configmap cm-test-opt-upd-1511e0c6-aee3-40ae-9486-c67d0cfeb1d2
STEP: Creating configMap with name cm-test-opt-create-17ef2a5c-d358-4766-af5c-5cdfe3ae8f45
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:12:09.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7642" for this suite.
Dec 23 18:12:31.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:12:31.754: INFO: namespace configmap-7642 deletion completed in 22.363120983s

• [SLOW TEST:26.637 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:12:31.759: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-zbl4
STEP: Creating a pod to test atomic-volume-subpath
Dec 23 18:12:31.845: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-zbl4" in namespace "subpath-8006" to be "success or failure"
Dec 23 18:12:31.851: INFO: Pod "pod-subpath-test-downwardapi-zbl4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.590341ms
Dec 23 18:12:33.857: INFO: Pod "pod-subpath-test-downwardapi-zbl4": Phase="Running", Reason="", readiness=true. Elapsed: 2.011559561s
Dec 23 18:12:35.862: INFO: Pod "pod-subpath-test-downwardapi-zbl4": Phase="Running", Reason="", readiness=true. Elapsed: 4.017189746s
Dec 23 18:12:37.869: INFO: Pod "pod-subpath-test-downwardapi-zbl4": Phase="Running", Reason="", readiness=true. Elapsed: 6.023634583s
Dec 23 18:12:39.877: INFO: Pod "pod-subpath-test-downwardapi-zbl4": Phase="Running", Reason="", readiness=true. Elapsed: 8.03180345s
Dec 23 18:12:41.884: INFO: Pod "pod-subpath-test-downwardapi-zbl4": Phase="Running", Reason="", readiness=true. Elapsed: 10.038458905s
Dec 23 18:12:43.890: INFO: Pod "pod-subpath-test-downwardapi-zbl4": Phase="Running", Reason="", readiness=true. Elapsed: 12.044628563s
Dec 23 18:12:45.895: INFO: Pod "pod-subpath-test-downwardapi-zbl4": Phase="Running", Reason="", readiness=true. Elapsed: 14.050268562s
Dec 23 18:12:47.902: INFO: Pod "pod-subpath-test-downwardapi-zbl4": Phase="Running", Reason="", readiness=true. Elapsed: 16.056460977s
Dec 23 18:12:49.908: INFO: Pod "pod-subpath-test-downwardapi-zbl4": Phase="Running", Reason="", readiness=true. Elapsed: 18.062705111s
Dec 23 18:12:51.913: INFO: Pod "pod-subpath-test-downwardapi-zbl4": Phase="Running", Reason="", readiness=true. Elapsed: 20.068165996s
Dec 23 18:12:53.920: INFO: Pod "pod-subpath-test-downwardapi-zbl4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.074614648s
STEP: Saw pod success
Dec 23 18:12:53.920: INFO: Pod "pod-subpath-test-downwardapi-zbl4" satisfied condition "success or failure"
Dec 23 18:12:53.927: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod pod-subpath-test-downwardapi-zbl4 container test-container-subpath-downwardapi-zbl4: <nil>
STEP: delete the pod
Dec 23 18:12:53.964: INFO: Waiting for pod pod-subpath-test-downwardapi-zbl4 to disappear
Dec 23 18:12:53.969: INFO: Pod pod-subpath-test-downwardapi-zbl4 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-zbl4
Dec 23 18:12:53.970: INFO: Deleting pod "pod-subpath-test-downwardapi-zbl4" in namespace "subpath-8006"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:12:53.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8006" for this suite.
Dec 23 18:13:00.006: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:13:00.220: INFO: namespace subpath-8006 deletion completed in 6.234816726s

• [SLOW TEST:28.461 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:13:00.225: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 23 18:13:00.291: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ccbb2631-33dd-4d9c-a0bf-6f4fb96b4e60" in namespace "projected-6012" to be "success or failure"
Dec 23 18:13:00.297: INFO: Pod "downwardapi-volume-ccbb2631-33dd-4d9c-a0bf-6f4fb96b4e60": Phase="Pending", Reason="", readiness=false. Elapsed: 6.193453ms
Dec 23 18:13:02.303: INFO: Pod "downwardapi-volume-ccbb2631-33dd-4d9c-a0bf-6f4fb96b4e60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012076097s
STEP: Saw pod success
Dec 23 18:13:02.304: INFO: Pod "downwardapi-volume-ccbb2631-33dd-4d9c-a0bf-6f4fb96b4e60" satisfied condition "success or failure"
Dec 23 18:13:02.308: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod downwardapi-volume-ccbb2631-33dd-4d9c-a0bf-6f4fb96b4e60 container client-container: <nil>
STEP: delete the pod
Dec 23 18:13:02.350: INFO: Waiting for pod downwardapi-volume-ccbb2631-33dd-4d9c-a0bf-6f4fb96b4e60 to disappear
Dec 23 18:13:02.363: INFO: Pod downwardapi-volume-ccbb2631-33dd-4d9c-a0bf-6f4fb96b4e60 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:13:02.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6012" for this suite.
Dec 23 18:13:08.401: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:13:08.607: INFO: namespace projected-6012 deletion completed in 6.233630454s

• [SLOW TEST:8.382 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:13:08.614: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec 23 18:13:11.768: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:13:11.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1198" for this suite.
Dec 23 18:13:25.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:13:26.064: INFO: namespace replicaset-1198 deletion completed in 14.241432423s

• [SLOW TEST:17.450 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:13:26.068: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-1029
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 23 18:13:26.169: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 23 18:13:52.542: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.206.101:8080/dial?request=hostName&protocol=http&host=10.42.92.7&port=8080&tries=1'] Namespace:pod-network-test-1029 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:13:52.542: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:13:52.774: INFO: Waiting for endpoints: map[]
Dec 23 18:13:52.780: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.206.101:8080/dial?request=hostName&protocol=http&host=10.42.97.5&port=8080&tries=1'] Namespace:pod-network-test-1029 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:13:52.780: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:13:52.857: INFO: Waiting for endpoints: map[]
Dec 23 18:13:52.863: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.206.101:8080/dial?request=hostName&protocol=http&host=10.42.108.201&port=8080&tries=1'] Namespace:pod-network-test-1029 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:13:52.863: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:13:52.960: INFO: Waiting for endpoints: map[]
Dec 23 18:13:52.966: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.206.101:8080/dial?request=hostName&protocol=http&host=10.42.209.135&port=8080&tries=1'] Namespace:pod-network-test-1029 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:13:52.967: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:13:53.078: INFO: Waiting for endpoints: map[]
Dec 23 18:13:53.084: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.206.101:8080/dial?request=hostName&protocol=http&host=10.42.132.136&port=8080&tries=1'] Namespace:pod-network-test-1029 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:13:53.085: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:13:53.183: INFO: Waiting for endpoints: map[]
Dec 23 18:13:53.188: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.206.101:8080/dial?request=hostName&protocol=http&host=10.42.209.101&port=8080&tries=1'] Namespace:pod-network-test-1029 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:13:53.188: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:13:53.279: INFO: Waiting for endpoints: map[]
Dec 23 18:13:53.284: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.206.101:8080/dial?request=hostName&protocol=http&host=10.42.206.100&port=8080&tries=1'] Namespace:pod-network-test-1029 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:13:53.285: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:13:53.362: INFO: Waiting for endpoints: map[]
Dec 23 18:13:53.368: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.206.101:8080/dial?request=hostName&protocol=http&host=10.42.150.6&port=8080&tries=1'] Namespace:pod-network-test-1029 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:13:53.368: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:13:53.494: INFO: Waiting for endpoints: map[]
Dec 23 18:13:53.500: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.206.101:8080/dial?request=hostName&protocol=http&host=10.42.17.199&port=8080&tries=1'] Namespace:pod-network-test-1029 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:13:53.501: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:13:53.588: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:13:53.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1029" for this suite.
Dec 23 18:14:17.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:14:17.834: INFO: namespace pod-network-test-1029 deletion completed in 24.236847462s

• [SLOW TEST:51.766 seconds]
[sig-network] Networking
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:14:17.840: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W1223 18:14:28.130817      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 23 18:14:28.131: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:14:28.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6953" for this suite.
Dec 23 18:14:34.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:14:34.411: INFO: namespace gc-6953 deletion completed in 6.271757323s

• [SLOW TEST:16.572 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:14:34.431: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W1223 18:14:35.240793      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 23 18:14:35.241: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:14:35.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7949" for this suite.
Dec 23 18:14:41.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:14:41.631: INFO: namespace gc-7949 deletion completed in 6.378308425s

• [SLOW TEST:7.201 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:14:41.635: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-b3c1b29b-081b-44a3-a876-e1a0b8c4c711 in namespace container-probe-2159
Dec 23 18:14:43.777: INFO: Started pod busybox-b3c1b29b-081b-44a3-a876-e1a0b8c4c711 in namespace container-probe-2159
STEP: checking the pod's current state and verifying that restartCount is present
Dec 23 18:14:43.782: INFO: Initial restart count of pod busybox-b3c1b29b-081b-44a3-a876-e1a0b8c4c711 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:18:44.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2159" for this suite.
Dec 23 18:18:50.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:18:50.845: INFO: namespace container-probe-2159 deletion completed in 6.237623024s

• [SLOW TEST:249.211 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:18:50.847: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:18:56.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3548" for this suite.
Dec 23 18:19:02.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:19:02.729: INFO: namespace watch-3548 deletion completed in 6.310856906s

• [SLOW TEST:11.882 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:19:02.732: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 23 18:19:02.846: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6a82c90e-3cc2-4acd-9620-ec96b383bc1c" in namespace "projected-9115" to be "success or failure"
Dec 23 18:19:02.852: INFO: Pod "downwardapi-volume-6a82c90e-3cc2-4acd-9620-ec96b383bc1c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.185156ms
Dec 23 18:19:04.859: INFO: Pod "downwardapi-volume-6a82c90e-3cc2-4acd-9620-ec96b383bc1c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012843217s
STEP: Saw pod success
Dec 23 18:19:04.859: INFO: Pod "downwardapi-volume-6a82c90e-3cc2-4acd-9620-ec96b383bc1c" satisfied condition "success or failure"
Dec 23 18:19:04.864: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod downwardapi-volume-6a82c90e-3cc2-4acd-9620-ec96b383bc1c container client-container: <nil>
STEP: delete the pod
Dec 23 18:19:04.900: INFO: Waiting for pod downwardapi-volume-6a82c90e-3cc2-4acd-9620-ec96b383bc1c to disappear
Dec 23 18:19:04.905: INFO: Pod downwardapi-volume-6a82c90e-3cc2-4acd-9620-ec96b383bc1c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:19:04.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9115" for this suite.
Dec 23 18:19:10.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:19:11.197: INFO: namespace projected-9115 deletion completed in 6.282249898s

• [SLOW TEST:8.466 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:19:11.221: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec 23 18:19:11.327: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:19:14.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9213" for this suite.
Dec 23 18:19:20.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:19:20.252: INFO: namespace init-container-9213 deletion completed in 6.231628921s

• [SLOW TEST:9.032 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:19:20.257: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1420
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 23 18:19:20.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-44'
Dec 23 18:19:20.629: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 23 18:19:20.629: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1426
Dec 23 18:19:20.644: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 delete deployment e2e-test-nginx-deployment --namespace=kubectl-44'
Dec 23 18:19:20.916: INFO: stderr: ""
Dec 23 18:19:20.916: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:19:20.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-44" for this suite.
Dec 23 18:19:26.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:19:27.164: INFO: namespace kubectl-44 deletion completed in 6.239432707s

• [SLOW TEST:6.908 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:19:27.170: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-2800
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2800 to expose endpoints map[]
Dec 23 18:19:27.311: INFO: Get endpoints failed (8.557775ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Dec 23 18:19:28.318: INFO: successfully validated that service multi-endpoint-test in namespace services-2800 exposes endpoints map[] (1.015378933s elapsed)
STEP: Creating pod pod1 in namespace services-2800
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2800 to expose endpoints map[pod1:[100]]
Dec 23 18:19:29.363: INFO: successfully validated that service multi-endpoint-test in namespace services-2800 exposes endpoints map[pod1:[100]] (1.034077734s elapsed)
STEP: Creating pod pod2 in namespace services-2800
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2800 to expose endpoints map[pod1:[100] pod2:[101]]
Dec 23 18:19:30.410: INFO: successfully validated that service multi-endpoint-test in namespace services-2800 exposes endpoints map[pod1:[100] pod2:[101]] (1.036777566s elapsed)
STEP: Deleting pod pod1 in namespace services-2800
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2800 to expose endpoints map[pod2:[101]]
Dec 23 18:19:30.452: INFO: successfully validated that service multi-endpoint-test in namespace services-2800 exposes endpoints map[pod2:[101]] (20.997104ms elapsed)
STEP: Deleting pod pod2 in namespace services-2800
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2800 to expose endpoints map[]
Dec 23 18:19:31.529: INFO: successfully validated that service multi-endpoint-test in namespace services-2800 exposes endpoints map[] (1.064928649s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:19:31.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2800" for this suite.
Dec 23 18:19:53.608: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:19:53.820: INFO: namespace services-2800 deletion completed in 22.237006275s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:26.651 seconds]
[sig-network] Services
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:19:53.825: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-414a9e29-b11b-4293-a22a-b44c84d9b065 in namespace container-probe-6456
Dec 23 18:19:59.911: INFO: Started pod liveness-414a9e29-b11b-4293-a22a-b44c84d9b065 in namespace container-probe-6456
STEP: checking the pod's current state and verifying that restartCount is present
Dec 23 18:19:59.917: INFO: Initial restart count of pod liveness-414a9e29-b11b-4293-a22a-b44c84d9b065 is 0
Dec 23 18:20:19.985: INFO: Restart count of pod container-probe-6456/liveness-414a9e29-b11b-4293-a22a-b44c84d9b065 is now 1 (20.068439345s elapsed)
Dec 23 18:20:40.051: INFO: Restart count of pod container-probe-6456/liveness-414a9e29-b11b-4293-a22a-b44c84d9b065 is now 2 (40.134129593s elapsed)
Dec 23 18:21:00.114: INFO: Restart count of pod container-probe-6456/liveness-414a9e29-b11b-4293-a22a-b44c84d9b065 is now 3 (1m0.197509726s elapsed)
Dec 23 18:21:20.178: INFO: Restart count of pod container-probe-6456/liveness-414a9e29-b11b-4293-a22a-b44c84d9b065 is now 4 (1m20.260814273s elapsed)
Dec 23 18:22:20.370: INFO: Restart count of pod container-probe-6456/liveness-414a9e29-b11b-4293-a22a-b44c84d9b065 is now 5 (2m20.453042566s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:22:20.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6456" for this suite.
Dec 23 18:22:26.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:22:26.658: INFO: namespace container-probe-6456 deletion completed in 6.226090074s

• [SLOW TEST:152.833 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:22:26.662: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-9345
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-9345
STEP: Deleting pre-stop pod
Dec 23 18:22:39.865: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:22:39.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9345" for this suite.
Dec 23 18:23:17.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:23:18.150: INFO: namespace prestop-9345 deletion completed in 38.258848295s

• [SLOW TEST:51.488 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:23:18.151: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 23 18:23:20.819: INFO: Successfully updated pod "pod-update-34a46765-3034-4781-a900-fbb27e21267d"
STEP: verifying the updated pod is in kubernetes
Dec 23 18:23:20.834: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:23:20.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4140" for this suite.
Dec 23 18:23:42.867: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:23:43.073: INFO: namespace pods-4140 deletion completed in 22.229207404s

• [SLOW TEST:24.923 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:23:43.088: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 23 18:23:43.222: INFO: Waiting up to 5m0s for pod "downwardapi-volume-be00242c-8088-4e82-9563-95322bc852be" in namespace "downward-api-535" to be "success or failure"
Dec 23 18:23:43.233: INFO: Pod "downwardapi-volume-be00242c-8088-4e82-9563-95322bc852be": Phase="Pending", Reason="", readiness=false. Elapsed: 10.916075ms
Dec 23 18:23:45.240: INFO: Pod "downwardapi-volume-be00242c-8088-4e82-9563-95322bc852be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018147231s
STEP: Saw pod success
Dec 23 18:23:45.240: INFO: Pod "downwardapi-volume-be00242c-8088-4e82-9563-95322bc852be" satisfied condition "success or failure"
Dec 23 18:23:45.245: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod downwardapi-volume-be00242c-8088-4e82-9563-95322bc852be container client-container: <nil>
STEP: delete the pod
Dec 23 18:23:45.283: INFO: Waiting for pod downwardapi-volume-be00242c-8088-4e82-9563-95322bc852be to disappear
Dec 23 18:23:45.289: INFO: Pod downwardapi-volume-be00242c-8088-4e82-9563-95322bc852be no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:23:45.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-535" for this suite.
Dec 23 18:23:51.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:23:51.639: INFO: namespace downward-api-535 deletion completed in 6.341274432s

• [SLOW TEST:8.552 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:23:51.662: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Dec 23 18:23:51.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 create -f - --namespace=kubectl-5030'
Dec 23 18:23:52.760: INFO: stderr: ""
Dec 23 18:23:52.760: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 23 18:23:52.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5030'
Dec 23 18:23:53.061: INFO: stderr: ""
Dec 23 18:23:53.061: INFO: stdout: "update-demo-nautilus-4khdh update-demo-nautilus-gfgxk "
Dec 23 18:23:53.061: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-nautilus-4khdh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5030'
Dec 23 18:23:53.293: INFO: stderr: ""
Dec 23 18:23:53.293: INFO: stdout: ""
Dec 23 18:23:53.293: INFO: update-demo-nautilus-4khdh is created but not running
Dec 23 18:23:58.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5030'
Dec 23 18:23:58.556: INFO: stderr: ""
Dec 23 18:23:58.556: INFO: stdout: "update-demo-nautilus-4khdh update-demo-nautilus-gfgxk "
Dec 23 18:23:58.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-nautilus-4khdh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5030'
Dec 23 18:23:58.774: INFO: stderr: ""
Dec 23 18:23:58.774: INFO: stdout: "true"
Dec 23 18:23:58.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-nautilus-4khdh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5030'
Dec 23 18:23:59.043: INFO: stderr: ""
Dec 23 18:23:59.043: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 23 18:23:59.043: INFO: validating pod update-demo-nautilus-4khdh
Dec 23 18:23:59.052: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 23 18:23:59.052: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 23 18:23:59.052: INFO: update-demo-nautilus-4khdh is verified up and running
Dec 23 18:23:59.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-nautilus-gfgxk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5030'
Dec 23 18:23:59.287: INFO: stderr: ""
Dec 23 18:23:59.287: INFO: stdout: "true"
Dec 23 18:23:59.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-nautilus-gfgxk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5030'
Dec 23 18:23:59.516: INFO: stderr: ""
Dec 23 18:23:59.516: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 23 18:23:59.516: INFO: validating pod update-demo-nautilus-gfgxk
Dec 23 18:23:59.522: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 23 18:23:59.522: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 23 18:23:59.522: INFO: update-demo-nautilus-gfgxk is verified up and running
STEP: scaling down the replication controller
Dec 23 18:23:59.532: INFO: scanned /root for discovery docs: <nil>
Dec 23 18:23:59.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-5030'
Dec 23 18:24:00.858: INFO: stderr: ""
Dec 23 18:24:00.858: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 23 18:24:00.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5030'
Dec 23 18:24:01.142: INFO: stderr: ""
Dec 23 18:24:01.143: INFO: stdout: "update-demo-nautilus-4khdh update-demo-nautilus-gfgxk "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 23 18:24:06.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5030'
Dec 23 18:24:06.391: INFO: stderr: ""
Dec 23 18:24:06.391: INFO: stdout: "update-demo-nautilus-4khdh "
Dec 23 18:24:06.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-nautilus-4khdh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5030'
Dec 23 18:24:06.604: INFO: stderr: ""
Dec 23 18:24:06.604: INFO: stdout: "true"
Dec 23 18:24:06.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-nautilus-4khdh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5030'
Dec 23 18:24:06.823: INFO: stderr: ""
Dec 23 18:24:06.824: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 23 18:24:06.824: INFO: validating pod update-demo-nautilus-4khdh
Dec 23 18:24:06.837: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 23 18:24:06.837: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 23 18:24:06.837: INFO: update-demo-nautilus-4khdh is verified up and running
STEP: scaling up the replication controller
Dec 23 18:24:06.847: INFO: scanned /root for discovery docs: <nil>
Dec 23 18:24:06.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-5030'
Dec 23 18:24:08.231: INFO: stderr: ""
Dec 23 18:24:08.231: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 23 18:24:08.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-5030'
Dec 23 18:24:08.512: INFO: stderr: ""
Dec 23 18:24:08.512: INFO: stdout: "update-demo-nautilus-4khdh update-demo-nautilus-prmrw "
Dec 23 18:24:08.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-nautilus-4khdh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5030'
Dec 23 18:24:08.708: INFO: stderr: ""
Dec 23 18:24:08.708: INFO: stdout: "true"
Dec 23 18:24:08.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-nautilus-4khdh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5030'
Dec 23 18:24:08.924: INFO: stderr: ""
Dec 23 18:24:08.924: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 23 18:24:08.924: INFO: validating pod update-demo-nautilus-4khdh
Dec 23 18:24:08.931: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 23 18:24:08.931: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 23 18:24:08.931: INFO: update-demo-nautilus-4khdh is verified up and running
Dec 23 18:24:08.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-nautilus-prmrw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-5030'
Dec 23 18:24:09.113: INFO: stderr: ""
Dec 23 18:24:09.113: INFO: stdout: "true"
Dec 23 18:24:09.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-nautilus-prmrw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-5030'
Dec 23 18:24:09.260: INFO: stderr: ""
Dec 23 18:24:09.260: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 23 18:24:09.260: INFO: validating pod update-demo-nautilus-prmrw
Dec 23 18:24:09.266: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 23 18:24:09.266: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 23 18:24:09.266: INFO: update-demo-nautilus-prmrw is verified up and running
STEP: using delete to clean up resources
Dec 23 18:24:09.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 delete --grace-period=0 --force -f - --namespace=kubectl-5030'
Dec 23 18:24:09.410: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 23 18:24:09.410: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 23 18:24:09.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-5030'
Dec 23 18:24:09.708: INFO: stderr: "No resources found.\n"
Dec 23 18:24:09.708: INFO: stdout: ""
Dec 23 18:24:09.708: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods -l name=update-demo --namespace=kubectl-5030 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 23 18:24:09.962: INFO: stderr: ""
Dec 23 18:24:09.962: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:24:09.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5030" for this suite.
Dec 23 18:24:32.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:24:32.224: INFO: namespace kubectl-5030 deletion completed in 22.224146691s

• [SLOW TEST:40.563 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:24:32.226: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1685
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 23 18:24:32.338: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2588'
Dec 23 18:24:32.634: INFO: stderr: ""
Dec 23 18:24:32.634: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1690
Dec 23 18:24:32.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 delete pods e2e-test-nginx-pod --namespace=kubectl-2588'
Dec 23 18:24:33.424: INFO: stderr: ""
Dec 23 18:24:33.424: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:24:33.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2588" for this suite.
Dec 23 18:24:39.464: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:24:39.663: INFO: namespace kubectl-2588 deletion completed in 6.225897114s

• [SLOW TEST:7.438 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:24:39.667: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 23 18:24:39.880: INFO: Number of nodes with available pods: 0
Dec 23 18:24:39.882: INFO: Node k8s-conformance-1-15-control-1 is running more than one daemon pod
Dec 23 18:24:40.901: INFO: Number of nodes with available pods: 0
Dec 23 18:24:40.903: INFO: Node k8s-conformance-1-15-control-1 is running more than one daemon pod
Dec 23 18:24:41.905: INFO: Number of nodes with available pods: 7
Dec 23 18:24:41.905: INFO: Node k8s-conformance-1-15-control-2 is running more than one daemon pod
Dec 23 18:24:42.904: INFO: Number of nodes with available pods: 9
Dec 23 18:24:42.905: INFO: Number of running nodes: 9, number of available pods: 9
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec 23 18:24:42.968: INFO: Number of nodes with available pods: 8
Dec 23 18:24:42.968: INFO: Node k8s-conformance-1-15-etcd-2 is running more than one daemon pod
Dec 23 18:24:43.988: INFO: Number of nodes with available pods: 8
Dec 23 18:24:43.989: INFO: Node k8s-conformance-1-15-etcd-2 is running more than one daemon pod
Dec 23 18:24:44.987: INFO: Number of nodes with available pods: 9
Dec 23 18:24:44.988: INFO: Number of running nodes: 9, number of available pods: 9
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3671, will wait for the garbage collector to delete the pods
Dec 23 18:24:45.063: INFO: Deleting DaemonSet.extensions daemon-set took: 11.39479ms
Dec 23 18:24:45.595: INFO: Terminating DaemonSet.extensions daemon-set pods took: 531.792023ms
Dec 23 18:24:56.301: INFO: Number of nodes with available pods: 0
Dec 23 18:24:56.301: INFO: Number of running nodes: 0, number of available pods: 0
Dec 23 18:24:56.311: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3671/daemonsets","resourceVersion":"21328"},"items":null}

Dec 23 18:24:56.316: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3671/pods","resourceVersion":"21328"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:24:56.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3671" for this suite.
Dec 23 18:25:02.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:25:02.640: INFO: namespace daemonsets-3671 deletion completed in 6.264717687s

• [SLOW TEST:22.973 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:25:02.642: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 23 18:25:04.729: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:25:04.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3708" for this suite.
Dec 23 18:25:10.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:25:11.025: INFO: namespace container-runtime-3708 deletion completed in 6.252022509s

• [SLOW TEST:8.383 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:25:11.029: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec 23 18:25:11.085: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:25:14.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9538" for this suite.
Dec 23 18:25:20.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:25:20.861: INFO: namespace init-container-9538 deletion completed in 6.237621496s

• [SLOW TEST:9.833 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:25:20.865: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 23 18:25:20.918: INFO: Creating deployment "nginx-deployment"
Dec 23 18:25:20.929: INFO: Waiting for observed generation 1
Dec 23 18:25:22.943: INFO: Waiting for all required pods to come up
Dec 23 18:25:23.005: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec 23 18:25:25.021: INFO: Waiting for deployment "nginx-deployment" to complete
Dec 23 18:25:25.031: INFO: Updating deployment "nginx-deployment" with a non-existent image
Dec 23 18:25:25.057: INFO: Updating deployment nginx-deployment
Dec 23 18:25:25.057: INFO: Waiting for observed generation 2
Dec 23 18:25:27.073: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec 23 18:25:27.087: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec 23 18:25:27.091: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec 23 18:25:27.105: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec 23 18:25:27.105: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec 23 18:25:27.109: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Dec 23 18:25:27.120: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Dec 23 18:25:27.121: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Dec 23 18:25:27.138: INFO: Updating deployment nginx-deployment
Dec 23 18:25:27.138: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Dec 23 18:25:27.158: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec 23 18:25:27.192: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec 23 18:25:29.242: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-1808,SelfLink:/apis/apps/v1/namespaces/deployment-1808/deployments/nginx-deployment,UID:6a1b66a7-14a9-42a7-b731-2bb13f04520c,ResourceVersion:21886,Generation:3,CreationTimestamp:2019-12-23 18:25:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-12-23 18:25:27 +0000 UTC 2019-12-23 18:25:27 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-12-23 18:25:27 +0000 UTC 2019-12-23 18:25:20 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Dec 23 18:25:29.270: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-1808,SelfLink:/apis/apps/v1/namespaces/deployment-1808/replicasets/nginx-deployment-55fb7cb77f,UID:837f1679-beb9-49ad-a585-4958e5a5af28,ResourceVersion:21871,Generation:3,CreationTimestamp:2019-12-23 18:25:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 6a1b66a7-14a9-42a7-b731-2bb13f04520c 0xc002c50247 0xc002c50248}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 23 18:25:29.270: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Dec 23 18:25:29.270: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-1808,SelfLink:/apis/apps/v1/namespaces/deployment-1808/replicasets/nginx-deployment-7b8c6f4498,UID:f05dbc5a-eab4-4377-91e9-e78b98b20fe3,ResourceVersion:21879,Generation:3,CreationTimestamp:2019-12-23 18:25:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 6a1b66a7-14a9-42a7-b731-2bb13f04520c 0xc002c50317 0xc002c50318}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Dec 23 18:25:29.310: INFO: Pod "nginx-deployment-55fb7cb77f-6jjj8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-6jjj8,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-55fb7cb77f-6jjj8,UID:0b6cd85f-925e-4ff6-bfc3-7e6ad239382f,ResourceVersion:21883,Generation:0,CreationTimestamp:2019-12-23 18:25:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 837f1679-beb9-49ad-a585-4958e5a5af28 0xc002c50f27 0xc002c50f28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-control-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c50f90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c50fb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  }],Message:,Reason:,HostIP:72.2.118.117,PodIP:,StartTime:2019-12-23 18:25:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.312: INFO: Pod "nginx-deployment-55fb7cb77f-6m8gf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-6m8gf,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-55fb7cb77f-6m8gf,UID:92313c2e-9936-481a-abce-0402ad656298,ResourceVersion:22012,Generation:0,CreationTimestamp:2019-12-23 18:25:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.209.115/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 837f1679-beb9-49ad-a585-4958e5a5af28 0xc002c51090 0xc002c51091}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-etcd-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c51100} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c51120}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  }],Message:,Reason:,HostIP:72.2.119.135,PodIP:,StartTime:2019-12-23 18:25:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.313: INFO: Pod "nginx-deployment-55fb7cb77f-6rssm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-6rssm,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-55fb7cb77f-6rssm,UID:b8c44d3b-f94a-4fa7-aedf-21a3bf84dbe8,ResourceVersion:21968,Generation:0,CreationTimestamp:2019-12-23 18:25:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.132.140/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 837f1679-beb9-49ad-a585-4958e5a5af28 0xc002c51200 0xc002c51201}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c51270} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c51290}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  }],Message:,Reason:,HostIP:72.2.118.151,PodIP:,StartTime:2019-12-23 18:25:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.314: INFO: Pod "nginx-deployment-55fb7cb77f-7cwtl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-7cwtl,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-55fb7cb77f-7cwtl,UID:e71a3822-4a34-4e08-8374-173f3026847b,ResourceVersion:21937,Generation:0,CreationTimestamp:2019-12-23 18:25:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.209.113/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 837f1679-beb9-49ad-a585-4958e5a5af28 0xc002c51370 0xc002c51371}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-etcd-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c513e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c51400}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:25 +0000 UTC  }],Message:,Reason:,HostIP:72.2.119.135,PodIP:10.42.209.113,StartTime:2019-12-23 18:25:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "nginx:404",} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.320: INFO: Pod "nginx-deployment-55fb7cb77f-8knnn" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-8knnn,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-55fb7cb77f-8knnn,UID:c97243b2-d14c-428b-892c-3d39a8c0f165,ResourceVersion:21991,Generation:0,CreationTimestamp:2019-12-23 18:25:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.108.208/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 837f1679-beb9-49ad-a585-4958e5a5af28 0xc002c51500 0xc002c51501}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-etcd-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c51570} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c51590}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:25 +0000 UTC  }],Message:,Reason:,HostIP:72.2.118.33,PodIP:,StartTime:2019-12-23 18:25:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.321: INFO: Pod "nginx-deployment-55fb7cb77f-8z6n8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-8z6n8,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-55fb7cb77f-8z6n8,UID:16c25d2e-78c1-467f-a145-1f8b72a06866,ResourceVersion:21910,Generation:0,CreationTimestamp:2019-12-23 18:25:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 837f1679-beb9-49ad-a585-4958e5a5af28 0xc002c51670 0xc002c51671}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-control-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c516e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c51700}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  }],Message:,Reason:,HostIP:72.2.118.40,PodIP:,StartTime:2019-12-23 18:25:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.322: INFO: Pod "nginx-deployment-55fb7cb77f-fzhgh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-fzhgh,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-55fb7cb77f-fzhgh,UID:22aa94e7-85c6-46ce-bdcb-7e7a1db6372f,ResourceVersion:21905,Generation:0,CreationTimestamp:2019-12-23 18:25:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.97.9/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 837f1679-beb9-49ad-a585-4958e5a5af28 0xc002c517e0 0xc002c517e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c51850} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c51870}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:25 +0000 UTC  }],Message:,Reason:,HostIP:72.2.119.97,PodIP:,StartTime:2019-12-23 18:25:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.323: INFO: Pod "nginx-deployment-55fb7cb77f-p9hrh" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-p9hrh,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-55fb7cb77f-p9hrh,UID:2227f12e-3ded-4d0c-a689-27318a09aaee,ResourceVersion:21986,Generation:0,CreationTimestamp:2019-12-23 18:25:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.206.118/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 837f1679-beb9-49ad-a585-4958e5a5af28 0xc002c51960 0xc002c51961}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-control-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c519d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c519f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  }],Message:,Reason:,HostIP:72.2.119.160,PodIP:,StartTime:2019-12-23 18:25:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.323: INFO: Pod "nginx-deployment-55fb7cb77f-qnmf5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-qnmf5,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-55fb7cb77f-qnmf5,UID:c37f065a-d1b7-4fe5-b13e-490aba83638e,ResourceVersion:21975,Generation:0,CreationTimestamp:2019-12-23 18:25:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.150.10/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 837f1679-beb9-49ad-a585-4958e5a5af28 0xc002c51ae0 0xc002c51ae1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-etcd-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c51b50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c51b70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:25 +0000 UTC  }],Message:,Reason:,HostIP:72.2.119.25,PodIP:10.42.150.10,StartTime:2019-12-23 18:25:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "nginx:404",} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.326: INFO: Pod "nginx-deployment-55fb7cb77f-rmtxd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-rmtxd,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-55fb7cb77f-rmtxd,UID:a8516d66-294d-44c1-8231-616c85d5d93e,ResourceVersion:21893,Generation:0,CreationTimestamp:2019-12-23 18:25:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 837f1679-beb9-49ad-a585-4958e5a5af28 0xc002c51c80 0xc002c51c81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c51cf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c51d10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  }],Message:,Reason:,HostIP:72.2.119.97,PodIP:,StartTime:2019-12-23 18:25:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.327: INFO: Pod "nginx-deployment-55fb7cb77f-tgmh7" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-tgmh7,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-55fb7cb77f-tgmh7,UID:46141f11-2e4d-45ec-8579-9becbffb2b4f,ResourceVersion:21932,Generation:0,CreationTimestamp:2019-12-23 18:25:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.206.117/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 837f1679-beb9-49ad-a585-4958e5a5af28 0xc002c51df0 0xc002c51df1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-control-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c51e60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002c51e80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:25 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:25 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:25 +0000 UTC  }],Message:,Reason:,HostIP:72.2.119.160,PodIP:10.42.206.117,StartTime:2019-12-23 18:25:25 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "nginx:404",} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.329: INFO: Pod "nginx-deployment-55fb7cb77f-xz5h4" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-xz5h4,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-55fb7cb77f-xz5h4,UID:ee19eb50-207c-46ef-b23e-c125d8ffa3a8,ResourceVersion:21971,Generation:0,CreationTimestamp:2019-12-23 18:25:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.17.203/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 837f1679-beb9-49ad-a585-4958e5a5af28 0xc002c51f80 0xc002c51f81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002c51ff0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026b4010}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  }],Message:,Reason:,HostIP:72.2.118.98,PodIP:10.42.17.203,StartTime:2019-12-23 18:25:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.331: INFO: Pod "nginx-deployment-55fb7cb77f-xzvhq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-xzvhq,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-55fb7cb77f-xzvhq,UID:0deafb20-6a27-48f9-ab6c-a95ecf246837,ResourceVersion:21996,Generation:0,CreationTimestamp:2019-12-23 18:25:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.108.207/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 837f1679-beb9-49ad-a585-4958e5a5af28 0xc0026b4120 0xc0026b4121}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-etcd-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026b4190} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026b41b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  }],Message:,Reason:,HostIP:72.2.118.33,PodIP:,StartTime:2019-12-23 18:25:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.334: INFO: Pod "nginx-deployment-7b8c6f4498-4q996" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4q996,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-7b8c6f4498-4q996,UID:1fbc7785-7966-4f34-878c-d107b9cdc54e,ResourceVersion:21941,Generation:0,CreationTimestamp:2019-12-23 18:25:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f05dbc5a-eab4-4377-91e9-e78b98b20fe3 0xc0026b42a0 0xc0026b42a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-etcd-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026b4300} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026b4320}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  }],Message:,Reason:,HostIP:72.2.119.135,PodIP:,StartTime:2019-12-23 18:25:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.337: INFO: Pod "nginx-deployment-7b8c6f4498-4zdrt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4zdrt,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-7b8c6f4498-4zdrt,UID:c24e307b-0545-45ac-bee8-c55fb0753a15,ResourceVersion:21849,Generation:0,CreationTimestamp:2019-12-23 18:25:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f05dbc5a-eab4-4377-91e9-e78b98b20fe3 0xc0026b43e7 0xc0026b43e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026b4450} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026b4470}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  }],Message:,Reason:,HostIP:72.2.119.97,PodIP:,StartTime:2019-12-23 18:25:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.339: INFO: Pod "nginx-deployment-7b8c6f4498-9qn2h" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-9qn2h,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-7b8c6f4498-9qn2h,UID:3eb0f946-ee5f-4b6a-8906-b184b6ef429b,ResourceVersion:21669,Generation:0,CreationTimestamp:2019-12-23 18:25:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.206.115/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f05dbc5a-eab4-4377-91e9-e78b98b20fe3 0xc0026b4580 0xc0026b4581}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-control-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026b45e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026b4600}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:21 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:21 +0000 UTC  }],Message:,Reason:,HostIP:72.2.119.160,PodIP:10.42.206.115,StartTime:2019-12-23 18:25:21 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-23 18:25:22 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://69163b382e66dd6a44bae780041bb1314c24c4a71ae21446641b51f6ab7cd17b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.340: INFO: Pod "nginx-deployment-7b8c6f4498-cvq4q" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-cvq4q,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-7b8c6f4498-cvq4q,UID:c703acad-ae45-407c-8960-4d71330e495f,ResourceVersion:21918,Generation:0,CreationTimestamp:2019-12-23 18:25:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f05dbc5a-eab4-4377-91e9-e78b98b20fe3 0xc0026b46d7 0xc0026b46d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-etcd-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026b4740} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026b4760}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  }],Message:,Reason:,HostIP:72.2.119.25,PodIP:,StartTime:2019-12-23 18:25:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.341: INFO: Pod "nginx-deployment-7b8c6f4498-dck4j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-dck4j,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-7b8c6f4498-dck4j,UID:32384664-0bf9-4b08-8572-67bcf6f0bdeb,ResourceVersion:21884,Generation:0,CreationTimestamp:2019-12-23 18:25:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f05dbc5a-eab4-4377-91e9-e78b98b20fe3 0xc0026b4820 0xc0026b4821}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026b4880} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026b48a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  }],Message:,Reason:,HostIP:72.2.119.97,PodIP:,StartTime:2019-12-23 18:25:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.350: INFO: Pod "nginx-deployment-7b8c6f4498-fp4bx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fp4bx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-7b8c6f4498-fp4bx,UID:6d1ff918-8d68-4363-970d-61d68da3f789,ResourceVersion:21645,Generation:0,CreationTimestamp:2019-12-23 18:25:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.132.139/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f05dbc5a-eab4-4377-91e9-e78b98b20fe3 0xc0026b4980 0xc0026b4981}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026b49e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026b4a00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:21 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:21 +0000 UTC  }],Message:,Reason:,HostIP:72.2.118.151,PodIP:10.42.132.139,StartTime:2019-12-23 18:25:21 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-23 18:25:22 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c370974ba57d9a72c37f717878de115784593e01f87df752a43966ab959ee46f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.354: INFO: Pod "nginx-deployment-7b8c6f4498-gskrr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-gskrr,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-7b8c6f4498-gskrr,UID:1b70aac7-8a45-412a-9adf-0707cea6084a,ResourceVersion:21657,Generation:0,CreationTimestamp:2019-12-23 18:25:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.209.112/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f05dbc5a-eab4-4377-91e9-e78b98b20fe3 0xc0026b4af7 0xc0026b4af8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-etcd-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026b4b60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026b4b80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:20 +0000 UTC  }],Message:,Reason:,HostIP:72.2.119.135,PodIP:10.42.209.112,StartTime:2019-12-23 18:25:20 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-23 18:25:21 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://df86d98529759d1e1e347f220848e07a1f5233f6e3ea636eace45adec0694b65}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.355: INFO: Pod "nginx-deployment-7b8c6f4498-k9h6s" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-k9h6s,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-7b8c6f4498-k9h6s,UID:57594588-2187-4c71-9c77-b4e2e12ab4ae,ResourceVersion:21998,Generation:0,CreationTimestamp:2019-12-23 18:25:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.132.141/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f05dbc5a-eab4-4377-91e9-e78b98b20fe3 0xc0026b4c67 0xc0026b4c68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026b4cd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026b4cf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  }],Message:,Reason:,HostIP:72.2.118.151,PodIP:,StartTime:2019-12-23 18:25:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.356: INFO: Pod "nginx-deployment-7b8c6f4498-kmdbw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-kmdbw,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-7b8c6f4498-kmdbw,UID:b3e5c824-b4f7-4b17-8833-bf29325f0ca4,ResourceVersion:21672,Generation:0,CreationTimestamp:2019-12-23 18:25:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.206.114/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f05dbc5a-eab4-4377-91e9-e78b98b20fe3 0xc0026b4dc7 0xc0026b4dc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-control-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026b4e30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026b4e50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:20 +0000 UTC  }],Message:,Reason:,HostIP:72.2.119.160,PodIP:10.42.206.114,StartTime:2019-12-23 18:25:20 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-23 18:25:22 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://1e0bd8cc79c29713d7025bf91257c05bf7d3671c1fba3e7340303d9822db5549}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.359: INFO: Pod "nginx-deployment-7b8c6f4498-lvgnr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-lvgnr,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-7b8c6f4498-lvgnr,UID:655706da-f5ff-4fd9-9c7c-9da006d122e9,ResourceVersion:21881,Generation:0,CreationTimestamp:2019-12-23 18:25:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f05dbc5a-eab4-4377-91e9-e78b98b20fe3 0xc0026b4f37 0xc0026b4f38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-etcd-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026b4fa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026b4fc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  }],Message:,Reason:,HostIP:72.2.118.33,PodIP:,StartTime:2019-12-23 18:25:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.360: INFO: Pod "nginx-deployment-7b8c6f4498-m2vwg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-m2vwg,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-7b8c6f4498-m2vwg,UID:fed5ced0-8988-4aa6-9eed-af1894c4e94b,ResourceVersion:21908,Generation:0,CreationTimestamp:2019-12-23 18:25:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f05dbc5a-eab4-4377-91e9-e78b98b20fe3 0xc0026b5080 0xc0026b5081}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-control-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026b50e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026b5100}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  }],Message:,Reason:,HostIP:72.2.118.117,PodIP:,StartTime:2019-12-23 18:25:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.362: INFO: Pod "nginx-deployment-7b8c6f4498-m2xjx" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-m2xjx,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-7b8c6f4498-m2xjx,UID:3234df7c-88eb-44d1-abc4-a5233e8512b5,ResourceVersion:21683,Generation:0,CreationTimestamp:2019-12-23 18:25:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.92.10/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f05dbc5a-eab4-4377-91e9-e78b98b20fe3 0xc0026b51d7 0xc0026b51d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-control-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026b5240} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026b5260}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:21 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:20 +0000 UTC  }],Message:,Reason:,HostIP:72.2.118.117,PodIP:10.42.92.10,StartTime:2019-12-23 18:25:21 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-23 18:25:22 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://e37312519e00af649175f886b610971a0a8f3d6532459d60f7213444c36e1945}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.363: INFO: Pod "nginx-deployment-7b8c6f4498-mhg4g" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-mhg4g,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-7b8c6f4498-mhg4g,UID:598e7629-b75a-463c-9c50-dcfc711708e2,ResourceVersion:21677,Generation:0,CreationTimestamp:2019-12-23 18:25:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.108.205/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f05dbc5a-eab4-4377-91e9-e78b98b20fe3 0xc0026b5340 0xc0026b5341}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-etcd-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026b53a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026b53c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:21 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:21 +0000 UTC  }],Message:,Reason:,HostIP:72.2.118.33,PodIP:10.42.108.205,StartTime:2019-12-23 18:25:21 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-23 18:25:22 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://2eade260a564381dbec0e8cb8279b01022a6a860f694574213ddd1fe9c2b6419}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.364: INFO: Pod "nginx-deployment-7b8c6f4498-pchtw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-pchtw,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-7b8c6f4498-pchtw,UID:ad3b6b40-f65a-48f5-a317-396caa575f1c,ResourceVersion:22008,Generation:0,CreationTimestamp:2019-12-23 18:25:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.17.204/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f05dbc5a-eab4-4377-91e9-e78b98b20fe3 0xc0026b54a0 0xc0026b54a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026b5500} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026b5520}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  }],Message:,Reason:,HostIP:72.2.118.98,PodIP:,StartTime:2019-12-23 18:25:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.367: INFO: Pod "nginx-deployment-7b8c6f4498-ptkkp" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-ptkkp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-7b8c6f4498-ptkkp,UID:9eefb7f3-6d8d-4813-8cf2-a01472e5a9d4,ResourceVersion:21688,Generation:0,CreationTimestamp:2019-12-23 18:25:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.209.138/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f05dbc5a-eab4-4377-91e9-e78b98b20fe3 0xc0026b55f0 0xc0026b55f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-control-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026b5650} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026b5670}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:21 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:21 +0000 UTC  }],Message:,Reason:,HostIP:72.2.118.40,PodIP:10.42.209.138,StartTime:2019-12-23 18:25:21 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-23 18:25:22 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://dbe5d1bd432e282b25fd260520c51935c96b91da09c3192704e5affadc42d324}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.369: INFO: Pod "nginx-deployment-7b8c6f4498-r7hdm" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-r7hdm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-7b8c6f4498-r7hdm,UID:bb8a5ffe-6c04-4622-a748-44536d6b71cd,ResourceVersion:21662,Generation:0,CreationTimestamp:2019-12-23 18:25:21 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.17.202/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f05dbc5a-eab4-4377-91e9-e78b98b20fe3 0xc0026b5750 0xc0026b5751}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-worker-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026b57b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026b57d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:21 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:21 +0000 UTC  }],Message:,Reason:,HostIP:72.2.118.98,PodIP:10.42.17.202,StartTime:2019-12-23 18:25:21 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-12-23 18:25:22 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://35c4440173031477e67af33c4202ca0f9c6c6aac56d31b550d6a76b274154f2d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.370: INFO: Pod "nginx-deployment-7b8c6f4498-skxwr" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-skxwr,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-7b8c6f4498-skxwr,UID:71aa65c8-21cb-4955-968c-ea1dcc0a4615,ResourceVersion:21926,Generation:0,CreationTimestamp:2019-12-23 18:25:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f05dbc5a-eab4-4377-91e9-e78b98b20fe3 0xc0026b58a0 0xc0026b58a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-control-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026b5900} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026b5920}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  }],Message:,Reason:,HostIP:72.2.118.40,PodIP:,StartTime:2019-12-23 18:25:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.377: INFO: Pod "nginx-deployment-7b8c6f4498-ttgzd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-ttgzd,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-7b8c6f4498-ttgzd,UID:0d9b882f-e19d-4781-8e94-11130d2a5573,ResourceVersion:21966,Generation:0,CreationTimestamp:2019-12-23 18:25:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.209.114/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f05dbc5a-eab4-4377-91e9-e78b98b20fe3 0xc0026b59f0 0xc0026b59f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-etcd-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026b5a50} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026b5a70}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  }],Message:,Reason:,HostIP:72.2.119.135,PodIP:,StartTime:2019-12-23 18:25:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.379: INFO: Pod "nginx-deployment-7b8c6f4498-xpqj5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xpqj5,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-7b8c6f4498-xpqj5,UID:688c9525-5eea-4d5c-b12e-4a05458d0a13,ResourceVersion:21936,Generation:0,CreationTimestamp:2019-12-23 18:25:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f05dbc5a-eab4-4377-91e9-e78b98b20fe3 0xc0026b5b57 0xc0026b5b58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-worker-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026b5bc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026b5be0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  }],Message:,Reason:,HostIP:72.2.118.151,PodIP:,StartTime:2019-12-23 18:25:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Dec 23 18:25:29.380: INFO: Pod "nginx-deployment-7b8c6f4498-zbcrq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-zbcrq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-1808,SelfLink:/api/v1/namespaces/deployment-1808/pods/nginx-deployment-7b8c6f4498-zbcrq,UID:dd3ffddd-17aa-4530-8e94-7810f92041a8,ResourceVersion:21928,Generation:0,CreationTimestamp:2019-12-23 18:25:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f05dbc5a-eab4-4377-91e9-e78b98b20fe3 0xc0026b5ca7 0xc0026b5ca8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-z2pfc {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-z2pfc,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-z2pfc true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-etcd-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0026b5d10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0026b5d30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:25:27 +0000 UTC  }],Message:,Reason:,HostIP:72.2.119.25,PodIP:,StartTime:2019-12-23 18:25:27 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:25:29.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1808" for this suite.
Dec 23 18:25:37.488: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:25:37.789: INFO: namespace deployment-1808 deletion completed in 8.39315985s

• [SLOW TEST:16.924 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:25:37.791: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3854.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3854.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3854.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3854.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3854.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3854.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3854.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3854.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3854.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3854.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3854.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 22.247.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.247.22_udp@PTR;check="$$(dig +tcp +noall +answer +search 22.247.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.247.22_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3854.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3854.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3854.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3854.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3854.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3854.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3854.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3854.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3854.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3854.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3854.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 22.247.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.247.22_udp@PTR;check="$$(dig +tcp +noall +answer +search 22.247.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.247.22_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 23 18:25:58.012: INFO: Unable to read wheezy_udp@dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:25:58.019: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:25:58.025: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:25:58.031: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:25:58.074: INFO: Unable to read jessie_udp@dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:25:58.079: INFO: Unable to read jessie_tcp@dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:25:58.088: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:25:58.093: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:25:58.124: INFO: Lookups using dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d failed for: [wheezy_udp@dns-test-service.dns-3854.svc.cluster.local wheezy_tcp@dns-test-service.dns-3854.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local jessie_udp@dns-test-service.dns-3854.svc.cluster.local jessie_tcp@dns-test-service.dns-3854.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local]

Dec 23 18:26:03.133: INFO: Unable to read wheezy_udp@dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:26:03.141: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:26:03.146: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:26:03.153: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:26:03.200: INFO: Unable to read jessie_udp@dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:26:03.205: INFO: Unable to read jessie_tcp@dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:26:03.210: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:26:03.215: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:26:03.250: INFO: Lookups using dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d failed for: [wheezy_udp@dns-test-service.dns-3854.svc.cluster.local wheezy_tcp@dns-test-service.dns-3854.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local jessie_udp@dns-test-service.dns-3854.svc.cluster.local jessie_tcp@dns-test-service.dns-3854.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local]

Dec 23 18:26:08.135: INFO: Unable to read wheezy_udp@dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:26:08.141: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:26:08.146: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:26:08.152: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:26:08.198: INFO: Unable to read jessie_udp@dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:26:08.204: INFO: Unable to read jessie_tcp@dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:26:08.209: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:26:08.214: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:26:08.268: INFO: Lookups using dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d failed for: [wheezy_udp@dns-test-service.dns-3854.svc.cluster.local wheezy_tcp@dns-test-service.dns-3854.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local jessie_udp@dns-test-service.dns-3854.svc.cluster.local jessie_tcp@dns-test-service.dns-3854.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local]

Dec 23 18:26:13.132: INFO: Unable to read wheezy_udp@dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:26:13.141: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:26:13.155: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:26:13.202: INFO: Unable to read jessie_udp@dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:26:13.208: INFO: Unable to read jessie_tcp@dns-test-service.dns-3854.svc.cluster.local from pod dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d: the server could not find the requested resource (get pods dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d)
Dec 23 18:26:13.253: INFO: Lookups using dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d failed for: [wheezy_udp@dns-test-service.dns-3854.svc.cluster.local wheezy_tcp@dns-test-service.dns-3854.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3854.svc.cluster.local jessie_udp@dns-test-service.dns-3854.svc.cluster.local jessie_tcp@dns-test-service.dns-3854.svc.cluster.local]

Dec 23 18:26:18.243: INFO: DNS probes using dns-3854/dns-test-62fa3bfa-e762-416e-b678-918f5ccf286d succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:26:18.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3854" for this suite.
Dec 23 18:26:24.458: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:26:24.647: INFO: namespace dns-3854 deletion completed in 6.212370485s

• [SLOW TEST:46.857 seconds]
[sig-network] DNS
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:26:24.666: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec 23 18:26:24.783: INFO: Pod name pod-release: Found 0 pods out of 1
Dec 23 18:26:29.790: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:26:30.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1497" for this suite.
Dec 23 18:26:36.855: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:26:37.080: INFO: namespace replication-controller-1497 deletion completed in 6.249696272s

• [SLOW TEST:12.415 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:26:37.085: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 23 18:26:37.210: INFO: Waiting up to 5m0s for pod "downwardapi-volume-31bb6874-15a6-4ee7-8671-42871c1191cf" in namespace "projected-4955" to be "success or failure"
Dec 23 18:26:37.216: INFO: Pod "downwardapi-volume-31bb6874-15a6-4ee7-8671-42871c1191cf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.963618ms
Dec 23 18:26:39.226: INFO: Pod "downwardapi-volume-31bb6874-15a6-4ee7-8671-42871c1191cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015928665s
STEP: Saw pod success
Dec 23 18:26:39.226: INFO: Pod "downwardapi-volume-31bb6874-15a6-4ee7-8671-42871c1191cf" satisfied condition "success or failure"
Dec 23 18:26:39.231: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod downwardapi-volume-31bb6874-15a6-4ee7-8671-42871c1191cf container client-container: <nil>
STEP: delete the pod
Dec 23 18:26:39.265: INFO: Waiting for pod downwardapi-volume-31bb6874-15a6-4ee7-8671-42871c1191cf to disappear
Dec 23 18:26:39.271: INFO: Pod downwardapi-volume-31bb6874-15a6-4ee7-8671-42871c1191cf no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:26:39.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4955" for this suite.
Dec 23 18:26:45.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:26:45.538: INFO: namespace projected-4955 deletion completed in 6.255833579s

• [SLOW TEST:8.453 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:26:45.540: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 23 18:26:45.611: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec 23 18:26:50.618: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 23 18:26:50.618: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec 23 18:26:52.624: INFO: Creating deployment "test-rollover-deployment"
Dec 23 18:26:52.641: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec 23 18:26:54.654: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec 23 18:26:54.667: INFO: Ensure that both replica sets have 1 created replica
Dec 23 18:26:54.676: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec 23 18:26:54.692: INFO: Updating deployment test-rollover-deployment
Dec 23 18:26:54.694: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec 23 18:26:56.708: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec 23 18:26:56.717: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec 23 18:26:56.726: INFO: all replica sets need to contain the pod-template-hash label
Dec 23 18:26:56.726: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722414, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 18:26:58.738: INFO: all replica sets need to contain the pod-template-hash label
Dec 23 18:26:58.739: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722414, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 18:27:00.737: INFO: all replica sets need to contain the pod-template-hash label
Dec 23 18:27:00.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722414, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 18:27:02.737: INFO: all replica sets need to contain the pod-template-hash label
Dec 23 18:27:02.737: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722421, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 18:27:04.737: INFO: all replica sets need to contain the pod-template-hash label
Dec 23 18:27:04.738: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722421, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 18:27:06.738: INFO: all replica sets need to contain the pod-template-hash label
Dec 23 18:27:06.739: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722421, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 18:27:08.744: INFO: all replica sets need to contain the pod-template-hash label
Dec 23 18:27:08.746: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722421, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 18:27:10.738: INFO: all replica sets need to contain the pod-template-hash label
Dec 23 18:27:10.738: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722421, loc:(*time.Location)(0x7ed1a20)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712722412, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 23 18:27:12.737: INFO: 
Dec 23 18:27:12.737: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec 23 18:27:12.754: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-4203,SelfLink:/apis/apps/v1/namespaces/deployment-4203/deployments/test-rollover-deployment,UID:7efb6aa6-17c7-4bf6-8250-4015fb5b89e4,ResourceVersion:23057,Generation:2,CreationTimestamp:2019-12-23 18:26:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-12-23 18:26:52 +0000 UTC 2019-12-23 18:26:52 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-12-23 18:27:11 +0000 UTC 2019-12-23 18:26:52 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Dec 23 18:27:12.761: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-4203,SelfLink:/apis/apps/v1/namespaces/deployment-4203/replicasets/test-rollover-deployment-854595fc44,UID:8bcefe1c-c6ff-46be-82b7-13e54ef19bcd,ResourceVersion:23047,Generation:2,CreationTimestamp:2019-12-23 18:26:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 7efb6aa6-17c7-4bf6-8250-4015fb5b89e4 0xc0025f2487 0xc0025f2488}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Dec 23 18:27:12.762: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec 23 18:27:12.762: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-4203,SelfLink:/apis/apps/v1/namespaces/deployment-4203/replicasets/test-rollover-controller,UID:b68ab524-ce79-4f67-a4b3-bc2cc22ce647,ResourceVersion:23056,Generation:2,CreationTimestamp:2019-12-23 18:26:45 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 7efb6aa6-17c7-4bf6-8250-4015fb5b89e4 0xc0025f23b7 0xc0025f23b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 23 18:27:12.762: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-4203,SelfLink:/apis/apps/v1/namespaces/deployment-4203/replicasets/test-rollover-deployment-9b8b997cf,UID:3e384ff4-0a60-4955-b089-73a2442c1c74,ResourceVersion:22984,Generation:2,CreationTimestamp:2019-12-23 18:26:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 7efb6aa6-17c7-4bf6-8250-4015fb5b89e4 0xc0025f2550 0xc0025f2551}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 23 18:27:12.768: INFO: Pod "test-rollover-deployment-854595fc44-5wwpj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-5wwpj,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-4203,SelfLink:/api/v1/namespaces/deployment-4203/pods/test-rollover-deployment-854595fc44-5wwpj,UID:6a1ca153-7b54-4732-8607-3a12e312ddd7,ResourceVersion:23021,Generation:0,CreationTimestamp:2019-12-23 18:26:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.42.209.142/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 8bcefe1c-c6ff-46be-82b7-13e54ef19bcd 0xc0025f31c7 0xc0025f31c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-wpgzr {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-wpgzr,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-wpgzr true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-control-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025f3230} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025f3250}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:26:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:27:01 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:27:01 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:26:54 +0000 UTC  }],Message:,Reason:,HostIP:72.2.118.40,PodIP:10.42.209.142,StartTime:2019-12-23 18:26:54 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-12-23 18:27:00 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://2d5de32194bfe325096ee692f43b4427914a6b93a03e693aefa339e7838f3322}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:27:12.768: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4203" for this suite.
Dec 23 18:27:18.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:27:19.050: INFO: namespace deployment-4203 deletion completed in 6.265806953s

• [SLOW TEST:33.511 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:27:19.053: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-b094bfb9-984f-4445-9cd7-6d3b6e5db8eb
STEP: Creating a pod to test consume secrets
Dec 23 18:27:19.183: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e5618bf0-b20a-4aa8-b2e5-c72a4aa687c6" in namespace "projected-1681" to be "success or failure"
Dec 23 18:27:19.191: INFO: Pod "pod-projected-secrets-e5618bf0-b20a-4aa8-b2e5-c72a4aa687c6": Phase="Pending", Reason="", readiness=false. Elapsed: 7.946331ms
Dec 23 18:27:21.201: INFO: Pod "pod-projected-secrets-e5618bf0-b20a-4aa8-b2e5-c72a4aa687c6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018309285s
STEP: Saw pod success
Dec 23 18:27:21.202: INFO: Pod "pod-projected-secrets-e5618bf0-b20a-4aa8-b2e5-c72a4aa687c6" satisfied condition "success or failure"
Dec 23 18:27:21.207: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-projected-secrets-e5618bf0-b20a-4aa8-b2e5-c72a4aa687c6 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 23 18:27:21.237: INFO: Waiting for pod pod-projected-secrets-e5618bf0-b20a-4aa8-b2e5-c72a4aa687c6 to disappear
Dec 23 18:27:21.246: INFO: Pod pod-projected-secrets-e5618bf0-b20a-4aa8-b2e5-c72a4aa687c6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:27:21.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1681" for this suite.
Dec 23 18:27:27.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:27:27.491: INFO: namespace projected-1681 deletion completed in 6.234845278s

• [SLOW TEST:8.439 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:27:27.494: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1557
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 23 18:27:27.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/apps.v1 --namespace=kubectl-2529'
Dec 23 18:27:27.881: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 23 18:27:27.881: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1562
Dec 23 18:27:29.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 delete deployment e2e-test-nginx-deployment --namespace=kubectl-2529'
Dec 23 18:27:30.201: INFO: stderr: ""
Dec 23 18:27:30.201: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:27:30.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2529" for this suite.
Dec 23 18:27:52.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:27:52.443: INFO: namespace kubectl-2529 deletion completed in 22.225183162s

• [SLOW TEST:24.950 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:27:52.447: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-3735
I1223 18:27:52.568259      14 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-3735, replica count: 1
I1223 18:27:53.619992      14 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1223 18:27:54.620888      14 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 23 18:27:54.745: INFO: Created: latency-svc-t24rm
Dec 23 18:27:54.755: INFO: Got endpoints: latency-svc-t24rm [34.406859ms]
Dec 23 18:27:54.782: INFO: Created: latency-svc-gfxvk
Dec 23 18:27:54.787: INFO: Got endpoints: latency-svc-gfxvk [30.708557ms]
Dec 23 18:27:54.797: INFO: Created: latency-svc-9nd4g
Dec 23 18:27:54.801: INFO: Got endpoints: latency-svc-9nd4g [41.464606ms]
Dec 23 18:27:54.810: INFO: Created: latency-svc-vbtjv
Dec 23 18:27:54.813: INFO: Got endpoints: latency-svc-vbtjv [52.407688ms]
Dec 23 18:27:54.820: INFO: Created: latency-svc-fnx6v
Dec 23 18:27:54.829: INFO: Got endpoints: latency-svc-fnx6v [68.803878ms]
Dec 23 18:27:54.836: INFO: Created: latency-svc-lqvh7
Dec 23 18:27:54.842: INFO: Got endpoints: latency-svc-lqvh7 [83.851755ms]
Dec 23 18:27:54.850: INFO: Created: latency-svc-m24pf
Dec 23 18:27:54.854: INFO: Got endpoints: latency-svc-m24pf [94.296665ms]
Dec 23 18:27:54.863: INFO: Created: latency-svc-qqfwx
Dec 23 18:27:54.866: INFO: Got endpoints: latency-svc-qqfwx [105.959312ms]
Dec 23 18:27:54.873: INFO: Created: latency-svc-5m6kg
Dec 23 18:27:54.882: INFO: Got endpoints: latency-svc-5m6kg [122.105608ms]
Dec 23 18:27:54.891: INFO: Created: latency-svc-zm5dz
Dec 23 18:27:54.895: INFO: Got endpoints: latency-svc-zm5dz [134.902671ms]
Dec 23 18:27:54.905: INFO: Created: latency-svc-9nbr8
Dec 23 18:27:54.911: INFO: Got endpoints: latency-svc-9nbr8 [150.249266ms]
Dec 23 18:27:54.916: INFO: Created: latency-svc-wnvvc
Dec 23 18:27:54.923: INFO: Got endpoints: latency-svc-wnvvc [162.830457ms]
Dec 23 18:27:54.925: INFO: Created: latency-svc-2g62d
Dec 23 18:27:54.940: INFO: Got endpoints: latency-svc-2g62d [179.190078ms]
Dec 23 18:27:54.943: INFO: Created: latency-svc-rkj7k
Dec 23 18:27:54.950: INFO: Got endpoints: latency-svc-rkj7k [189.195542ms]
Dec 23 18:27:54.954: INFO: Created: latency-svc-tpvkv
Dec 23 18:27:54.963: INFO: Got endpoints: latency-svc-tpvkv [202.689897ms]
Dec 23 18:27:54.967: INFO: Created: latency-svc-dw9rm
Dec 23 18:27:54.977: INFO: Got endpoints: latency-svc-dw9rm [216.140421ms]
Dec 23 18:27:54.985: INFO: Created: latency-svc-jljh2
Dec 23 18:27:54.989: INFO: Got endpoints: latency-svc-jljh2 [201.272246ms]
Dec 23 18:27:54.997: INFO: Created: latency-svc-9c76t
Dec 23 18:27:55.002: INFO: Got endpoints: latency-svc-9c76t [200.444988ms]
Dec 23 18:27:55.010: INFO: Created: latency-svc-rr5nv
Dec 23 18:27:55.017: INFO: Got endpoints: latency-svc-rr5nv [204.015532ms]
Dec 23 18:27:55.026: INFO: Created: latency-svc-f4shq
Dec 23 18:27:55.038: INFO: Got endpoints: latency-svc-f4shq [209.009669ms]
Dec 23 18:27:55.040: INFO: Created: latency-svc-tdcsw
Dec 23 18:27:55.045: INFO: Got endpoints: latency-svc-tdcsw [202.568227ms]
Dec 23 18:27:55.053: INFO: Created: latency-svc-lwrtg
Dec 23 18:27:55.060: INFO: Got endpoints: latency-svc-lwrtg [205.820527ms]
Dec 23 18:27:55.068: INFO: Created: latency-svc-scwct
Dec 23 18:27:55.074: INFO: Got endpoints: latency-svc-scwct [207.399531ms]
Dec 23 18:27:55.081: INFO: Created: latency-svc-plvv8
Dec 23 18:27:55.086: INFO: Got endpoints: latency-svc-plvv8 [203.855372ms]
Dec 23 18:27:55.094: INFO: Created: latency-svc-9zzk4
Dec 23 18:27:55.100: INFO: Got endpoints: latency-svc-9zzk4 [204.752347ms]
Dec 23 18:27:55.108: INFO: Created: latency-svc-sp8wn
Dec 23 18:27:55.112: INFO: Got endpoints: latency-svc-sp8wn [201.076915ms]
Dec 23 18:27:55.119: INFO: Created: latency-svc-42zx8
Dec 23 18:27:55.123: INFO: Got endpoints: latency-svc-42zx8 [199.568122ms]
Dec 23 18:27:55.129: INFO: Created: latency-svc-vmqdn
Dec 23 18:27:55.137: INFO: Got endpoints: latency-svc-vmqdn [196.802269ms]
Dec 23 18:27:55.145: INFO: Created: latency-svc-hm45z
Dec 23 18:27:55.150: INFO: Got endpoints: latency-svc-hm45z [200.194665ms]
Dec 23 18:27:55.159: INFO: Created: latency-svc-8mvhz
Dec 23 18:27:55.162: INFO: Got endpoints: latency-svc-8mvhz [198.386249ms]
Dec 23 18:27:55.175: INFO: Created: latency-svc-vx2ps
Dec 23 18:27:55.186: INFO: Got endpoints: latency-svc-vx2ps [208.646308ms]
Dec 23 18:27:55.196: INFO: Created: latency-svc-87s74
Dec 23 18:27:55.200: INFO: Got endpoints: latency-svc-87s74 [211.0736ms]
Dec 23 18:27:55.208: INFO: Created: latency-svc-qvvgm
Dec 23 18:27:55.221: INFO: Got endpoints: latency-svc-qvvgm [219.361818ms]
Dec 23 18:27:55.229: INFO: Created: latency-svc-ppnbc
Dec 23 18:27:55.233: INFO: Got endpoints: latency-svc-ppnbc [214.943139ms]
Dec 23 18:27:55.246: INFO: Created: latency-svc-97czs
Dec 23 18:27:55.252: INFO: Got endpoints: latency-svc-97czs [213.57196ms]
Dec 23 18:27:55.258: INFO: Created: latency-svc-sn8gg
Dec 23 18:27:55.264: INFO: Got endpoints: latency-svc-sn8gg [219.598462ms]
Dec 23 18:27:55.275: INFO: Created: latency-svc-9kq2b
Dec 23 18:27:55.279: INFO: Got endpoints: latency-svc-9kq2b [218.641649ms]
Dec 23 18:27:55.288: INFO: Created: latency-svc-jj9wb
Dec 23 18:27:55.293: INFO: Got endpoints: latency-svc-jj9wb [218.929169ms]
Dec 23 18:27:55.302: INFO: Created: latency-svc-rb2d8
Dec 23 18:27:55.306: INFO: Got endpoints: latency-svc-rb2d8 [219.244313ms]
Dec 23 18:27:55.311: INFO: Created: latency-svc-s968m
Dec 23 18:27:55.317: INFO: Got endpoints: latency-svc-s968m [216.296142ms]
Dec 23 18:27:55.323: INFO: Created: latency-svc-zmh2k
Dec 23 18:27:55.333: INFO: Got endpoints: latency-svc-zmh2k [220.661852ms]
Dec 23 18:27:55.335: INFO: Created: latency-svc-2drn2
Dec 23 18:27:55.342: INFO: Got endpoints: latency-svc-2drn2 [218.71962ms]
Dec 23 18:27:55.345: INFO: Created: latency-svc-gwf5n
Dec 23 18:27:55.371: INFO: Created: latency-svc-wn4fm
Dec 23 18:27:55.405: INFO: Got endpoints: latency-svc-gwf5n [267.619885ms]
Dec 23 18:27:55.412: INFO: Created: latency-svc-jjgfc
Dec 23 18:27:55.433: INFO: Created: latency-svc-svb94
Dec 23 18:27:55.442: INFO: Got endpoints: latency-svc-wn4fm [291.201931ms]
Dec 23 18:27:55.445: INFO: Created: latency-svc-jwkmm
Dec 23 18:27:55.457: INFO: Created: latency-svc-9qvk7
Dec 23 18:27:55.468: INFO: Created: latency-svc-kcdqz
Dec 23 18:27:55.480: INFO: Created: latency-svc-bns2z
Dec 23 18:27:55.491: INFO: Created: latency-svc-lmtnp
Dec 23 18:27:55.501: INFO: Created: latency-svc-78c4r
Dec 23 18:27:55.511: INFO: Got endpoints: latency-svc-jjgfc [348.776891ms]
Dec 23 18:27:55.515: INFO: Created: latency-svc-xjvb5
Dec 23 18:27:55.529: INFO: Created: latency-svc-d5cl4
Dec 23 18:27:55.538: INFO: Created: latency-svc-m6wll
Dec 23 18:27:55.545: INFO: Created: latency-svc-zcf7z
Dec 23 18:27:55.554: INFO: Created: latency-svc-mhrh6
Dec 23 18:27:55.563: INFO: Created: latency-svc-6vftr
Dec 23 18:27:55.566: INFO: Got endpoints: latency-svc-svb94 [380.178139ms]
Dec 23 18:27:55.576: INFO: Created: latency-svc-mqxfx
Dec 23 18:27:55.584: INFO: Created: latency-svc-rx9bs
Dec 23 18:27:55.594: INFO: Got endpoints: latency-svc-jwkmm [393.948389ms]
Dec 23 18:27:55.598: INFO: Created: latency-svc-gxjx4
Dec 23 18:27:55.610: INFO: Created: latency-svc-frn6n
Dec 23 18:27:55.642: INFO: Got endpoints: latency-svc-9qvk7 [421.095137ms]
Dec 23 18:27:55.690: INFO: Created: latency-svc-jf6dn
Dec 23 18:27:55.693: INFO: Got endpoints: latency-svc-kcdqz [460.177713ms]
Dec 23 18:27:55.707: INFO: Created: latency-svc-7729h
Dec 23 18:27:55.742: INFO: Got endpoints: latency-svc-bns2z [489.989488ms]
Dec 23 18:27:55.758: INFO: Created: latency-svc-t4prd
Dec 23 18:27:55.793: INFO: Got endpoints: latency-svc-lmtnp [528.549336ms]
Dec 23 18:27:55.809: INFO: Created: latency-svc-zhh9t
Dec 23 18:27:55.842: INFO: Got endpoints: latency-svc-78c4r [562.556095ms]
Dec 23 18:27:55.858: INFO: Created: latency-svc-xd9cj
Dec 23 18:27:55.893: INFO: Got endpoints: latency-svc-xjvb5 [599.987233ms]
Dec 23 18:27:55.909: INFO: Created: latency-svc-8rwtx
Dec 23 18:27:55.942: INFO: Got endpoints: latency-svc-d5cl4 [636.083518ms]
Dec 23 18:27:55.957: INFO: Created: latency-svc-sztt2
Dec 23 18:27:55.994: INFO: Got endpoints: latency-svc-m6wll [677.396226ms]
Dec 23 18:27:56.009: INFO: Created: latency-svc-5tm5k
Dec 23 18:27:56.044: INFO: Got endpoints: latency-svc-zcf7z [711.40251ms]
Dec 23 18:27:56.063: INFO: Created: latency-svc-x8gtr
Dec 23 18:27:56.094: INFO: Got endpoints: latency-svc-mhrh6 [751.699561ms]
Dec 23 18:27:56.111: INFO: Created: latency-svc-j67vf
Dec 23 18:27:56.143: INFO: Got endpoints: latency-svc-6vftr [737.852657ms]
Dec 23 18:27:56.159: INFO: Created: latency-svc-2qg29
Dec 23 18:27:56.194: INFO: Got endpoints: latency-svc-mqxfx [752.402309ms]
Dec 23 18:27:56.211: INFO: Created: latency-svc-s5vt2
Dec 23 18:27:56.247: INFO: Got endpoints: latency-svc-rx9bs [735.350069ms]
Dec 23 18:27:56.262: INFO: Created: latency-svc-brgt9
Dec 23 18:27:56.293: INFO: Got endpoints: latency-svc-gxjx4 [726.321882ms]
Dec 23 18:27:56.306: INFO: Created: latency-svc-vhjl2
Dec 23 18:27:56.343: INFO: Got endpoints: latency-svc-frn6n [748.708412ms]
Dec 23 18:27:56.357: INFO: Created: latency-svc-64whr
Dec 23 18:27:56.394: INFO: Got endpoints: latency-svc-jf6dn [751.009562ms]
Dec 23 18:27:56.434: INFO: Created: latency-svc-pslrg
Dec 23 18:27:56.463: INFO: Got endpoints: latency-svc-7729h [769.932202ms]
Dec 23 18:27:56.512: INFO: Got endpoints: latency-svc-t4prd [769.83612ms]
Dec 23 18:27:56.516: INFO: Created: latency-svc-7h2z7
Dec 23 18:27:56.553: INFO: Got endpoints: latency-svc-zhh9t [760.18158ms]
Dec 23 18:27:56.556: INFO: Created: latency-svc-tzsgc
Dec 23 18:27:56.572: INFO: Created: latency-svc-qsmpl
Dec 23 18:27:56.648: INFO: Got endpoints: latency-svc-8rwtx [754.324823ms]
Dec 23 18:27:56.649: INFO: Got endpoints: latency-svc-xd9cj [806.939409ms]
Dec 23 18:27:56.674: INFO: Created: latency-svc-xlpc5
Dec 23 18:27:56.682: INFO: Created: latency-svc-9xhh7
Dec 23 18:27:56.880: INFO: Got endpoints: latency-svc-sztt2 [938.306065ms]
Dec 23 18:27:56.950: INFO: Got endpoints: latency-svc-x8gtr [905.429728ms]
Dec 23 18:27:56.952: INFO: Got endpoints: latency-svc-5tm5k [957.307483ms]
Dec 23 18:27:56.953: INFO: Got endpoints: latency-svc-j67vf [858.641215ms]
Dec 23 18:27:56.953: INFO: Got endpoints: latency-svc-2qg29 [810.557579ms]
Dec 23 18:27:56.959: INFO: Got endpoints: latency-svc-s5vt2 [764.219771ms]
Dec 23 18:27:56.975: INFO: Created: latency-svc-tjn4x
Dec 23 18:27:56.997: INFO: Created: latency-svc-rwnkt
Dec 23 18:27:57.004: INFO: Got endpoints: latency-svc-brgt9 [757.054087ms]
Dec 23 18:27:57.015: INFO: Created: latency-svc-m9gj5
Dec 23 18:27:57.042: INFO: Created: latency-svc-bsb5t
Dec 23 18:27:57.049: INFO: Got endpoints: latency-svc-vhjl2 [756.62518ms]
Dec 23 18:27:57.058: INFO: Created: latency-svc-sqfv2
Dec 23 18:27:57.069: INFO: Created: latency-svc-qmhdj
Dec 23 18:27:57.099: INFO: Got endpoints: latency-svc-64whr [756.523307ms]
Dec 23 18:27:57.101: INFO: Created: latency-svc-945b9
Dec 23 18:27:57.101: INFO: Created: latency-svc-nhmsk
Dec 23 18:27:57.117: INFO: Created: latency-svc-jqwvc
Dec 23 18:27:57.141: INFO: Got endpoints: latency-svc-pslrg [747.631436ms]
Dec 23 18:27:57.158: INFO: Created: latency-svc-5thwm
Dec 23 18:27:57.197: INFO: Got endpoints: latency-svc-7h2z7 [733.36105ms]
Dec 23 18:27:57.214: INFO: Created: latency-svc-hdgvj
Dec 23 18:27:57.243: INFO: Got endpoints: latency-svc-tzsgc [731.428825ms]
Dec 23 18:27:57.259: INFO: Created: latency-svc-v78lg
Dec 23 18:27:57.298: INFO: Got endpoints: latency-svc-qsmpl [744.154983ms]
Dec 23 18:27:57.315: INFO: Created: latency-svc-mj8zr
Dec 23 18:27:57.343: INFO: Got endpoints: latency-svc-xlpc5 [694.981001ms]
Dec 23 18:27:57.358: INFO: Created: latency-svc-9fxfk
Dec 23 18:27:57.392: INFO: Got endpoints: latency-svc-9xhh7 [743.254621ms]
Dec 23 18:27:57.409: INFO: Created: latency-svc-vtcfc
Dec 23 18:27:57.445: INFO: Got endpoints: latency-svc-tjn4x [564.248597ms]
Dec 23 18:27:57.463: INFO: Created: latency-svc-jcv8q
Dec 23 18:27:57.493: INFO: Got endpoints: latency-svc-rwnkt [542.368126ms]
Dec 23 18:27:57.513: INFO: Created: latency-svc-snjwd
Dec 23 18:27:57.543: INFO: Got endpoints: latency-svc-m9gj5 [589.094026ms]
Dec 23 18:27:57.558: INFO: Created: latency-svc-skhgg
Dec 23 18:27:57.599: INFO: Got endpoints: latency-svc-bsb5t [646.825039ms]
Dec 23 18:27:57.615: INFO: Created: latency-svc-phvqh
Dec 23 18:27:57.643: INFO: Got endpoints: latency-svc-sqfv2 [690.265633ms]
Dec 23 18:27:57.659: INFO: Created: latency-svc-8jdts
Dec 23 18:27:57.696: INFO: Got endpoints: latency-svc-qmhdj [736.819403ms]
Dec 23 18:27:57.714: INFO: Created: latency-svc-dwttz
Dec 23 18:27:57.743: INFO: Got endpoints: latency-svc-945b9 [738.660174ms]
Dec 23 18:27:57.758: INFO: Created: latency-svc-74s86
Dec 23 18:27:57.793: INFO: Got endpoints: latency-svc-nhmsk [743.768419ms]
Dec 23 18:27:57.809: INFO: Created: latency-svc-ntnw2
Dec 23 18:27:57.847: INFO: Got endpoints: latency-svc-jqwvc [747.10464ms]
Dec 23 18:27:57.873: INFO: Created: latency-svc-kkbf7
Dec 23 18:27:57.895: INFO: Got endpoints: latency-svc-5thwm [753.856619ms]
Dec 23 18:27:57.914: INFO: Created: latency-svc-r865n
Dec 23 18:27:57.944: INFO: Got endpoints: latency-svc-hdgvj [746.95671ms]
Dec 23 18:27:57.965: INFO: Created: latency-svc-w9kfk
Dec 23 18:27:57.994: INFO: Got endpoints: latency-svc-v78lg [750.366735ms]
Dec 23 18:27:58.012: INFO: Created: latency-svc-47jmz
Dec 23 18:27:58.046: INFO: Got endpoints: latency-svc-mj8zr [748.129685ms]
Dec 23 18:27:58.062: INFO: Created: latency-svc-xn7s9
Dec 23 18:27:58.095: INFO: Got endpoints: latency-svc-9fxfk [752.507127ms]
Dec 23 18:27:58.113: INFO: Created: latency-svc-tltvl
Dec 23 18:27:58.144: INFO: Got endpoints: latency-svc-vtcfc [751.767199ms]
Dec 23 18:27:58.159: INFO: Created: latency-svc-85lfq
Dec 23 18:27:58.195: INFO: Got endpoints: latency-svc-jcv8q [750.183562ms]
Dec 23 18:27:58.211: INFO: Created: latency-svc-vp8g2
Dec 23 18:27:58.244: INFO: Got endpoints: latency-svc-snjwd [751.054502ms]
Dec 23 18:27:58.271: INFO: Created: latency-svc-w72xc
Dec 23 18:27:58.293: INFO: Got endpoints: latency-svc-skhgg [750.538308ms]
Dec 23 18:27:58.312: INFO: Created: latency-svc-2nbbh
Dec 23 18:27:58.343: INFO: Got endpoints: latency-svc-phvqh [743.854945ms]
Dec 23 18:27:58.359: INFO: Created: latency-svc-m455n
Dec 23 18:27:58.392: INFO: Got endpoints: latency-svc-8jdts [748.933811ms]
Dec 23 18:27:58.409: INFO: Created: latency-svc-8tk2c
Dec 23 18:27:58.442: INFO: Got endpoints: latency-svc-dwttz [746.220497ms]
Dec 23 18:27:58.459: INFO: Created: latency-svc-54m2g
Dec 23 18:27:58.512: INFO: Got endpoints: latency-svc-74s86 [768.491449ms]
Dec 23 18:27:58.530: INFO: Created: latency-svc-xrk5f
Dec 23 18:27:58.542: INFO: Got endpoints: latency-svc-ntnw2 [748.599571ms]
Dec 23 18:27:58.558: INFO: Created: latency-svc-f9sw6
Dec 23 18:27:58.595: INFO: Got endpoints: latency-svc-kkbf7 [747.713293ms]
Dec 23 18:27:58.611: INFO: Created: latency-svc-5qpqn
Dec 23 18:27:58.642: INFO: Got endpoints: latency-svc-r865n [746.498564ms]
Dec 23 18:27:58.659: INFO: Created: latency-svc-f5cd2
Dec 23 18:27:58.693: INFO: Got endpoints: latency-svc-w9kfk [749.037588ms]
Dec 23 18:27:58.712: INFO: Created: latency-svc-dhjwq
Dec 23 18:27:58.742: INFO: Got endpoints: latency-svc-47jmz [748.230715ms]
Dec 23 18:27:58.758: INFO: Created: latency-svc-8smds
Dec 23 18:27:58.793: INFO: Got endpoints: latency-svc-xn7s9 [746.725314ms]
Dec 23 18:27:58.809: INFO: Created: latency-svc-fxz5q
Dec 23 18:27:58.844: INFO: Got endpoints: latency-svc-tltvl [748.706114ms]
Dec 23 18:27:58.867: INFO: Created: latency-svc-tq4xr
Dec 23 18:27:58.894: INFO: Got endpoints: latency-svc-85lfq [749.228126ms]
Dec 23 18:27:58.911: INFO: Created: latency-svc-xg4ww
Dec 23 18:27:58.944: INFO: Got endpoints: latency-svc-vp8g2 [748.901901ms]
Dec 23 18:27:58.962: INFO: Created: latency-svc-n74k2
Dec 23 18:27:58.994: INFO: Got endpoints: latency-svc-w72xc [749.948059ms]
Dec 23 18:27:59.009: INFO: Created: latency-svc-grlh8
Dec 23 18:27:59.045: INFO: Got endpoints: latency-svc-2nbbh [751.495341ms]
Dec 23 18:27:59.063: INFO: Created: latency-svc-w4djx
Dec 23 18:27:59.094: INFO: Got endpoints: latency-svc-m455n [750.914924ms]
Dec 23 18:27:59.111: INFO: Created: latency-svc-pqvq4
Dec 23 18:27:59.145: INFO: Got endpoints: latency-svc-8tk2c [752.20691ms]
Dec 23 18:27:59.165: INFO: Created: latency-svc-f2pxt
Dec 23 18:27:59.195: INFO: Got endpoints: latency-svc-54m2g [752.418401ms]
Dec 23 18:27:59.210: INFO: Created: latency-svc-r7kjz
Dec 23 18:27:59.256: INFO: Got endpoints: latency-svc-xrk5f [744.004495ms]
Dec 23 18:27:59.277: INFO: Created: latency-svc-shgn6
Dec 23 18:27:59.292: INFO: Got endpoints: latency-svc-f9sw6 [750.097142ms]
Dec 23 18:27:59.308: INFO: Created: latency-svc-d7vp4
Dec 23 18:27:59.345: INFO: Got endpoints: latency-svc-5qpqn [750.134122ms]
Dec 23 18:27:59.360: INFO: Created: latency-svc-n4z59
Dec 23 18:27:59.395: INFO: Got endpoints: latency-svc-f5cd2 [752.94101ms]
Dec 23 18:27:59.411: INFO: Created: latency-svc-b492m
Dec 23 18:27:59.444: INFO: Got endpoints: latency-svc-dhjwq [751.046231ms]
Dec 23 18:27:59.460: INFO: Created: latency-svc-r8zsc
Dec 23 18:27:59.495: INFO: Got endpoints: latency-svc-8smds [753.014073ms]
Dec 23 18:27:59.526: INFO: Created: latency-svc-9ssk6
Dec 23 18:27:59.544: INFO: Got endpoints: latency-svc-fxz5q [750.746801ms]
Dec 23 18:27:59.570: INFO: Created: latency-svc-7bzpm
Dec 23 18:27:59.592: INFO: Got endpoints: latency-svc-tq4xr [747.827103ms]
Dec 23 18:27:59.615: INFO: Created: latency-svc-sqdqk
Dec 23 18:27:59.642: INFO: Got endpoints: latency-svc-xg4ww [748.24349ms]
Dec 23 18:27:59.657: INFO: Created: latency-svc-wrw96
Dec 23 18:27:59.697: INFO: Got endpoints: latency-svc-n74k2 [753.081623ms]
Dec 23 18:27:59.714: INFO: Created: latency-svc-s7kns
Dec 23 18:27:59.743: INFO: Got endpoints: latency-svc-grlh8 [748.326553ms]
Dec 23 18:27:59.758: INFO: Created: latency-svc-jqc2f
Dec 23 18:27:59.793: INFO: Got endpoints: latency-svc-w4djx [747.65256ms]
Dec 23 18:27:59.809: INFO: Created: latency-svc-wqcqv
Dec 23 18:27:59.844: INFO: Got endpoints: latency-svc-pqvq4 [749.399433ms]
Dec 23 18:27:59.860: INFO: Created: latency-svc-st956
Dec 23 18:27:59.893: INFO: Got endpoints: latency-svc-f2pxt [748.241251ms]
Dec 23 18:27:59.910: INFO: Created: latency-svc-trmk5
Dec 23 18:27:59.942: INFO: Got endpoints: latency-svc-r7kjz [747.843026ms]
Dec 23 18:27:59.963: INFO: Created: latency-svc-jsmb9
Dec 23 18:27:59.993: INFO: Got endpoints: latency-svc-shgn6 [737.131968ms]
Dec 23 18:28:00.010: INFO: Created: latency-svc-fkzsf
Dec 23 18:28:00.044: INFO: Got endpoints: latency-svc-d7vp4 [751.866641ms]
Dec 23 18:28:00.066: INFO: Created: latency-svc-c7k6k
Dec 23 18:28:00.099: INFO: Got endpoints: latency-svc-n4z59 [753.940228ms]
Dec 23 18:28:00.120: INFO: Created: latency-svc-r7brr
Dec 23 18:28:00.145: INFO: Got endpoints: latency-svc-b492m [749.922106ms]
Dec 23 18:28:00.165: INFO: Created: latency-svc-8qmkp
Dec 23 18:28:00.194: INFO: Got endpoints: latency-svc-r8zsc [749.426454ms]
Dec 23 18:28:00.209: INFO: Created: latency-svc-dqrvv
Dec 23 18:28:00.244: INFO: Got endpoints: latency-svc-9ssk6 [748.559205ms]
Dec 23 18:28:00.260: INFO: Created: latency-svc-5tn4p
Dec 23 18:28:00.294: INFO: Got endpoints: latency-svc-7bzpm [750.456285ms]
Dec 23 18:28:00.311: INFO: Created: latency-svc-bg252
Dec 23 18:28:00.344: INFO: Got endpoints: latency-svc-sqdqk [751.651377ms]
Dec 23 18:28:00.359: INFO: Created: latency-svc-lwn4v
Dec 23 18:28:00.393: INFO: Got endpoints: latency-svc-wrw96 [750.987538ms]
Dec 23 18:28:00.414: INFO: Created: latency-svc-mbj2k
Dec 23 18:28:00.467: INFO: Got endpoints: latency-svc-s7kns [769.310549ms]
Dec 23 18:28:00.499: INFO: Got endpoints: latency-svc-jqc2f [756.421526ms]
Dec 23 18:28:00.523: INFO: Created: latency-svc-kt5ds
Dec 23 18:28:00.532: INFO: Created: latency-svc-dwnjr
Dec 23 18:28:00.542: INFO: Got endpoints: latency-svc-wqcqv [749.091857ms]
Dec 23 18:28:00.557: INFO: Created: latency-svc-h8mv2
Dec 23 18:28:00.597: INFO: Got endpoints: latency-svc-st956 [752.752345ms]
Dec 23 18:28:00.615: INFO: Created: latency-svc-rtt44
Dec 23 18:28:00.644: INFO: Got endpoints: latency-svc-trmk5 [751.025958ms]
Dec 23 18:28:00.660: INFO: Created: latency-svc-gv5l4
Dec 23 18:28:00.694: INFO: Got endpoints: latency-svc-jsmb9 [751.7158ms]
Dec 23 18:28:00.710: INFO: Created: latency-svc-pbp5d
Dec 23 18:28:00.743: INFO: Got endpoints: latency-svc-fkzsf [749.977577ms]
Dec 23 18:28:00.758: INFO: Created: latency-svc-8dm8x
Dec 23 18:28:00.793: INFO: Got endpoints: latency-svc-c7k6k [749.094682ms]
Dec 23 18:28:00.809: INFO: Created: latency-svc-xn4tv
Dec 23 18:28:00.843: INFO: Got endpoints: latency-svc-r7brr [744.20199ms]
Dec 23 18:28:00.858: INFO: Created: latency-svc-5p74f
Dec 23 18:28:00.894: INFO: Got endpoints: latency-svc-8qmkp [748.376585ms]
Dec 23 18:28:00.909: INFO: Created: latency-svc-fzkvb
Dec 23 18:28:00.942: INFO: Got endpoints: latency-svc-dqrvv [748.347206ms]
Dec 23 18:28:00.957: INFO: Created: latency-svc-vsrfp
Dec 23 18:28:00.993: INFO: Got endpoints: latency-svc-5tn4p [748.336803ms]
Dec 23 18:28:01.010: INFO: Created: latency-svc-8dq7q
Dec 23 18:28:01.045: INFO: Got endpoints: latency-svc-bg252 [750.782951ms]
Dec 23 18:28:01.060: INFO: Created: latency-svc-nd6qk
Dec 23 18:28:01.094: INFO: Got endpoints: latency-svc-lwn4v [749.584286ms]
Dec 23 18:28:01.108: INFO: Created: latency-svc-wmq7l
Dec 23 18:28:01.142: INFO: Got endpoints: latency-svc-mbj2k [748.449543ms]
Dec 23 18:28:01.159: INFO: Created: latency-svc-9gtk8
Dec 23 18:28:01.194: INFO: Got endpoints: latency-svc-kt5ds [726.373091ms]
Dec 23 18:28:01.217: INFO: Created: latency-svc-8zfmb
Dec 23 18:28:01.245: INFO: Got endpoints: latency-svc-dwnjr [745.428926ms]
Dec 23 18:28:01.271: INFO: Created: latency-svc-gqr42
Dec 23 18:28:01.293: INFO: Got endpoints: latency-svc-h8mv2 [751.329728ms]
Dec 23 18:28:01.308: INFO: Created: latency-svc-vfvss
Dec 23 18:28:01.343: INFO: Got endpoints: latency-svc-rtt44 [746.800243ms]
Dec 23 18:28:01.361: INFO: Created: latency-svc-blnvr
Dec 23 18:28:01.406: INFO: Got endpoints: latency-svc-gv5l4 [760.870915ms]
Dec 23 18:28:01.433: INFO: Created: latency-svc-7rvsr
Dec 23 18:28:01.449: INFO: Got endpoints: latency-svc-pbp5d [754.71716ms]
Dec 23 18:28:01.468: INFO: Created: latency-svc-8hl88
Dec 23 18:28:01.494: INFO: Got endpoints: latency-svc-8dm8x [750.232962ms]
Dec 23 18:28:01.518: INFO: Created: latency-svc-dg8m6
Dec 23 18:28:01.701: INFO: Got endpoints: latency-svc-vsrfp [758.393881ms]
Dec 23 18:28:01.702: INFO: Got endpoints: latency-svc-xn4tv [908.706001ms]
Dec 23 18:28:01.704: INFO: Got endpoints: latency-svc-5p74f [860.772954ms]
Dec 23 18:28:01.707: INFO: Got endpoints: latency-svc-fzkvb [813.596891ms]
Dec 23 18:28:01.717: INFO: Created: latency-svc-krns4
Dec 23 18:28:01.727: INFO: Created: latency-svc-4p99j
Dec 23 18:28:01.736: INFO: Created: latency-svc-vdhrc
Dec 23 18:28:01.744: INFO: Got endpoints: latency-svc-8dq7q [750.972512ms]
Dec 23 18:28:01.746: INFO: Created: latency-svc-wdc4w
Dec 23 18:28:01.759: INFO: Created: latency-svc-ccqt7
Dec 23 18:28:01.792: INFO: Got endpoints: latency-svc-nd6qk [746.554492ms]
Dec 23 18:28:01.807: INFO: Created: latency-svc-hbld9
Dec 23 18:28:01.843: INFO: Got endpoints: latency-svc-wmq7l [749.348651ms]
Dec 23 18:28:01.860: INFO: Created: latency-svc-mv9gq
Dec 23 18:28:01.893: INFO: Got endpoints: latency-svc-9gtk8 [750.457411ms]
Dec 23 18:28:01.907: INFO: Created: latency-svc-p5g58
Dec 23 18:28:01.942: INFO: Got endpoints: latency-svc-8zfmb [747.842954ms]
Dec 23 18:28:01.957: INFO: Created: latency-svc-bhvff
Dec 23 18:28:01.992: INFO: Got endpoints: latency-svc-gqr42 [747.075524ms]
Dec 23 18:28:02.006: INFO: Created: latency-svc-wzpfl
Dec 23 18:28:02.048: INFO: Got endpoints: latency-svc-vfvss [754.246143ms]
Dec 23 18:28:02.064: INFO: Created: latency-svc-9mm62
Dec 23 18:28:02.094: INFO: Got endpoints: latency-svc-blnvr [750.436991ms]
Dec 23 18:28:02.110: INFO: Created: latency-svc-zg55h
Dec 23 18:28:02.142: INFO: Got endpoints: latency-svc-7rvsr [736.371411ms]
Dec 23 18:28:02.159: INFO: Created: latency-svc-wjjp4
Dec 23 18:28:02.194: INFO: Got endpoints: latency-svc-8hl88 [744.220367ms]
Dec 23 18:28:02.211: INFO: Created: latency-svc-d98vf
Dec 23 18:28:02.244: INFO: Got endpoints: latency-svc-dg8m6 [749.813275ms]
Dec 23 18:28:02.260: INFO: Created: latency-svc-zs2fg
Dec 23 18:28:02.291: INFO: Got endpoints: latency-svc-krns4 [590.036325ms]
Dec 23 18:28:02.307: INFO: Created: latency-svc-4996x
Dec 23 18:28:02.343: INFO: Got endpoints: latency-svc-4p99j [640.513514ms]
Dec 23 18:28:02.358: INFO: Created: latency-svc-6v6mf
Dec 23 18:28:02.394: INFO: Got endpoints: latency-svc-vdhrc [688.865209ms]
Dec 23 18:28:02.410: INFO: Created: latency-svc-gv9ln
Dec 23 18:28:02.446: INFO: Got endpoints: latency-svc-wdc4w [737.306911ms]
Dec 23 18:28:02.464: INFO: Created: latency-svc-wm2rd
Dec 23 18:28:02.494: INFO: Got endpoints: latency-svc-ccqt7 [750.588072ms]
Dec 23 18:28:02.509: INFO: Created: latency-svc-2fhbv
Dec 23 18:28:02.542: INFO: Got endpoints: latency-svc-hbld9 [750.020778ms]
Dec 23 18:28:02.557: INFO: Created: latency-svc-6zz94
Dec 23 18:28:02.595: INFO: Got endpoints: latency-svc-mv9gq [751.805426ms]
Dec 23 18:28:02.646: INFO: Got endpoints: latency-svc-p5g58 [752.853584ms]
Dec 23 18:28:02.693: INFO: Got endpoints: latency-svc-bhvff [750.874115ms]
Dec 23 18:28:02.744: INFO: Got endpoints: latency-svc-wzpfl [751.960489ms]
Dec 23 18:28:02.796: INFO: Got endpoints: latency-svc-9mm62 [747.947044ms]
Dec 23 18:28:02.844: INFO: Got endpoints: latency-svc-zg55h [749.684228ms]
Dec 23 18:28:02.894: INFO: Got endpoints: latency-svc-wjjp4 [751.547634ms]
Dec 23 18:28:02.944: INFO: Got endpoints: latency-svc-d98vf [750.502556ms]
Dec 23 18:28:02.993: INFO: Got endpoints: latency-svc-zs2fg [749.198719ms]
Dec 23 18:28:03.045: INFO: Got endpoints: latency-svc-4996x [753.14308ms]
Dec 23 18:28:03.092: INFO: Got endpoints: latency-svc-6v6mf [749.472575ms]
Dec 23 18:28:03.143: INFO: Got endpoints: latency-svc-gv9ln [749.060542ms]
Dec 23 18:28:03.193: INFO: Got endpoints: latency-svc-wm2rd [747.168797ms]
Dec 23 18:28:03.246: INFO: Got endpoints: latency-svc-2fhbv [751.815967ms]
Dec 23 18:28:03.295: INFO: Got endpoints: latency-svc-6zz94 [753.162783ms]
Dec 23 18:28:03.295: INFO: Latencies: [30.708557ms 41.464606ms 52.407688ms 68.803878ms 83.851755ms 94.296665ms 105.959312ms 122.105608ms 134.902671ms 150.249266ms 162.830457ms 179.190078ms 189.195542ms 196.802269ms 198.386249ms 199.568122ms 200.194665ms 200.444988ms 201.076915ms 201.272246ms 202.568227ms 202.689897ms 203.855372ms 204.015532ms 204.752347ms 205.820527ms 207.399531ms 208.646308ms 209.009669ms 211.0736ms 213.57196ms 214.943139ms 216.140421ms 216.296142ms 218.641649ms 218.71962ms 218.929169ms 219.244313ms 219.361818ms 219.598462ms 220.661852ms 267.619885ms 291.201931ms 348.776891ms 380.178139ms 393.948389ms 421.095137ms 460.177713ms 489.989488ms 528.549336ms 542.368126ms 562.556095ms 564.248597ms 589.094026ms 590.036325ms 599.987233ms 636.083518ms 640.513514ms 646.825039ms 677.396226ms 688.865209ms 690.265633ms 694.981001ms 711.40251ms 726.321882ms 726.373091ms 731.428825ms 733.36105ms 735.350069ms 736.371411ms 736.819403ms 737.131968ms 737.306911ms 737.852657ms 738.660174ms 743.254621ms 743.768419ms 743.854945ms 744.004495ms 744.154983ms 744.20199ms 744.220367ms 745.428926ms 746.220497ms 746.498564ms 746.554492ms 746.725314ms 746.800243ms 746.95671ms 747.075524ms 747.10464ms 747.168797ms 747.631436ms 747.65256ms 747.713293ms 747.827103ms 747.842954ms 747.843026ms 747.947044ms 748.129685ms 748.230715ms 748.241251ms 748.24349ms 748.326553ms 748.336803ms 748.347206ms 748.376585ms 748.449543ms 748.559205ms 748.599571ms 748.706114ms 748.708412ms 748.901901ms 748.933811ms 749.037588ms 749.060542ms 749.091857ms 749.094682ms 749.198719ms 749.228126ms 749.348651ms 749.399433ms 749.426454ms 749.472575ms 749.584286ms 749.684228ms 749.813275ms 749.922106ms 749.948059ms 749.977577ms 750.020778ms 750.097142ms 750.134122ms 750.183562ms 750.232962ms 750.366735ms 750.436991ms 750.456285ms 750.457411ms 750.502556ms 750.538308ms 750.588072ms 750.746801ms 750.782951ms 750.874115ms 750.914924ms 750.972512ms 750.987538ms 751.009562ms 751.025958ms 751.046231ms 751.054502ms 751.329728ms 751.495341ms 751.547634ms 751.651377ms 751.699561ms 751.7158ms 751.767199ms 751.805426ms 751.815967ms 751.866641ms 751.960489ms 752.20691ms 752.402309ms 752.418401ms 752.507127ms 752.752345ms 752.853584ms 752.94101ms 753.014073ms 753.081623ms 753.14308ms 753.162783ms 753.856619ms 753.940228ms 754.246143ms 754.324823ms 754.71716ms 756.421526ms 756.523307ms 756.62518ms 757.054087ms 758.393881ms 760.18158ms 760.870915ms 764.219771ms 768.491449ms 769.310549ms 769.83612ms 769.932202ms 806.939409ms 810.557579ms 813.596891ms 858.641215ms 860.772954ms 905.429728ms 908.706001ms 938.306065ms 957.307483ms]
Dec 23 18:28:03.296: INFO: 50 %ile: 748.230715ms
Dec 23 18:28:03.296: INFO: 90 %ile: 756.523307ms
Dec 23 18:28:03.296: INFO: 99 %ile: 938.306065ms
Dec 23 18:28:03.296: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:28:03.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-3735" for this suite.
Dec 23 18:28:17.326: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:28:17.550: INFO: namespace svc-latency-3735 deletion completed in 14.244794952s

• [SLOW TEST:25.103 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:28:17.553: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-8499
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8499 to expose endpoints map[]
Dec 23 18:28:17.680: INFO: Get endpoints failed (8.008468ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Dec 23 18:28:18.685: INFO: Get endpoints failed (1.013563728s elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Dec 23 18:28:19.691: INFO: Get endpoints failed (2.019012905s elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Dec 23 18:28:20.697: INFO: Get endpoints failed (3.024721691s elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Dec 23 18:28:21.702: INFO: Get endpoints failed (4.029712468s elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Dec 23 18:28:22.707: INFO: Get endpoints failed (5.034808186s elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Dec 23 18:28:23.711: INFO: Get endpoints failed (6.03965455s elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Dec 23 18:28:24.716: INFO: Get endpoints failed (7.044372702s elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Dec 23 18:28:25.721: INFO: successfully validated that service endpoint-test2 in namespace services-8499 exposes endpoints map[] (8.049322612s elapsed)
STEP: Creating pod pod1 in namespace services-8499
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8499 to expose endpoints map[pod1:[80]]
Dec 23 18:28:27.780: INFO: successfully validated that service endpoint-test2 in namespace services-8499 exposes endpoints map[pod1:[80]] (2.04431488s elapsed)
STEP: Creating pod pod2 in namespace services-8499
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8499 to expose endpoints map[pod1:[80] pod2:[80]]
Dec 23 18:28:29.845: INFO: successfully validated that service endpoint-test2 in namespace services-8499 exposes endpoints map[pod1:[80] pod2:[80]] (2.053727579s elapsed)
STEP: Deleting pod pod1 in namespace services-8499
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8499 to expose endpoints map[pod2:[80]]
Dec 23 18:28:29.919: INFO: successfully validated that service endpoint-test2 in namespace services-8499 exposes endpoints map[pod2:[80]] (35.620144ms elapsed)
STEP: Deleting pod pod2 in namespace services-8499
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8499 to expose endpoints map[]
Dec 23 18:28:29.943: INFO: successfully validated that service endpoint-test2 in namespace services-8499 exposes endpoints map[] (7.151129ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:28:29.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8499" for this suite.
Dec 23 18:28:52.013: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:28:52.219: INFO: namespace services-8499 deletion completed in 22.228950299s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:34.666 seconds]
[sig-network] Services
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:28:52.223: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 23 18:28:52.322: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 version'
Dec 23 18:28:52.563: INFO: stderr: ""
Dec 23 18:28:52.563: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.6\", GitCommit:\"7015f71e75f670eb9e7ebd4b5749639d42e20079\", GitTreeState:\"clean\", BuildDate:\"2019-11-13T11:20:18Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.6\", GitCommit:\"7015f71e75f670eb9e7ebd4b5749639d42e20079\", GitTreeState:\"clean\", BuildDate:\"2019-11-13T11:11:50Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:28:52.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7916" for this suite.
Dec 23 18:28:58.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:28:58.809: INFO: namespace kubectl-7916 deletion completed in 6.235850313s

• [SLOW TEST:6.587 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:28:58.818: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Dec 23 18:28:58.923: INFO: namespace kubectl-8770
Dec 23 18:28:58.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 create -f - --namespace=kubectl-8770'
Dec 23 18:28:59.706: INFO: stderr: ""
Dec 23 18:28:59.706: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 23 18:29:00.712: INFO: Selector matched 1 pods for map[app:redis]
Dec 23 18:29:00.712: INFO: Found 0 / 1
Dec 23 18:29:01.712: INFO: Selector matched 1 pods for map[app:redis]
Dec 23 18:29:01.713: INFO: Found 1 / 1
Dec 23 18:29:01.713: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 23 18:29:01.717: INFO: Selector matched 1 pods for map[app:redis]
Dec 23 18:29:01.718: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 23 18:29:01.718: INFO: wait on redis-master startup in kubectl-8770 
Dec 23 18:29:01.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 logs redis-master-hdrt4 redis-master --namespace=kubectl-8770'
Dec 23 18:29:01.997: INFO: stderr: ""
Dec 23 18:29:01.997: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 23 Dec 18:29:00.454 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 23 Dec 18:29:00.454 # Server started, Redis version 3.2.12\n1:M 23 Dec 18:29:00.454 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 23 Dec 18:29:00.454 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Dec 23 18:29:01.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-8770'
Dec 23 18:29:02.264: INFO: stderr: ""
Dec 23 18:29:02.264: INFO: stdout: "service/rm2 exposed\n"
Dec 23 18:29:02.271: INFO: Service rm2 in namespace kubectl-8770 found.
STEP: exposing service
Dec 23 18:29:04.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-8770'
Dec 23 18:29:04.593: INFO: stderr: ""
Dec 23 18:29:04.593: INFO: stdout: "service/rm3 exposed\n"
Dec 23 18:29:04.598: INFO: Service rm3 in namespace kubectl-8770 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:29:06.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8770" for this suite.
Dec 23 18:29:28.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:29:28.852: INFO: namespace kubectl-8770 deletion completed in 22.234464393s

• [SLOW TEST:30.034 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:29:28.853: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:29:52.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2105" for this suite.
Dec 23 18:29:58.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:29:58.602: INFO: namespace container-runtime-2105 deletion completed in 6.234108728s

• [SLOW TEST:29.750 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:29:58.617: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9765.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9765.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9765.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9765.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 23 18:30:00.787: INFO: DNS probes using dns-test-7a4a6b8d-6fba-4c0c-ac71-0dfbe6f5cb79 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9765.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-9765.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9765.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-9765.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 23 18:30:22.899: INFO: File jessie_udp@dns-test-service-3.dns-9765.svc.cluster.local from pod  dns-9765/dns-test-4162e319-fcce-4d62-88ee-45a27a3cba59 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 23 18:30:22.899: INFO: Lookups using dns-9765/dns-test-4162e319-fcce-4d62-88ee-45a27a3cba59 failed for: [jessie_udp@dns-test-service-3.dns-9765.svc.cluster.local]

Dec 23 18:30:27.906: INFO: File wheezy_udp@dns-test-service-3.dns-9765.svc.cluster.local from pod  dns-9765/dns-test-4162e319-fcce-4d62-88ee-45a27a3cba59 contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 23 18:30:27.912: INFO: Lookups using dns-9765/dns-test-4162e319-fcce-4d62-88ee-45a27a3cba59 failed for: [wheezy_udp@dns-test-service-3.dns-9765.svc.cluster.local]

Dec 23 18:30:32.913: INFO: DNS probes using dns-test-4162e319-fcce-4d62-88ee-45a27a3cba59 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9765.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-9765.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-9765.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-9765.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 23 18:30:35.109: INFO: DNS probes using dns-test-7d32f234-1476-4950-b3d8-5f9d7dcf0774 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:30:35.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9765" for this suite.
Dec 23 18:30:41.262: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:30:41.625: INFO: namespace dns-9765 deletion completed in 6.400788799s

• [SLOW TEST:43.009 seconds]
[sig-network] DNS
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:30:41.629: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-f6d7c887-f436-44ac-a96d-828e226f4145
STEP: Creating a pod to test consume secrets
Dec 23 18:30:41.754: INFO: Waiting up to 5m0s for pod "pod-secrets-0d488c3a-7d68-423b-a0f4-f9f3a5c8c7e0" in namespace "secrets-8581" to be "success or failure"
Dec 23 18:30:41.764: INFO: Pod "pod-secrets-0d488c3a-7d68-423b-a0f4-f9f3a5c8c7e0": Phase="Pending", Reason="", readiness=false. Elapsed: 9.670999ms
Dec 23 18:30:43.769: INFO: Pod "pod-secrets-0d488c3a-7d68-423b-a0f4-f9f3a5c8c7e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015492323s
STEP: Saw pod success
Dec 23 18:30:43.769: INFO: Pod "pod-secrets-0d488c3a-7d68-423b-a0f4-f9f3a5c8c7e0" satisfied condition "success or failure"
Dec 23 18:30:43.784: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-secrets-0d488c3a-7d68-423b-a0f4-f9f3a5c8c7e0 container secret-volume-test: <nil>
STEP: delete the pod
Dec 23 18:30:43.818: INFO: Waiting for pod pod-secrets-0d488c3a-7d68-423b-a0f4-f9f3a5c8c7e0 to disappear
Dec 23 18:30:43.826: INFO: Pod pod-secrets-0d488c3a-7d68-423b-a0f4-f9f3a5c8c7e0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:30:43.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8581" for this suite.
Dec 23 18:30:49.879: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:30:50.098: INFO: namespace secrets-8581 deletion completed in 6.262757513s

• [SLOW TEST:8.469 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:30:50.101: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 23 18:30:50.222: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec 23 18:30:50.238: INFO: Number of nodes with available pods: 0
Dec 23 18:30:50.239: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec 23 18:30:50.295: INFO: Number of nodes with available pods: 0
Dec 23 18:30:50.295: INFO: Node k8s-conformance-1-15-control-1 is running more than one daemon pod
Dec 23 18:30:51.300: INFO: Number of nodes with available pods: 0
Dec 23 18:30:51.300: INFO: Node k8s-conformance-1-15-control-1 is running more than one daemon pod
Dec 23 18:30:52.301: INFO: Number of nodes with available pods: 1
Dec 23 18:30:52.301: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec 23 18:30:52.330: INFO: Number of nodes with available pods: 1
Dec 23 18:30:52.331: INFO: Number of running nodes: 0, number of available pods: 1
Dec 23 18:30:53.337: INFO: Number of nodes with available pods: 0
Dec 23 18:30:53.337: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec 23 18:30:53.379: INFO: Number of nodes with available pods: 0
Dec 23 18:30:53.379: INFO: Node k8s-conformance-1-15-control-1 is running more than one daemon pod
Dec 23 18:30:54.387: INFO: Number of nodes with available pods: 0
Dec 23 18:30:54.387: INFO: Node k8s-conformance-1-15-control-1 is running more than one daemon pod
Dec 23 18:30:55.385: INFO: Number of nodes with available pods: 0
Dec 23 18:30:55.385: INFO: Node k8s-conformance-1-15-control-1 is running more than one daemon pod
Dec 23 18:30:56.386: INFO: Number of nodes with available pods: 0
Dec 23 18:30:56.386: INFO: Node k8s-conformance-1-15-control-1 is running more than one daemon pod
Dec 23 18:30:57.386: INFO: Number of nodes with available pods: 0
Dec 23 18:30:57.386: INFO: Node k8s-conformance-1-15-control-1 is running more than one daemon pod
Dec 23 18:30:58.385: INFO: Number of nodes with available pods: 0
Dec 23 18:30:58.386: INFO: Node k8s-conformance-1-15-control-1 is running more than one daemon pod
Dec 23 18:30:59.385: INFO: Number of nodes with available pods: 0
Dec 23 18:30:59.385: INFO: Node k8s-conformance-1-15-control-1 is running more than one daemon pod
Dec 23 18:31:00.385: INFO: Number of nodes with available pods: 0
Dec 23 18:31:00.386: INFO: Node k8s-conformance-1-15-control-1 is running more than one daemon pod
Dec 23 18:31:01.386: INFO: Number of nodes with available pods: 0
Dec 23 18:31:01.386: INFO: Node k8s-conformance-1-15-control-1 is running more than one daemon pod
Dec 23 18:31:02.384: INFO: Number of nodes with available pods: 0
Dec 23 18:31:02.385: INFO: Node k8s-conformance-1-15-control-1 is running more than one daemon pod
Dec 23 18:31:03.384: INFO: Number of nodes with available pods: 0
Dec 23 18:31:03.384: INFO: Node k8s-conformance-1-15-control-1 is running more than one daemon pod
Dec 23 18:31:04.385: INFO: Number of nodes with available pods: 0
Dec 23 18:31:04.386: INFO: Node k8s-conformance-1-15-control-1 is running more than one daemon pod
Dec 23 18:31:05.385: INFO: Number of nodes with available pods: 1
Dec 23 18:31:05.385: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9951, will wait for the garbage collector to delete the pods
Dec 23 18:31:05.463: INFO: Deleting DaemonSet.extensions daemon-set took: 13.589851ms
Dec 23 18:31:05.864: INFO: Terminating DaemonSet.extensions daemon-set pods took: 400.381659ms
Dec 23 18:31:13.369: INFO: Number of nodes with available pods: 0
Dec 23 18:31:13.369: INFO: Number of running nodes: 0, number of available pods: 0
Dec 23 18:31:13.373: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9951/daemonsets","resourceVersion":"25591"},"items":null}

Dec 23 18:31:13.378: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9951/pods","resourceVersion":"25591"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:31:13.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9951" for this suite.
Dec 23 18:31:19.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:31:19.715: INFO: namespace daemonsets-9951 deletion completed in 6.223580246s

• [SLOW TEST:29.614 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:31:19.717: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 23 18:31:22.389: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b9f6e650-aa3b-443f-8230-2ede9e6dfc03"
Dec 23 18:31:22.389: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b9f6e650-aa3b-443f-8230-2ede9e6dfc03" in namespace "pods-9903" to be "terminated due to deadline exceeded"
Dec 23 18:31:22.394: INFO: Pod "pod-update-activedeadlineseconds-b9f6e650-aa3b-443f-8230-2ede9e6dfc03": Phase="Running", Reason="", readiness=true. Elapsed: 4.409184ms
Dec 23 18:31:24.400: INFO: Pod "pod-update-activedeadlineseconds-b9f6e650-aa3b-443f-8230-2ede9e6dfc03": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 2.010475815s
Dec 23 18:31:24.400: INFO: Pod "pod-update-activedeadlineseconds-b9f6e650-aa3b-443f-8230-2ede9e6dfc03" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:31:24.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9903" for this suite.
Dec 23 18:31:30.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:31:30.637: INFO: namespace pods-9903 deletion completed in 6.227158666s

• [SLOW TEST:10.921 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:31:30.646: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-c28c606d-bf1d-48dc-a786-9bec04be648b
STEP: Creating a pod to test consume configMaps
Dec 23 18:31:30.744: INFO: Waiting up to 5m0s for pod "pod-configmaps-cee6ba3d-5215-4882-9d70-2e9832b4bdf2" in namespace "configmap-8295" to be "success or failure"
Dec 23 18:31:30.757: INFO: Pod "pod-configmaps-cee6ba3d-5215-4882-9d70-2e9832b4bdf2": Phase="Pending", Reason="", readiness=false. Elapsed: 12.744382ms
Dec 23 18:31:32.765: INFO: Pod "pod-configmaps-cee6ba3d-5215-4882-9d70-2e9832b4bdf2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020847822s
STEP: Saw pod success
Dec 23 18:31:32.766: INFO: Pod "pod-configmaps-cee6ba3d-5215-4882-9d70-2e9832b4bdf2" satisfied condition "success or failure"
Dec 23 18:31:32.771: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-configmaps-cee6ba3d-5215-4882-9d70-2e9832b4bdf2 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 23 18:31:32.802: INFO: Waiting for pod pod-configmaps-cee6ba3d-5215-4882-9d70-2e9832b4bdf2 to disappear
Dec 23 18:31:32.807: INFO: Pod pod-configmaps-cee6ba3d-5215-4882-9d70-2e9832b4bdf2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:31:32.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8295" for this suite.
Dec 23 18:31:38.841: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:31:39.054: INFO: namespace configmap-8295 deletion completed in 6.238247246s

• [SLOW TEST:8.408 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:31:39.077: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 23 18:31:39.159: INFO: (0) /api/v1/nodes/k8s-conformance-1-15-control-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 16.481476ms)
Dec 23 18:31:39.167: INFO: (1) /api/v1/nodes/k8s-conformance-1-15-control-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 7.482762ms)
Dec 23 18:31:39.172: INFO: (2) /api/v1/nodes/k8s-conformance-1-15-control-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.683491ms)
Dec 23 18:31:39.178: INFO: (3) /api/v1/nodes/k8s-conformance-1-15-control-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.06663ms)
Dec 23 18:31:39.182: INFO: (4) /api/v1/nodes/k8s-conformance-1-15-control-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.435725ms)
Dec 23 18:31:39.191: INFO: (5) /api/v1/nodes/k8s-conformance-1-15-control-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 9.166496ms)
Dec 23 18:31:39.201: INFO: (6) /api/v1/nodes/k8s-conformance-1-15-control-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 9.444385ms)
Dec 23 18:31:39.207: INFO: (7) /api/v1/nodes/k8s-conformance-1-15-control-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.576163ms)
Dec 23 18:31:39.212: INFO: (8) /api/v1/nodes/k8s-conformance-1-15-control-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.779344ms)
Dec 23 18:31:39.218: INFO: (9) /api/v1/nodes/k8s-conformance-1-15-control-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.2874ms)
Dec 23 18:31:39.222: INFO: (10) /api/v1/nodes/k8s-conformance-1-15-control-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.425998ms)
Dec 23 18:31:39.227: INFO: (11) /api/v1/nodes/k8s-conformance-1-15-control-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.120503ms)
Dec 23 18:31:39.233: INFO: (12) /api/v1/nodes/k8s-conformance-1-15-control-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.177209ms)
Dec 23 18:31:39.242: INFO: (13) /api/v1/nodes/k8s-conformance-1-15-control-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 9.053759ms)
Dec 23 18:31:39.249: INFO: (14) /api/v1/nodes/k8s-conformance-1-15-control-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 6.138593ms)
Dec 23 18:31:39.260: INFO: (15) /api/v1/nodes/k8s-conformance-1-15-control-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 10.580175ms)
Dec 23 18:31:39.266: INFO: (16) /api/v1/nodes/k8s-conformance-1-15-control-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.471545ms)
Dec 23 18:31:39.271: INFO: (17) /api/v1/nodes/k8s-conformance-1-15-control-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.89596ms)
Dec 23 18:31:39.276: INFO: (18) /api/v1/nodes/k8s-conformance-1-15-control-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.822021ms)
Dec 23 18:31:39.281: INFO: (19) /api/v1/nodes/k8s-conformance-1-15-control-1:10250/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.907326ms)
[AfterEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:31:39.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8214" for this suite.
Dec 23 18:31:45.315: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:31:45.544: INFO: namespace proxy-8214 deletion completed in 6.251491659s

• [SLOW TEST:6.469 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:31:45.554: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Dec 23 18:31:45.625: INFO: Waiting up to 5m0s for pod "pod-b7afa160-5e60-46ad-b7b1-6f1ffe025c4b" in namespace "emptydir-9166" to be "success or failure"
Dec 23 18:31:45.641: INFO: Pod "pod-b7afa160-5e60-46ad-b7b1-6f1ffe025c4b": Phase="Pending", Reason="", readiness=false. Elapsed: 14.935728ms
Dec 23 18:31:47.647: INFO: Pod "pod-b7afa160-5e60-46ad-b7b1-6f1ffe025c4b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020882643s
STEP: Saw pod success
Dec 23 18:31:47.647: INFO: Pod "pod-b7afa160-5e60-46ad-b7b1-6f1ffe025c4b" satisfied condition "success or failure"
Dec 23 18:31:47.652: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod pod-b7afa160-5e60-46ad-b7b1-6f1ffe025c4b container test-container: <nil>
STEP: delete the pod
Dec 23 18:31:47.690: INFO: Waiting for pod pod-b7afa160-5e60-46ad-b7b1-6f1ffe025c4b to disappear
Dec 23 18:31:47.695: INFO: Pod pod-b7afa160-5e60-46ad-b7b1-6f1ffe025c4b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:31:47.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9166" for this suite.
Dec 23 18:31:53.726: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:31:53.926: INFO: namespace emptydir-9166 deletion completed in 6.221497204s

• [SLOW TEST:8.372 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:31:53.930: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-9489
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 23 18:31:54.034: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 23 18:32:22.383: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.209.143:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9489 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:32:22.383: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:32:22.538: INFO: Found all expected endpoints: [netserver-0]
Dec 23 18:32:22.543: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.209.68:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9489 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:32:22.543: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:32:22.650: INFO: Found all expected endpoints: [netserver-1]
Dec 23 18:32:22.655: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.92.14:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9489 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:32:22.655: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:32:22.759: INFO: Found all expected endpoints: [netserver-2]
Dec 23 18:32:22.764: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.108.214:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9489 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:32:22.764: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:32:22.864: INFO: Found all expected endpoints: [netserver-3]
Dec 23 18:32:22.871: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.206.70:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9489 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:32:22.871: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:32:22.966: INFO: Found all expected endpoints: [netserver-4]
Dec 23 18:32:22.973: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.132.145:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9489 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:32:22.973: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:32:23.050: INFO: Found all expected endpoints: [netserver-5]
Dec 23 18:32:23.056: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.150.13:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9489 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:32:23.056: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:32:23.188: INFO: Found all expected endpoints: [netserver-6]
Dec 23 18:32:23.195: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.97.15:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9489 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:32:23.195: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:32:23.305: INFO: Found all expected endpoints: [netserver-7]
Dec 23 18:32:23.312: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.17.205:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9489 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:32:23.312: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:32:23.404: INFO: Found all expected endpoints: [netserver-8]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:32:23.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9489" for this suite.
Dec 23 18:32:47.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:32:47.668: INFO: namespace pod-network-test-9489 deletion completed in 24.253321529s

• [SLOW TEST:53.739 seconds]
[sig-network] Networking
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:32:47.672: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-6e3abc4d-69b1-4ea3-8993-5a172b0babad
STEP: Creating a pod to test consume configMaps
Dec 23 18:32:47.789: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-93304764-7e66-4d7a-9d73-040720d985ca" in namespace "projected-7809" to be "success or failure"
Dec 23 18:32:47.796: INFO: Pod "pod-projected-configmaps-93304764-7e66-4d7a-9d73-040720d985ca": Phase="Pending", Reason="", readiness=false. Elapsed: 6.896574ms
Dec 23 18:32:49.802: INFO: Pod "pod-projected-configmaps-93304764-7e66-4d7a-9d73-040720d985ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012501791s
STEP: Saw pod success
Dec 23 18:32:49.802: INFO: Pod "pod-projected-configmaps-93304764-7e66-4d7a-9d73-040720d985ca" satisfied condition "success or failure"
Dec 23 18:32:49.812: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-projected-configmaps-93304764-7e66-4d7a-9d73-040720d985ca container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 23 18:32:49.849: INFO: Waiting for pod pod-projected-configmaps-93304764-7e66-4d7a-9d73-040720d985ca to disappear
Dec 23 18:32:49.853: INFO: Pod pod-projected-configmaps-93304764-7e66-4d7a-9d73-040720d985ca no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:32:49.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7809" for this suite.
Dec 23 18:32:55.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:32:56.125: INFO: namespace projected-7809 deletion completed in 6.259648016s

• [SLOW TEST:8.453 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:32:56.133: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-552850c2-1b92-4637-b0f5-c9b4f2245c35
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:32:58.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5714" for this suite.
Dec 23 18:33:20.351: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:33:20.558: INFO: namespace configmap-5714 deletion completed in 22.232883619s

• [SLOW TEST:24.425 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:33:20.562: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Dec 23 18:33:20.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 create -f - --namespace=kubectl-2888'
Dec 23 18:33:21.280: INFO: stderr: ""
Dec 23 18:33:21.280: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 23 18:33:21.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2888'
Dec 23 18:33:21.783: INFO: stderr: ""
Dec 23 18:33:21.783: INFO: stdout: "update-demo-nautilus-lgdzv update-demo-nautilus-xhljg "
Dec 23 18:33:21.783: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-nautilus-lgdzv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2888'
Dec 23 18:33:22.087: INFO: stderr: ""
Dec 23 18:33:22.087: INFO: stdout: ""
Dec 23 18:33:22.087: INFO: update-demo-nautilus-lgdzv is created but not running
Dec 23 18:33:27.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2888'
Dec 23 18:33:27.345: INFO: stderr: ""
Dec 23 18:33:27.345: INFO: stdout: "update-demo-nautilus-lgdzv update-demo-nautilus-xhljg "
Dec 23 18:33:27.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-nautilus-lgdzv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2888'
Dec 23 18:33:27.588: INFO: stderr: ""
Dec 23 18:33:27.589: INFO: stdout: "true"
Dec 23 18:33:27.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-nautilus-lgdzv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2888'
Dec 23 18:33:27.844: INFO: stderr: ""
Dec 23 18:33:27.844: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 23 18:33:27.844: INFO: validating pod update-demo-nautilus-lgdzv
Dec 23 18:33:27.851: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 23 18:33:27.851: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 23 18:33:27.851: INFO: update-demo-nautilus-lgdzv is verified up and running
Dec 23 18:33:27.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-nautilus-xhljg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2888'
Dec 23 18:33:28.077: INFO: stderr: ""
Dec 23 18:33:28.078: INFO: stdout: "true"
Dec 23 18:33:28.078: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-nautilus-xhljg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2888'
Dec 23 18:33:28.305: INFO: stderr: ""
Dec 23 18:33:28.305: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 23 18:33:28.305: INFO: validating pod update-demo-nautilus-xhljg
Dec 23 18:33:28.313: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 23 18:33:28.313: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 23 18:33:28.313: INFO: update-demo-nautilus-xhljg is verified up and running
STEP: rolling-update to new replication controller
Dec 23 18:33:28.320: INFO: scanned /root for discovery docs: <nil>
Dec 23 18:33:28.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-2888'
Dec 23 18:33:50.359: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 23 18:33:50.359: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 23 18:33:50.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2888'
Dec 23 18:33:50.668: INFO: stderr: ""
Dec 23 18:33:50.668: INFO: stdout: "update-demo-kitten-5tpgd update-demo-kitten-dmjvz update-demo-nautilus-xhljg "
STEP: Replicas for name=update-demo: expected=2 actual=3
Dec 23 18:33:55.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2888'
Dec 23 18:33:56.126: INFO: stderr: ""
Dec 23 18:33:56.126: INFO: stdout: "update-demo-kitten-5tpgd update-demo-kitten-dmjvz "
Dec 23 18:33:56.127: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-kitten-5tpgd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2888'
Dec 23 18:33:56.366: INFO: stderr: ""
Dec 23 18:33:56.366: INFO: stdout: "true"
Dec 23 18:33:56.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-kitten-5tpgd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2888'
Dec 23 18:33:56.600: INFO: stderr: ""
Dec 23 18:33:56.600: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 23 18:33:56.600: INFO: validating pod update-demo-kitten-5tpgd
Dec 23 18:33:56.607: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 23 18:33:56.607: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 23 18:33:56.607: INFO: update-demo-kitten-5tpgd is verified up and running
Dec 23 18:33:56.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-kitten-dmjvz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2888'
Dec 23 18:33:56.838: INFO: stderr: ""
Dec 23 18:33:56.838: INFO: stdout: "true"
Dec 23 18:33:56.838: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods update-demo-kitten-dmjvz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2888'
Dec 23 18:33:57.096: INFO: stderr: ""
Dec 23 18:33:57.096: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 23 18:33:57.096: INFO: validating pod update-demo-kitten-dmjvz
Dec 23 18:33:57.106: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 23 18:33:57.106: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 23 18:33:57.106: INFO: update-demo-kitten-dmjvz is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:33:57.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2888" for this suite.
Dec 23 18:34:19.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:34:19.357: INFO: namespace kubectl-2888 deletion completed in 22.241104956s

• [SLOW TEST:58.795 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:34:19.361: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 23 18:34:19.544: INFO: Create a RollingUpdate DaemonSet
Dec 23 18:34:19.562: INFO: Check that daemon pods launch on every node of the cluster
Dec 23 18:34:19.622: INFO: Number of nodes with available pods: 0
Dec 23 18:34:19.622: INFO: Node k8s-conformance-1-15-control-1 is running more than one daemon pod
Dec 23 18:34:20.642: INFO: Number of nodes with available pods: 0
Dec 23 18:34:20.642: INFO: Node k8s-conformance-1-15-control-1 is running more than one daemon pod
Dec 23 18:34:21.662: INFO: Number of nodes with available pods: 6
Dec 23 18:34:21.662: INFO: Node k8s-conformance-1-15-control-2 is running more than one daemon pod
Dec 23 18:34:22.639: INFO: Number of nodes with available pods: 9
Dec 23 18:34:22.640: INFO: Number of running nodes: 9, number of available pods: 9
Dec 23 18:34:22.640: INFO: Update the DaemonSet to trigger a rollout
Dec 23 18:34:22.651: INFO: Updating DaemonSet daemon-set
Dec 23 18:34:28.689: INFO: Roll back the DaemonSet before rollout is complete
Dec 23 18:34:28.708: INFO: Updating DaemonSet daemon-set
Dec 23 18:34:28.708: INFO: Make sure DaemonSet rollback is complete
Dec 23 18:34:28.717: INFO: Wrong image for pod: daemon-set-ws5q5. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec 23 18:34:28.718: INFO: Pod daemon-set-ws5q5 is not available
Dec 23 18:34:29.735: INFO: Wrong image for pod: daemon-set-ws5q5. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Dec 23 18:34:29.736: INFO: Pod daemon-set-ws5q5 is not available
Dec 23 18:34:30.735: INFO: Pod daemon-set-zdwdc is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4711, will wait for the garbage collector to delete the pods
Dec 23 18:34:30.827: INFO: Deleting DaemonSet.extensions daemon-set took: 12.872801ms
Dec 23 18:34:31.327: INFO: Terminating DaemonSet.extensions daemon-set pods took: 500.450862ms
Dec 23 18:34:44.635: INFO: Number of nodes with available pods: 0
Dec 23 18:34:44.635: INFO: Number of running nodes: 0, number of available pods: 0
Dec 23 18:34:44.640: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4711/daemonsets","resourceVersion":"26992"},"items":null}

Dec 23 18:34:44.645: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4711/pods","resourceVersion":"26992"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:34:44.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4711" for this suite.
Dec 23 18:34:50.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:34:50.972: INFO: namespace daemonsets-4711 deletion completed in 6.269329778s

• [SLOW TEST:31.612 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:34:50.976: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 23 18:34:51.080: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:34:53.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3393" for this suite.
Dec 23 18:35:37.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:35:37.563: INFO: namespace pods-3393 deletion completed in 44.248345084s

• [SLOW TEST:46.588 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:35:37.573: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:72
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 23 18:35:37.680: INFO: Creating deployment "test-recreate-deployment"
Dec 23 18:35:37.688: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec 23 18:35:37.704: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
Dec 23 18:35:39.714: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec 23 18:35:39.719: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec 23 18:35:39.733: INFO: Updating deployment test-recreate-deployment
Dec 23 18:35:39.733: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:66
Dec 23 18:35:39.870: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-5275,SelfLink:/apis/apps/v1/namespaces/deployment-5275/deployments/test-recreate-deployment,UID:f8c4a9b6-7fb2-4ac6-991b-ad3fd8ad5a88,ResourceVersion:27288,Generation:2,CreationTimestamp:2019-12-23 18:35:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-12-23 18:35:39 +0000 UTC 2019-12-23 18:35:39 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-12-23 18:35:39 +0000 UTC 2019-12-23 18:35:37 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Dec 23 18:35:39.876: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-5275,SelfLink:/apis/apps/v1/namespaces/deployment-5275/replicasets/test-recreate-deployment-5c8c9cc69d,UID:b319b12a-155d-4fa2-ac38-5901c33249f2,ResourceVersion:27285,Generation:1,CreationTimestamp:2019-12-23 18:35:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment f8c4a9b6-7fb2-4ac6-991b-ad3fd8ad5a88 0xc0037342e7 0xc0037342e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 23 18:35:39.876: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec 23 18:35:39.877: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-5275,SelfLink:/apis/apps/v1/namespaces/deployment-5275/replicasets/test-recreate-deployment-6df85df6b9,UID:6afde519-3e75-4a97-b5de-3ed09d79d930,ResourceVersion:27277,Generation:2,CreationTimestamp:2019-12-23 18:35:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment f8c4a9b6-7fb2-4ac6-991b-ad3fd8ad5a88 0xc0037343b7 0xc0037343b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Dec 23 18:35:39.887: INFO: Pod "test-recreate-deployment-5c8c9cc69d-gwg44" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-gwg44,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-5275,SelfLink:/api/v1/namespaces/deployment-5275/pods/test-recreate-deployment-5c8c9cc69d-gwg44,UID:236392a8-acbe-4d26-979d-fcd568c611dd,ResourceVersion:27283,Generation:0,CreationTimestamp:2019-12-23 18:35:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d b319b12a-155d-4fa2-ac38-5901c33249f2 0xc003734d37 0xc003734d38}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-k4lkt {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-k4lkt,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-k4lkt true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8s-conformance-1-15-etcd-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003734da0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003734dc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 18:35:39 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:35:39.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5275" for this suite.
Dec 23 18:35:45.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:35:46.130: INFO: namespace deployment-5275 deletion completed in 6.228359775s

• [SLOW TEST:8.558 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:35:46.137: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-5ae44dab-e1aa-45f8-8663-68ab50e56c40
STEP: Creating a pod to test consume secrets
Dec 23 18:35:46.258: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-92481213-999a-405e-bada-abf3cbe1be16" in namespace "projected-209" to be "success or failure"
Dec 23 18:35:46.272: INFO: Pod "pod-projected-secrets-92481213-999a-405e-bada-abf3cbe1be16": Phase="Pending", Reason="", readiness=false. Elapsed: 14.239858ms
Dec 23 18:35:48.278: INFO: Pod "pod-projected-secrets-92481213-999a-405e-bada-abf3cbe1be16": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020382726s
STEP: Saw pod success
Dec 23 18:35:48.279: INFO: Pod "pod-projected-secrets-92481213-999a-405e-bada-abf3cbe1be16" satisfied condition "success or failure"
Dec 23 18:35:48.283: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod pod-projected-secrets-92481213-999a-405e-bada-abf3cbe1be16 container secret-volume-test: <nil>
STEP: delete the pod
Dec 23 18:35:48.328: INFO: Waiting for pod pod-projected-secrets-92481213-999a-405e-bada-abf3cbe1be16 to disappear
Dec 23 18:35:48.333: INFO: Pod pod-projected-secrets-92481213-999a-405e-bada-abf3cbe1be16 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:35:48.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-209" for this suite.
Dec 23 18:35:54.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:35:54.580: INFO: namespace projected-209 deletion completed in 6.237687529s

• [SLOW TEST:8.444 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:35:54.585: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 23 18:35:58.774: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 23 18:35:58.784: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 23 18:36:00.784: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 23 18:36:00.790: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 23 18:36:02.784: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 23 18:36:02.790: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 23 18:36:04.784: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 23 18:36:04.792: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 23 18:36:06.784: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 23 18:36:06.791: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 23 18:36:08.784: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 23 18:36:08.790: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 23 18:36:10.784: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 23 18:36:10.790: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 23 18:36:12.784: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 23 18:36:12.791: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 23 18:36:14.784: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 23 18:36:14.792: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 23 18:36:16.784: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 23 18:36:16.790: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 23 18:36:18.784: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 23 18:36:18.791: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 23 18:36:20.784: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 23 18:36:20.792: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 23 18:36:22.784: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 23 18:36:22.790: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 23 18:36:24.784: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 23 18:36:24.791: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:36:24.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-249" for this suite.
Dec 23 18:36:46.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:36:47.061: INFO: namespace container-lifecycle-hook-249 deletion completed in 22.248923781s

• [SLOW TEST:52.478 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:36:47.061: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-2193
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-2193
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2193
Dec 23 18:36:47.242: INFO: Found 0 stateful pods, waiting for 1
Dec 23 18:36:57.250: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec 23 18:36:57.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-2193 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 23 18:36:57.677: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 23 18:36:57.678: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 23 18:36:57.678: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 23 18:36:57.689: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 23 18:37:07.696: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 23 18:37:07.697: INFO: Waiting for statefulset status.replicas updated to 0
Dec 23 18:37:07.722: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999947s
Dec 23 18:37:08.728: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993238566s
Dec 23 18:37:09.733: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.986910681s
Dec 23 18:37:10.740: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.981560621s
Dec 23 18:37:11.746: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.9748884s
Dec 23 18:37:12.753: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.968684082s
Dec 23 18:37:13.760: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.96195771s
Dec 23 18:37:14.767: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.954876995s
Dec 23 18:37:15.777: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.947713198s
Dec 23 18:37:16.783: INFO: Verifying statefulset ss doesn't scale past 1 for another 937.910902ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2193
Dec 23 18:37:17.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-2193 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 18:37:18.131: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 23 18:37:18.131: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 23 18:37:18.131: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 23 18:37:18.137: INFO: Found 1 stateful pods, waiting for 3
Dec 23 18:37:28.143: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 23 18:37:28.143: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 23 18:37:28.143: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec 23 18:37:28.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-2193 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 23 18:37:28.480: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 23 18:37:28.480: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 23 18:37:28.480: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 23 18:37:28.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-2193 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 23 18:37:28.819: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 23 18:37:28.819: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 23 18:37:28.819: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 23 18:37:28.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-2193 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 23 18:37:29.391: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 23 18:37:29.391: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 23 18:37:29.391: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 23 18:37:29.391: INFO: Waiting for statefulset status.replicas updated to 0
Dec 23 18:37:29.397: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Dec 23 18:37:39.407: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 23 18:37:39.407: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 23 18:37:39.408: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 23 18:37:39.432: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999377s
Dec 23 18:37:40.440: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.992862073s
Dec 23 18:37:41.447: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.985565816s
Dec 23 18:37:42.455: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.978077516s
Dec 23 18:37:43.463: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970061885s
Dec 23 18:37:44.471: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.961386245s
Dec 23 18:37:45.484: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.95280414s
Dec 23 18:37:46.490: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.940681486s
Dec 23 18:37:47.499: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.933895294s
Dec 23 18:37:48.506: INFO: Verifying statefulset ss doesn't scale past 3 for another 926.017794ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2193
Dec 23 18:37:49.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-2193 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 18:37:49.901: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 23 18:37:49.901: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 23 18:37:49.901: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 23 18:37:49.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-2193 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 18:37:50.265: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 23 18:37:50.265: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 23 18:37:50.265: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 23 18:37:50.265: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-2193 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 18:37:50.813: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 23 18:37:50.813: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 23 18:37:50.813: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 23 18:37:50.813: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec 23 18:38:20.843: INFO: Deleting all statefulset in ns statefulset-2193
Dec 23 18:38:20.849: INFO: Scaling statefulset ss to 0
Dec 23 18:38:20.865: INFO: Waiting for statefulset status.replicas updated to 0
Dec 23 18:38:20.870: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:38:20.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2193" for this suite.
Dec 23 18:38:26.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:38:27.199: INFO: namespace statefulset-2193 deletion completed in 6.280611413s

• [SLOW TEST:100.138 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:38:27.203: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec 23 18:38:27.273: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6739,SelfLink:/api/v1/namespaces/watch-6739/configmaps/e2e-watch-test-configmap-a,UID:194a1779-9fba-4ec6-bf8d-0799b272525d,ResourceVersion:28064,Generation:0,CreationTimestamp:2019-12-23 18:38:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 23 18:38:27.274: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6739,SelfLink:/api/v1/namespaces/watch-6739/configmaps/e2e-watch-test-configmap-a,UID:194a1779-9fba-4ec6-bf8d-0799b272525d,ResourceVersion:28064,Generation:0,CreationTimestamp:2019-12-23 18:38:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec 23 18:38:37.288: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6739,SelfLink:/api/v1/namespaces/watch-6739/configmaps/e2e-watch-test-configmap-a,UID:194a1779-9fba-4ec6-bf8d-0799b272525d,ResourceVersion:28090,Generation:0,CreationTimestamp:2019-12-23 18:38:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 23 18:38:37.289: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6739,SelfLink:/api/v1/namespaces/watch-6739/configmaps/e2e-watch-test-configmap-a,UID:194a1779-9fba-4ec6-bf8d-0799b272525d,ResourceVersion:28090,Generation:0,CreationTimestamp:2019-12-23 18:38:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec 23 18:38:47.302: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6739,SelfLink:/api/v1/namespaces/watch-6739/configmaps/e2e-watch-test-configmap-a,UID:194a1779-9fba-4ec6-bf8d-0799b272525d,ResourceVersion:28115,Generation:0,CreationTimestamp:2019-12-23 18:38:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 23 18:38:47.303: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6739,SelfLink:/api/v1/namespaces/watch-6739/configmaps/e2e-watch-test-configmap-a,UID:194a1779-9fba-4ec6-bf8d-0799b272525d,ResourceVersion:28115,Generation:0,CreationTimestamp:2019-12-23 18:38:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec 23 18:38:57.313: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6739,SelfLink:/api/v1/namespaces/watch-6739/configmaps/e2e-watch-test-configmap-a,UID:194a1779-9fba-4ec6-bf8d-0799b272525d,ResourceVersion:28144,Generation:0,CreationTimestamp:2019-12-23 18:38:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 23 18:38:57.314: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6739,SelfLink:/api/v1/namespaces/watch-6739/configmaps/e2e-watch-test-configmap-a,UID:194a1779-9fba-4ec6-bf8d-0799b272525d,ResourceVersion:28144,Generation:0,CreationTimestamp:2019-12-23 18:38:27 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec 23 18:39:07.326: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6739,SelfLink:/api/v1/namespaces/watch-6739/configmaps/e2e-watch-test-configmap-b,UID:86669fb0-de57-470b-8c4f-8fdcc8985360,ResourceVersion:28171,Generation:0,CreationTimestamp:2019-12-23 18:39:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 23 18:39:07.327: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6739,SelfLink:/api/v1/namespaces/watch-6739/configmaps/e2e-watch-test-configmap-b,UID:86669fb0-de57-470b-8c4f-8fdcc8985360,ResourceVersion:28171,Generation:0,CreationTimestamp:2019-12-23 18:39:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec 23 18:39:17.342: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6739,SelfLink:/api/v1/namespaces/watch-6739/configmaps/e2e-watch-test-configmap-b,UID:86669fb0-de57-470b-8c4f-8fdcc8985360,ResourceVersion:28196,Generation:0,CreationTimestamp:2019-12-23 18:39:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 23 18:39:17.342: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6739,SelfLink:/api/v1/namespaces/watch-6739/configmaps/e2e-watch-test-configmap-b,UID:86669fb0-de57-470b-8c4f-8fdcc8985360,ResourceVersion:28196,Generation:0,CreationTimestamp:2019-12-23 18:39:07 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:39:27.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6739" for this suite.
Dec 23 18:39:33.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:39:33.587: INFO: namespace watch-6739 deletion completed in 6.23219951s

• [SLOW TEST:66.385 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:39:33.592: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-0fb50aa0-8f3c-41ad-89d1-924789f4d709
STEP: Creating a pod to test consume configMaps
Dec 23 18:39:33.724: INFO: Waiting up to 5m0s for pod "pod-configmaps-ca5991a1-36d4-44fa-8bc4-a0f21cd30bfc" in namespace "configmap-5625" to be "success or failure"
Dec 23 18:39:33.728: INFO: Pod "pod-configmaps-ca5991a1-36d4-44fa-8bc4-a0f21cd30bfc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.863197ms
Dec 23 18:39:35.736: INFO: Pod "pod-configmaps-ca5991a1-36d4-44fa-8bc4-a0f21cd30bfc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012805621s
STEP: Saw pod success
Dec 23 18:39:35.736: INFO: Pod "pod-configmaps-ca5991a1-36d4-44fa-8bc4-a0f21cd30bfc" satisfied condition "success or failure"
Dec 23 18:39:35.742: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod pod-configmaps-ca5991a1-36d4-44fa-8bc4-a0f21cd30bfc container configmap-volume-test: <nil>
STEP: delete the pod
Dec 23 18:39:35.775: INFO: Waiting for pod pod-configmaps-ca5991a1-36d4-44fa-8bc4-a0f21cd30bfc to disappear
Dec 23 18:39:35.782: INFO: Pod pod-configmaps-ca5991a1-36d4-44fa-8bc4-a0f21cd30bfc no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:39:35.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5625" for this suite.
Dec 23 18:39:41.817: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:39:42.016: INFO: namespace configmap-5625 deletion completed in 6.223846061s

• [SLOW TEST:8.425 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:39:42.022: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:39:42.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-652" for this suite.
Dec 23 18:40:04.143: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:40:04.352: INFO: namespace kubelet-test-652 deletion completed in 22.231489205s

• [SLOW TEST:22.330 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:40:04.360: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 23 18:40:04.481: INFO: Waiting up to 5m0s for pod "pod-631cb9f1-ff6d-40a2-9477-21feb2394a64" in namespace "emptydir-7589" to be "success or failure"
Dec 23 18:40:04.489: INFO: Pod "pod-631cb9f1-ff6d-40a2-9477-21feb2394a64": Phase="Pending", Reason="", readiness=false. Elapsed: 7.312485ms
Dec 23 18:40:06.496: INFO: Pod "pod-631cb9f1-ff6d-40a2-9477-21feb2394a64": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014391716s
STEP: Saw pod success
Dec 23 18:40:06.496: INFO: Pod "pod-631cb9f1-ff6d-40a2-9477-21feb2394a64" satisfied condition "success or failure"
Dec 23 18:40:06.502: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod pod-631cb9f1-ff6d-40a2-9477-21feb2394a64 container test-container: <nil>
STEP: delete the pod
Dec 23 18:40:06.541: INFO: Waiting for pod pod-631cb9f1-ff6d-40a2-9477-21feb2394a64 to disappear
Dec 23 18:40:06.546: INFO: Pod pod-631cb9f1-ff6d-40a2-9477-21feb2394a64 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:40:06.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7589" for this suite.
Dec 23 18:40:12.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:40:12.785: INFO: namespace emptydir-7589 deletion completed in 6.228345681s

• [SLOW TEST:8.426 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:40:12.787: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-bb687fd6-ebc4-43db-a744-7500f4912b9f
STEP: Creating configMap with name cm-test-opt-upd-dd80c775-1bfc-411f-aac8-303ca508d524
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-bb687fd6-ebc4-43db-a744-7500f4912b9f
STEP: Updating configmap cm-test-opt-upd-dd80c775-1bfc-411f-aac8-303ca508d524
STEP: Creating configMap with name cm-test-opt-create-bfdefe70-dfcf-47f5-964c-90558a3918d8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:40:17.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8553" for this suite.
Dec 23 18:40:39.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:40:39.277: INFO: namespace projected-8553 deletion completed in 22.251870933s

• [SLOW TEST:26.490 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:40:39.279: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-7728b71a-fe02-43de-93fd-08152b384d3b
STEP: Creating a pod to test consume configMaps
Dec 23 18:40:39.400: INFO: Waiting up to 5m0s for pod "pod-configmaps-29ccc5e4-e964-40eb-9bf2-342a175c083c" in namespace "configmap-2050" to be "success or failure"
Dec 23 18:40:39.414: INFO: Pod "pod-configmaps-29ccc5e4-e964-40eb-9bf2-342a175c083c": Phase="Pending", Reason="", readiness=false. Elapsed: 13.805967ms
Dec 23 18:40:41.424: INFO: Pod "pod-configmaps-29ccc5e4-e964-40eb-9bf2-342a175c083c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023917665s
STEP: Saw pod success
Dec 23 18:40:41.425: INFO: Pod "pod-configmaps-29ccc5e4-e964-40eb-9bf2-342a175c083c" satisfied condition "success or failure"
Dec 23 18:40:41.429: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod pod-configmaps-29ccc5e4-e964-40eb-9bf2-342a175c083c container configmap-volume-test: <nil>
STEP: delete the pod
Dec 23 18:40:41.469: INFO: Waiting for pod pod-configmaps-29ccc5e4-e964-40eb-9bf2-342a175c083c to disappear
Dec 23 18:40:41.475: INFO: Pod pod-configmaps-29ccc5e4-e964-40eb-9bf2-342a175c083c no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:40:41.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2050" for this suite.
Dec 23 18:40:47.630: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:40:47.843: INFO: namespace configmap-2050 deletion completed in 6.353283932s

• [SLOW TEST:8.565 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:40:47.849: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-f5a923e1-2604-4a69-99c8-94a429a44daa
STEP: Creating a pod to test consume secrets
Dec 23 18:40:48.028: INFO: Waiting up to 5m0s for pod "pod-secrets-339f41cb-02cc-44af-94bc-b7f9e03e2f0d" in namespace "secrets-2565" to be "success or failure"
Dec 23 18:40:48.036: INFO: Pod "pod-secrets-339f41cb-02cc-44af-94bc-b7f9e03e2f0d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.99651ms
Dec 23 18:40:50.043: INFO: Pod "pod-secrets-339f41cb-02cc-44af-94bc-b7f9e03e2f0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014644475s
STEP: Saw pod success
Dec 23 18:40:50.043: INFO: Pod "pod-secrets-339f41cb-02cc-44af-94bc-b7f9e03e2f0d" satisfied condition "success or failure"
Dec 23 18:40:50.048: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-secrets-339f41cb-02cc-44af-94bc-b7f9e03e2f0d container secret-volume-test: <nil>
STEP: delete the pod
Dec 23 18:40:50.088: INFO: Waiting for pod pod-secrets-339f41cb-02cc-44af-94bc-b7f9e03e2f0d to disappear
Dec 23 18:40:50.095: INFO: Pod pod-secrets-339f41cb-02cc-44af-94bc-b7f9e03e2f0d no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:40:50.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2565" for this suite.
Dec 23 18:40:56.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:40:56.340: INFO: namespace secrets-2565 deletion completed in 6.232858068s
STEP: Destroying namespace "secret-namespace-4619" for this suite.
Dec 23 18:41:02.364: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:41:02.559: INFO: namespace secret-namespace-4619 deletion completed in 6.219199968s

• [SLOW TEST:14.711 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:41:02.562: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-9603
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 23 18:41:02.619: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 23 18:41:24.939: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.206.83:8080/dial?request=hostName&protocol=udp&host=10.42.132.149&port=8081&tries=1'] Namespace:pod-network-test-9603 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:41:24.939: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:41:25.015: INFO: Waiting for endpoints: map[]
Dec 23 18:41:25.020: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.206.83:8080/dial?request=hostName&protocol=udp&host=10.42.108.216&port=8081&tries=1'] Namespace:pod-network-test-9603 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:41:25.020: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:41:25.103: INFO: Waiting for endpoints: map[]
Dec 23 18:41:25.110: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.206.83:8080/dial?request=hostName&protocol=udp&host=10.42.150.15&port=8081&tries=1'] Namespace:pod-network-test-9603 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:41:25.110: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:41:25.208: INFO: Waiting for endpoints: map[]
Dec 23 18:41:25.216: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.206.83:8080/dial?request=hostName&protocol=udp&host=10.42.92.18&port=8081&tries=1'] Namespace:pod-network-test-9603 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:41:25.217: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:41:25.300: INFO: Waiting for endpoints: map[]
Dec 23 18:41:25.305: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.206.83:8080/dial?request=hostName&protocol=udp&host=10.42.17.207&port=8081&tries=1'] Namespace:pod-network-test-9603 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:41:25.306: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:41:25.386: INFO: Waiting for endpoints: map[]
Dec 23 18:41:25.395: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.206.83:8080/dial?request=hostName&protocol=udp&host=10.42.206.82&port=8081&tries=1'] Namespace:pod-network-test-9603 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:41:25.395: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:41:25.491: INFO: Waiting for endpoints: map[]
Dec 23 18:41:25.496: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.206.83:8080/dial?request=hostName&protocol=udp&host=10.42.209.79&port=8081&tries=1'] Namespace:pod-network-test-9603 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:41:25.496: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:41:25.575: INFO: Waiting for endpoints: map[]
Dec 23 18:41:25.580: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.206.83:8080/dial?request=hostName&protocol=udp&host=10.42.209.145&port=8081&tries=1'] Namespace:pod-network-test-9603 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:41:25.581: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:41:25.672: INFO: Waiting for endpoints: map[]
Dec 23 18:41:25.678: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.206.83:8080/dial?request=hostName&protocol=udp&host=10.42.97.17&port=8081&tries=1'] Namespace:pod-network-test-9603 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 18:41:25.678: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 18:41:25.785: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:41:25.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-9603" for this suite.
Dec 23 18:41:49.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:41:50.045: INFO: namespace pod-network-test-9603 deletion completed in 24.249551643s

• [SLOW TEST:47.484 seconds]
[sig-network] Networking
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:41:50.048: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1292
STEP: creating an rc
Dec 23 18:41:50.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 create -f - --namespace=kubectl-4027'
Dec 23 18:41:50.642: INFO: stderr: ""
Dec 23 18:41:50.645: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Dec 23 18:41:51.652: INFO: Selector matched 1 pods for map[app:redis]
Dec 23 18:41:51.652: INFO: Found 0 / 1
Dec 23 18:41:52.650: INFO: Selector matched 1 pods for map[app:redis]
Dec 23 18:41:52.651: INFO: Found 1 / 1
Dec 23 18:41:52.651: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 23 18:41:52.657: INFO: Selector matched 1 pods for map[app:redis]
Dec 23 18:41:52.657: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Dec 23 18:41:52.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 logs redis-master-vrm2c redis-master --namespace=kubectl-4027'
Dec 23 18:41:52.957: INFO: stderr: ""
Dec 23 18:41:52.957: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 23 Dec 18:41:51.480 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 23 Dec 18:41:51.480 # Server started, Redis version 3.2.12\n1:M 23 Dec 18:41:51.481 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 23 Dec 18:41:51.481 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Dec 23 18:41:52.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 logs redis-master-vrm2c redis-master --namespace=kubectl-4027 --tail=1'
Dec 23 18:41:53.223: INFO: stderr: ""
Dec 23 18:41:53.224: INFO: stdout: "1:M 23 Dec 18:41:51.481 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Dec 23 18:41:53.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 logs redis-master-vrm2c redis-master --namespace=kubectl-4027 --limit-bytes=1'
Dec 23 18:41:53.544: INFO: stderr: ""
Dec 23 18:41:53.544: INFO: stdout: " "
STEP: exposing timestamps
Dec 23 18:41:53.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 logs redis-master-vrm2c redis-master --namespace=kubectl-4027 --tail=1 --timestamps'
Dec 23 18:41:53.675: INFO: stderr: ""
Dec 23 18:41:53.675: INFO: stdout: "2019-12-23T18:41:51.481865059Z 1:M 23 Dec 18:41:51.481 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Dec 23 18:41:56.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 logs redis-master-vrm2c redis-master --namespace=kubectl-4027 --since=1s'
Dec 23 18:41:56.428: INFO: stderr: ""
Dec 23 18:41:56.428: INFO: stdout: ""
Dec 23 18:41:56.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 logs redis-master-vrm2c redis-master --namespace=kubectl-4027 --since=24h'
Dec 23 18:41:56.664: INFO: stderr: ""
Dec 23 18:41:56.664: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 23 Dec 18:41:51.480 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 23 Dec 18:41:51.480 # Server started, Redis version 3.2.12\n1:M 23 Dec 18:41:51.481 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 23 Dec 18:41:51.481 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1298
STEP: using delete to clean up resources
Dec 23 18:41:56.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 delete --grace-period=0 --force -f - --namespace=kubectl-4027'
Dec 23 18:41:56.900: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 23 18:41:56.900: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Dec 23 18:41:56.900: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get rc,svc -l name=nginx --no-headers --namespace=kubectl-4027'
Dec 23 18:41:57.275: INFO: stderr: "No resources found.\n"
Dec 23 18:41:57.275: INFO: stdout: ""
Dec 23 18:41:57.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods -l name=nginx --namespace=kubectl-4027 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 23 18:41:57.567: INFO: stderr: ""
Dec 23 18:41:57.567: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:41:57.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4027" for this suite.
Dec 23 18:42:19.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:42:19.805: INFO: namespace kubectl-4027 deletion completed in 22.227808038s

• [SLOW TEST:29.757 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:42:19.829: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:42:22.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5850" for this suite.
Dec 23 18:42:45.025: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:42:45.235: INFO: namespace replication-controller-5850 deletion completed in 22.233134528s

• [SLOW TEST:25.406 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:42:45.236: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 23 18:42:45.452: INFO: Number of nodes with available pods: 0
Dec 23 18:42:45.452: INFO: Node k8s-conformance-1-15-control-1 is running more than one daemon pod
Dec 23 18:42:46.472: INFO: Number of nodes with available pods: 1
Dec 23 18:42:46.473: INFO: Node k8s-conformance-1-15-control-2 is running more than one daemon pod
Dec 23 18:42:47.485: INFO: Number of nodes with available pods: 7
Dec 23 18:42:47.486: INFO: Node k8s-conformance-1-15-control-2 is running more than one daemon pod
Dec 23 18:42:48.469: INFO: Number of nodes with available pods: 9
Dec 23 18:42:48.470: INFO: Number of running nodes: 9, number of available pods: 9
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec 23 18:42:48.511: INFO: Number of nodes with available pods: 8
Dec 23 18:42:48.511: INFO: Node k8s-conformance-1-15-etcd-2 is running more than one daemon pod
Dec 23 18:42:49.529: INFO: Number of nodes with available pods: 8
Dec 23 18:42:49.529: INFO: Node k8s-conformance-1-15-etcd-2 is running more than one daemon pod
Dec 23 18:42:50.531: INFO: Number of nodes with available pods: 8
Dec 23 18:42:50.531: INFO: Node k8s-conformance-1-15-etcd-2 is running more than one daemon pod
Dec 23 18:42:51.589: INFO: Number of nodes with available pods: 8
Dec 23 18:42:51.589: INFO: Node k8s-conformance-1-15-etcd-2 is running more than one daemon pod
Dec 23 18:42:52.530: INFO: Number of nodes with available pods: 9
Dec 23 18:42:52.531: INFO: Number of running nodes: 9, number of available pods: 9
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2803, will wait for the garbage collector to delete the pods
Dec 23 18:42:52.604: INFO: Deleting DaemonSet.extensions daemon-set took: 14.222835ms
Dec 23 18:42:53.110: INFO: Terminating DaemonSet.extensions daemon-set pods took: 505.959311ms
Dec 23 18:43:06.316: INFO: Number of nodes with available pods: 0
Dec 23 18:43:06.316: INFO: Number of running nodes: 0, number of available pods: 0
Dec 23 18:43:06.320: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2803/daemonsets","resourceVersion":"29557"},"items":null}

Dec 23 18:43:06.330: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2803/pods","resourceVersion":"29557"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:43:06.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2803" for this suite.
Dec 23 18:43:12.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:43:12.642: INFO: namespace daemonsets-2803 deletion completed in 6.248237166s

• [SLOW TEST:27.407 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:43:12.649: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Dec 23 18:43:14.752: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-424598258 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec 23 18:43:25.036: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:43:25.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2756" for this suite.
Dec 23 18:43:31.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:43:31.305: INFO: namespace pods-2756 deletion completed in 6.249564522s

• [SLOW TEST:18.656 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:43:31.307: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec 23 18:43:31.424: INFO: Waiting up to 5m0s for pod "downward-api-165b9e48-34a8-412e-b5d6-c4e7b9cb6fcd" in namespace "downward-api-1008" to be "success or failure"
Dec 23 18:43:31.431: INFO: Pod "downward-api-165b9e48-34a8-412e-b5d6-c4e7b9cb6fcd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.89546ms
Dec 23 18:43:33.437: INFO: Pod "downward-api-165b9e48-34a8-412e-b5d6-c4e7b9cb6fcd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012913138s
STEP: Saw pod success
Dec 23 18:43:33.437: INFO: Pod "downward-api-165b9e48-34a8-412e-b5d6-c4e7b9cb6fcd" satisfied condition "success or failure"
Dec 23 18:43:33.442: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod downward-api-165b9e48-34a8-412e-b5d6-c4e7b9cb6fcd container dapi-container: <nil>
STEP: delete the pod
Dec 23 18:43:33.480: INFO: Waiting for pod downward-api-165b9e48-34a8-412e-b5d6-c4e7b9cb6fcd to disappear
Dec 23 18:43:33.486: INFO: Pod downward-api-165b9e48-34a8-412e-b5d6-c4e7b9cb6fcd no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:43:33.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1008" for this suite.
Dec 23 18:43:39.518: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:43:39.727: INFO: namespace downward-api-1008 deletion completed in 6.23101044s

• [SLOW TEST:8.421 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:43:39.730: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W1223 18:43:49.871746      14 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 23 18:43:49.872: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:43:49.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6569" for this suite.
Dec 23 18:43:55.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:43:56.106: INFO: namespace gc-6569 deletion completed in 6.22536699s

• [SLOW TEST:16.377 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:43:56.109: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-46f1ed37-a21e-4db9-aba1-09bcb51711ec
STEP: Creating a pod to test consume secrets
Dec 23 18:43:56.183: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c405ea26-57ed-4fac-ad7e-40d91e0d9d15" in namespace "projected-3239" to be "success or failure"
Dec 23 18:43:56.231: INFO: Pod "pod-projected-secrets-c405ea26-57ed-4fac-ad7e-40d91e0d9d15": Phase="Pending", Reason="", readiness=false. Elapsed: 44.987154ms
Dec 23 18:43:58.237: INFO: Pod "pod-projected-secrets-c405ea26-57ed-4fac-ad7e-40d91e0d9d15": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.050981424s
STEP: Saw pod success
Dec 23 18:43:58.237: INFO: Pod "pod-projected-secrets-c405ea26-57ed-4fac-ad7e-40d91e0d9d15" satisfied condition "success or failure"
Dec 23 18:43:58.243: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod pod-projected-secrets-c405ea26-57ed-4fac-ad7e-40d91e0d9d15 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 23 18:43:58.283: INFO: Waiting for pod pod-projected-secrets-c405ea26-57ed-4fac-ad7e-40d91e0d9d15 to disappear
Dec 23 18:43:58.288: INFO: Pod pod-projected-secrets-c405ea26-57ed-4fac-ad7e-40d91e0d9d15 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:43:58.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3239" for this suite.
Dec 23 18:44:04.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:44:04.521: INFO: namespace projected-3239 deletion completed in 6.2234597s

• [SLOW TEST:8.413 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:44:04.523: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-8905
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Dec 23 18:44:04.601: INFO: Found 0 stateful pods, waiting for 3
Dec 23 18:44:14.607: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 23 18:44:14.608: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 23 18:44:14.608: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec 23 18:44:14.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-8905 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 23 18:44:15.210: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 23 18:44:15.210: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 23 18:44:15.210: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec 23 18:44:25.258: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec 23 18:44:35.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-8905 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 18:44:35.622: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 23 18:44:35.622: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 23 18:44:35.622: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 23 18:44:45.657: INFO: Waiting for StatefulSet statefulset-8905/ss2 to complete update
Dec 23 18:44:45.657: INFO: Waiting for Pod statefulset-8905/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec 23 18:44:45.657: INFO: Waiting for Pod statefulset-8905/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec 23 18:44:55.669: INFO: Waiting for StatefulSet statefulset-8905/ss2 to complete update
Dec 23 18:44:55.669: INFO: Waiting for Pod statefulset-8905/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec 23 18:44:55.670: INFO: Waiting for Pod statefulset-8905/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec 23 18:45:05.670: INFO: Waiting for StatefulSet statefulset-8905/ss2 to complete update
Dec 23 18:45:05.671: INFO: Waiting for Pod statefulset-8905/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec 23 18:45:15.669: INFO: Waiting for StatefulSet statefulset-8905/ss2 to complete update
STEP: Rolling back to a previous revision
Dec 23 18:45:25.668: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-8905 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 23 18:45:25.999: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 23 18:45:25.999: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 23 18:45:25.999: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 23 18:45:36.045: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec 23 18:45:46.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-8905 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 18:45:46.441: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 23 18:45:46.441: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 23 18:45:46.441: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 23 18:46:06.473: INFO: Waiting for StatefulSet statefulset-8905/ss2 to complete update
Dec 23 18:46:06.474: INFO: Waiting for Pod statefulset-8905/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
Dec 23 18:46:16.487: INFO: Waiting for StatefulSet statefulset-8905/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec 23 18:46:26.486: INFO: Deleting all statefulset in ns statefulset-8905
Dec 23 18:46:26.493: INFO: Scaling statefulset ss2 to 0
Dec 23 18:46:46.521: INFO: Waiting for statefulset status.replicas updated to 0
Dec 23 18:46:46.527: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:46:46.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8905" for this suite.
Dec 23 18:46:52.600: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:46:52.805: INFO: namespace statefulset-8905 deletion completed in 6.231411225s

• [SLOW TEST:168.282 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:46:52.813: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-70173c11-fcd2-4c83-8034-80fae413b1aa
STEP: Creating a pod to test consume configMaps
Dec 23 18:46:52.881: INFO: Waiting up to 5m0s for pod "pod-configmaps-89b6592c-9a28-4ac5-bbf5-c5b1465be4ef" in namespace "configmap-8921" to be "success or failure"
Dec 23 18:46:52.901: INFO: Pod "pod-configmaps-89b6592c-9a28-4ac5-bbf5-c5b1465be4ef": Phase="Pending", Reason="", readiness=false. Elapsed: 20.440319ms
Dec 23 18:46:54.908: INFO: Pod "pod-configmaps-89b6592c-9a28-4ac5-bbf5-c5b1465be4ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026814395s
STEP: Saw pod success
Dec 23 18:46:54.908: INFO: Pod "pod-configmaps-89b6592c-9a28-4ac5-bbf5-c5b1465be4ef" satisfied condition "success or failure"
Dec 23 18:46:54.912: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod pod-configmaps-89b6592c-9a28-4ac5-bbf5-c5b1465be4ef container configmap-volume-test: <nil>
STEP: delete the pod
Dec 23 18:46:54.953: INFO: Waiting for pod pod-configmaps-89b6592c-9a28-4ac5-bbf5-c5b1465be4ef to disappear
Dec 23 18:46:54.958: INFO: Pod pod-configmaps-89b6592c-9a28-4ac5-bbf5-c5b1465be4ef no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:46:54.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8921" for this suite.
Dec 23 18:47:00.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:47:01.277: INFO: namespace configmap-8921 deletion completed in 6.306813163s

• [SLOW TEST:8.464 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:47:01.281: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 23 18:47:01.443: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec 23 18:47:01.464: INFO: Number of nodes with available pods: 0
Dec 23 18:47:01.464: INFO: Node k8s-conformance-1-15-control-1 is running more than one daemon pod
Dec 23 18:47:02.494: INFO: Number of nodes with available pods: 1
Dec 23 18:47:02.494: INFO: Node k8s-conformance-1-15-control-2 is running more than one daemon pod
Dec 23 18:47:03.481: INFO: Number of nodes with available pods: 7
Dec 23 18:47:03.482: INFO: Node k8s-conformance-1-15-control-2 is running more than one daemon pod
Dec 23 18:47:04.480: INFO: Number of nodes with available pods: 9
Dec 23 18:47:04.480: INFO: Number of running nodes: 9, number of available pods: 9
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec 23 18:47:04.533: INFO: Wrong image for pod: daemon-set-4crg2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:04.533: INFO: Wrong image for pod: daemon-set-hcfcv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:04.534: INFO: Wrong image for pod: daemon-set-jdqkf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:04.534: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:04.534: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:04.534: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:04.537: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:04.537: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:04.540: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:05.559: INFO: Wrong image for pod: daemon-set-4crg2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:05.560: INFO: Wrong image for pod: daemon-set-hcfcv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:05.561: INFO: Wrong image for pod: daemon-set-jdqkf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:05.561: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:05.561: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:05.562: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:05.562: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:05.562: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:05.563: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:06.559: INFO: Wrong image for pod: daemon-set-4crg2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:06.559: INFO: Wrong image for pod: daemon-set-hcfcv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:06.559: INFO: Wrong image for pod: daemon-set-jdqkf. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:06.559: INFO: Pod daemon-set-jdqkf is not available
Dec 23 18:47:06.559: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:06.559: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:06.559: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:06.560: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:06.560: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:06.560: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:07.559: INFO: Wrong image for pod: daemon-set-4crg2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:07.560: INFO: Pod daemon-set-gb9vn is not available
Dec 23 18:47:07.560: INFO: Wrong image for pod: daemon-set-hcfcv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:07.560: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:07.561: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:07.561: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:07.561: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:07.561: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:07.561: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:08.559: INFO: Wrong image for pod: daemon-set-4crg2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:08.559: INFO: Wrong image for pod: daemon-set-hcfcv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:08.559: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:08.560: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:08.560: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:08.560: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:08.560: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:08.560: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:09.559: INFO: Wrong image for pod: daemon-set-4crg2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:09.559: INFO: Wrong image for pod: daemon-set-hcfcv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:09.559: INFO: Pod daemon-set-hcfcv is not available
Dec 23 18:47:09.559: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:09.560: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:09.560: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:09.560: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:09.560: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:09.560: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:10.558: INFO: Wrong image for pod: daemon-set-4crg2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:10.558: INFO: Wrong image for pod: daemon-set-hcfcv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:10.558: INFO: Pod daemon-set-hcfcv is not available
Dec 23 18:47:10.559: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:10.559: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:10.559: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:10.559: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:10.559: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:10.559: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:11.569: INFO: Wrong image for pod: daemon-set-4crg2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:11.570: INFO: Wrong image for pod: daemon-set-hcfcv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:11.570: INFO: Pod daemon-set-hcfcv is not available
Dec 23 18:47:11.571: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:11.572: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:11.572: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:11.572: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:11.573: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:11.573: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:12.560: INFO: Wrong image for pod: daemon-set-4crg2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:12.560: INFO: Wrong image for pod: daemon-set-hcfcv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:12.560: INFO: Pod daemon-set-hcfcv is not available
Dec 23 18:47:12.561: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:12.561: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:12.561: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:12.561: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:12.561: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:12.561: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:13.560: INFO: Wrong image for pod: daemon-set-4crg2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:13.560: INFO: Wrong image for pod: daemon-set-hcfcv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:13.561: INFO: Pod daemon-set-hcfcv is not available
Dec 23 18:47:13.561: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:13.561: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:13.561: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:13.561: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:13.561: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:13.561: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:14.561: INFO: Wrong image for pod: daemon-set-4crg2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:14.562: INFO: Wrong image for pod: daemon-set-hcfcv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:14.562: INFO: Pod daemon-set-hcfcv is not available
Dec 23 18:47:14.562: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:14.563: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:14.564: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:14.564: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:14.565: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:14.565: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:15.559: INFO: Wrong image for pod: daemon-set-4crg2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:15.559: INFO: Wrong image for pod: daemon-set-hcfcv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:15.559: INFO: Pod daemon-set-hcfcv is not available
Dec 23 18:47:15.559: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:15.560: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:15.560: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:15.560: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:15.560: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:15.560: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:16.559: INFO: Wrong image for pod: daemon-set-4crg2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:16.560: INFO: Wrong image for pod: daemon-set-hcfcv. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:16.560: INFO: Pod daemon-set-hcfcv is not available
Dec 23 18:47:16.560: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:16.560: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:16.560: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:16.561: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:16.561: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:16.561: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:17.560: INFO: Wrong image for pod: daemon-set-4crg2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:17.561: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:17.561: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:17.561: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:17.561: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:17.561: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:17.561: INFO: Pod daemon-set-qskxv is not available
Dec 23 18:47:17.561: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:18.559: INFO: Wrong image for pod: daemon-set-4crg2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:18.559: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:18.559: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:18.559: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:18.559: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:18.559: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:18.559: INFO: Pod daemon-set-qskxv is not available
Dec 23 18:47:18.559: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:19.559: INFO: Wrong image for pod: daemon-set-4crg2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:19.560: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:19.560: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:19.560: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:19.560: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:19.560: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:19.560: INFO: Pod daemon-set-qskxv is not available
Dec 23 18:47:19.561: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:20.558: INFO: Wrong image for pod: daemon-set-4crg2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:20.559: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:20.559: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:20.559: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:20.559: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:20.560: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:20.560: INFO: Pod daemon-set-qskxv is not available
Dec 23 18:47:20.560: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:21.558: INFO: Wrong image for pod: daemon-set-4crg2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:21.558: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:21.558: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:21.559: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:21.559: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:21.559: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:21.559: INFO: Pod daemon-set-qskxv is not available
Dec 23 18:47:21.559: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:22.560: INFO: Wrong image for pod: daemon-set-4crg2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:22.560: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:22.560: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:22.560: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:22.560: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:22.560: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:22.560: INFO: Pod daemon-set-qskxv is not available
Dec 23 18:47:22.560: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:23.558: INFO: Wrong image for pod: daemon-set-4crg2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:23.558: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:23.558: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:23.558: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:23.559: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:23.559: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:23.559: INFO: Pod daemon-set-qskxv is not available
Dec 23 18:47:23.559: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:24.559: INFO: Wrong image for pod: daemon-set-4crg2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:24.560: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:24.560: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:24.560: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:24.560: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:24.560: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:24.560: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:25.558: INFO: Wrong image for pod: daemon-set-4crg2. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:25.558: INFO: Pod daemon-set-4crg2 is not available
Dec 23 18:47:25.559: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:25.559: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:25.559: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:25.559: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:25.559: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:25.559: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:26.559: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:26.560: INFO: Pod daemon-set-lh8bh is not available
Dec 23 18:47:26.561: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:26.561: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:26.562: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:26.562: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:26.562: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:27.560: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:27.561: INFO: Pod daemon-set-lh8bh is not available
Dec 23 18:47:27.561: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:27.561: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:27.561: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:27.561: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:27.561: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:28.562: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:28.562: INFO: Pod daemon-set-lh8bh is not available
Dec 23 18:47:28.562: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:28.562: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:28.562: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:28.562: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:28.563: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:29.559: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:29.559: INFO: Pod daemon-set-lh8bh is not available
Dec 23 18:47:29.560: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:29.560: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:29.561: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:29.561: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:29.561: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:30.560: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:30.561: INFO: Pod daemon-set-lh8bh is not available
Dec 23 18:47:30.561: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:30.561: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:30.561: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:30.561: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:30.562: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:31.559: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:31.559: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:31.560: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:31.560: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:31.560: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:31.560: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:32.561: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:32.561: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:32.562: INFO: Wrong image for pod: daemon-set-pplzn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:32.562: INFO: Pod daemon-set-pplzn is not available
Dec 23 18:47:32.562: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:32.562: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:32.562: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:33.560: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:33.561: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:33.562: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:33.562: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:33.563: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:33.563: INFO: Pod daemon-set-vnhfb is not available
Dec 23 18:47:34.560: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:34.561: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:34.562: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:34.562: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:34.563: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:34.563: INFO: Pod daemon-set-vnhfb is not available
Dec 23 18:47:35.559: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:35.560: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:35.560: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:35.560: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:35.560: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:35.560: INFO: Pod daemon-set-vnhfb is not available
Dec 23 18:47:36.559: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:36.559: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:36.560: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:36.560: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:36.560: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:36.560: INFO: Pod daemon-set-vnhfb is not available
Dec 23 18:47:37.559: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:37.559: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:37.560: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:37.560: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:37.560: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:37.560: INFO: Pod daemon-set-vnhfb is not available
Dec 23 18:47:38.559: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:38.559: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:38.559: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:38.559: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:38.559: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:38.559: INFO: Pod daemon-set-vnhfb is not available
Dec 23 18:47:39.560: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:39.560: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:39.560: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:39.561: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:39.561: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:40.560: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:40.560: INFO: Wrong image for pod: daemon-set-mphvq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:40.561: INFO: Pod daemon-set-mphvq is not available
Dec 23 18:47:40.561: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:40.561: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:40.561: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:41.579: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:41.579: INFO: Pod daemon-set-nrqbf is not available
Dec 23 18:47:41.579: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:41.579: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:41.579: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:42.560: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:42.560: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:42.560: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:42.561: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:43.558: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:43.559: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:43.559: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:43.559: INFO: Pod daemon-set-qs27b is not available
Dec 23 18:47:43.559: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:44.559: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:44.560: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:44.560: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:44.560: INFO: Pod daemon-set-qs27b is not available
Dec 23 18:47:44.560: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:45.560: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:45.561: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:45.562: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:45.563: INFO: Pod daemon-set-qs27b is not available
Dec 23 18:47:45.563: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:46.560: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:46.560: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:46.560: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:46.560: INFO: Pod daemon-set-qs27b is not available
Dec 23 18:47:46.561: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:47.560: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:47.561: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:47.561: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:47.562: INFO: Pod daemon-set-qs27b is not available
Dec 23 18:47:47.562: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:48.559: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:48.560: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:48.560: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:48.560: INFO: Pod daemon-set-qs27b is not available
Dec 23 18:47:48.560: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:49.560: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:49.561: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:49.561: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:49.561: INFO: Pod daemon-set-qs27b is not available
Dec 23 18:47:49.561: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:50.560: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:50.563: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:50.564: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:50.564: INFO: Pod daemon-set-qs27b is not available
Dec 23 18:47:50.564: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:51.578: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:51.578: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:51.578: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:51.579: INFO: Pod daemon-set-qs27b is not available
Dec 23 18:47:51.579: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:52.562: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:52.562: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:52.562: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:52.562: INFO: Pod daemon-set-qs27b is not available
Dec 23 18:47:52.563: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:53.559: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:53.559: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:53.559: INFO: Wrong image for pod: daemon-set-qs27b. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:53.559: INFO: Pod daemon-set-qs27b is not available
Dec 23 18:47:53.559: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:54.557: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:54.558: INFO: Pod daemon-set-pp7hk is not available
Dec 23 18:47:54.558: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:54.558: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:55.559: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:55.560: INFO: Pod daemon-set-pp7hk is not available
Dec 23 18:47:55.560: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:55.560: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:56.563: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:56.564: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:56.564: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:57.559: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:57.559: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:57.559: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:58.561: INFO: Wrong image for pod: daemon-set-jp8b8. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:58.561: INFO: Pod daemon-set-jp8b8 is not available
Dec 23 18:47:58.561: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:58.561: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:59.559: INFO: Pod daemon-set-4n66b is not available
Dec 23 18:47:59.559: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:47:59.559: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:48:00.559: INFO: Pod daemon-set-4n66b is not available
Dec 23 18:48:00.560: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:48:00.560: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:48:01.582: INFO: Pod daemon-set-4n66b is not available
Dec 23 18:48:01.583: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:48:01.583: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:48:02.559: INFO: Pod daemon-set-4n66b is not available
Dec 23 18:48:02.559: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:48:02.559: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:48:03.561: INFO: Pod daemon-set-4n66b is not available
Dec 23 18:48:03.561: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:48:03.561: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:48:04.560: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:48:04.560: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:48:05.560: INFO: Wrong image for pod: daemon-set-qm4pq. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:48:05.561: INFO: Pod daemon-set-qm4pq is not available
Dec 23 18:48:05.561: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:48:06.560: INFO: Pod daemon-set-bh9m7 is not available
Dec 23 18:48:06.561: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:48:07.559: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:48:08.559: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:48:08.559: INFO: Pod daemon-set-rcc7t is not available
Dec 23 18:48:09.564: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:48:09.565: INFO: Pod daemon-set-rcc7t is not available
Dec 23 18:48:10.561: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:48:10.561: INFO: Pod daemon-set-rcc7t is not available
Dec 23 18:48:11.559: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:48:11.559: INFO: Pod daemon-set-rcc7t is not available
Dec 23 18:48:12.561: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:48:12.561: INFO: Pod daemon-set-rcc7t is not available
Dec 23 18:48:13.560: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:48:13.560: INFO: Pod daemon-set-rcc7t is not available
Dec 23 18:48:14.559: INFO: Wrong image for pod: daemon-set-rcc7t. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Dec 23 18:48:14.560: INFO: Pod daemon-set-rcc7t is not available
Dec 23 18:48:15.560: INFO: Pod daemon-set-t4gf4 is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Dec 23 18:48:15.588: INFO: Number of nodes with available pods: 8
Dec 23 18:48:15.588: INFO: Node k8s-conformance-1-15-worker-1 is running more than one daemon pod
Dec 23 18:48:16.607: INFO: Number of nodes with available pods: 8
Dec 23 18:48:16.608: INFO: Node k8s-conformance-1-15-worker-1 is running more than one daemon pod
Dec 23 18:48:17.606: INFO: Number of nodes with available pods: 8
Dec 23 18:48:17.608: INFO: Node k8s-conformance-1-15-worker-1 is running more than one daemon pod
Dec 23 18:48:18.607: INFO: Number of nodes with available pods: 8
Dec 23 18:48:18.608: INFO: Node k8s-conformance-1-15-worker-1 is running more than one daemon pod
Dec 23 18:48:19.606: INFO: Number of nodes with available pods: 8
Dec 23 18:48:19.606: INFO: Node k8s-conformance-1-15-worker-1 is running more than one daemon pod
Dec 23 18:48:20.608: INFO: Number of nodes with available pods: 8
Dec 23 18:48:20.609: INFO: Node k8s-conformance-1-15-worker-1 is running more than one daemon pod
Dec 23 18:48:21.609: INFO: Number of nodes with available pods: 9
Dec 23 18:48:21.609: INFO: Number of running nodes: 9, number of available pods: 9
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7641, will wait for the garbage collector to delete the pods
Dec 23 18:48:21.706: INFO: Deleting DaemonSet.extensions daemon-set took: 9.950058ms
Dec 23 18:48:22.220: INFO: Terminating DaemonSet.extensions daemon-set pods took: 513.839908ms
Dec 23 18:48:35.125: INFO: Number of nodes with available pods: 0
Dec 23 18:48:35.125: INFO: Number of running nodes: 0, number of available pods: 0
Dec 23 18:48:35.130: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-7641/daemonsets","resourceVersion":"31487"},"items":null}

Dec 23 18:48:35.135: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-7641/pods","resourceVersion":"31487"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:48:35.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7641" for this suite.
Dec 23 18:48:43.226: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:48:43.443: INFO: namespace daemonsets-7641 deletion completed in 8.24134455s

• [SLOW TEST:102.163 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:48:43.444: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Dec 23 18:48:43.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 --namespace=kubectl-371 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Dec 23 18:48:44.853: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Dec 23 18:48:44.853: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:48:46.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-371" for this suite.
Dec 23 18:48:56.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:48:57.104: INFO: namespace kubectl-371 deletion completed in 10.231598154s

• [SLOW TEST:13.661 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:48:57.106: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-6601/configmap-test-6c64c307-2bb3-4d07-8eb7-7e950b89ca3b
STEP: Creating a pod to test consume configMaps
Dec 23 18:48:57.211: INFO: Waiting up to 5m0s for pod "pod-configmaps-7227964f-fa52-456e-b49d-f7600fc28d30" in namespace "configmap-6601" to be "success or failure"
Dec 23 18:48:57.226: INFO: Pod "pod-configmaps-7227964f-fa52-456e-b49d-f7600fc28d30": Phase="Pending", Reason="", readiness=false. Elapsed: 14.377398ms
Dec 23 18:48:59.234: INFO: Pod "pod-configmaps-7227964f-fa52-456e-b49d-f7600fc28d30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021786745s
STEP: Saw pod success
Dec 23 18:48:59.234: INFO: Pod "pod-configmaps-7227964f-fa52-456e-b49d-f7600fc28d30" satisfied condition "success or failure"
Dec 23 18:48:59.240: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod pod-configmaps-7227964f-fa52-456e-b49d-f7600fc28d30 container env-test: <nil>
STEP: delete the pod
Dec 23 18:48:59.277: INFO: Waiting for pod pod-configmaps-7227964f-fa52-456e-b49d-f7600fc28d30 to disappear
Dec 23 18:48:59.283: INFO: Pod pod-configmaps-7227964f-fa52-456e-b49d-f7600fc28d30 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:48:59.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6601" for this suite.
Dec 23 18:49:05.314: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:49:05.541: INFO: namespace configmap-6601 deletion completed in 6.246437423s

• [SLOW TEST:8.435 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:49:05.544: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-6d223a69-1204-446b-a810-61850fbd6154
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:49:05.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6423" for this suite.
Dec 23 18:49:11.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:49:11.898: INFO: namespace secrets-6423 deletion completed in 6.231684513s

• [SLOW TEST:6.355 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:49:11.908: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-724de0b4-09dc-411f-a182-a5ab26013847
STEP: Creating a pod to test consume secrets
Dec 23 18:49:12.033: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2877f573-61f1-452f-92f3-71107befdc1f" in namespace "projected-2180" to be "success or failure"
Dec 23 18:49:12.041: INFO: Pod "pod-projected-secrets-2877f573-61f1-452f-92f3-71107befdc1f": Phase="Pending", Reason="", readiness=false. Elapsed: 7.905375ms
Dec 23 18:49:14.047: INFO: Pod "pod-projected-secrets-2877f573-61f1-452f-92f3-71107befdc1f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014005527s
STEP: Saw pod success
Dec 23 18:49:14.056: INFO: Pod "pod-projected-secrets-2877f573-61f1-452f-92f3-71107befdc1f" satisfied condition "success or failure"
Dec 23 18:49:14.062: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-projected-secrets-2877f573-61f1-452f-92f3-71107befdc1f container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 23 18:49:14.106: INFO: Waiting for pod pod-projected-secrets-2877f573-61f1-452f-92f3-71107befdc1f to disappear
Dec 23 18:49:14.112: INFO: Pod pod-projected-secrets-2877f573-61f1-452f-92f3-71107befdc1f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:49:14.113: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2180" for this suite.
Dec 23 18:49:20.148: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:49:20.352: INFO: namespace projected-2180 deletion completed in 6.227234872s

• [SLOW TEST:8.444 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:49:20.352: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1903.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1903.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1903.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1903.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1903.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1903.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 23 18:49:22.530: INFO: DNS probes using dns-1903/dns-test-c2e4bfcf-caf3-4141-8cca-51e1469b207d succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:49:22.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1903" for this suite.
Dec 23 18:49:28.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:49:28.832: INFO: namespace dns-1903 deletion completed in 6.237607858s

• [SLOW TEST:8.480 seconds]
[sig-network] DNS
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:49:28.852: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-97108a63-51cc-4df3-a55d-904fca13ed9b
STEP: Creating a pod to test consume configMaps
Dec 23 18:49:28.983: INFO: Waiting up to 5m0s for pod "pod-configmaps-5eacb956-21a6-4ad1-98f4-6d5832f0f369" in namespace "configmap-113" to be "success or failure"
Dec 23 18:49:28.989: INFO: Pod "pod-configmaps-5eacb956-21a6-4ad1-98f4-6d5832f0f369": Phase="Pending", Reason="", readiness=false. Elapsed: 6.435048ms
Dec 23 18:49:31.001: INFO: Pod "pod-configmaps-5eacb956-21a6-4ad1-98f4-6d5832f0f369": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.017933526s
STEP: Saw pod success
Dec 23 18:49:31.001: INFO: Pod "pod-configmaps-5eacb956-21a6-4ad1-98f4-6d5832f0f369" satisfied condition "success or failure"
Dec 23 18:49:31.005: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-configmaps-5eacb956-21a6-4ad1-98f4-6d5832f0f369 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 23 18:49:31.039: INFO: Waiting for pod pod-configmaps-5eacb956-21a6-4ad1-98f4-6d5832f0f369 to disappear
Dec 23 18:49:31.045: INFO: Pod pod-configmaps-5eacb956-21a6-4ad1-98f4-6d5832f0f369 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:49:31.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-113" for this suite.
Dec 23 18:49:37.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:49:37.288: INFO: namespace configmap-113 deletion completed in 6.232695612s

• [SLOW TEST:8.437 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:49:37.291: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4708
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Dec 23 18:49:37.438: INFO: Found 0 stateful pods, waiting for 3
Dec 23 18:49:47.446: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 23 18:49:47.446: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 23 18:49:47.446: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Dec 23 18:49:47.486: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec 23 18:49:57.536: INFO: Updating stateful set ss2
Dec 23 18:49:57.562: INFO: Waiting for Pod statefulset-4708/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Dec 23 18:50:07.720: INFO: Found 2 stateful pods, waiting for 3
Dec 23 18:50:17.727: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 23 18:50:17.727: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 23 18:50:17.728: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec 23 18:50:17.760: INFO: Updating stateful set ss2
Dec 23 18:50:17.776: INFO: Waiting for Pod statefulset-4708/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Dec 23 18:50:27.814: INFO: Updating stateful set ss2
Dec 23 18:50:27.870: INFO: Waiting for StatefulSet statefulset-4708/ss2 to complete update
Dec 23 18:50:27.870: INFO: Waiting for Pod statefulset-4708/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec 23 18:50:37.883: INFO: Deleting all statefulset in ns statefulset-4708
Dec 23 18:50:37.888: INFO: Scaling statefulset ss2 to 0
Dec 23 18:50:57.930: INFO: Waiting for statefulset status.replicas updated to 0
Dec 23 18:50:57.935: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:50:57.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4708" for this suite.
Dec 23 18:51:03.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:51:04.212: INFO: namespace statefulset-4708 deletion completed in 6.237932787s

• [SLOW TEST:86.921 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:51:04.217: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 23 18:51:04.336: INFO: Waiting up to 5m0s for pod "downwardapi-volume-32fd059f-05d4-4d6a-afc6-1d3d6bb140a5" in namespace "projected-9344" to be "success or failure"
Dec 23 18:51:04.350: INFO: Pod "downwardapi-volume-32fd059f-05d4-4d6a-afc6-1d3d6bb140a5": Phase="Pending", Reason="", readiness=false. Elapsed: 13.732861ms
Dec 23 18:51:06.356: INFO: Pod "downwardapi-volume-32fd059f-05d4-4d6a-afc6-1d3d6bb140a5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019205054s
STEP: Saw pod success
Dec 23 18:51:06.356: INFO: Pod "downwardapi-volume-32fd059f-05d4-4d6a-afc6-1d3d6bb140a5" satisfied condition "success or failure"
Dec 23 18:51:06.360: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod downwardapi-volume-32fd059f-05d4-4d6a-afc6-1d3d6bb140a5 container client-container: <nil>
STEP: delete the pod
Dec 23 18:51:06.396: INFO: Waiting for pod downwardapi-volume-32fd059f-05d4-4d6a-afc6-1d3d6bb140a5 to disappear
Dec 23 18:51:06.401: INFO: Pod downwardapi-volume-32fd059f-05d4-4d6a-afc6-1d3d6bb140a5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:51:06.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9344" for this suite.
Dec 23 18:51:12.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:51:12.731: INFO: namespace projected-9344 deletion completed in 6.320733243s

• [SLOW TEST:8.514 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:51:12.732: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 23 18:51:12.828: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39120287-b472-4ffe-9637-784275c19fab" in namespace "downward-api-5570" to be "success or failure"
Dec 23 18:51:12.865: INFO: Pod "downwardapi-volume-39120287-b472-4ffe-9637-784275c19fab": Phase="Pending", Reason="", readiness=false. Elapsed: 36.834084ms
Dec 23 18:51:14.871: INFO: Pod "downwardapi-volume-39120287-b472-4ffe-9637-784275c19fab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.042528775s
STEP: Saw pod success
Dec 23 18:51:14.871: INFO: Pod "downwardapi-volume-39120287-b472-4ffe-9637-784275c19fab" satisfied condition "success or failure"
Dec 23 18:51:14.876: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod downwardapi-volume-39120287-b472-4ffe-9637-784275c19fab container client-container: <nil>
STEP: delete the pod
Dec 23 18:51:14.913: INFO: Waiting for pod downwardapi-volume-39120287-b472-4ffe-9637-784275c19fab to disappear
Dec 23 18:51:14.918: INFO: Pod downwardapi-volume-39120287-b472-4ffe-9637-784275c19fab no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:51:14.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5570" for this suite.
Dec 23 18:51:20.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:51:21.167: INFO: namespace downward-api-5570 deletion completed in 6.239470646s

• [SLOW TEST:8.436 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:51:21.178: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1210
STEP: creating the pod
Dec 23 18:51:21.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 create -f - --namespace=kubectl-9560'
Dec 23 18:51:22.151: INFO: stderr: ""
Dec 23 18:51:22.151: INFO: stdout: "pod/pause created\n"
Dec 23 18:51:22.151: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec 23 18:51:22.151: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-9560" to be "running and ready"
Dec 23 18:51:22.160: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 9.210147ms
Dec 23 18:51:24.166: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.014804544s
Dec 23 18:51:24.166: INFO: Pod "pause" satisfied condition "running and ready"
Dec 23 18:51:24.166: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Dec 23 18:51:24.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 label pods pause testing-label=testing-label-value --namespace=kubectl-9560'
Dec 23 18:51:24.399: INFO: stderr: ""
Dec 23 18:51:24.399: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec 23 18:51:24.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pod pause -L testing-label --namespace=kubectl-9560'
Dec 23 18:51:24.711: INFO: stderr: ""
Dec 23 18:51:24.711: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec 23 18:51:24.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 label pods pause testing-label- --namespace=kubectl-9560'
Dec 23 18:51:24.980: INFO: stderr: ""
Dec 23 18:51:24.980: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec 23 18:51:24.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pod pause -L testing-label --namespace=kubectl-9560'
Dec 23 18:51:25.245: INFO: stderr: ""
Dec 23 18:51:25.245: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1217
STEP: using delete to clean up resources
Dec 23 18:51:25.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 delete --grace-period=0 --force -f - --namespace=kubectl-9560'
Dec 23 18:51:25.524: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 23 18:51:25.524: INFO: stdout: "pod \"pause\" force deleted\n"
Dec 23 18:51:25.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get rc,svc -l name=pause --no-headers --namespace=kubectl-9560'
Dec 23 18:51:25.807: INFO: stderr: "No resources found.\n"
Dec 23 18:51:25.807: INFO: stdout: ""
Dec 23 18:51:25.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pods -l name=pause --namespace=kubectl-9560 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 23 18:51:26.047: INFO: stderr: ""
Dec 23 18:51:26.047: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:51:26.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9560" for this suite.
Dec 23 18:51:32.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:51:32.293: INFO: namespace kubectl-9560 deletion completed in 6.235259982s

• [SLOW TEST:11.115 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:51:32.300: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Dec 23 18:51:32.406: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 23 18:51:32.424: INFO: Waiting for terminating namespaces to be deleted...
Dec 23 18:51:32.429: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-control-1 before test
Dec 23 18:51:32.439: INFO: calico-node-drwck from kube-system started at 2019-12-23 16:49:25 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.439: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 18:51:32.440: INFO: cattle-node-agent-5hgvg from cattle-system started at 2019-12-23 16:51:44 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.440: INFO: 	Container agent ready: true, restart count 0
Dec 23 18:51:32.440: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-vcnpn from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 18:51:32.440: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 23 18:51:32.440: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 18:51:32.441: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-control-2 before test
Dec 23 18:51:32.460: INFO: rke-network-plugin-deploy-job-xvn4m from kube-system started at 2019-12-23 16:48:57 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.460: INFO: 	Container rke-network-plugin-pod ready: false, restart count 0
Dec 23 18:51:32.461: INFO: rke-coredns-addon-deploy-job-lppwq from kube-system started at 2019-12-23 16:51:09 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.461: INFO: 	Container rke-coredns-addon-pod ready: false, restart count 0
Dec 23 18:51:32.461: INFO: calico-node-v9rvr from kube-system started at 2019-12-23 16:49:25 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.461: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 18:51:32.462: INFO: rke-metrics-addon-deploy-job-glj7v from kube-system started at 2019-12-23 16:51:14 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.462: INFO: 	Container rke-metrics-addon-pod ready: false, restart count 0
Dec 23 18:51:32.462: INFO: rke-ingress-controller-deploy-job-bz9f6 from kube-system started at 2019-12-23 16:51:19 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.462: INFO: 	Container rke-ingress-controller-pod ready: false, restart count 0
Dec 23 18:51:32.462: INFO: cattle-node-agent-9jbrb from cattle-system started at 2019-12-23 16:51:44 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.462: INFO: 	Container agent ready: true, restart count 0
Dec 23 18:51:32.462: INFO: sonobuoy from sonobuoy started at 2019-12-23 17:51:06 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.463: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 23 18:51:32.463: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-s25j6 from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 18:51:32.464: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 23 18:51:32.464: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 18:51:32.464: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-control-3 before test
Dec 23 18:51:32.481: INFO: calico-node-ptzn9 from kube-system started at 2019-12-23 16:49:25 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.482: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 18:51:32.482: INFO: cattle-cluster-agent-6dfc4fd4fc-gsf4h from cattle-system started at 2019-12-23 16:51:44 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.482: INFO: 	Container cluster-register ready: true, restart count 0
Dec 23 18:51:32.482: INFO: cattle-node-agent-whghm from cattle-system started at 2019-12-23 16:51:44 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.482: INFO: 	Container agent ready: true, restart count 0
Dec 23 18:51:32.482: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-97q92 from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 18:51:32.483: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 23 18:51:32.483: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 18:51:32.483: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-etcd-1 before test
Dec 23 18:51:32.500: INFO: calico-node-qv786 from kube-system started at 2019-12-23 16:49:25 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.500: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 18:51:32.500: INFO: cattle-node-agent-bckfv from cattle-system started at 2019-12-23 16:51:44 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.500: INFO: 	Container agent ready: true, restart count 0
Dec 23 18:51:32.500: INFO: sonobuoy-e2e-job-7470178446ee4019 from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 18:51:32.500: INFO: 	Container e2e ready: true, restart count 0
Dec 23 18:51:32.500: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 23 18:51:32.500: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-sxmpv from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 18:51:32.501: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 23 18:51:32.501: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 18:51:32.501: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-etcd-2 before test
Dec 23 18:51:32.511: INFO: calico-kube-controllers-77cd95cb44-b9g9b from kube-system started at 2019-12-23 16:49:25 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.512: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Dec 23 18:51:32.512: INFO: cattle-node-agent-7jfhx from cattle-system started at 2019-12-23 16:51:44 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.512: INFO: 	Container agent ready: true, restart count 0
Dec 23 18:51:32.512: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-2cg6j from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 18:51:32.512: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 23 18:51:32.512: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 18:51:32.512: INFO: calico-node-648gc from kube-system started at 2019-12-23 16:49:25 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.512: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 18:51:32.512: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-etcd-3 before test
Dec 23 18:51:32.521: INFO: calico-node-vk4b5 from kube-system started at 2019-12-23 16:49:25 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.521: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 18:51:32.521: INFO: cattle-node-agent-28j5k from cattle-system started at 2019-12-23 16:51:44 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.521: INFO: 	Container agent ready: true, restart count 0
Dec 23 18:51:32.521: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-xm8d7 from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 18:51:32.522: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 23 18:51:32.522: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 18:51:32.522: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-worker-1 before test
Dec 23 18:51:32.539: INFO: calico-node-7c2k8 from kube-system started at 2019-12-23 16:53:55 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.540: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 18:51:32.540: INFO: cattle-node-agent-9brqm from cattle-system started at 2019-12-23 16:53:55 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.540: INFO: 	Container agent ready: true, restart count 0
Dec 23 18:51:32.540: INFO: coredns-799dffd9c4-2459g from kube-system started at 2019-12-23 16:53:56 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.541: INFO: 	Container coredns ready: true, restart count 0
Dec 23 18:51:32.541: INFO: nginx-ingress-controller-ddhqm from ingress-nginx started at 2019-12-23 16:53:56 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.541: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec 23 18:51:32.541: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-mjhc2 from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 18:51:32.541: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 23 18:51:32.542: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 18:51:32.542: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-worker-2 before test
Dec 23 18:51:32.557: INFO: calico-node-cvld6 from kube-system started at 2019-12-23 16:52:48 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.558: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 18:51:32.558: INFO: nginx-ingress-controller-nczlg from ingress-nginx started at 2019-12-23 16:52:48 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.558: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec 23 18:51:32.558: INFO: default-http-backend-5bcc9fd598-pz56l from ingress-nginx started at 2019-12-23 16:52:49 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.558: INFO: 	Container default-http-backend ready: true, restart count 0
Dec 23 18:51:32.559: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-zgfft from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 18:51:32.559: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 23 18:51:32.559: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 23 18:51:32.560: INFO: cattle-node-agent-l6k7z from cattle-system started at 2019-12-23 16:52:48 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.560: INFO: 	Container agent ready: true, restart count 0
Dec 23 18:51:32.560: INFO: coredns-autoscaler-84766fbb4-2s6b6 from kube-system started at 2019-12-23 16:52:49 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.560: INFO: 	Container autoscaler ready: true, restart count 0
Dec 23 18:51:32.560: INFO: metrics-server-59c6fd6767-4nvkr from kube-system started at 2019-12-23 16:52:49 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.560: INFO: 	Container metrics-server ready: true, restart count 0
Dec 23 18:51:32.560: INFO: coredns-799dffd9c4-2swvh from kube-system started at 2019-12-23 16:53:56 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.561: INFO: 	Container coredns ready: true, restart count 0
Dec 23 18:51:32.561: INFO: 
Logging pods the kubelet thinks is on node k8s-conformance-1-15-worker-3 before test
Dec 23 18:51:32.572: INFO: calico-node-pgzp6 from kube-system started at 2019-12-23 16:52:46 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.573: INFO: 	Container calico-node ready: true, restart count 0
Dec 23 18:51:32.573: INFO: cattle-node-agent-22s8h from cattle-system started at 2019-12-23 16:52:46 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.573: INFO: 	Container agent ready: true, restart count 0
Dec 23 18:51:32.573: INFO: nginx-ingress-controller-sq5vm from ingress-nginx started at 2019-12-23 16:52:46 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.573: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec 23 18:51:32.573: INFO: coredns-799dffd9c4-gc76w from kube-system started at 2019-12-23 16:52:49 +0000 UTC (1 container statuses recorded)
Dec 23 18:51:32.574: INFO: 	Container coredns ready: true, restart count 0
Dec 23 18:51:32.574: INFO: sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-ptdwt from sonobuoy started at 2019-12-23 17:51:08 +0000 UTC (2 container statuses recorded)
Dec 23 18:51:32.574: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 23 18:51:32.574: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-e30f781c-6008-4e84-b607-52794324a5f8 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-e30f781c-6008-4e84-b607-52794324a5f8 off the node k8s-conformance-1-15-etcd-3
STEP: verifying the node doesn't have the label kubernetes.io/e2e-e30f781c-6008-4e84-b607-52794324a5f8
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:51:36.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5786" for this suite.
Dec 23 18:51:56.750: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:51:56.954: INFO: namespace sched-pred-5786 deletion completed in 20.223660839s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:24.654 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:51:56.963: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Dec 23 18:51:59.121: INFO: Pod pod-hostip-a7b6fd3d-6cf9-4057-98c8-20730d253f64 has hostIP: 72.2.119.160
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:51:59.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7312" for this suite.
Dec 23 18:52:21.158: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:52:21.362: INFO: namespace pods-7312 deletion completed in 22.230295498s

• [SLOW TEST:24.401 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:52:21.364: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 23 18:52:21.487: INFO: (0) /api/v1/nodes/k8s-conformance-1-15-control-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 6.602689ms)
Dec 23 18:52:21.495: INFO: (1) /api/v1/nodes/k8s-conformance-1-15-control-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 7.86416ms)
Dec 23 18:52:21.500: INFO: (2) /api/v1/nodes/k8s-conformance-1-15-control-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.922345ms)
Dec 23 18:52:21.505: INFO: (3) /api/v1/nodes/k8s-conformance-1-15-control-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.379045ms)
Dec 23 18:52:21.510: INFO: (4) /api/v1/nodes/k8s-conformance-1-15-control-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.908185ms)
Dec 23 18:52:21.515: INFO: (5) /api/v1/nodes/k8s-conformance-1-15-control-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.293965ms)
Dec 23 18:52:21.520: INFO: (6) /api/v1/nodes/k8s-conformance-1-15-control-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.584126ms)
Dec 23 18:52:21.525: INFO: (7) /api/v1/nodes/k8s-conformance-1-15-control-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.972802ms)
Dec 23 18:52:21.530: INFO: (8) /api/v1/nodes/k8s-conformance-1-15-control-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.881451ms)
Dec 23 18:52:21.535: INFO: (9) /api/v1/nodes/k8s-conformance-1-15-control-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 5.099671ms)
Dec 23 18:52:21.540: INFO: (10) /api/v1/nodes/k8s-conformance-1-15-control-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.277662ms)
Dec 23 18:52:21.544: INFO: (11) /api/v1/nodes/k8s-conformance-1-15-control-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.165545ms)
Dec 23 18:52:21.549: INFO: (12) /api/v1/nodes/k8s-conformance-1-15-control-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.818729ms)
Dec 23 18:52:21.554: INFO: (13) /api/v1/nodes/k8s-conformance-1-15-control-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.687108ms)
Dec 23 18:52:21.558: INFO: (14) /api/v1/nodes/k8s-conformance-1-15-control-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.29886ms)
Dec 23 18:52:21.563: INFO: (15) /api/v1/nodes/k8s-conformance-1-15-control-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.464672ms)
Dec 23 18:52:21.567: INFO: (16) /api/v1/nodes/k8s-conformance-1-15-control-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.349876ms)
Dec 23 18:52:21.572: INFO: (17) /api/v1/nodes/k8s-conformance-1-15-control-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.579617ms)
Dec 23 18:52:21.576: INFO: (18) /api/v1/nodes/k8s-conformance-1-15-control-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.202521ms)
Dec 23 18:52:21.580: INFO: (19) /api/v1/nodes/k8s-conformance-1-15-control-1/proxy/logs/: <pre>
<a href="containers/">containers/</a>
<a href="pods/">pods/</a>
</pre>
 (200; 4.499848ms)
[AfterEach] version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:52:21.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-8154" for this suite.
Dec 23 18:52:27.724: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:52:27.934: INFO: namespace proxy-8154 deletion completed in 6.339479221s

• [SLOW TEST:6.571 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:52:27.937: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-97b1e57a-7a77-45c3-9b2e-5f2950796327
STEP: Creating a pod to test consume secrets
Dec 23 18:52:28.059: INFO: Waiting up to 5m0s for pod "pod-secrets-74847c7b-f7b8-4226-890b-04fc88efe569" in namespace "secrets-5812" to be "success or failure"
Dec 23 18:52:28.065: INFO: Pod "pod-secrets-74847c7b-f7b8-4226-890b-04fc88efe569": Phase="Pending", Reason="", readiness=false. Elapsed: 6.022511ms
Dec 23 18:52:30.072: INFO: Pod "pod-secrets-74847c7b-f7b8-4226-890b-04fc88efe569": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01304204s
STEP: Saw pod success
Dec 23 18:52:30.073: INFO: Pod "pod-secrets-74847c7b-f7b8-4226-890b-04fc88efe569" satisfied condition "success or failure"
Dec 23 18:52:30.079: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-secrets-74847c7b-f7b8-4226-890b-04fc88efe569 container secret-volume-test: <nil>
STEP: delete the pod
Dec 23 18:52:30.146: INFO: Waiting for pod pod-secrets-74847c7b-f7b8-4226-890b-04fc88efe569 to disappear
Dec 23 18:52:30.153: INFO: Pod pod-secrets-74847c7b-f7b8-4226-890b-04fc88efe569 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:52:30.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5812" for this suite.
Dec 23 18:52:36.191: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:52:36.411: INFO: namespace secrets-5812 deletion completed in 6.2474183s

• [SLOW TEST:8.475 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:52:36.415: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-36412419-93a0-4afd-ac8d-a58e45c3aa37
STEP: Creating a pod to test consume secrets
Dec 23 18:52:36.495: INFO: Waiting up to 5m0s for pod "pod-secrets-42eb8afc-5029-4e63-bf86-4f11433491f1" in namespace "secrets-6008" to be "success or failure"
Dec 23 18:52:36.512: INFO: Pod "pod-secrets-42eb8afc-5029-4e63-bf86-4f11433491f1": Phase="Pending", Reason="", readiness=false. Elapsed: 16.764084ms
Dec 23 18:52:38.518: INFO: Pod "pod-secrets-42eb8afc-5029-4e63-bf86-4f11433491f1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023013229s
STEP: Saw pod success
Dec 23 18:52:38.518: INFO: Pod "pod-secrets-42eb8afc-5029-4e63-bf86-4f11433491f1" satisfied condition "success or failure"
Dec 23 18:52:38.524: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod pod-secrets-42eb8afc-5029-4e63-bf86-4f11433491f1 container secret-volume-test: <nil>
STEP: delete the pod
Dec 23 18:52:38.563: INFO: Waiting for pod pod-secrets-42eb8afc-5029-4e63-bf86-4f11433491f1 to disappear
Dec 23 18:52:38.571: INFO: Pod pod-secrets-42eb8afc-5029-4e63-bf86-4f11433491f1 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:52:38.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6008" for this suite.
Dec 23 18:52:44.611: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:52:44.822: INFO: namespace secrets-6008 deletion completed in 6.241148103s

• [SLOW TEST:8.407 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:52:44.824: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec 23 18:52:45.298: INFO: Pod name wrapped-volume-race-a6ed8108-d355-4df9-a7bf-44dd0578fcb6: Found 0 pods out of 5
Dec 23 18:52:50.310: INFO: Pod name wrapped-volume-race-a6ed8108-d355-4df9-a7bf-44dd0578fcb6: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-a6ed8108-d355-4df9-a7bf-44dd0578fcb6 in namespace emptydir-wrapper-4687, will wait for the garbage collector to delete the pods
Dec 23 18:53:02.419: INFO: Deleting ReplicationController wrapped-volume-race-a6ed8108-d355-4df9-a7bf-44dd0578fcb6 took: 12.801003ms
Dec 23 18:53:02.919: INFO: Terminating ReplicationController wrapped-volume-race-a6ed8108-d355-4df9-a7bf-44dd0578fcb6 pods took: 500.511138ms
STEP: Creating RC which spawns configmap-volume pods
Dec 23 18:53:43.757: INFO: Pod name wrapped-volume-race-30354dc6-1fa8-4139-879c-caec03d254a6: Found 0 pods out of 5
Dec 23 18:53:48.770: INFO: Pod name wrapped-volume-race-30354dc6-1fa8-4139-879c-caec03d254a6: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-30354dc6-1fa8-4139-879c-caec03d254a6 in namespace emptydir-wrapper-4687, will wait for the garbage collector to delete the pods
Dec 23 18:53:58.879: INFO: Deleting ReplicationController wrapped-volume-race-30354dc6-1fa8-4139-879c-caec03d254a6 took: 15.138702ms
Dec 23 18:53:59.386: INFO: Terminating ReplicationController wrapped-volume-race-30354dc6-1fa8-4139-879c-caec03d254a6 pods took: 506.939094ms
STEP: Creating RC which spawns configmap-volume pods
Dec 23 18:54:43.817: INFO: Pod name wrapped-volume-race-16a1932d-2207-4e2b-8e1d-9bd62ce2fc02: Found 0 pods out of 5
Dec 23 18:54:48.830: INFO: Pod name wrapped-volume-race-16a1932d-2207-4e2b-8e1d-9bd62ce2fc02: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-16a1932d-2207-4e2b-8e1d-9bd62ce2fc02 in namespace emptydir-wrapper-4687, will wait for the garbage collector to delete the pods
Dec 23 18:54:58.937: INFO: Deleting ReplicationController wrapped-volume-race-16a1932d-2207-4e2b-8e1d-9bd62ce2fc02 took: 14.482465ms
Dec 23 18:54:59.438: INFO: Terminating ReplicationController wrapped-volume-race-16a1932d-2207-4e2b-8e1d-9bd62ce2fc02 pods took: 500.851087ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:55:43.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-4687" for this suite.
Dec 23 18:55:51.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:55:52.142: INFO: namespace emptydir-wrapper-4687 deletion completed in 8.227127713s

• [SLOW TEST:187.318 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:55:52.149: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:55:52.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1981" for this suite.
Dec 23 18:55:58.289: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:55:58.488: INFO: namespace services-1981 deletion completed in 6.218551205s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.340 seconds]
[sig-network] Services
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:55:58.500: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-2ae72e0d-bd7d-49c0-b359-fb951c5e2df4
Dec 23 18:55:58.627: INFO: Pod name my-hostname-basic-2ae72e0d-bd7d-49c0-b359-fb951c5e2df4: Found 0 pods out of 1
Dec 23 18:56:03.634: INFO: Pod name my-hostname-basic-2ae72e0d-bd7d-49c0-b359-fb951c5e2df4: Found 1 pods out of 1
Dec 23 18:56:03.634: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-2ae72e0d-bd7d-49c0-b359-fb951c5e2df4" are running
Dec 23 18:56:03.641: INFO: Pod "my-hostname-basic-2ae72e0d-bd7d-49c0-b359-fb951c5e2df4-4tzhj" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-23 18:55:58 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-23 18:55:59 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-23 18:55:59 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-23 18:55:58 +0000 UTC Reason: Message:}])
Dec 23 18:56:03.641: INFO: Trying to dial the pod
Dec 23 18:56:08.658: INFO: Controller my-hostname-basic-2ae72e0d-bd7d-49c0-b359-fb951c5e2df4: Got expected result from replica 1 [my-hostname-basic-2ae72e0d-bd7d-49c0-b359-fb951c5e2df4-4tzhj]: "my-hostname-basic-2ae72e0d-bd7d-49c0-b359-fb951c5e2df4-4tzhj", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:56:08.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5398" for this suite.
Dec 23 18:56:14.694: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:56:14.950: INFO: namespace replication-controller-5398 deletion completed in 6.279968709s

• [SLOW TEST:16.451 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:56:14.958: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-xr4w
STEP: Creating a pod to test atomic-volume-subpath
Dec 23 18:56:15.050: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-xr4w" in namespace "subpath-2897" to be "success or failure"
Dec 23 18:56:15.087: INFO: Pod "pod-subpath-test-projected-xr4w": Phase="Pending", Reason="", readiness=false. Elapsed: 36.546142ms
Dec 23 18:56:17.094: INFO: Pod "pod-subpath-test-projected-xr4w": Phase="Running", Reason="", readiness=true. Elapsed: 2.043188412s
Dec 23 18:56:19.099: INFO: Pod "pod-subpath-test-projected-xr4w": Phase="Running", Reason="", readiness=true. Elapsed: 4.048870705s
Dec 23 18:56:21.105: INFO: Pod "pod-subpath-test-projected-xr4w": Phase="Running", Reason="", readiness=true. Elapsed: 6.054347868s
Dec 23 18:56:23.111: INFO: Pod "pod-subpath-test-projected-xr4w": Phase="Running", Reason="", readiness=true. Elapsed: 8.060797672s
Dec 23 18:56:25.118: INFO: Pod "pod-subpath-test-projected-xr4w": Phase="Running", Reason="", readiness=true. Elapsed: 10.067194422s
Dec 23 18:56:27.123: INFO: Pod "pod-subpath-test-projected-xr4w": Phase="Running", Reason="", readiness=true. Elapsed: 12.07273478s
Dec 23 18:56:29.130: INFO: Pod "pod-subpath-test-projected-xr4w": Phase="Running", Reason="", readiness=true. Elapsed: 14.079852378s
Dec 23 18:56:31.136: INFO: Pod "pod-subpath-test-projected-xr4w": Phase="Running", Reason="", readiness=true. Elapsed: 16.084919592s
Dec 23 18:56:33.141: INFO: Pod "pod-subpath-test-projected-xr4w": Phase="Running", Reason="", readiness=true. Elapsed: 18.09048349s
Dec 23 18:56:35.147: INFO: Pod "pod-subpath-test-projected-xr4w": Phase="Running", Reason="", readiness=true. Elapsed: 20.096334493s
Dec 23 18:56:37.154: INFO: Pod "pod-subpath-test-projected-xr4w": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.103141927s
STEP: Saw pod success
Dec 23 18:56:37.154: INFO: Pod "pod-subpath-test-projected-xr4w" satisfied condition "success or failure"
Dec 23 18:56:37.159: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod pod-subpath-test-projected-xr4w container test-container-subpath-projected-xr4w: <nil>
STEP: delete the pod
Dec 23 18:56:37.193: INFO: Waiting for pod pod-subpath-test-projected-xr4w to disappear
Dec 23 18:56:37.202: INFO: Pod pod-subpath-test-projected-xr4w no longer exists
STEP: Deleting pod pod-subpath-test-projected-xr4w
Dec 23 18:56:37.203: INFO: Deleting pod "pod-subpath-test-projected-xr4w" in namespace "subpath-2897"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:56:37.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2897" for this suite.
Dec 23 18:56:43.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:56:43.443: INFO: namespace subpath-2897 deletion completed in 6.226907409s

• [SLOW TEST:28.486 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:56:43.446: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Dec 23 18:56:43.511: INFO: Waiting up to 5m0s for pod "client-containers-ffc4062b-1b06-4bfb-8137-6076cc11b1f6" in namespace "containers-6499" to be "success or failure"
Dec 23 18:56:43.516: INFO: Pod "client-containers-ffc4062b-1b06-4bfb-8137-6076cc11b1f6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.199632ms
Dec 23 18:56:45.522: INFO: Pod "client-containers-ffc4062b-1b06-4bfb-8137-6076cc11b1f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011488736s
STEP: Saw pod success
Dec 23 18:56:45.523: INFO: Pod "client-containers-ffc4062b-1b06-4bfb-8137-6076cc11b1f6" satisfied condition "success or failure"
Dec 23 18:56:45.528: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod client-containers-ffc4062b-1b06-4bfb-8137-6076cc11b1f6 container test-container: <nil>
STEP: delete the pod
Dec 23 18:56:45.568: INFO: Waiting for pod client-containers-ffc4062b-1b06-4bfb-8137-6076cc11b1f6 to disappear
Dec 23 18:56:45.573: INFO: Pod client-containers-ffc4062b-1b06-4bfb-8137-6076cc11b1f6 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:56:45.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6499" for this suite.
Dec 23 18:56:51.613: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:56:51.808: INFO: namespace containers-6499 deletion completed in 6.223358708s

• [SLOW TEST:8.363 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:56:51.814: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-b323daa2-4a8f-49a9-a9bf-c659c1d90bb9
STEP: Creating a pod to test consume configMaps
Dec 23 18:56:51.931: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8856a6e3-ff2f-4610-b2ff-5099c55f3176" in namespace "projected-3278" to be "success or failure"
Dec 23 18:56:51.974: INFO: Pod "pod-projected-configmaps-8856a6e3-ff2f-4610-b2ff-5099c55f3176": Phase="Pending", Reason="", readiness=false. Elapsed: 41.429771ms
Dec 23 18:56:53.981: INFO: Pod "pod-projected-configmaps-8856a6e3-ff2f-4610-b2ff-5099c55f3176": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.048145378s
STEP: Saw pod success
Dec 23 18:56:53.981: INFO: Pod "pod-projected-configmaps-8856a6e3-ff2f-4610-b2ff-5099c55f3176" satisfied condition "success or failure"
Dec 23 18:56:53.987: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod pod-projected-configmaps-8856a6e3-ff2f-4610-b2ff-5099c55f3176 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 23 18:56:54.025: INFO: Waiting for pod pod-projected-configmaps-8856a6e3-ff2f-4610-b2ff-5099c55f3176 to disappear
Dec 23 18:56:54.032: INFO: Pod pod-projected-configmaps-8856a6e3-ff2f-4610-b2ff-5099c55f3176 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:56:54.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3278" for this suite.
Dec 23 18:57:00.071: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:57:00.288: INFO: namespace projected-3278 deletion completed in 6.244306054s

• [SLOW TEST:8.475 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:57:00.291: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Dec 23 18:57:00.395: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 create -f - --namespace=kubectl-9743'
Dec 23 18:57:01.342: INFO: stderr: ""
Dec 23 18:57:01.343: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 23 18:57:02.358: INFO: Selector matched 1 pods for map[app:redis]
Dec 23 18:57:02.358: INFO: Found 0 / 1
Dec 23 18:57:03.357: INFO: Selector matched 1 pods for map[app:redis]
Dec 23 18:57:03.357: INFO: Found 1 / 1
Dec 23 18:57:03.357: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec 23 18:57:03.365: INFO: Selector matched 1 pods for map[app:redis]
Dec 23 18:57:03.365: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 23 18:57:03.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 patch pod redis-master-qwtns --namespace=kubectl-9743 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec 23 18:57:03.512: INFO: stderr: ""
Dec 23 18:57:03.512: INFO: stdout: "pod/redis-master-qwtns patched\n"
STEP: checking annotations
Dec 23 18:57:03.518: INFO: Selector matched 1 pods for map[app:redis]
Dec 23 18:57:03.518: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:57:03.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9743" for this suite.
Dec 23 18:57:25.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:57:25.753: INFO: namespace kubectl-9743 deletion completed in 22.224664976s

• [SLOW TEST:25.462 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:57:25.758: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-038e9ffc-f688-47fd-ac3a-674427e12c9b
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-038e9ffc-f688-47fd-ac3a-674427e12c9b
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:58:32.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6851" for this suite.
Dec 23 18:58:54.355: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:58:54.558: INFO: namespace projected-6851 deletion completed in 22.224576824s

• [SLOW TEST:88.800 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:58:54.562: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 23 18:58:54.715: INFO: Waiting up to 5m0s for pod "pod-cc2ec9ea-e3d2-41be-820b-3c43e23b5d96" in namespace "emptydir-9748" to be "success or failure"
Dec 23 18:58:54.724: INFO: Pod "pod-cc2ec9ea-e3d2-41be-820b-3c43e23b5d96": Phase="Pending", Reason="", readiness=false. Elapsed: 9.254535ms
Dec 23 18:58:56.730: INFO: Pod "pod-cc2ec9ea-e3d2-41be-820b-3c43e23b5d96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014614772s
STEP: Saw pod success
Dec 23 18:58:56.730: INFO: Pod "pod-cc2ec9ea-e3d2-41be-820b-3c43e23b5d96" satisfied condition "success or failure"
Dec 23 18:58:56.735: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-cc2ec9ea-e3d2-41be-820b-3c43e23b5d96 container test-container: <nil>
STEP: delete the pod
Dec 23 18:58:56.769: INFO: Waiting for pod pod-cc2ec9ea-e3d2-41be-820b-3c43e23b5d96 to disappear
Dec 23 18:58:56.774: INFO: Pod pod-cc2ec9ea-e3d2-41be-820b-3c43e23b5d96 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:58:56.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9748" for this suite.
Dec 23 18:59:02.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:59:03.034: INFO: namespace emptydir-9748 deletion completed in 6.249845871s

• [SLOW TEST:8.472 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:59:03.037: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:59:05.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9469" for this suite.
Dec 23 18:59:43.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 18:59:43.453: INFO: namespace kubelet-test-9469 deletion completed in 38.227646992s

• [SLOW TEST:40.416 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 18:59:43.455: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec 23 18:59:46.135: INFO: Successfully updated pod "annotationupdate06260e7f-64c9-486e-a8db-c51521f4d9ff"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 18:59:50.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2863" for this suite.
Dec 23 19:00:12.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:00:12.420: INFO: namespace projected-2863 deletion completed in 22.230952549s

• [SLOW TEST:28.965 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:00:12.425: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-f45df108-5eac-4c43-b46a-f07c10bd10f1
STEP: Creating a pod to test consume secrets
Dec 23 19:00:12.506: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-34c95d4c-3551-4483-ba0b-f66dd296e76f" in namespace "projected-7225" to be "success or failure"
Dec 23 19:00:12.523: INFO: Pod "pod-projected-secrets-34c95d4c-3551-4483-ba0b-f66dd296e76f": Phase="Pending", Reason="", readiness=false. Elapsed: 16.97548ms
Dec 23 19:00:14.531: INFO: Pod "pod-projected-secrets-34c95d4c-3551-4483-ba0b-f66dd296e76f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.025210575s
STEP: Saw pod success
Dec 23 19:00:14.531: INFO: Pod "pod-projected-secrets-34c95d4c-3551-4483-ba0b-f66dd296e76f" satisfied condition "success or failure"
Dec 23 19:00:14.537: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod pod-projected-secrets-34c95d4c-3551-4483-ba0b-f66dd296e76f container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 23 19:00:14.580: INFO: Waiting for pod pod-projected-secrets-34c95d4c-3551-4483-ba0b-f66dd296e76f to disappear
Dec 23 19:00:14.591: INFO: Pod pod-projected-secrets-34c95d4c-3551-4483-ba0b-f66dd296e76f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:00:14.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7225" for this suite.
Dec 23 19:00:20.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:00:20.829: INFO: namespace projected-7225 deletion completed in 6.227895675s

• [SLOW TEST:8.405 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:00:20.836: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec 23 19:00:20.956: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-7974,SelfLink:/api/v1/namespaces/watch-7974/configmaps/e2e-watch-test-watch-closed,UID:5030012f-da86-4b44-bf4f-6e76008b4ef7,ResourceVersion:35610,Generation:0,CreationTimestamp:2019-12-23 19:00:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 23 19:00:20.956: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-7974,SelfLink:/api/v1/namespaces/watch-7974/configmaps/e2e-watch-test-watch-closed,UID:5030012f-da86-4b44-bf4f-6e76008b4ef7,ResourceVersion:35611,Generation:0,CreationTimestamp:2019-12-23 19:00:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec 23 19:00:20.991: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-7974,SelfLink:/api/v1/namespaces/watch-7974/configmaps/e2e-watch-test-watch-closed,UID:5030012f-da86-4b44-bf4f-6e76008b4ef7,ResourceVersion:35613,Generation:0,CreationTimestamp:2019-12-23 19:00:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 23 19:00:20.992: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-7974,SelfLink:/api/v1/namespaces/watch-7974/configmaps/e2e-watch-test-watch-closed,UID:5030012f-da86-4b44-bf4f-6e76008b4ef7,ResourceVersion:35614,Generation:0,CreationTimestamp:2019-12-23 19:00:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:00:20.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7974" for this suite.
Dec 23 19:00:27.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:00:27.246: INFO: namespace watch-7974 deletion completed in 6.244950428s

• [SLOW TEST:6.411 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:00:27.249: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-cb4d85b6-ebf5-4202-99d2-a53599463ff9
STEP: Creating secret with name s-test-opt-upd-76180775-8c0b-4750-98f4-1f2be870b82a
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-cb4d85b6-ebf5-4202-99d2-a53599463ff9
STEP: Updating secret s-test-opt-upd-76180775-8c0b-4750-98f4-1f2be870b82a
STEP: Creating secret with name s-test-opt-create-ce08b9f0-266f-45ec-ad69-6db9b1ee1636
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:00:31.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8248" for this suite.
Dec 23 19:00:53.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:00:53.765: INFO: namespace secrets-8248 deletion completed in 22.217762131s

• [SLOW TEST:26.517 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:00:53.769: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-bf3afe41-674b-4ce2-bced-f440271f1be3
STEP: Creating a pod to test consume secrets
Dec 23 19:00:53.856: INFO: Waiting up to 5m0s for pod "pod-secrets-46b06646-d7ac-453c-b0cf-b2b84ae772be" in namespace "secrets-3749" to be "success or failure"
Dec 23 19:00:53.875: INFO: Pod "pod-secrets-46b06646-d7ac-453c-b0cf-b2b84ae772be": Phase="Pending", Reason="", readiness=false. Elapsed: 18.173207ms
Dec 23 19:00:55.881: INFO: Pod "pod-secrets-46b06646-d7ac-453c-b0cf-b2b84ae772be": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.02437352s
STEP: Saw pod success
Dec 23 19:00:55.881: INFO: Pod "pod-secrets-46b06646-d7ac-453c-b0cf-b2b84ae772be" satisfied condition "success or failure"
Dec 23 19:00:55.886: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod pod-secrets-46b06646-d7ac-453c-b0cf-b2b84ae772be container secret-volume-test: <nil>
STEP: delete the pod
Dec 23 19:00:55.924: INFO: Waiting for pod pod-secrets-46b06646-d7ac-453c-b0cf-b2b84ae772be to disappear
Dec 23 19:00:55.929: INFO: Pod pod-secrets-46b06646-d7ac-453c-b0cf-b2b84ae772be no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:00:55.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3749" for this suite.
Dec 23 19:01:01.975: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:01:02.176: INFO: namespace secrets-3749 deletion completed in 6.237233367s

• [SLOW TEST:8.408 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:01:02.185: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 23 19:01:02.251: INFO: Waiting up to 5m0s for pod "pod-d9464e3d-ba05-43a3-af6a-00417c0a1c66" in namespace "emptydir-218" to be "success or failure"
Dec 23 19:01:02.258: INFO: Pod "pod-d9464e3d-ba05-43a3-af6a-00417c0a1c66": Phase="Pending", Reason="", readiness=false. Elapsed: 6.868101ms
Dec 23 19:01:04.264: INFO: Pod "pod-d9464e3d-ba05-43a3-af6a-00417c0a1c66": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013063384s
STEP: Saw pod success
Dec 23 19:01:04.264: INFO: Pod "pod-d9464e3d-ba05-43a3-af6a-00417c0a1c66" satisfied condition "success or failure"
Dec 23 19:01:04.270: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-d9464e3d-ba05-43a3-af6a-00417c0a1c66 container test-container: <nil>
STEP: delete the pod
Dec 23 19:01:04.304: INFO: Waiting for pod pod-d9464e3d-ba05-43a3-af6a-00417c0a1c66 to disappear
Dec 23 19:01:04.311: INFO: Pod pod-d9464e3d-ba05-43a3-af6a-00417c0a1c66 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:01:04.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-218" for this suite.
Dec 23 19:01:10.343: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:01:10.566: INFO: namespace emptydir-218 deletion completed in 6.245165957s

• [SLOW TEST:8.382 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:01:10.574: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-d3336430-d8d2-41c3-bdbc-03b1366476e8 in namespace container-probe-6323
Dec 23 19:01:12.658: INFO: Started pod busybox-d3336430-d8d2-41c3-bdbc-03b1366476e8 in namespace container-probe-6323
STEP: checking the pod's current state and verifying that restartCount is present
Dec 23 19:01:12.663: INFO: Initial restart count of pod busybox-d3336430-d8d2-41c3-bdbc-03b1366476e8 is 0
Dec 23 19:02:04.828: INFO: Restart count of pod container-probe-6323/busybox-d3336430-d8d2-41c3-bdbc-03b1366476e8 is now 1 (52.164104741s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:02:04.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6323" for this suite.
Dec 23 19:02:10.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:02:11.115: INFO: namespace container-probe-6323 deletion completed in 6.231929906s

• [SLOW TEST:60.542 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:02:11.119: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec 23 19:02:11.222: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-6000,SelfLink:/api/v1/namespaces/watch-6000/configmaps/e2e-watch-test-resource-version,UID:64d7225e-cdc9-4091-9cf4-e1c18c50f4aa,ResourceVersion:36070,Generation:0,CreationTimestamp:2019-12-23 19:02:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 23 19:02:11.225: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-6000,SelfLink:/api/v1/namespaces/watch-6000/configmaps/e2e-watch-test-resource-version,UID:64d7225e-cdc9-4091-9cf4-e1c18c50f4aa,ResourceVersion:36072,Generation:0,CreationTimestamp:2019-12-23 19:02:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:02:11.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6000" for this suite.
Dec 23 19:02:17.260: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:02:17.464: INFO: namespace watch-6000 deletion completed in 6.226422796s

• [SLOW TEST:6.346 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:02:17.491: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec 23 19:02:17.607: INFO: PodSpec: initContainers in spec.initContainers
Dec 23 19:03:02.651: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-abfc4a0e-758d-4bba-b35d-95fedc190719", GenerateName:"", Namespace:"init-container-3171", SelfLink:"/api/v1/namespaces/init-container-3171/pods/pod-init-abfc4a0e-758d-4bba-b35d-95fedc190719", UID:"a0c7c851-c329-4ecf-ac8b-97475e256220", ResourceVersion:"36244", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63712724537, loc:(*time.Location)(0x7ed1a20)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"606358392"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.42.209.105/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-cq2qf", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002a4fe00), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-cq2qf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-cq2qf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-cq2qf", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002b86318), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8s-conformance-1-15-etcd-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc002d6c7e0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002b86390)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002b863b0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002b863b8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002b863bc), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712724537, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712724537, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712724537, loc:(*time.Location)(0x7ed1a20)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63712724537, loc:(*time.Location)(0x7ed1a20)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"72.2.119.135", PodIP:"10.42.209.105", StartTime:(*v1.Time)(0xc0022ea380), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0028bd9d0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0028bda40)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://6ada1567e0bfe1213eed6ed8136b664ab8e370e98549c56df786bc80ef1f7c06"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0022ea3c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0022ea3a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:03:02.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3171" for this suite.
Dec 23 19:03:24.687: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:03:24.897: INFO: namespace init-container-3171 deletion completed in 22.233431935s

• [SLOW TEST:67.406 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:03:24.903: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec 23 19:03:25.014: INFO: Waiting up to 5m0s for pod "downward-api-9eece8a0-4f99-41d4-8a0d-f74f06dfc075" in namespace "downward-api-961" to be "success or failure"
Dec 23 19:03:25.035: INFO: Pod "downward-api-9eece8a0-4f99-41d4-8a0d-f74f06dfc075": Phase="Pending", Reason="", readiness=false. Elapsed: 20.109217ms
Dec 23 19:03:27.042: INFO: Pod "downward-api-9eece8a0-4f99-41d4-8a0d-f74f06dfc075": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.026872783s
STEP: Saw pod success
Dec 23 19:03:27.042: INFO: Pod "downward-api-9eece8a0-4f99-41d4-8a0d-f74f06dfc075" satisfied condition "success or failure"
Dec 23 19:03:27.049: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod downward-api-9eece8a0-4f99-41d4-8a0d-f74f06dfc075 container dapi-container: <nil>
STEP: delete the pod
Dec 23 19:03:27.088: INFO: Waiting for pod downward-api-9eece8a0-4f99-41d4-8a0d-f74f06dfc075 to disappear
Dec 23 19:03:27.093: INFO: Pod downward-api-9eece8a0-4f99-41d4-8a0d-f74f06dfc075 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:03:27.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-961" for this suite.
Dec 23 19:03:33.127: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:03:33.345: INFO: namespace downward-api-961 deletion completed in 6.240744032s

• [SLOW TEST:8.445 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:03:33.352: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Dec 23 19:03:33.431: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-6076" to be "success or failure"
Dec 23 19:03:33.439: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 7.540064ms
Dec 23 19:03:35.445: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013412157s
STEP: Saw pod success
Dec 23 19:03:35.445: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Dec 23 19:03:35.449: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Dec 23 19:03:35.485: INFO: Waiting for pod pod-host-path-test to disappear
Dec 23 19:03:35.493: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:03:35.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-6076" for this suite.
Dec 23 19:03:41.576: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:03:41.767: INFO: namespace hostpath-6076 deletion completed in 6.263102531s

• [SLOW TEST:8.415 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:03:41.770: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 23 19:03:41.881: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:03:44.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-5257" for this suite.
Dec 23 19:03:50.536: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:03:50.734: INFO: namespace custom-resource-definition-5257 deletion completed in 6.222890527s

• [SLOW TEST:8.965 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:03:50.739: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 23 19:03:52.914: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:03:52.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7102" for this suite.
Dec 23 19:03:58.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:03:59.209: INFO: namespace container-runtime-7102 deletion completed in 6.245335273s

• [SLOW TEST:8.471 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:03:59.217: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-czjt
STEP: Creating a pod to test atomic-volume-subpath
Dec 23 19:03:59.345: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-czjt" in namespace "subpath-3133" to be "success or failure"
Dec 23 19:03:59.354: INFO: Pod "pod-subpath-test-secret-czjt": Phase="Pending", Reason="", readiness=false. Elapsed: 5.854022ms
Dec 23 19:04:01.361: INFO: Pod "pod-subpath-test-secret-czjt": Phase="Running", Reason="", readiness=true. Elapsed: 2.013309226s
Dec 23 19:04:03.366: INFO: Pod "pod-subpath-test-secret-czjt": Phase="Running", Reason="", readiness=true. Elapsed: 4.018402941s
Dec 23 19:04:05.372: INFO: Pod "pod-subpath-test-secret-czjt": Phase="Running", Reason="", readiness=true. Elapsed: 6.02411439s
Dec 23 19:04:07.381: INFO: Pod "pod-subpath-test-secret-czjt": Phase="Running", Reason="", readiness=true. Elapsed: 8.032748776s
Dec 23 19:04:09.386: INFO: Pod "pod-subpath-test-secret-czjt": Phase="Running", Reason="", readiness=true. Elapsed: 10.038322134s
Dec 23 19:04:11.393: INFO: Pod "pod-subpath-test-secret-czjt": Phase="Running", Reason="", readiness=true. Elapsed: 12.044968203s
Dec 23 19:04:13.399: INFO: Pod "pod-subpath-test-secret-czjt": Phase="Running", Reason="", readiness=true. Elapsed: 14.050852071s
Dec 23 19:04:15.404: INFO: Pod "pod-subpath-test-secret-czjt": Phase="Running", Reason="", readiness=true. Elapsed: 16.056621888s
Dec 23 19:04:17.414: INFO: Pod "pod-subpath-test-secret-czjt": Phase="Running", Reason="", readiness=true. Elapsed: 18.066711813s
Dec 23 19:04:19.421: INFO: Pod "pod-subpath-test-secret-czjt": Phase="Running", Reason="", readiness=true. Elapsed: 20.072894114s
Dec 23 19:04:21.428: INFO: Pod "pod-subpath-test-secret-czjt": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.080541637s
STEP: Saw pod success
Dec 23 19:04:21.429: INFO: Pod "pod-subpath-test-secret-czjt" satisfied condition "success or failure"
Dec 23 19:04:21.433: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-subpath-test-secret-czjt container test-container-subpath-secret-czjt: <nil>
STEP: delete the pod
Dec 23 19:04:21.465: INFO: Waiting for pod pod-subpath-test-secret-czjt to disappear
Dec 23 19:04:21.474: INFO: Pod pod-subpath-test-secret-czjt no longer exists
STEP: Deleting pod pod-subpath-test-secret-czjt
Dec 23 19:04:21.474: INFO: Deleting pod "pod-subpath-test-secret-czjt" in namespace "subpath-3133"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:04:21.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3133" for this suite.
Dec 23 19:04:27.626: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:04:27.845: INFO: namespace subpath-3133 deletion completed in 6.239123906s

• [SLOW TEST:28.628 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:04:27.854: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 23 19:04:27.976: INFO: Waiting up to 5m0s for pod "pod-20d9d94d-62ef-4b3c-af6f-1b826a95eaf5" in namespace "emptydir-9927" to be "success or failure"
Dec 23 19:04:28.009: INFO: Pod "pod-20d9d94d-62ef-4b3c-af6f-1b826a95eaf5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.925784ms
Dec 23 19:04:30.016: INFO: Pod "pod-20d9d94d-62ef-4b3c-af6f-1b826a95eaf5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.039414025s
STEP: Saw pod success
Dec 23 19:04:30.016: INFO: Pod "pod-20d9d94d-62ef-4b3c-af6f-1b826a95eaf5" satisfied condition "success or failure"
Dec 23 19:04:30.023: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod pod-20d9d94d-62ef-4b3c-af6f-1b826a95eaf5 container test-container: <nil>
STEP: delete the pod
Dec 23 19:04:30.065: INFO: Waiting for pod pod-20d9d94d-62ef-4b3c-af6f-1b826a95eaf5 to disappear
Dec 23 19:04:30.074: INFO: Pod pod-20d9d94d-62ef-4b3c-af6f-1b826a95eaf5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:04:30.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9927" for this suite.
Dec 23 19:04:36.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:04:36.350: INFO: namespace emptydir-9927 deletion completed in 6.266904034s

• [SLOW TEST:8.497 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:04:36.353: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec 23 19:04:36.443: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2394,SelfLink:/api/v1/namespaces/watch-2394/configmaps/e2e-watch-test-label-changed,UID:cace875a-9fc7-4fb5-b573-dc605ae9cfec,ResourceVersion:36708,Generation:0,CreationTimestamp:2019-12-23 19:04:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 23 19:04:36.444: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2394,SelfLink:/api/v1/namespaces/watch-2394/configmaps/e2e-watch-test-label-changed,UID:cace875a-9fc7-4fb5-b573-dc605ae9cfec,ResourceVersion:36709,Generation:0,CreationTimestamp:2019-12-23 19:04:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 23 19:04:36.444: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2394,SelfLink:/api/v1/namespaces/watch-2394/configmaps/e2e-watch-test-label-changed,UID:cace875a-9fc7-4fb5-b573-dc605ae9cfec,ResourceVersion:36710,Generation:0,CreationTimestamp:2019-12-23 19:04:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec 23 19:04:46.490: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2394,SelfLink:/api/v1/namespaces/watch-2394/configmaps/e2e-watch-test-label-changed,UID:cace875a-9fc7-4fb5-b573-dc605ae9cfec,ResourceVersion:36736,Generation:0,CreationTimestamp:2019-12-23 19:04:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 23 19:04:46.490: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2394,SelfLink:/api/v1/namespaces/watch-2394/configmaps/e2e-watch-test-label-changed,UID:cace875a-9fc7-4fb5-b573-dc605ae9cfec,ResourceVersion:36737,Generation:0,CreationTimestamp:2019-12-23 19:04:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Dec 23 19:04:46.491: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-2394,SelfLink:/api/v1/namespaces/watch-2394/configmaps/e2e-watch-test-label-changed,UID:cace875a-9fc7-4fb5-b573-dc605ae9cfec,ResourceVersion:36738,Generation:0,CreationTimestamp:2019-12-23 19:04:36 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:04:46.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2394" for this suite.
Dec 23 19:04:52.524: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:04:52.724: INFO: namespace watch-2394 deletion completed in 6.222277334s

• [SLOW TEST:16.372 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:04:52.726: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:04:56.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6292" for this suite.
Dec 23 19:05:02.909: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:05:03.124: INFO: namespace kubelet-test-6292 deletion completed in 6.254634374s

• [SLOW TEST:10.398 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:05:03.160: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:05:05.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-3993" for this suite.
Dec 23 19:05:11.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:05:11.714: INFO: namespace emptydir-wrapper-3993 deletion completed in 6.352186223s

• [SLOW TEST:8.555 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:05:11.718: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1721
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 23 19:05:11.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-8858'
Dec 23 19:05:12.117: INFO: stderr: ""
Dec 23 19:05:12.117: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Dec 23 19:05:17.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 get pod e2e-test-nginx-pod --namespace=kubectl-8858 -o json'
Dec 23 19:05:17.404: INFO: stderr: ""
Dec 23 19:05:17.404: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.42.209.109/32\"\n        },\n        \"creationTimestamp\": \"2019-12-23T19:05:12Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-8858\",\n        \"resourceVersion\": \"36901\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-8858/pods/e2e-test-nginx-pod\",\n        \"uid\": \"6a65390d-55b6-4c83-8cf4-421693c482d2\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-dfssw\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8s-conformance-1-15-etcd-3\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-dfssw\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-dfssw\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-23T19:05:12Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-23T19:05:13Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-23T19:05:13Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-23T19:05:12Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://e2b11b83e9a2f0237324f8f24bc63b31eb41cc58ddc0ab5017e601cfd222a137\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-12-23T19:05:12Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"72.2.119.135\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.42.209.109\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-12-23T19:05:12Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec 23 19:05:17.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 replace -f - --namespace=kubectl-8858'
Dec 23 19:05:17.962: INFO: stderr: ""
Dec 23 19:05:17.962: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1726
Dec 23 19:05:17.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 delete pods e2e-test-nginx-pod --namespace=kubectl-8858'
Dec 23 19:05:21.261: INFO: stderr: ""
Dec 23 19:05:21.261: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:05:21.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8858" for this suite.
Dec 23 19:05:27.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:05:27.495: INFO: namespace kubectl-8858 deletion completed in 6.224409007s

• [SLOW TEST:15.777 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:05:27.499: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Dec 23 19:05:27.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 cluster-info'
Dec 23 19:05:27.872: INFO: stderr: ""
Dec 23 19:05:27.872: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.43.0.1:443\x1b[0m\n\x1b[0;32mCoreDNS\x1b[0m is running at \x1b[0;33mhttps://10.43.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:05:27.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8381" for this suite.
Dec 23 19:05:33.907: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:05:34.108: INFO: namespace kubectl-8381 deletion completed in 6.226241032s

• [SLOW TEST:6.609 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:05:34.114: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-a185e8a8-981a-46b1-86e0-9040917b59db
STEP: Creating secret with name s-test-opt-upd-2caedb78-3d8e-4afe-822f-4d2bc01fd1a0
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-a185e8a8-981a-46b1-86e0-9040917b59db
STEP: Updating secret s-test-opt-upd-2caedb78-3d8e-4afe-822f-4d2bc01fd1a0
STEP: Creating secret with name s-test-opt-create-0621dfa7-b301-4963-b248-2fa2fc884457
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:05:38.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9616" for this suite.
Dec 23 19:06:00.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:06:00.632: INFO: namespace projected-9616 deletion completed in 22.240938348s

• [SLOW TEST:26.518 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:06:00.636: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-63a33088-db44-47eb-9c9d-2efe2f2a0cf8
STEP: Creating a pod to test consume configMaps
Dec 23 19:06:00.714: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-55e428d0-0e81-40dc-82d9-108834be6e5a" in namespace "projected-6680" to be "success or failure"
Dec 23 19:06:00.722: INFO: Pod "pod-projected-configmaps-55e428d0-0e81-40dc-82d9-108834be6e5a": Phase="Pending", Reason="", readiness=false. Elapsed: 7.805671ms
Dec 23 19:06:02.728: INFO: Pod "pod-projected-configmaps-55e428d0-0e81-40dc-82d9-108834be6e5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014033615s
STEP: Saw pod success
Dec 23 19:06:02.729: INFO: Pod "pod-projected-configmaps-55e428d0-0e81-40dc-82d9-108834be6e5a" satisfied condition "success or failure"
Dec 23 19:06:02.734: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-projected-configmaps-55e428d0-0e81-40dc-82d9-108834be6e5a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 23 19:06:02.770: INFO: Waiting for pod pod-projected-configmaps-55e428d0-0e81-40dc-82d9-108834be6e5a to disappear
Dec 23 19:06:02.775: INFO: Pod pod-projected-configmaps-55e428d0-0e81-40dc-82d9-108834be6e5a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:06:02.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6680" for this suite.
Dec 23 19:06:08.808: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:06:09.013: INFO: namespace projected-6680 deletion completed in 6.228356369s

• [SLOW TEST:8.377 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:06:09.021: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 23 19:06:09.140: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6eec56e4-cc91-4350-97c9-21d256ddcefb" in namespace "downward-api-1066" to be "success or failure"
Dec 23 19:06:09.152: INFO: Pod "downwardapi-volume-6eec56e4-cc91-4350-97c9-21d256ddcefb": Phase="Pending", Reason="", readiness=false. Elapsed: 12.636742ms
Dec 23 19:06:11.161: INFO: Pod "downwardapi-volume-6eec56e4-cc91-4350-97c9-21d256ddcefb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.020805952s
STEP: Saw pod success
Dec 23 19:06:11.162: INFO: Pod "downwardapi-volume-6eec56e4-cc91-4350-97c9-21d256ddcefb" satisfied condition "success or failure"
Dec 23 19:06:11.171: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod downwardapi-volume-6eec56e4-cc91-4350-97c9-21d256ddcefb container client-container: <nil>
STEP: delete the pod
Dec 23 19:06:11.212: INFO: Waiting for pod downwardapi-volume-6eec56e4-cc91-4350-97c9-21d256ddcefb to disappear
Dec 23 19:06:11.224: INFO: Pod downwardapi-volume-6eec56e4-cc91-4350-97c9-21d256ddcefb no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:06:11.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1066" for this suite.
Dec 23 19:06:17.261: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:06:17.469: INFO: namespace downward-api-1066 deletion completed in 6.233826784s

• [SLOW TEST:8.448 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:06:17.471: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec 23 19:06:17.558: INFO: Waiting up to 5m0s for pod "downward-api-ce2c602b-67d2-4f62-bf04-aaf98a036915" in namespace "downward-api-3375" to be "success or failure"
Dec 23 19:06:17.565: INFO: Pod "downward-api-ce2c602b-67d2-4f62-bf04-aaf98a036915": Phase="Pending", Reason="", readiness=false. Elapsed: 6.053291ms
Dec 23 19:06:19.572: INFO: Pod "downward-api-ce2c602b-67d2-4f62-bf04-aaf98a036915": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01335305s
STEP: Saw pod success
Dec 23 19:06:19.572: INFO: Pod "downward-api-ce2c602b-67d2-4f62-bf04-aaf98a036915" satisfied condition "success or failure"
Dec 23 19:06:19.578: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod downward-api-ce2c602b-67d2-4f62-bf04-aaf98a036915 container dapi-container: <nil>
STEP: delete the pod
Dec 23 19:06:19.612: INFO: Waiting for pod downward-api-ce2c602b-67d2-4f62-bf04-aaf98a036915 to disappear
Dec 23 19:06:19.617: INFO: Pod downward-api-ce2c602b-67d2-4f62-bf04-aaf98a036915 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:06:19.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3375" for this suite.
Dec 23 19:06:25.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:06:25.853: INFO: namespace downward-api-3375 deletion completed in 6.225664428s

• [SLOW TEST:8.382 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:06:25.858: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3451.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3451.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 23 19:06:28.014: INFO: DNS probes using dns-3451/dns-test-63e5d4d0-876e-41bb-a021-675e203f5d43 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:06:28.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3451" for this suite.
Dec 23 19:06:34.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:06:34.298: INFO: namespace dns-3451 deletion completed in 6.226975272s

• [SLOW TEST:8.440 seconds]
[sig-network] DNS
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:06:34.302: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1612
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Dec 23 19:06:34.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-3430'
Dec 23 19:06:34.657: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 23 19:06:34.657: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1617
Dec 23 19:06:34.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 delete jobs e2e-test-nginx-job --namespace=kubectl-3430'
Dec 23 19:06:34.938: INFO: stderr: ""
Dec 23 19:06:34.938: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:06:34.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3430" for this suite.
Dec 23 19:06:56.985: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:06:57.192: INFO: namespace kubectl-3430 deletion completed in 22.233962876s

• [SLOW TEST:22.891 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:06:57.197: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 23 19:06:57.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 create -f - --namespace=kubectl-2366'
Dec 23 19:06:57.948: INFO: stderr: ""
Dec 23 19:06:57.948: INFO: stdout: "replicationcontroller/redis-master created\n"
Dec 23 19:06:57.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 create -f - --namespace=kubectl-2366'
Dec 23 19:06:58.572: INFO: stderr: ""
Dec 23 19:06:58.572: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 23 19:06:59.579: INFO: Selector matched 1 pods for map[app:redis]
Dec 23 19:06:59.579: INFO: Found 1 / 1
Dec 23 19:06:59.579: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 23 19:06:59.583: INFO: Selector matched 1 pods for map[app:redis]
Dec 23 19:06:59.584: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 23 19:06:59.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 describe pod redis-master-tgbn8 --namespace=kubectl-2366'
Dec 23 19:06:59.849: INFO: stderr: ""
Dec 23 19:06:59.850: INFO: stdout: "Name:           redis-master-tgbn8\nNamespace:      kubectl-2366\nPriority:       0\nNode:           k8s-conformance-1-15-control-1/72.2.119.160\nStart Time:     Mon, 23 Dec 2019 19:06:57 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    cni.projectcalico.org/podIP: 10.42.206.70/32\nStatus:         Running\nIP:             10.42.206.70\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://b9cdaa155fb4d1082a3df27fd8a6850716bb3b53c99e377a2b0e4cfd788ddf29\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Mon, 23 Dec 2019 19:06:58 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-p68xf (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-p68xf:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-p68xf\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                     Message\n  ----    ------     ----  ----                                     -------\n  Normal  Scheduled  2s    default-scheduler                        Successfully assigned kubectl-2366/redis-master-tgbn8 to k8s-conformance-1-15-control-1\n  Normal  Pulled     1s    kubelet, k8s-conformance-1-15-control-1  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, k8s-conformance-1-15-control-1  Created container redis-master\n  Normal  Started    1s    kubelet, k8s-conformance-1-15-control-1  Started container redis-master\n"
Dec 23 19:06:59.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 describe rc redis-master --namespace=kubectl-2366'
Dec 23 19:07:00.193: INFO: stderr: ""
Dec 23 19:07:00.193: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-2366\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-tgbn8\n"
Dec 23 19:07:00.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 describe service redis-master --namespace=kubectl-2366'
Dec 23 19:07:00.474: INFO: stderr: ""
Dec 23 19:07:00.474: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-2366\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.43.219.225\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.42.206.70:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec 23 19:07:00.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 describe node k8s-conformance-1-15-control-1'
Dec 23 19:07:00.757: INFO: stderr: ""
Dec 23 19:07:00.757: INFO: stdout: "Name:               k8s-conformance-1-15-control-1\nRoles:              controlplane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8s-conformance-1-15-control-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/controlplane=true\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 72.2.119.160/23\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.42.206.64\n                    rke.cattle.io/external-ip: 72.2.119.160\n                    rke.cattle.io/internal-ip: 72.2.119.160\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Mon, 23 Dec 2019 16:48:43 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Mon, 23 Dec 2019 16:49:59 +0000   Mon, 23 Dec 2019 16:49:59 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Mon, 23 Dec 2019 19:06:22 +0000   Mon, 23 Dec 2019 16:48:43 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Mon, 23 Dec 2019 19:06:22 +0000   Mon, 23 Dec 2019 16:48:43 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Mon, 23 Dec 2019 19:06:22 +0000   Mon, 23 Dec 2019 16:48:43 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Mon, 23 Dec 2019 19:06:22 +0000   Mon, 23 Dec 2019 16:50:03 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  72.2.119.160\n  Hostname:    k8s-conformance-1-15-control-1\nCapacity:\n cpu:                1\n ephemeral-storage:  7688776Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             1790080Ki\n pods:               110\nAllocatable:\n cpu:                1\n ephemeral-storage:  7085975950\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             1687680Ki\n pods:               110\nSystem Info:\n Machine ID:                 311936e2303b034fe7ef70182235b8cb\n System UUID:                DE037603-5B8F-67F1-EA32-B447CCB95C8A\n Boot ID:                    ff0e7223-2f93-458e-b18a-512d71c7cce1\n Kernel Version:             4.4.0-154-generic\n OS Image:                   Ubuntu 16.04.6 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://17.3.2\n Kubelet Version:            v1.15.6\n Kube-Proxy Version:         v1.15.6\nPodCIDR:                     10.42.0.0/24\nNon-terminated Pods:         (4 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  cattle-system              cattle-node-agent-5hgvg                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         135m\n  kube-system                calico-node-drwck                                          250m (25%)    0 (0%)      0 (0%)           0 (0%)         137m\n  kubectl-2366               redis-master-tgbn8                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         3s\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-ffc6411083f441c4-vcnpn    0 (0%)        0 (0%)      0 (0%)           0 (0%)         75m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                250m (25%)  0 (0%)\n  memory             0 (0%)      0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:              <none>\n"
Dec 23 19:07:00.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 describe namespace kubectl-2366'
Dec 23 19:07:01.053: INFO: stderr: ""
Dec 23 19:07:01.053: INFO: stdout: "Name:         kubectl-2366\nLabels:       e2e-framework=kubectl\n              e2e-run=b3f6ae40-7c58-4c32-ade4-3ef9573f8b96\nAnnotations:  cattle.io/status:\n                {\"Conditions\":[{\"Type\":\"ResourceQuotaInit\",\"Status\":\"True\",\"Message\":\"\",\"LastUpdateTime\":\"2019-12-23T19:06:58Z\"},{\"Type\":\"InitialRolesPopu...\n              lifecycle.cattle.io/create.namespace-auth: true\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:07:01.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2366" for this suite.
Dec 23 19:07:23.102: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:07:23.315: INFO: namespace kubectl-2366 deletion completed in 22.240062975s

• [SLOW TEST:26.119 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:07:23.317: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec 23 19:07:27.492: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3897 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 19:07:27.492: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 19:07:27.593: INFO: Exec stderr: ""
Dec 23 19:07:27.594: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3897 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 19:07:27.594: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 19:07:27.688: INFO: Exec stderr: ""
Dec 23 19:07:27.689: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3897 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 19:07:27.689: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 19:07:27.803: INFO: Exec stderr: ""
Dec 23 19:07:27.803: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3897 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 19:07:27.803: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 19:07:27.884: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec 23 19:07:27.885: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3897 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 19:07:27.885: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 19:07:27.945: INFO: Exec stderr: ""
Dec 23 19:07:27.946: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3897 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 19:07:27.946: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 19:07:28.009: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec 23 19:07:28.010: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3897 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 19:07:28.011: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 19:07:28.086: INFO: Exec stderr: ""
Dec 23 19:07:28.087: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3897 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 19:07:28.087: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 19:07:28.159: INFO: Exec stderr: ""
Dec 23 19:07:28.159: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3897 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 19:07:28.159: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 19:07:28.254: INFO: Exec stderr: ""
Dec 23 19:07:28.254: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3897 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 19:07:28.254: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 19:07:28.344: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:07:28.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-3897" for this suite.
Dec 23 19:08:20.377: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:08:20.610: INFO: namespace e2e-kubelet-etc-hosts-3897 deletion completed in 52.255460073s

• [SLOW TEST:57.293 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:08:20.614: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 23 19:08:20.721: INFO: Waiting up to 5m0s for pod "pod-a10b74ad-de43-488f-b15d-96ffbba228c2" in namespace "emptydir-8279" to be "success or failure"
Dec 23 19:08:20.727: INFO: Pod "pod-a10b74ad-de43-488f-b15d-96ffbba228c2": Phase="Pending", Reason="", readiness=false. Elapsed: 5.916354ms
Dec 23 19:08:22.734: INFO: Pod "pod-a10b74ad-de43-488f-b15d-96ffbba228c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012865524s
STEP: Saw pod success
Dec 23 19:08:22.734: INFO: Pod "pod-a10b74ad-de43-488f-b15d-96ffbba228c2" satisfied condition "success or failure"
Dec 23 19:08:22.740: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-a10b74ad-de43-488f-b15d-96ffbba228c2 container test-container: <nil>
STEP: delete the pod
Dec 23 19:08:22.774: INFO: Waiting for pod pod-a10b74ad-de43-488f-b15d-96ffbba228c2 to disappear
Dec 23 19:08:22.791: INFO: Pod pod-a10b74ad-de43-488f-b15d-96ffbba228c2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:08:22.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8279" for this suite.
Dec 23 19:08:28.825: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:08:29.049: INFO: namespace emptydir-8279 deletion completed in 6.247015455s

• [SLOW TEST:8.436 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:08:29.051: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 23 19:08:29.147: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec 23 19:08:31.232: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:08:31.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4713" for this suite.
Dec 23 19:08:37.288: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:08:37.503: INFO: namespace replication-controller-4713 deletion completed in 6.24359537s

• [SLOW TEST:8.453 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:08:37.507: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:08:43.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6441" for this suite.
Dec 23 19:08:49.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:08:50.059: INFO: namespace namespaces-6441 deletion completed in 6.244676644s
STEP: Destroying namespace "nsdeletetest-6377" for this suite.
Dec 23 19:08:50.063: INFO: Namespace nsdeletetest-6377 was already deleted
STEP: Destroying namespace "nsdeletetest-7654" for this suite.
Dec 23 19:08:56.090: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:08:56.296: INFO: namespace nsdeletetest-7654 deletion completed in 6.23217691s

• [SLOW TEST:18.790 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:08:56.317: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 23 19:08:56.435: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d3980635-1c48-4027-882d-73c233f51881" in namespace "projected-6269" to be "success or failure"
Dec 23 19:08:56.453: INFO: Pod "downwardapi-volume-d3980635-1c48-4027-882d-73c233f51881": Phase="Pending", Reason="", readiness=false. Elapsed: 17.373028ms
Dec 23 19:08:58.458: INFO: Pod "downwardapi-volume-d3980635-1c48-4027-882d-73c233f51881": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023084163s
STEP: Saw pod success
Dec 23 19:08:58.460: INFO: Pod "downwardapi-volume-d3980635-1c48-4027-882d-73c233f51881" satisfied condition "success or failure"
Dec 23 19:08:58.466: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod downwardapi-volume-d3980635-1c48-4027-882d-73c233f51881 container client-container: <nil>
STEP: delete the pod
Dec 23 19:08:58.508: INFO: Waiting for pod downwardapi-volume-d3980635-1c48-4027-882d-73c233f51881 to disappear
Dec 23 19:08:58.512: INFO: Pod downwardapi-volume-d3980635-1c48-4027-882d-73c233f51881 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:08:58.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6269" for this suite.
Dec 23 19:09:04.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:09:04.749: INFO: namespace projected-6269 deletion completed in 6.226918032s

• [SLOW TEST:8.433 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:09:04.751: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec 23 19:09:07.406: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3150 pod-service-account-1318831c-c33a-45b8-83dc-810b9831da33 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec 23 19:09:07.952: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3150 pod-service-account-1318831c-c33a-45b8-83dc-810b9831da33 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec 23 19:09:08.316: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3150 pod-service-account-1318831c-c33a-45b8-83dc-810b9831da33 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:09:08.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3150" for this suite.
Dec 23 19:09:14.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:09:14.896: INFO: namespace svcaccounts-3150 deletion completed in 6.232617866s

• [SLOW TEST:10.145 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:09:14.900: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-4069
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-4069
STEP: Creating statefulset with conflicting port in namespace statefulset-4069
STEP: Waiting until pod test-pod will start running in namespace statefulset-4069
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-4069
Dec 23 19:09:17.134: INFO: Observed stateful pod in namespace: statefulset-4069, name: ss-0, uid: c6393e94-55ba-4a49-8eaf-f33436f921b7, status phase: Pending. Waiting for statefulset controller to delete.
Dec 23 19:09:17.293: INFO: Observed stateful pod in namespace: statefulset-4069, name: ss-0, uid: c6393e94-55ba-4a49-8eaf-f33436f921b7, status phase: Failed. Waiting for statefulset controller to delete.
Dec 23 19:09:17.307: INFO: Observed stateful pod in namespace: statefulset-4069, name: ss-0, uid: c6393e94-55ba-4a49-8eaf-f33436f921b7, status phase: Failed. Waiting for statefulset controller to delete.
Dec 23 19:09:17.315: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-4069
STEP: Removing pod with conflicting port in namespace statefulset-4069
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-4069 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec 23 19:09:19.371: INFO: Deleting all statefulset in ns statefulset-4069
Dec 23 19:09:19.376: INFO: Scaling statefulset ss to 0
Dec 23 19:09:29.402: INFO: Waiting for statefulset status.replicas updated to 0
Dec 23 19:09:29.414: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:09:29.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4069" for this suite.
Dec 23 19:09:35.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:09:35.688: INFO: namespace statefulset-4069 deletion completed in 6.239954236s

• [SLOW TEST:20.788 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:09:35.692: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 23 19:09:39.893: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 23 19:09:39.903: INFO: Pod pod-with-poststart-http-hook still exists
Dec 23 19:09:41.904: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 23 19:09:41.909: INFO: Pod pod-with-poststart-http-hook still exists
Dec 23 19:09:43.904: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 23 19:09:43.911: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:09:43.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-186" for this suite.
Dec 23 19:10:05.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:10:06.153: INFO: namespace container-lifecycle-hook-186 deletion completed in 22.23041384s

• [SLOW TEST:30.462 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:10:06.157: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-25b9befa-3abd-436b-8b77-38387795d858
STEP: Creating a pod to test consume secrets
Dec 23 19:10:06.287: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-a38ee9c9-f1ab-4052-96ee-9f76e1e2c640" in namespace "projected-3427" to be "success or failure"
Dec 23 19:10:06.303: INFO: Pod "pod-projected-secrets-a38ee9c9-f1ab-4052-96ee-9f76e1e2c640": Phase="Pending", Reason="", readiness=false. Elapsed: 15.648301ms
Dec 23 19:10:08.309: INFO: Pod "pod-projected-secrets-a38ee9c9-f1ab-4052-96ee-9f76e1e2c640": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.021032358s
STEP: Saw pod success
Dec 23 19:10:08.309: INFO: Pod "pod-projected-secrets-a38ee9c9-f1ab-4052-96ee-9f76e1e2c640" satisfied condition "success or failure"
Dec 23 19:10:08.313: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod pod-projected-secrets-a38ee9c9-f1ab-4052-96ee-9f76e1e2c640 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 23 19:10:08.353: INFO: Waiting for pod pod-projected-secrets-a38ee9c9-f1ab-4052-96ee-9f76e1e2c640 to disappear
Dec 23 19:10:08.358: INFO: Pod pod-projected-secrets-a38ee9c9-f1ab-4052-96ee-9f76e1e2c640 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:10:08.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3427" for this suite.
Dec 23 19:10:14.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:10:14.609: INFO: namespace projected-3427 deletion completed in 6.23965412s

• [SLOW TEST:8.453 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:10:14.611: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 23 19:10:14.709: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:10:16.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5124" for this suite.
Dec 23 19:10:56.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:10:57.056: INFO: namespace pods-5124 deletion completed in 40.255295138s

• [SLOW TEST:42.445 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:10:57.062: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec 23 19:10:57.172: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:11:13.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4100" for this suite.
Dec 23 19:11:19.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:11:19.536: INFO: namespace pods-4100 deletion completed in 6.220331029s

• [SLOW TEST:22.475 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:11:19.540: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-802bfd18-d212-4cca-bc74-2c86324ff068
STEP: Creating a pod to test consume configMaps
Dec 23 19:11:19.657: INFO: Waiting up to 5m0s for pod "pod-configmaps-5d52f54c-d49d-4b61-b15f-4d0f9c14f090" in namespace "configmap-1396" to be "success or failure"
Dec 23 19:11:19.668: INFO: Pod "pod-configmaps-5d52f54c-d49d-4b61-b15f-4d0f9c14f090": Phase="Pending", Reason="", readiness=false. Elapsed: 10.333321ms
Dec 23 19:11:21.673: INFO: Pod "pod-configmaps-5d52f54c-d49d-4b61-b15f-4d0f9c14f090": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.016129847s
STEP: Saw pod success
Dec 23 19:11:21.674: INFO: Pod "pod-configmaps-5d52f54c-d49d-4b61-b15f-4d0f9c14f090" satisfied condition "success or failure"
Dec 23 19:11:21.678: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-configmaps-5d52f54c-d49d-4b61-b15f-4d0f9c14f090 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 23 19:11:21.721: INFO: Waiting for pod pod-configmaps-5d52f54c-d49d-4b61-b15f-4d0f9c14f090 to disappear
Dec 23 19:11:21.727: INFO: Pod pod-configmaps-5d52f54c-d49d-4b61-b15f-4d0f9c14f090 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:11:21.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1396" for this suite.
Dec 23 19:11:27.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:11:27.978: INFO: namespace configmap-1396 deletion completed in 6.240373103s

• [SLOW TEST:8.439 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:11:27.982: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-cb5c2853-a066-4501-ad40-ea8a266eac3d
STEP: Creating a pod to test consume configMaps
Dec 23 19:11:28.059: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bd0d00b9-089e-46b4-9a82-e4df11cabb5a" in namespace "projected-1049" to be "success or failure"
Dec 23 19:11:28.077: INFO: Pod "pod-projected-configmaps-bd0d00b9-089e-46b4-9a82-e4df11cabb5a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.470711ms
Dec 23 19:11:30.084: INFO: Pod "pod-projected-configmaps-bd0d00b9-089e-46b4-9a82-e4df11cabb5a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024515532s
STEP: Saw pod success
Dec 23 19:11:30.085: INFO: Pod "pod-projected-configmaps-bd0d00b9-089e-46b4-9a82-e4df11cabb5a" satisfied condition "success or failure"
Dec 23 19:11:30.098: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod pod-projected-configmaps-bd0d00b9-089e-46b4-9a82-e4df11cabb5a container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 23 19:11:30.144: INFO: Waiting for pod pod-projected-configmaps-bd0d00b9-089e-46b4-9a82-e4df11cabb5a to disappear
Dec 23 19:11:30.150: INFO: Pod pod-projected-configmaps-bd0d00b9-089e-46b4-9a82-e4df11cabb5a no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:11:30.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1049" for this suite.
Dec 23 19:11:36.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:11:36.386: INFO: namespace projected-1049 deletion completed in 6.225325849s

• [SLOW TEST:8.405 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:11:36.390: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 23 19:11:36.506: INFO: Waiting up to 5m0s for pod "downwardapi-volume-73ce64de-d554-465e-ab31-662bacf845d1" in namespace "downward-api-7717" to be "success or failure"
Dec 23 19:11:36.512: INFO: Pod "downwardapi-volume-73ce64de-d554-465e-ab31-662bacf845d1": Phase="Pending", Reason="", readiness=false. Elapsed: 6.568352ms
Dec 23 19:11:38.521: INFO: Pod "downwardapi-volume-73ce64de-d554-465e-ab31-662bacf845d1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015139992s
STEP: Saw pod success
Dec 23 19:11:38.521: INFO: Pod "downwardapi-volume-73ce64de-d554-465e-ab31-662bacf845d1" satisfied condition "success or failure"
Dec 23 19:11:38.526: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod downwardapi-volume-73ce64de-d554-465e-ab31-662bacf845d1 container client-container: <nil>
STEP: delete the pod
Dec 23 19:11:38.560: INFO: Waiting for pod downwardapi-volume-73ce64de-d554-465e-ab31-662bacf845d1 to disappear
Dec 23 19:11:38.580: INFO: Pod downwardapi-volume-73ce64de-d554-465e-ab31-662bacf845d1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:11:38.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7717" for this suite.
Dec 23 19:11:44.624: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:11:44.840: INFO: namespace downward-api-7717 deletion completed in 6.245331994s

• [SLOW TEST:8.451 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:11:44.857: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec 23 19:11:44.975: INFO: Waiting up to 5m0s for pod "pod-5948e28b-098f-4db6-b29b-deddd0b409e0" in namespace "emptydir-7221" to be "success or failure"
Dec 23 19:11:44.987: INFO: Pod "pod-5948e28b-098f-4db6-b29b-deddd0b409e0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.965523ms
Dec 23 19:11:46.992: INFO: Pod "pod-5948e28b-098f-4db6-b29b-deddd0b409e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01743801s
STEP: Saw pod success
Dec 23 19:11:46.993: INFO: Pod "pod-5948e28b-098f-4db6-b29b-deddd0b409e0" satisfied condition "success or failure"
Dec 23 19:11:46.998: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod pod-5948e28b-098f-4db6-b29b-deddd0b409e0 container test-container: <nil>
STEP: delete the pod
Dec 23 19:11:47.047: INFO: Waiting for pod pod-5948e28b-098f-4db6-b29b-deddd0b409e0 to disappear
Dec 23 19:11:47.052: INFO: Pod pod-5948e28b-098f-4db6-b29b-deddd0b409e0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:11:47.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7221" for this suite.
Dec 23 19:11:53.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:11:53.302: INFO: namespace emptydir-7221 deletion completed in 6.23898521s

• [SLOW TEST:8.446 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:11:53.305: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-003dd582-b773-4a3b-ad52-8070677bc8c1
STEP: Creating a pod to test consume secrets
Dec 23 19:11:53.446: INFO: Waiting up to 5m0s for pod "pod-secrets-ebf6f5da-1084-479c-92ca-84094272ea91" in namespace "secrets-9582" to be "success or failure"
Dec 23 19:11:53.454: INFO: Pod "pod-secrets-ebf6f5da-1084-479c-92ca-84094272ea91": Phase="Pending", Reason="", readiness=false. Elapsed: 7.713275ms
Dec 23 19:11:55.462: INFO: Pod "pod-secrets-ebf6f5da-1084-479c-92ca-84094272ea91": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0158655s
STEP: Saw pod success
Dec 23 19:11:55.463: INFO: Pod "pod-secrets-ebf6f5da-1084-479c-92ca-84094272ea91" satisfied condition "success or failure"
Dec 23 19:11:55.468: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-secrets-ebf6f5da-1084-479c-92ca-84094272ea91 container secret-volume-test: <nil>
STEP: delete the pod
Dec 23 19:11:55.501: INFO: Waiting for pod pod-secrets-ebf6f5da-1084-479c-92ca-84094272ea91 to disappear
Dec 23 19:11:55.509: INFO: Pod pod-secrets-ebf6f5da-1084-479c-92ca-84094272ea91 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:11:55.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9582" for this suite.
Dec 23 19:12:01.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:12:01.793: INFO: namespace secrets-9582 deletion completed in 6.273713138s

• [SLOW TEST:8.489 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:12:01.796: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Dec 23 19:12:01.909: INFO: Waiting up to 5m0s for pod "client-containers-c5c60f4d-5e96-4c9c-be8f-a7ac655f4fa5" in namespace "containers-4982" to be "success or failure"
Dec 23 19:12:01.916: INFO: Pod "client-containers-c5c60f4d-5e96-4c9c-be8f-a7ac655f4fa5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.803632ms
Dec 23 19:12:03.922: INFO: Pod "client-containers-c5c60f4d-5e96-4c9c-be8f-a7ac655f4fa5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01289763s
Dec 23 19:12:05.930: INFO: Pod "client-containers-c5c60f4d-5e96-4c9c-be8f-a7ac655f4fa5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020243524s
Dec 23 19:12:07.936: INFO: Pod "client-containers-c5c60f4d-5e96-4c9c-be8f-a7ac655f4fa5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026308627s
STEP: Saw pod success
Dec 23 19:12:07.936: INFO: Pod "client-containers-c5c60f4d-5e96-4c9c-be8f-a7ac655f4fa5" satisfied condition "success or failure"
Dec 23 19:12:07.940: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod client-containers-c5c60f4d-5e96-4c9c-be8f-a7ac655f4fa5 container test-container: <nil>
STEP: delete the pod
Dec 23 19:12:07.977: INFO: Waiting for pod client-containers-c5c60f4d-5e96-4c9c-be8f-a7ac655f4fa5 to disappear
Dec 23 19:12:07.985: INFO: Pod client-containers-c5c60f4d-5e96-4c9c-be8f-a7ac655f4fa5 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:12:07.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4982" for this suite.
Dec 23 19:12:14.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:12:14.221: INFO: namespace containers-4982 deletion completed in 6.2261047s

• [SLOW TEST:12.426 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:12:14.224: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-8243, will wait for the garbage collector to delete the pods
Dec 23 19:12:16.398: INFO: Deleting Job.batch foo took: 9.919544ms
Dec 23 19:12:16.799: INFO: Terminating Job.batch foo pods took: 400.510484ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:12:54.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8243" for this suite.
Dec 23 19:13:00.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:13:01.135: INFO: namespace job-8243 deletion completed in 6.221519919s

• [SLOW TEST:46.912 seconds]
[sig-apps] Job
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:13:01.140: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-6967
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 23 19:13:01.247: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 23 19:13:25.569: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.209.152 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6967 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 19:13:25.570: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 19:13:26.633: INFO: Found all expected endpoints: [netserver-0]
Dec 23 19:13:26.640: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.108.221 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6967 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 19:13:26.640: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 19:13:27.710: INFO: Found all expected endpoints: [netserver-1]
Dec 23 19:13:27.717: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.97.21 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6967 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 19:13:27.717: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 19:13:28.805: INFO: Found all expected endpoints: [netserver-2]
Dec 23 19:13:28.810: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.206.82 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6967 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 19:13:28.811: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 19:13:29.895: INFO: Found all expected endpoints: [netserver-3]
Dec 23 19:13:29.902: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.92.25 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6967 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 19:13:29.902: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 19:13:30.994: INFO: Found all expected endpoints: [netserver-4]
Dec 23 19:13:30.999: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.150.19 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6967 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 19:13:30.999: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 19:13:32.064: INFO: Found all expected endpoints: [netserver-5]
Dec 23 19:13:32.072: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.17.211 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6967 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 19:13:32.072: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 19:13:33.132: INFO: Found all expected endpoints: [netserver-6]
Dec 23 19:13:33.138: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.132.153 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6967 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 19:13:33.138: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 19:13:34.207: INFO: Found all expected endpoints: [netserver-7]
Dec 23 19:13:34.213: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.209.124 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6967 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 23 19:13:34.213: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
Dec 23 19:13:35.291: INFO: Found all expected endpoints: [netserver-8]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:13:35.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-6967" for this suite.
Dec 23 19:13:59.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:13:59.545: INFO: namespace pod-network-test-6967 deletion completed in 24.243571532s

• [SLOW TEST:58.406 seconds]
[sig-network] Networking
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:13:59.551: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-wrk6
STEP: Creating a pod to test atomic-volume-subpath
Dec 23 19:13:59.683: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-wrk6" in namespace "subpath-9920" to be "success or failure"
Dec 23 19:13:59.690: INFO: Pod "pod-subpath-test-configmap-wrk6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.886613ms
Dec 23 19:14:01.695: INFO: Pod "pod-subpath-test-configmap-wrk6": Phase="Running", Reason="", readiness=true. Elapsed: 2.01201753s
Dec 23 19:14:03.702: INFO: Pod "pod-subpath-test-configmap-wrk6": Phase="Running", Reason="", readiness=true. Elapsed: 4.019030032s
Dec 23 19:14:05.708: INFO: Pod "pod-subpath-test-configmap-wrk6": Phase="Running", Reason="", readiness=true. Elapsed: 6.025297275s
Dec 23 19:14:07.714: INFO: Pod "pod-subpath-test-configmap-wrk6": Phase="Running", Reason="", readiness=true. Elapsed: 8.031131058s
Dec 23 19:14:09.720: INFO: Pod "pod-subpath-test-configmap-wrk6": Phase="Running", Reason="", readiness=true. Elapsed: 10.037224777s
Dec 23 19:14:11.726: INFO: Pod "pod-subpath-test-configmap-wrk6": Phase="Running", Reason="", readiness=true. Elapsed: 12.043174607s
Dec 23 19:14:13.736: INFO: Pod "pod-subpath-test-configmap-wrk6": Phase="Running", Reason="", readiness=true. Elapsed: 14.053365836s
Dec 23 19:14:15.745: INFO: Pod "pod-subpath-test-configmap-wrk6": Phase="Running", Reason="", readiness=true. Elapsed: 16.062191866s
Dec 23 19:14:17.751: INFO: Pod "pod-subpath-test-configmap-wrk6": Phase="Running", Reason="", readiness=true. Elapsed: 18.067883608s
Dec 23 19:14:19.758: INFO: Pod "pod-subpath-test-configmap-wrk6": Phase="Running", Reason="", readiness=true. Elapsed: 20.074696473s
Dec 23 19:14:21.763: INFO: Pod "pod-subpath-test-configmap-wrk6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.080161835s
STEP: Saw pod success
Dec 23 19:14:21.764: INFO: Pod "pod-subpath-test-configmap-wrk6" satisfied condition "success or failure"
Dec 23 19:14:21.769: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-subpath-test-configmap-wrk6 container test-container-subpath-configmap-wrk6: <nil>
STEP: delete the pod
Dec 23 19:14:21.803: INFO: Waiting for pod pod-subpath-test-configmap-wrk6 to disappear
Dec 23 19:14:21.809: INFO: Pod pod-subpath-test-configmap-wrk6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-wrk6
Dec 23 19:14:21.809: INFO: Deleting pod "pod-subpath-test-configmap-wrk6" in namespace "subpath-9920"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:14:21.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9920" for this suite.
Dec 23 19:14:27.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:14:28.050: INFO: namespace subpath-9920 deletion completed in 6.221889174s

• [SLOW TEST:28.503 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:14:28.054: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-1957
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-1957
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1957
Dec 23 19:14:28.190: INFO: Found 0 stateful pods, waiting for 1
Dec 23 19:14:38.196: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec 23 19:14:38.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 23 19:14:38.540: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 23 19:14:38.540: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 23 19:14:38.540: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 23 19:14:38.546: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 23 19:14:48.552: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 23 19:14:48.552: INFO: Waiting for statefulset status.replicas updated to 0
Dec 23 19:14:48.581: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Dec 23 19:14:48.581: INFO: ss-0  k8s-conformance-1-15-control-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:39 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:28 +0000 UTC  }]
Dec 23 19:14:48.581: INFO: ss-1                                  Pending         []
Dec 23 19:14:48.581: INFO: 
Dec 23 19:14:48.582: INFO: StatefulSet ss has not reached scale 3, at 2
Dec 23 19:14:49.589: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990636081s
Dec 23 19:14:50.595: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983946643s
Dec 23 19:14:51.604: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.977461025s
Dec 23 19:14:52.611: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.969038757s
Dec 23 19:14:53.618: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.961379472s
Dec 23 19:14:54.625: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.953621853s
Dec 23 19:14:55.633: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.947241896s
Dec 23 19:14:56.640: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.939377549s
Dec 23 19:14:57.647: INFO: Verifying statefulset ss doesn't scale past 3 for another 932.516444ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1957
Dec 23 19:14:58.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:14:59.027: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Dec 23 19:14:59.027: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 23 19:14:59.027: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 23 19:14:59.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:14:59.423: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 23 19:14:59.423: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 23 19:14:59.423: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 23 19:14:59.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:14:59.822: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 23 19:14:59.822: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Dec 23 19:14:59.822: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Dec 23 19:14:59.828: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 23 19:14:59.828: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 23 19:14:59.828: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec 23 19:14:59.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 23 19:15:00.194: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 23 19:15:00.194: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 23 19:15:00.194: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 23 19:15:00.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 23 19:15:00.632: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 23 19:15:00.632: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 23 19:15:00.632: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 23 19:15:00.632: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Dec 23 19:15:01.003: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Dec 23 19:15:01.003: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Dec 23 19:15:01.003: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Dec 23 19:15:01.003: INFO: Waiting for statefulset status.replicas updated to 0
Dec 23 19:15:01.010: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 1
Dec 23 19:15:11.024: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 23 19:15:11.024: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 23 19:15:11.024: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 23 19:15:11.046: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Dec 23 19:15:11.046: INFO: ss-0  k8s-conformance-1-15-control-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:28 +0000 UTC  }]
Dec 23 19:15:11.046: INFO: ss-1  k8s-conformance-1-15-etcd-3     Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:48 +0000 UTC  }]
Dec 23 19:15:11.046: INFO: ss-2  k8s-conformance-1-15-etcd-2     Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:48 +0000 UTC  }]
Dec 23 19:15:11.047: INFO: 
Dec 23 19:15:11.047: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 23 19:15:12.054: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Dec 23 19:15:12.055: INFO: ss-0  k8s-conformance-1-15-control-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:28 +0000 UTC  }]
Dec 23 19:15:12.055: INFO: ss-1  k8s-conformance-1-15-etcd-3     Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:48 +0000 UTC  }]
Dec 23 19:15:12.055: INFO: ss-2  k8s-conformance-1-15-etcd-2     Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:01 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:48 +0000 UTC  }]
Dec 23 19:15:12.055: INFO: 
Dec 23 19:15:12.056: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 23 19:15:13.062: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Dec 23 19:15:13.063: INFO: ss-0  k8s-conformance-1-15-control-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:28 +0000 UTC  }]
Dec 23 19:15:13.063: INFO: ss-1  k8s-conformance-1-15-etcd-3     Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:48 +0000 UTC  }]
Dec 23 19:15:13.063: INFO: 
Dec 23 19:15:13.063: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 23 19:15:14.070: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Dec 23 19:15:14.071: INFO: ss-0  k8s-conformance-1-15-control-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:28 +0000 UTC  }]
Dec 23 19:15:14.071: INFO: 
Dec 23 19:15:14.071: INFO: StatefulSet ss has not reached scale 0, at 1
Dec 23 19:15:15.078: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Dec 23 19:15:15.079: INFO: ss-0  k8s-conformance-1-15-control-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:28 +0000 UTC  }]
Dec 23 19:15:15.079: INFO: 
Dec 23 19:15:15.079: INFO: StatefulSet ss has not reached scale 0, at 1
Dec 23 19:15:16.085: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Dec 23 19:15:16.085: INFO: ss-0  k8s-conformance-1-15-control-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:28 +0000 UTC  }]
Dec 23 19:15:16.085: INFO: 
Dec 23 19:15:16.085: INFO: StatefulSet ss has not reached scale 0, at 1
Dec 23 19:15:17.099: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Dec 23 19:15:17.099: INFO: ss-0  k8s-conformance-1-15-control-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:28 +0000 UTC  }]
Dec 23 19:15:17.100: INFO: 
Dec 23 19:15:17.100: INFO: StatefulSet ss has not reached scale 0, at 1
Dec 23 19:15:18.108: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Dec 23 19:15:18.108: INFO: ss-0  k8s-conformance-1-15-control-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:28 +0000 UTC  }]
Dec 23 19:15:18.108: INFO: 
Dec 23 19:15:18.108: INFO: StatefulSet ss has not reached scale 0, at 1
Dec 23 19:15:19.116: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Dec 23 19:15:19.116: INFO: ss-0  k8s-conformance-1-15-control-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:28 +0000 UTC  }]
Dec 23 19:15:19.116: INFO: 
Dec 23 19:15:19.116: INFO: StatefulSet ss has not reached scale 0, at 1
Dec 23 19:15:20.123: INFO: POD   NODE                            PHASE    GRACE  CONDITIONS
Dec 23 19:15:20.124: INFO: ss-0  k8s-conformance-1-15-control-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:15:00 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-23 19:14:28 +0000 UTC  }]
Dec 23 19:15:20.124: INFO: 
Dec 23 19:15:20.124: INFO: StatefulSet ss has not reached scale 0, at 1
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1957
Dec 23 19:15:21.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:15:21.502: INFO: rc: 1
Dec 23 19:15:21.502: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc0024f9c80 exit status 1 <nil> <nil> true [0xc00320c5c0 0xc00320c5d8 0xc00320c5f0] [0xc00320c5c0 0xc00320c5d8 0xc00320c5f0] [0xc00320c5d0 0xc00320c5e8] [0xba6c10 0xba6c10] 0xc002fa0240 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Dec 23 19:15:31.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:15:31.772: INFO: rc: 1
Dec 23 19:15:31.772: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002ab0090 exit status 1 <nil> <nil> true [0xc00320c5f8 0xc00320c610 0xc00320c628] [0xc00320c5f8 0xc00320c610 0xc00320c628] [0xc00320c608 0xc00320c620] [0xba6c10 0xba6c10] 0xc002fa05a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:15:41.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:15:42.011: INFO: rc: 1
Dec 23 19:15:42.011: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002ab0420 exit status 1 <nil> <nil> true [0xc00320c630 0xc00320c648 0xc00320c660] [0xc00320c630 0xc00320c648 0xc00320c660] [0xc00320c640 0xc00320c658] [0xba6c10 0xba6c10] 0xc002fa0900 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:15:52.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:15:52.254: INFO: rc: 1
Dec 23 19:15:52.254: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002ab0840 exit status 1 <nil> <nil> true [0xc00320c668 0xc00320c680 0xc00320c698] [0xc00320c668 0xc00320c680 0xc00320c698] [0xc00320c678 0xc00320c690] [0xba6c10 0xba6c10] 0xc002fa0d80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:16:02.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:16:02.499: INFO: rc: 1
Dec 23 19:16:02.499: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002ab0ba0 exit status 1 <nil> <nil> true [0xc00320c6a0 0xc00320c6b8 0xc00320c6d0] [0xc00320c6a0 0xc00320c6b8 0xc00320c6d0] [0xc00320c6b0 0xc00320c6c8] [0xba6c10 0xba6c10] 0xc002fa1260 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:16:12.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:16:12.773: INFO: rc: 1
Dec 23 19:16:12.773: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002ab0f30 exit status 1 <nil> <nil> true [0xc00320c6d8 0xc00320c6f0 0xc00320c708] [0xc00320c6d8 0xc00320c6f0 0xc00320c708] [0xc00320c6e8 0xc00320c700] [0xba6c10 0xba6c10] 0xc002fa1620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:16:22.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:16:23.052: INFO: rc: 1
Dec 23 19:16:23.052: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024f8360 exit status 1 <nil> <nil> true [0xc003450008 0xc003450038 0xc003450078] [0xc003450008 0xc003450038 0xc003450078] [0xc003450018 0xc003450058] [0xba6c10 0xba6c10] 0xc0022aa2a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:16:33.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:16:33.272: INFO: rc: 1
Dec 23 19:16:33.272: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024f8840 exit status 1 <nil> <nil> true [0xc003450090 0xc0034500d0 0xc003450110] [0xc003450090 0xc0034500d0 0xc003450110] [0xc0034500c8 0xc0034500f8] [0xba6c10 0xba6c10] 0xc0022aa720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:16:43.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:16:43.489: INFO: rc: 1
Dec 23 19:16:43.489: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024f8ba0 exit status 1 <nil> <nil> true [0xc003450118 0xc003450148 0xc003450198] [0xc003450118 0xc003450148 0xc003450198] [0xc003450128 0xc003450178] [0xba6c10 0xba6c10] 0xc0022aad80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:16:53.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:16:53.713: INFO: rc: 1
Dec 23 19:16:53.713: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024f8f90 exit status 1 <nil> <nil> true [0xc0034501b0 0xc0034501f0 0xc003450208] [0xc0034501b0 0xc0034501f0 0xc003450208] [0xc0034501e8 0xc003450200] [0xba6c10 0xba6c10] 0xc0022ab440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:17:03.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:17:03.958: INFO: rc: 1
Dec 23 19:17:03.958: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024f9350 exit status 1 <nil> <nil> true [0xc003450210 0xc003450228 0xc003450260] [0xc003450210 0xc003450228 0xc003450260] [0xc003450220 0xc003450248] [0xba6c10 0xba6c10] 0xc0022abbc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:17:13.959: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:17:14.213: INFO: rc: 1
Dec 23 19:17:14.213: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024f9920 exit status 1 <nil> <nil> true [0xc003450268 0xc003450280 0xc0034502b8] [0xc003450268 0xc003450280 0xc0034502b8] [0xc003450278 0xc0034502b0] [0xba6c10 0xba6c10] 0xc002a88360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:17:24.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:17:24.442: INFO: rc: 1
Dec 23 19:17:24.442: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024f9dd0 exit status 1 <nil> <nil> true [0xc0034502c0 0xc0034502d8 0xc0034502f0] [0xc0034502c0 0xc0034502d8 0xc0034502f0] [0xc0034502d0 0xc0034502e8] [0xba6c10 0xba6c10] 0xc002a88a20 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:17:34.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:17:34.655: INFO: rc: 1
Dec 23 19:17:34.655: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b24300 exit status 1 <nil> <nil> true [0xc003450308 0xc003450330 0xc003450348] [0xc003450308 0xc003450330 0xc003450348] [0xc003450328 0xc003450340] [0xba6c10 0xba6c10] 0xc002a89020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:17:44.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:17:44.887: INFO: rc: 1
Dec 23 19:17:44.887: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b24660 exit status 1 <nil> <nil> true [0xc003450350 0xc003450380 0xc0034503a8] [0xc003450350 0xc003450380 0xc0034503a8] [0xc003450360 0xc0034503a0] [0xba6c10 0xba6c10] 0xc002a89860 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:17:54.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:17:55.181: INFO: rc: 1
Dec 23 19:17:55.182: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b249c0 exit status 1 <nil> <nil> true [0xc0034503b0 0xc0034503f0 0xc003450420] [0xc0034503b0 0xc0034503f0 0xc003450420] [0xc0034503e8 0xc003450400] [0xba6c10 0xba6c10] 0xc002a89c80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:18:05.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:18:05.458: INFO: rc: 1
Dec 23 19:18:05.458: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b24d20 exit status 1 <nil> <nil> true [0xc003450438 0xc003450450 0xc003450490] [0xc003450438 0xc003450450 0xc003450490] [0xc003450448 0xc003450488] [0xba6c10 0xba6c10] 0xc002b14120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:18:15.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:18:15.692: INFO: rc: 1
Dec 23 19:18:15.692: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b25080 exit status 1 <nil> <nil> true [0xc003450498 0xc0034504b0 0xc0034504c8] [0xc003450498 0xc0034504b0 0xc0034504c8] [0xc0034504a8 0xc0034504c0] [0xba6c10 0xba6c10] 0xc002b148a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:18:25.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:18:25.933: INFO: rc: 1
Dec 23 19:18:25.933: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024f8390 exit status 1 <nil> <nil> true [0xc003450008 0xc003450038 0xc003450078] [0xc003450008 0xc003450038 0xc003450078] [0xc003450018 0xc003450058] [0xba6c10 0xba6c10] 0xc002a885a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:18:35.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:18:36.152: INFO: rc: 1
Dec 23 19:18:36.152: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024f88a0 exit status 1 <nil> <nil> true [0xc003450090 0xc0034500d0 0xc003450110] [0xc003450090 0xc0034500d0 0xc003450110] [0xc0034500c8 0xc0034500f8] [0xba6c10 0xba6c10] 0xc002a88c60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:18:46.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:18:46.389: INFO: rc: 1
Dec 23 19:18:46.389: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024f8c30 exit status 1 <nil> <nil> true [0xc003450118 0xc003450148 0xc003450198] [0xc003450118 0xc003450148 0xc003450198] [0xc003450128 0xc003450178] [0xba6c10 0xba6c10] 0xc002a89560 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:18:56.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:18:56.609: INFO: rc: 1
Dec 23 19:18:56.610: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024f90b0 exit status 1 <nil> <nil> true [0xc0034501b0 0xc0034501f0 0xc003450208] [0xc0034501b0 0xc0034501f0 0xc003450208] [0xc0034501e8 0xc003450200] [0xba6c10 0xba6c10] 0xc002a89980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:19:06.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:19:06.822: INFO: rc: 1
Dec 23 19:19:06.822: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024f94a0 exit status 1 <nil> <nil> true [0xc003450210 0xc003450228 0xc003450260] [0xc003450210 0xc003450228 0xc003450260] [0xc003450220 0xc003450248] [0xba6c10 0xba6c10] 0xc002a89da0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:19:16.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:19:17.311: INFO: rc: 1
Dec 23 19:19:17.311: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024f9a40 exit status 1 <nil> <nil> true [0xc003450268 0xc003450280 0xc0034502b8] [0xc003450268 0xc003450280 0xc0034502b8] [0xc003450278 0xc0034502b0] [0xba6c10 0xba6c10] 0xc0022aa0c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:19:27.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:19:27.558: INFO: rc: 1
Dec 23 19:19:27.558: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024f9ef0 exit status 1 <nil> <nil> true [0xc0034502c0 0xc0034502d8 0xc0034502f0] [0xc0034502c0 0xc0034502d8 0xc0034502f0] [0xc0034502d0 0xc0034502e8] [0xba6c10 0xba6c10] 0xc0022aa420 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:19:37.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:19:37.778: INFO: rc: 1
Dec 23 19:19:37.778: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b24270 exit status 1 <nil> <nil> true [0xc003450308 0xc003450330 0xc003450348] [0xc003450308 0xc003450330 0xc003450348] [0xc003450328 0xc003450340] [0xba6c10 0xba6c10] 0xc0022aa960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:19:47.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:19:48.014: INFO: rc: 1
Dec 23 19:19:48.014: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b24600 exit status 1 <nil> <nil> true [0xc003450350 0xc003450380 0xc0034503a8] [0xc003450350 0xc003450380 0xc0034503a8] [0xc003450360 0xc0034503a0] [0xba6c10 0xba6c10] 0xc0022ab080 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:19:58.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:19:58.263: INFO: rc: 1
Dec 23 19:19:58.263: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b24990 exit status 1 <nil> <nil> true [0xc0034503b0 0xc0034503f0 0xc003450420] [0xc0034503b0 0xc0034503f0 0xc003450420] [0xc0034503e8 0xc003450400] [0xba6c10 0xba6c10] 0xc0022ab7a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:20:08.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:20:08.477: INFO: rc: 1
Dec 23 19:20:08.477: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b24d50 exit status 1 <nil> <nil> true [0xc003450438 0xc003450450 0xc003450490] [0xc003450438 0xc003450450 0xc003450490] [0xc003450448 0xc003450488] [0xba6c10 0xba6c10] 0xc002b14120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:20:18.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:20:18.696: INFO: rc: 1
Dec 23 19:20:18.696: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b250e0 exit status 1 <nil> <nil> true [0xc003450498 0xc0034504b0 0xc0034504c8] [0xc003450498 0xc0034504b0 0xc0034504c8] [0xc0034504a8 0xc0034504c0] [0xba6c10 0xba6c10] 0xc002b148a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 23 19:20:28.696: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 exec --namespace=statefulset-1957 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Dec 23 19:20:28.931: INFO: rc: 1
Dec 23 19:20:28.931: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Dec 23 19:20:28.931: INFO: Scaling statefulset ss to 0
Dec 23 19:20:28.949: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Dec 23 19:20:28.954: INFO: Deleting all statefulset in ns statefulset-1957
Dec 23 19:20:28.959: INFO: Scaling statefulset ss to 0
Dec 23 19:20:28.975: INFO: Waiting for statefulset status.replicas updated to 0
Dec 23 19:20:28.980: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:20:29.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1957" for this suite.
Dec 23 19:20:35.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:20:35.271: INFO: namespace statefulset-1957 deletion completed in 6.253011296s

• [SLOW TEST:367.218 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:20:35.274: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 23 19:20:35.389: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dcbdc948-7e69-4c1e-accb-fb914a52ed7e" in namespace "downward-api-5928" to be "success or failure"
Dec 23 19:20:35.396: INFO: Pod "downwardapi-volume-dcbdc948-7e69-4c1e-accb-fb914a52ed7e": Phase="Pending", Reason="", readiness=false. Elapsed: 7.259677ms
Dec 23 19:20:37.401: INFO: Pod "downwardapi-volume-dcbdc948-7e69-4c1e-accb-fb914a52ed7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012457798s
STEP: Saw pod success
Dec 23 19:20:37.402: INFO: Pod "downwardapi-volume-dcbdc948-7e69-4c1e-accb-fb914a52ed7e" satisfied condition "success or failure"
Dec 23 19:20:37.415: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod downwardapi-volume-dcbdc948-7e69-4c1e-accb-fb914a52ed7e container client-container: <nil>
STEP: delete the pod
Dec 23 19:20:37.459: INFO: Waiting for pod downwardapi-volume-dcbdc948-7e69-4c1e-accb-fb914a52ed7e to disappear
Dec 23 19:20:37.470: INFO: Pod downwardapi-volume-dcbdc948-7e69-4c1e-accb-fb914a52ed7e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:20:37.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5928" for this suite.
Dec 23 19:20:43.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:20:43.704: INFO: namespace downward-api-5928 deletion completed in 6.221285219s

• [SLOW TEST:8.431 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:20:43.707: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-472a959c-5fb9-44a7-bd8a-b1bf1b5a274c
STEP: Creating a pod to test consume configMaps
Dec 23 19:20:43.829: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-1f23ad8d-dabe-4e51-aa9f-efaffc0a3916" in namespace "projected-1215" to be "success or failure"
Dec 23 19:20:43.846: INFO: Pod "pod-projected-configmaps-1f23ad8d-dabe-4e51-aa9f-efaffc0a3916": Phase="Pending", Reason="", readiness=false. Elapsed: 17.265175ms
Dec 23 19:20:45.854: INFO: Pod "pod-projected-configmaps-1f23ad8d-dabe-4e51-aa9f-efaffc0a3916": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.024788555s
STEP: Saw pod success
Dec 23 19:20:45.854: INFO: Pod "pod-projected-configmaps-1f23ad8d-dabe-4e51-aa9f-efaffc0a3916" satisfied condition "success or failure"
Dec 23 19:20:45.858: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod pod-projected-configmaps-1f23ad8d-dabe-4e51-aa9f-efaffc0a3916 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 23 19:20:45.896: INFO: Waiting for pod pod-projected-configmaps-1f23ad8d-dabe-4e51-aa9f-efaffc0a3916 to disappear
Dec 23 19:20:45.903: INFO: Pod pod-projected-configmaps-1f23ad8d-dabe-4e51-aa9f-efaffc0a3916 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:20:45.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1215" for this suite.
Dec 23 19:20:51.965: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:20:52.185: INFO: namespace projected-1215 deletion completed in 6.271168317s

• [SLOW TEST:8.478 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:20:52.190: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 23 19:20:52.308: INFO: Waiting up to 5m0s for pod "pod-0c8b7d88-1df2-4e5a-b6e5-86033e1eedca" in namespace "emptydir-8043" to be "success or failure"
Dec 23 19:20:52.322: INFO: Pod "pod-0c8b7d88-1df2-4e5a-b6e5-86033e1eedca": Phase="Pending", Reason="", readiness=false. Elapsed: 13.517363ms
Dec 23 19:20:54.328: INFO: Pod "pod-0c8b7d88-1df2-4e5a-b6e5-86033e1eedca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019412913s
STEP: Saw pod success
Dec 23 19:20:54.329: INFO: Pod "pod-0c8b7d88-1df2-4e5a-b6e5-86033e1eedca" satisfied condition "success or failure"
Dec 23 19:20:54.334: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-0c8b7d88-1df2-4e5a-b6e5-86033e1eedca container test-container: <nil>
STEP: delete the pod
Dec 23 19:20:54.367: INFO: Waiting for pod pod-0c8b7d88-1df2-4e5a-b6e5-86033e1eedca to disappear
Dec 23 19:20:54.375: INFO: Pod pod-0c8b7d88-1df2-4e5a-b6e5-86033e1eedca no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:20:54.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8043" for this suite.
Dec 23 19:21:00.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:21:00.615: INFO: namespace emptydir-8043 deletion completed in 6.229916654s

• [SLOW TEST:8.426 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:21:00.619: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec 23 19:21:03.299: INFO: Successfully updated pod "labelsupdatefaa876e5-eb15-4eae-aebd-345889d35d3e"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:21:07.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8043" for this suite.
Dec 23 19:21:29.363: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:21:29.586: INFO: namespace projected-8043 deletion completed in 22.243917222s

• [SLOW TEST:28.967 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:21:29.591: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Dec 23 19:21:51.736: INFO: Container started at 2019-12-23 19:21:30 +0000 UTC, pod became ready at 2019-12-23 19:21:51 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:21:51.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-6831" for this suite.
Dec 23 19:22:13.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:22:13.983: INFO: namespace container-probe-6831 deletion completed in 22.234296871s

• [SLOW TEST:44.392 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:22:13.986: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Dec 23 19:22:14.089: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:22:17.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9400" for this suite.
Dec 23 19:22:39.155: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:22:39.369: INFO: namespace init-container-9400 deletion completed in 22.238340466s

• [SLOW TEST:25.383 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:22:39.376: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-c080b8ec-26c1-4b57-ad12-509abb8b0d01
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-c080b8ec-26c1-4b57-ad12-509abb8b0d01
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:22:43.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5445" for this suite.
Dec 23 19:23:05.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:23:05.883: INFO: namespace configmap-5445 deletion completed in 22.234863472s

• [SLOW TEST:26.508 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:23:05.887: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Dec 23 19:23:05.996: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-424598258 proxy --unix-socket=/tmp/kubectl-proxy-unix224303853/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:23:06.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1358" for this suite.
Dec 23 19:23:12.180: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:23:12.389: INFO: namespace kubectl-1358 deletion completed in 6.231043789s

• [SLOW TEST:6.503 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:23:12.391: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 23 19:23:12.519: INFO: Waiting up to 5m0s for pod "pod-fb32d985-20c2-4965-ad74-8471b7552008" in namespace "emptydir-5346" to be "success or failure"
Dec 23 19:23:12.533: INFO: Pod "pod-fb32d985-20c2-4965-ad74-8471b7552008": Phase="Pending", Reason="", readiness=false. Elapsed: 14.043683ms
Dec 23 19:23:14.541: INFO: Pod "pod-fb32d985-20c2-4965-ad74-8471b7552008": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022006413s
Dec 23 19:23:16.546: INFO: Pod "pod-fb32d985-20c2-4965-ad74-8471b7552008": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027550535s
Dec 23 19:23:18.552: INFO: Pod "pod-fb32d985-20c2-4965-ad74-8471b7552008": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.033754729s
STEP: Saw pod success
Dec 23 19:23:18.553: INFO: Pod "pod-fb32d985-20c2-4965-ad74-8471b7552008" satisfied condition "success or failure"
Dec 23 19:23:18.558: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod pod-fb32d985-20c2-4965-ad74-8471b7552008 container test-container: <nil>
STEP: delete the pod
Dec 23 19:23:18.597: INFO: Waiting for pod pod-fb32d985-20c2-4965-ad74-8471b7552008 to disappear
Dec 23 19:23:18.602: INFO: Pod pod-fb32d985-20c2-4965-ad74-8471b7552008 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:23:18.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5346" for this suite.
Dec 23 19:23:24.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:23:24.826: INFO: namespace emptydir-5346 deletion completed in 6.213734314s

• [SLOW TEST:12.435 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:23:24.841: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Dec 23 19:23:24.917: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1194f27e-ef09-47e6-aecf-d0cb6a685a6c" in namespace "projected-886" to be "success or failure"
Dec 23 19:23:24.924: INFO: Pod "downwardapi-volume-1194f27e-ef09-47e6-aecf-d0cb6a685a6c": Phase="Pending", Reason="", readiness=false. Elapsed: 6.337729ms
Dec 23 19:23:26.930: INFO: Pod "downwardapi-volume-1194f27e-ef09-47e6-aecf-d0cb6a685a6c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012572183s
STEP: Saw pod success
Dec 23 19:23:26.931: INFO: Pod "downwardapi-volume-1194f27e-ef09-47e6-aecf-d0cb6a685a6c" satisfied condition "success or failure"
Dec 23 19:23:26.935: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod downwardapi-volume-1194f27e-ef09-47e6-aecf-d0cb6a685a6c container client-container: <nil>
STEP: delete the pod
Dec 23 19:23:26.967: INFO: Waiting for pod downwardapi-volume-1194f27e-ef09-47e6-aecf-d0cb6a685a6c to disappear
Dec 23 19:23:26.973: INFO: Pod downwardapi-volume-1194f27e-ef09-47e6-aecf-d0cb6a685a6c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:23:26.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-886" for this suite.
Dec 23 19:23:33.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:23:33.215: INFO: namespace projected-886 deletion completed in 6.21971432s

• [SLOW TEST:8.375 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:23:33.218: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Dec 23 19:23:33.344: INFO: Waiting up to 5m0s for pod "downward-api-116abed8-14ec-49ed-9e5a-41d6eda3e889" in namespace "downward-api-9849" to be "success or failure"
Dec 23 19:23:33.359: INFO: Pod "downward-api-116abed8-14ec-49ed-9e5a-41d6eda3e889": Phase="Pending", Reason="", readiness=false. Elapsed: 14.488053ms
Dec 23 19:23:35.365: INFO: Pod "downward-api-116abed8-14ec-49ed-9e5a-41d6eda3e889": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019937386s
STEP: Saw pod success
Dec 23 19:23:35.365: INFO: Pod "downward-api-116abed8-14ec-49ed-9e5a-41d6eda3e889" satisfied condition "success or failure"
Dec 23 19:23:35.370: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod downward-api-116abed8-14ec-49ed-9e5a-41d6eda3e889 container dapi-container: <nil>
STEP: delete the pod
Dec 23 19:23:35.406: INFO: Waiting for pod downward-api-116abed8-14ec-49ed-9e5a-41d6eda3e889 to disappear
Dec 23 19:23:35.414: INFO: Pod downward-api-116abed8-14ec-49ed-9e5a-41d6eda3e889 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:23:35.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9849" for this suite.
Dec 23 19:23:41.446: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:23:41.802: INFO: namespace downward-api-9849 deletion completed in 6.37736972s

• [SLOW TEST:8.584 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:23:41.804: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Dec 23 19:23:41.943: INFO: Waiting up to 5m0s for pod "var-expansion-1d93aebe-442f-4061-b3f4-c39134986854" in namespace "var-expansion-5962" to be "success or failure"
Dec 23 19:23:41.948: INFO: Pod "var-expansion-1d93aebe-442f-4061-b3f4-c39134986854": Phase="Pending", Reason="", readiness=false. Elapsed: 5.255887ms
Dec 23 19:23:43.959: INFO: Pod "var-expansion-1d93aebe-442f-4061-b3f4-c39134986854": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015960866s
STEP: Saw pod success
Dec 23 19:23:43.959: INFO: Pod "var-expansion-1d93aebe-442f-4061-b3f4-c39134986854" satisfied condition "success or failure"
Dec 23 19:23:43.964: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod var-expansion-1d93aebe-442f-4061-b3f4-c39134986854 container dapi-container: <nil>
STEP: delete the pod
Dec 23 19:23:43.998: INFO: Waiting for pod var-expansion-1d93aebe-442f-4061-b3f4-c39134986854 to disappear
Dec 23 19:23:44.005: INFO: Pod var-expansion-1d93aebe-442f-4061-b3f4-c39134986854 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:23:44.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5962" for this suite.
Dec 23 19:23:50.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:23:50.245: INFO: namespace var-expansion-5962 deletion completed in 6.225873734s

• [SLOW TEST:8.441 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:23:50.253: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:23:52.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3887" for this suite.
Dec 23 19:24:30.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:24:30.657: INFO: namespace kubelet-test-3887 deletion completed in 38.240110625s

• [SLOW TEST:40.405 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:24:30.661: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-3bf81af6-bc90-4d7b-8a0b-805de1367573
STEP: Creating a pod to test consume secrets
Dec 23 19:24:30.738: INFO: Waiting up to 5m0s for pod "pod-secrets-7be1c34c-b25b-4d66-8f78-c52f3f85edc0" in namespace "secrets-5857" to be "success or failure"
Dec 23 19:24:30.745: INFO: Pod "pod-secrets-7be1c34c-b25b-4d66-8f78-c52f3f85edc0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.361349ms
Dec 23 19:24:32.752: INFO: Pod "pod-secrets-7be1c34c-b25b-4d66-8f78-c52f3f85edc0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013475588s
STEP: Saw pod success
Dec 23 19:24:32.752: INFO: Pod "pod-secrets-7be1c34c-b25b-4d66-8f78-c52f3f85edc0" satisfied condition "success or failure"
Dec 23 19:24:32.758: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-secrets-7be1c34c-b25b-4d66-8f78-c52f3f85edc0 container secret-volume-test: <nil>
STEP: delete the pod
Dec 23 19:24:32.801: INFO: Waiting for pod pod-secrets-7be1c34c-b25b-4d66-8f78-c52f3f85edc0 to disappear
Dec 23 19:24:32.811: INFO: Pod pod-secrets-7be1c34c-b25b-4d66-8f78-c52f3f85edc0 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:24:32.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5857" for this suite.
Dec 23 19:24:38.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:24:39.076: INFO: namespace secrets-5857 deletion completed in 6.251231452s

• [SLOW TEST:8.415 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:24:39.079: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-12a429e6-6ceb-4a01-ab6f-06f1b94d8416 in namespace container-probe-7789
Dec 23 19:24:41.177: INFO: Started pod liveness-12a429e6-6ceb-4a01-ab6f-06f1b94d8416 in namespace container-probe-7789
STEP: checking the pod's current state and verifying that restartCount is present
Dec 23 19:24:41.182: INFO: Initial restart count of pod liveness-12a429e6-6ceb-4a01-ab6f-06f1b94d8416 is 0
Dec 23 19:24:57.234: INFO: Restart count of pod container-probe-7789/liveness-12a429e6-6ceb-4a01-ab6f-06f1b94d8416 is now 1 (16.051258796s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:24:57.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7789" for this suite.
Dec 23 19:25:03.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:25:03.518: INFO: namespace container-probe-7789 deletion completed in 6.241735343s

• [SLOW TEST:24.440 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:25:03.520: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Dec 23 19:25:06.177: INFO: Successfully updated pod "annotationupdated7328df4-35b4-43ce-937a-b882bd19ba41"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:25:10.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7729" for this suite.
Dec 23 19:25:32.248: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:25:32.445: INFO: namespace downward-api-7729 deletion completed in 22.218868459s

• [SLOW TEST:28.925 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:25:32.448: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:26:32.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8201" for this suite.
Dec 23 19:26:54.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:26:54.821: INFO: namespace container-probe-8201 deletion completed in 22.240532794s

• [SLOW TEST:82.374 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:26:54.825: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 23 19:26:54.938: INFO: Waiting up to 5m0s for pod "pod-51186bab-7d9c-47ad-b4fa-0b696ae434f2" in namespace "emptydir-4446" to be "success or failure"
Dec 23 19:26:54.945: INFO: Pod "pod-51186bab-7d9c-47ad-b4fa-0b696ae434f2": Phase="Pending", Reason="", readiness=false. Elapsed: 6.425871ms
Dec 23 19:26:56.953: INFO: Pod "pod-51186bab-7d9c-47ad-b4fa-0b696ae434f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015187907s
STEP: Saw pod success
Dec 23 19:26:56.953: INFO: Pod "pod-51186bab-7d9c-47ad-b4fa-0b696ae434f2" satisfied condition "success or failure"
Dec 23 19:26:56.960: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-51186bab-7d9c-47ad-b4fa-0b696ae434f2 container test-container: <nil>
STEP: delete the pod
Dec 23 19:26:57.004: INFO: Waiting for pod pod-51186bab-7d9c-47ad-b4fa-0b696ae434f2 to disappear
Dec 23 19:26:57.010: INFO: Pod pod-51186bab-7d9c-47ad-b4fa-0b696ae434f2 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:26:57.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4446" for this suite.
Dec 23 19:27:03.044: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:27:03.259: INFO: namespace emptydir-4446 deletion completed in 6.23772437s

• [SLOW TEST:8.435 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:27:03.269: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Dec 23 19:27:03.372: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-424598258 api-versions'
Dec 23 19:27:03.568: INFO: stderr: ""
Dec 23 19:27:03.568: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:27:03.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9590" for this suite.
Dec 23 19:27:09.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:27:09.815: INFO: namespace kubectl-9590 deletion completed in 6.237900256s

• [SLOW TEST:6.546 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:27:09.823: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-2000/secret-test-4f33b796-b9a6-4b2f-aec0-4023c7c784e2
STEP: Creating a pod to test consume secrets
Dec 23 19:27:09.948: INFO: Waiting up to 5m0s for pod "pod-configmaps-015afb5e-96f7-41e2-8d59-3bb61551dbde" in namespace "secrets-2000" to be "success or failure"
Dec 23 19:27:09.965: INFO: Pod "pod-configmaps-015afb5e-96f7-41e2-8d59-3bb61551dbde": Phase="Pending", Reason="", readiness=false. Elapsed: 17.457601ms
Dec 23 19:27:11.971: INFO: Pod "pod-configmaps-015afb5e-96f7-41e2-8d59-3bb61551dbde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.023635613s
STEP: Saw pod success
Dec 23 19:27:11.971: INFO: Pod "pod-configmaps-015afb5e-96f7-41e2-8d59-3bb61551dbde" satisfied condition "success or failure"
Dec 23 19:27:11.976: INFO: Trying to get logs from node k8s-conformance-1-15-control-1 pod pod-configmaps-015afb5e-96f7-41e2-8d59-3bb61551dbde container env-test: <nil>
STEP: delete the pod
Dec 23 19:27:12.015: INFO: Waiting for pod pod-configmaps-015afb5e-96f7-41e2-8d59-3bb61551dbde to disappear
Dec 23 19:27:12.021: INFO: Pod pod-configmaps-015afb5e-96f7-41e2-8d59-3bb61551dbde no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:27:12.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2000" for this suite.
Dec 23 19:27:18.055: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:27:18.269: INFO: namespace secrets-2000 deletion completed in 6.237907309s

• [SLOW TEST:8.447 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:27:18.274: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 23 19:27:18.339: INFO: Waiting up to 5m0s for pod "pod-c5a8fe42-a50b-4b1b-b5cf-dfef42f3d906" in namespace "emptydir-7784" to be "success or failure"
Dec 23 19:27:18.353: INFO: Pod "pod-c5a8fe42-a50b-4b1b-b5cf-dfef42f3d906": Phase="Pending", Reason="", readiness=false. Elapsed: 14.06631ms
Dec 23 19:27:20.359: INFO: Pod "pod-c5a8fe42-a50b-4b1b-b5cf-dfef42f3d906": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.019919865s
STEP: Saw pod success
Dec 23 19:27:20.359: INFO: Pod "pod-c5a8fe42-a50b-4b1b-b5cf-dfef42f3d906" satisfied condition "success or failure"
Dec 23 19:27:20.364: INFO: Trying to get logs from node k8s-conformance-1-15-etcd-3 pod pod-c5a8fe42-a50b-4b1b-b5cf-dfef42f3d906 container test-container: <nil>
STEP: delete the pod
Dec 23 19:27:20.396: INFO: Waiting for pod pod-c5a8fe42-a50b-4b1b-b5cf-dfef42f3d906 to disappear
Dec 23 19:27:20.401: INFO: Pod pod-c5a8fe42-a50b-4b1b-b5cf-dfef42f3d906 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:27:20.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7784" for this suite.
Dec 23 19:27:26.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:27:26.605: INFO: namespace emptydir-7784 deletion completed in 6.194101647s

• [SLOW TEST:8.331 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Dec 23 19:27:26.609: INFO: >>> kubeConfig: /tmp/kubeconfig-424598258
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 23 19:27:30.742: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 23 19:27:30.758: INFO: Pod pod-with-prestop-http-hook still exists
Dec 23 19:27:32.759: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 23 19:27:32.765: INFO: Pod pod-with-prestop-http-hook still exists
Dec 23 19:27:34.759: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 23 19:27:34.764: INFO: Pod pod-with-prestop-http-hook still exists
Dec 23 19:27:36.759: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 23 19:27:36.765: INFO: Pod pod-with-prestop-http-hook still exists
Dec 23 19:27:38.759: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 23 19:27:38.766: INFO: Pod pod-with-prestop-http-hook still exists
Dec 23 19:27:40.759: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 23 19:27:40.765: INFO: Pod pod-with-prestop-http-hook still exists
Dec 23 19:27:42.759: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 23 19:27:42.764: INFO: Pod pod-with-prestop-http-hook still exists
Dec 23 19:27:44.759: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 23 19:27:44.766: INFO: Pod pod-with-prestop-http-hook still exists
Dec 23 19:27:46.759: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 23 19:27:46.765: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Dec 23 19:27:46.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9146" for this suite.
Dec 23 19:28:08.807: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 23 19:28:09.001: INFO: namespace container-lifecycle-hook-9146 deletion completed in 22.2149758s

• [SLOW TEST:42.392 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.6-beta.0.38+7015f71e75f670/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSDec 23 19:28:09.007: INFO: Running AfterSuite actions on all nodes
Dec 23 19:28:09.007: INFO: Running AfterSuite actions on node 1
Dec 23 19:28:09.008: INFO: Skipping dumping logs from cluster

Ran 215 of 4413 Specs in 5795.534 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4198 Skipped
PASS

Ginkgo ran 1 suite in 1h36m38.363412037s
Test Suite Passed
