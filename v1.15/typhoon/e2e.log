I0622 17:51:03.744023      15 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-689426708
I0622 17:51:03.744328      15 e2e.go:241] Starting e2e run "333810d3-9a5c-4520-896c-f92b8062d9e7" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1561225862 - Will randomize all specs
Will run 215 of 4411 specs

Jun 22 17:51:04.056: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
Jun 22 17:51:04.058: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun 22 17:51:04.079: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun 22 17:51:04.111: INFO: 17 / 17 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun 22 17:51:04.111: INFO: expected 6 pod replicas in namespace 'kube-system', 6 are Running and Ready.
Jun 22 17:51:04.111: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun 22 17:51:04.119: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
Jun 22 17:51:04.119: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'kube-apiserver' (0 seconds elapsed)
Jun 22 17:51:04.120: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jun 22 17:51:04.120: INFO: 1 / 1 pods ready in namespace 'kube-system' in daemonset 'pod-checkpointer' (0 seconds elapsed)
Jun 22 17:51:04.120: INFO: e2e test version: v1.15.0
Jun 22 17:51:04.121: INFO: kube-apiserver version: v1.15.0
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 17:51:04.121: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename custom-resource-definition
Jun 22 17:51:04.144: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 22 17:51:04.146: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 17:51:05.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2638" for this suite.
Jun 22 17:51:11.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:51:11.356: INFO: namespace custom-resource-definition-2638 deletion completed in 6.101970609s

• [SLOW TEST:7.235 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 17:51:11.356: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jun 22 17:51:16.409: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 17:51:17.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2079" for this suite.
Jun 22 17:51:39.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:51:39.516: INFO: namespace replicaset-2079 deletion completed in 22.092728104s

• [SLOW TEST:28.159 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 17:51:39.516: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 17:51:45.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1507" for this suite.
Jun 22 17:51:51.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:51:51.352: INFO: namespace watch-1507 deletion completed in 6.180336057s

• [SLOW TEST:11.836 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 17:51:51.354: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Jun 22 17:51:51.383: INFO: Waiting up to 5m0s for pod "client-containers-195b80f1-bdc6-43fb-a9b4-788b05fc35ed" in namespace "containers-1463" to be "success or failure"
Jun 22 17:51:51.385: INFO: Pod "client-containers-195b80f1-bdc6-43fb-a9b4-788b05fc35ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.564283ms
Jun 22 17:51:53.388: INFO: Pod "client-containers-195b80f1-bdc6-43fb-a9b4-788b05fc35ed": Phase="Pending", Reason="", readiness=false. Elapsed: 2.005580812s
Jun 22 17:51:55.392: INFO: Pod "client-containers-195b80f1-bdc6-43fb-a9b4-788b05fc35ed": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008839618s
STEP: Saw pod success
Jun 22 17:51:55.392: INFO: Pod "client-containers-195b80f1-bdc6-43fb-a9b4-788b05fc35ed" satisfied condition "success or failure"
Jun 22 17:51:55.394: INFO: Trying to get logs from node ip-10-0-20-165 pod client-containers-195b80f1-bdc6-43fb-a9b4-788b05fc35ed container test-container: <nil>
STEP: delete the pod
Jun 22 17:51:55.420: INFO: Waiting for pod client-containers-195b80f1-bdc6-43fb-a9b4-788b05fc35ed to disappear
Jun 22 17:51:55.423: INFO: Pod client-containers-195b80f1-bdc6-43fb-a9b4-788b05fc35ed no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 17:51:55.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1463" for this suite.
Jun 22 17:52:01.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:52:01.520: INFO: namespace containers-1463 deletion completed in 6.094336202s

• [SLOW TEST:10.166 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 17:52:01.521: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jun 22 17:52:06.136: INFO: Successfully updated pod "labelsupdatec677ba1f-5d3f-4180-aadd-8ecf078d598f"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 17:52:08.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-28" for this suite.
Jun 22 17:52:40.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:52:40.247: INFO: namespace projected-28 deletion completed in 32.093355326s

• [SLOW TEST:38.725 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 17:52:40.247: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-2242
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-2242
STEP: Deleting pre-stop pod
Jun 22 17:52:51.307: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 17:52:51.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2242" for this suite.
Jun 22 17:53:29.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:53:29.407: INFO: namespace prestop-2242 deletion completed in 38.092434996s

• [SLOW TEST:49.160 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 17:53:29.408: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 22 17:53:29.434: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aaa04f80-0ee4-4bda-a811-11a9ef06fa93" in namespace "projected-4378" to be "success or failure"
Jun 22 17:53:29.439: INFO: Pod "downwardapi-volume-aaa04f80-0ee4-4bda-a811-11a9ef06fa93": Phase="Pending", Reason="", readiness=false. Elapsed: 5.382972ms
Jun 22 17:53:31.442: INFO: Pod "downwardapi-volume-aaa04f80-0ee4-4bda-a811-11a9ef06fa93": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008687502s
STEP: Saw pod success
Jun 22 17:53:31.442: INFO: Pod "downwardapi-volume-aaa04f80-0ee4-4bda-a811-11a9ef06fa93" satisfied condition "success or failure"
Jun 22 17:53:31.445: INFO: Trying to get logs from node ip-10-0-20-165 pod downwardapi-volume-aaa04f80-0ee4-4bda-a811-11a9ef06fa93 container client-container: <nil>
STEP: delete the pod
Jun 22 17:53:31.459: INFO: Waiting for pod downwardapi-volume-aaa04f80-0ee4-4bda-a811-11a9ef06fa93 to disappear
Jun 22 17:53:31.462: INFO: Pod downwardapi-volume-aaa04f80-0ee4-4bda-a811-11a9ef06fa93 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 17:53:31.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4378" for this suite.
Jun 22 17:53:37.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:53:37.568: INFO: namespace projected-4378 deletion completed in 6.103946604s

• [SLOW TEST:8.161 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 17:53:37.570: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-j8zw
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 17:53:37.604: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-j8zw" in namespace "subpath-1143" to be "success or failure"
Jun 22 17:53:37.608: INFO: Pod "pod-subpath-test-downwardapi-j8zw": Phase="Pending", Reason="", readiness=false. Elapsed: 4.096825ms
Jun 22 17:53:39.612: INFO: Pod "pod-subpath-test-downwardapi-j8zw": Phase="Running", Reason="", readiness=true. Elapsed: 2.007859048s
Jun 22 17:53:41.615: INFO: Pod "pod-subpath-test-downwardapi-j8zw": Phase="Running", Reason="", readiness=true. Elapsed: 4.011020034s
Jun 22 17:53:43.618: INFO: Pod "pod-subpath-test-downwardapi-j8zw": Phase="Running", Reason="", readiness=true. Elapsed: 6.014080951s
Jun 22 17:53:45.622: INFO: Pod "pod-subpath-test-downwardapi-j8zw": Phase="Running", Reason="", readiness=true. Elapsed: 8.017153537s
Jun 22 17:53:47.625: INFO: Pod "pod-subpath-test-downwardapi-j8zw": Phase="Running", Reason="", readiness=true. Elapsed: 10.020431385s
Jun 22 17:53:49.628: INFO: Pod "pod-subpath-test-downwardapi-j8zw": Phase="Running", Reason="", readiness=true. Elapsed: 12.023757535s
Jun 22 17:53:51.631: INFO: Pod "pod-subpath-test-downwardapi-j8zw": Phase="Running", Reason="", readiness=true. Elapsed: 14.02640342s
Jun 22 17:53:53.634: INFO: Pod "pod-subpath-test-downwardapi-j8zw": Phase="Running", Reason="", readiness=true. Elapsed: 16.029431383s
Jun 22 17:53:55.637: INFO: Pod "pod-subpath-test-downwardapi-j8zw": Phase="Running", Reason="", readiness=true. Elapsed: 18.032567716s
Jun 22 17:53:57.640: INFO: Pod "pod-subpath-test-downwardapi-j8zw": Phase="Running", Reason="", readiness=true. Elapsed: 20.035593882s
Jun 22 17:53:59.647: INFO: Pod "pod-subpath-test-downwardapi-j8zw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.042709592s
STEP: Saw pod success
Jun 22 17:53:59.647: INFO: Pod "pod-subpath-test-downwardapi-j8zw" satisfied condition "success or failure"
Jun 22 17:53:59.650: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-subpath-test-downwardapi-j8zw container test-container-subpath-downwardapi-j8zw: <nil>
STEP: delete the pod
Jun 22 17:53:59.667: INFO: Waiting for pod pod-subpath-test-downwardapi-j8zw to disappear
Jun 22 17:53:59.669: INFO: Pod pod-subpath-test-downwardapi-j8zw no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-j8zw
Jun 22 17:53:59.669: INFO: Deleting pod "pod-subpath-test-downwardapi-j8zw" in namespace "subpath-1143"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 17:53:59.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-1143" for this suite.
Jun 22 17:54:05.684: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:54:05.763: INFO: namespace subpath-1143 deletion completed in 6.089550021s

• [SLOW TEST:28.194 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 17:54:05.764: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Jun 22 17:54:05.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 api-versions'
Jun 22 17:54:05.887: INFO: stderr: ""
Jun 22 17:54:05.887: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.projectcalico.org/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 17:54:05.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3328" for this suite.
Jun 22 17:54:11.898: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:54:11.979: INFO: namespace kubectl-3328 deletion completed in 6.089688669s

• [SLOW TEST:6.215 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 17:54:11.981: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jun 22 17:54:12.261: INFO: Pod name wrapped-volume-race-2c514c18-8b59-4b3b-981b-54847ae80784: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-2c514c18-8b59-4b3b-981b-54847ae80784 in namespace emptydir-wrapper-6755, will wait for the garbage collector to delete the pods
Jun 22 17:54:30.365: INFO: Deleting ReplicationController wrapped-volume-race-2c514c18-8b59-4b3b-981b-54847ae80784 took: 5.29943ms
Jun 22 17:54:30.665: INFO: Terminating ReplicationController wrapped-volume-race-2c514c18-8b59-4b3b-981b-54847ae80784 pods took: 300.196274ms
STEP: Creating RC which spawns configmap-volume pods
Jun 22 17:55:12.993: INFO: Pod name wrapped-volume-race-35e36507-cc56-4644-b766-9931666c79da: Found 0 pods out of 5
Jun 22 17:55:17.998: INFO: Pod name wrapped-volume-race-35e36507-cc56-4644-b766-9931666c79da: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-35e36507-cc56-4644-b766-9931666c79da in namespace emptydir-wrapper-6755, will wait for the garbage collector to delete the pods
Jun 22 17:55:30.084: INFO: Deleting ReplicationController wrapped-volume-race-35e36507-cc56-4644-b766-9931666c79da took: 6.295973ms
Jun 22 17:55:30.384: INFO: Terminating ReplicationController wrapped-volume-race-35e36507-cc56-4644-b766-9931666c79da pods took: 300.172681ms
STEP: Creating RC which spawns configmap-volume pods
Jun 22 17:56:12.897: INFO: Pod name wrapped-volume-race-36711a2e-d454-4c66-824f-8f491d47c294: Found 0 pods out of 5
Jun 22 17:56:17.902: INFO: Pod name wrapped-volume-race-36711a2e-d454-4c66-824f-8f491d47c294: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-36711a2e-d454-4c66-824f-8f491d47c294 in namespace emptydir-wrapper-6755, will wait for the garbage collector to delete the pods
Jun 22 17:56:29.982: INFO: Deleting ReplicationController wrapped-volume-race-36711a2e-d454-4c66-824f-8f491d47c294 took: 6.921663ms
Jun 22 17:56:30.383: INFO: Terminating ReplicationController wrapped-volume-race-36711a2e-d454-4c66-824f-8f491d47c294 pods took: 400.214946ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 17:57:13.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6755" for this suite.
Jun 22 17:57:19.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:57:19.160: INFO: namespace emptydir-wrapper-6755 deletion completed in 6.086130633s

• [SLOW TEST:187.179 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 17:57:19.160: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 22 17:57:21.198: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 17:57:21.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-10" for this suite.
Jun 22 17:57:27.219: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:57:27.342: INFO: namespace container-runtime-10 deletion completed in 6.131218148s

• [SLOW TEST:8.182 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 17:57:27.342: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jun 22 17:57:29.918: INFO: Successfully updated pod "annotationupdatecf280de7-626b-47b6-bb29-0c40ee726aa1"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 17:57:33.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1583" for this suite.
Jun 22 17:57:55.951: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:57:56.032: INFO: namespace projected-1583 deletion completed in 22.090659325s

• [SLOW TEST:28.690 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 17:57:56.033: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 22 17:57:56.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-5115'
Jun 22 17:57:56.298: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 22 17:57:56.298: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Jun 22 17:57:56.302: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 delete jobs e2e-test-nginx-job --namespace=kubectl-5115'
Jun 22 17:57:56.387: INFO: stderr: ""
Jun 22 17:57:56.387: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 17:57:56.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5115" for this suite.
Jun 22 17:58:02.400: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:58:02.479: INFO: namespace kubectl-5115 deletion completed in 6.089233976s

• [SLOW TEST:6.446 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 17:58:02.479: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 22 17:58:10.586: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 17:58:10.589: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 17:58:12.589: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 17:58:12.592: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 17:58:14.589: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 17:58:14.593: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 17:58:16.590: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 17:58:16.593: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 17:58:18.589: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 17:58:18.593: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 17:58:20.589: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 17:58:20.593: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 17:58:22.589: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 17:58:22.593: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 17:58:24.589: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 17:58:24.593: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 17:58:26.589: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 17:58:26.592: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 17:58:28.589: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 17:58:28.593: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 22 17:58:30.590: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 22 17:58:30.599: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 17:58:30.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3468" for this suite.
Jun 22 17:58:52.618: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:58:52.701: INFO: namespace container-lifecycle-hook-3468 deletion completed in 22.091570825s

• [SLOW TEST:50.221 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 17:58:52.701: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-514e635e-9d67-4d15-972f-fbaf6b6e0b9b
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 17:58:52.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4866" for this suite.
Jun 22 17:58:58.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:58:58.820: INFO: namespace secrets-4866 deletion completed in 6.093215173s

• [SLOW TEST:6.119 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 17:58:58.820: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 22 17:58:58.846: INFO: Waiting up to 5m0s for pod "pod-cfc08164-ed73-4f99-9fc7-8046a2b32a26" in namespace "emptydir-7689" to be "success or failure"
Jun 22 17:58:58.848: INFO: Pod "pod-cfc08164-ed73-4f99-9fc7-8046a2b32a26": Phase="Pending", Reason="", readiness=false. Elapsed: 2.421384ms
Jun 22 17:59:00.852: INFO: Pod "pod-cfc08164-ed73-4f99-9fc7-8046a2b32a26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005641833s
STEP: Saw pod success
Jun 22 17:59:00.852: INFO: Pod "pod-cfc08164-ed73-4f99-9fc7-8046a2b32a26" satisfied condition "success or failure"
Jun 22 17:59:00.854: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-cfc08164-ed73-4f99-9fc7-8046a2b32a26 container test-container: <nil>
STEP: delete the pod
Jun 22 17:59:00.869: INFO: Waiting for pod pod-cfc08164-ed73-4f99-9fc7-8046a2b32a26 to disappear
Jun 22 17:59:00.871: INFO: Pod pod-cfc08164-ed73-4f99-9fc7-8046a2b32a26 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 17:59:00.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7689" for this suite.
Jun 22 17:59:06.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:59:06.966: INFO: namespace emptydir-7689 deletion completed in 6.092160879s

• [SLOW TEST:8.146 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 17:59:06.966: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-2022
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 22 17:59:06.988: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 22 17:59:29.074: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.2.133.18 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2022 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 17:59:29.074: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
Jun 22 17:59:30.164: INFO: Found all expected endpoints: [netserver-0]
Jun 22 17:59:30.167: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.2.104.13 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2022 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 17:59:30.167: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
Jun 22 17:59:31.253: INFO: Found all expected endpoints: [netserver-1]
Jun 22 17:59:31.255: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.2.145.3 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2022 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 17:59:31.255: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
Jun 22 17:59:32.342: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 17:59:32.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2022" for this suite.
Jun 22 17:59:54.354: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 17:59:54.455: INFO: namespace pod-network-test-2022 deletion completed in 22.110469335s

• [SLOW TEST:47.490 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 17:59:54.456: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 22 17:59:54.482: INFO: Waiting up to 5m0s for pod "downward-api-1e890088-a5af-4e9e-94e1-f62845ec09a1" in namespace "downward-api-5776" to be "success or failure"
Jun 22 17:59:54.486: INFO: Pod "downward-api-1e890088-a5af-4e9e-94e1-f62845ec09a1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.600885ms
Jun 22 17:59:56.489: INFO: Pod "downward-api-1e890088-a5af-4e9e-94e1-f62845ec09a1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006752324s
STEP: Saw pod success
Jun 22 17:59:56.489: INFO: Pod "downward-api-1e890088-a5af-4e9e-94e1-f62845ec09a1" satisfied condition "success or failure"
Jun 22 17:59:56.492: INFO: Trying to get logs from node ip-10-0-20-165 pod downward-api-1e890088-a5af-4e9e-94e1-f62845ec09a1 container dapi-container: <nil>
STEP: delete the pod
Jun 22 17:59:56.506: INFO: Waiting for pod downward-api-1e890088-a5af-4e9e-94e1-f62845ec09a1 to disappear
Jun 22 17:59:56.509: INFO: Pod downward-api-1e890088-a5af-4e9e-94e1-f62845ec09a1 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 17:59:56.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5776" for this suite.
Jun 22 18:00:02.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:00:02.609: INFO: namespace downward-api-5776 deletion completed in 6.097519717s

• [SLOW TEST:8.153 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:00:02.610: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-f3a65437-0a5b-4a54-82a8-be0569274a9d
STEP: Creating a pod to test consume secrets
Jun 22 18:00:02.696: INFO: Waiting up to 5m0s for pod "pod-secrets-f2ccb12b-ff28-424c-b75d-2706981db10c" in namespace "secrets-1471" to be "success or failure"
Jun 22 18:00:02.701: INFO: Pod "pod-secrets-f2ccb12b-ff28-424c-b75d-2706981db10c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.798978ms
Jun 22 18:00:04.705: INFO: Pod "pod-secrets-f2ccb12b-ff28-424c-b75d-2706981db10c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008766589s
Jun 22 18:00:06.708: INFO: Pod "pod-secrets-f2ccb12b-ff28-424c-b75d-2706981db10c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01177674s
STEP: Saw pod success
Jun 22 18:00:06.708: INFO: Pod "pod-secrets-f2ccb12b-ff28-424c-b75d-2706981db10c" satisfied condition "success or failure"
Jun 22 18:00:06.710: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-secrets-f2ccb12b-ff28-424c-b75d-2706981db10c container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 18:00:06.723: INFO: Waiting for pod pod-secrets-f2ccb12b-ff28-424c-b75d-2706981db10c to disappear
Jun 22 18:00:06.726: INFO: Pod pod-secrets-f2ccb12b-ff28-424c-b75d-2706981db10c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:00:06.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1471" for this suite.
Jun 22 18:00:12.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:00:12.827: INFO: namespace secrets-1471 deletion completed in 6.098325501s

• [SLOW TEST:10.217 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:00:12.827: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 22 18:00:15.369: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e64b7cba-0096-4908-93a8-a8b72a7fb503"
Jun 22 18:00:15.370: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-e64b7cba-0096-4908-93a8-a8b72a7fb503" in namespace "pods-326" to be "terminated due to deadline exceeded"
Jun 22 18:00:15.372: INFO: Pod "pod-update-activedeadlineseconds-e64b7cba-0096-4908-93a8-a8b72a7fb503": Phase="Running", Reason="", readiness=true. Elapsed: 2.776848ms
Jun 22 18:00:17.376: INFO: Pod "pod-update-activedeadlineseconds-e64b7cba-0096-4908-93a8-a8b72a7fb503": Phase="Running", Reason="", readiness=true. Elapsed: 2.006046986s
Jun 22 18:00:19.379: INFO: Pod "pod-update-activedeadlineseconds-e64b7cba-0096-4908-93a8-a8b72a7fb503": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.009361852s
Jun 22 18:00:19.379: INFO: Pod "pod-update-activedeadlineseconds-e64b7cba-0096-4908-93a8-a8b72a7fb503" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:00:19.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-326" for this suite.
Jun 22 18:00:25.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:00:25.472: INFO: namespace pods-326 deletion completed in 6.090225509s

• [SLOW TEST:12.645 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:00:25.473: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 22 18:00:25.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-2073'
Jun 22 18:00:25.571: INFO: stderr: ""
Jun 22 18:00:25.571: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Jun 22 18:00:25.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 delete pods e2e-test-nginx-pod --namespace=kubectl-2073'
Jun 22 18:00:33.648: INFO: stderr: ""
Jun 22 18:00:33.648: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:00:33.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2073" for this suite.
Jun 22 18:00:39.661: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:00:39.742: INFO: namespace kubectl-2073 deletion completed in 6.090288093s

• [SLOW TEST:14.269 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:00:39.742: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 22 18:00:39.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 version'
Jun 22 18:00:39.831: INFO: stderr: ""
Jun 22 18:00:39.831: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:40:16Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:32:14Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:00:39.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8844" for this suite.
Jun 22 18:00:45.848: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:00:45.931: INFO: namespace kubectl-8844 deletion completed in 6.096313315s

• [SLOW TEST:6.189 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:00:45.932: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:00:48.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-903" for this suite.
Jun 22 18:01:10.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:01:11.066: INFO: namespace replication-controller-903 deletion completed in 22.088854438s

• [SLOW TEST:25.134 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:01:11.066: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 22 18:01:11.092: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun 22 18:01:16.096: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 22 18:01:16.096: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun 22 18:01:18.099: INFO: Creating deployment "test-rollover-deployment"
Jun 22 18:01:18.115: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun 22 18:01:20.125: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun 22 18:01:20.131: INFO: Ensure that both replica sets have 1 created replica
Jun 22 18:01:20.135: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun 22 18:01:20.142: INFO: Updating deployment test-rollover-deployment
Jun 22 18:01:20.142: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun 22 18:01:22.155: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun 22 18:01:22.161: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun 22 18:01:22.165: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 18:01:22.165: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823278, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823278, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823280, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823278, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 18:01:24.172: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 18:01:24.172: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823278, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823278, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823282, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823278, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 18:01:26.171: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 18:01:26.171: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823278, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823278, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823282, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823278, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 18:01:28.171: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 18:01:28.171: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823278, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823278, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823282, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823278, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 18:01:30.171: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 18:01:30.171: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823278, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823278, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823282, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823278, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 18:01:32.176: INFO: all replica sets need to contain the pod-template-hash label
Jun 22 18:01:32.176: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823278, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823278, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823282, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696823278, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 18:01:34.171: INFO: 
Jun 22 18:01:34.171: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 22 18:01:34.178: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-3168,SelfLink:/apis/apps/v1/namespaces/deployment-3168/deployments/test-rollover-deployment,UID:348a809e-efa1-4de4-a50d-0a85b0437bc1,ResourceVersion:4176,Generation:2,CreationTimestamp:2019-06-22 18:01:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-06-22 18:01:18 +0000 UTC 2019-06-22 18:01:18 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-06-22 18:01:32 +0000 UTC 2019-06-22 18:01:18 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jun 22 18:01:34.180: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-3168,SelfLink:/apis/apps/v1/namespaces/deployment-3168/replicasets/test-rollover-deployment-854595fc44,UID:454c35ab-9cba-405b-9154-6d281fd9db51,ResourceVersion:4166,Generation:2,CreationTimestamp:2019-06-22 18:01:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 348a809e-efa1-4de4-a50d-0a85b0437bc1 0xc0030b7de7 0xc0030b7de8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 22 18:01:34.180: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun 22 18:01:34.180: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-3168,SelfLink:/apis/apps/v1/namespaces/deployment-3168/replicasets/test-rollover-controller,UID:7b3b8b21-bd28-4a86-b591-8fe9df1c09b3,ResourceVersion:4175,Generation:2,CreationTimestamp:2019-06-22 18:01:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 348a809e-efa1-4de4-a50d-0a85b0437bc1 0xc0030b7d0f 0xc0030b7d20}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 22 18:01:34.181: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-3168,SelfLink:/apis/apps/v1/namespaces/deployment-3168/replicasets/test-rollover-deployment-9b8b997cf,UID:6f617195-1b38-466c-8f8d-53a850658f1f,ResourceVersion:4130,Generation:2,CreationTimestamp:2019-06-22 18:01:18 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment 348a809e-efa1-4de4-a50d-0a85b0437bc1 0xc0030b7ea0 0xc0030b7ea1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 22 18:01:34.183: INFO: Pod "test-rollover-deployment-854595fc44-wzgb4" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-wzgb4,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-3168,SelfLink:/api/v1/namespaces/deployment-3168/pods/test-rollover-deployment-854595fc44-wzgb4,UID:899d0623-5749-4bb3-ad37-d432db5c84ad,ResourceVersion:4146,Generation:0,CreationTimestamp:2019-06-22 18:01:20 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.104.20/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 454c35ab-9cba-405b-9154-6d281fd9db51 0xc00279ea87 0xc00279ea88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-98nsp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-98nsp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-98nsp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-20-165,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00279eb00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00279eb20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:01:20 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:01:22 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:01:22 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:01:20 +0000 UTC  }],Message:,Reason:,HostIP:10.0.20.165,PodIP:10.2.104.20,StartTime:2019-06-22 18:01:20 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-06-22 18:01:22 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://0cd1787b4cfa9c795886555341ac19440a3f25ef3b3c50a526866c0808fad343}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:01:34.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3168" for this suite.
Jun 22 18:01:40.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:01:40.300: INFO: namespace deployment-3168 deletion completed in 6.113933685s

• [SLOW TEST:29.233 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:01:40.300: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-cde4a638-fd08-4cc7-a484-0b656846fb08
STEP: Creating a pod to test consume secrets
Jun 22 18:01:40.328: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2b47f35f-8800-487a-a001-ea2c30e463eb" in namespace "projected-4629" to be "success or failure"
Jun 22 18:01:40.331: INFO: Pod "pod-projected-secrets-2b47f35f-8800-487a-a001-ea2c30e463eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.743629ms
Jun 22 18:01:42.334: INFO: Pod "pod-projected-secrets-2b47f35f-8800-487a-a001-ea2c30e463eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006054507s
STEP: Saw pod success
Jun 22 18:01:42.334: INFO: Pod "pod-projected-secrets-2b47f35f-8800-487a-a001-ea2c30e463eb" satisfied condition "success or failure"
Jun 22 18:01:42.337: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-projected-secrets-2b47f35f-8800-487a-a001-ea2c30e463eb container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 18:01:42.353: INFO: Waiting for pod pod-projected-secrets-2b47f35f-8800-487a-a001-ea2c30e463eb to disappear
Jun 22 18:01:42.355: INFO: Pod pod-projected-secrets-2b47f35f-8800-487a-a001-ea2c30e463eb no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:01:42.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4629" for this suite.
Jun 22 18:01:48.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:01:48.446: INFO: namespace projected-4629 deletion completed in 6.088124406s

• [SLOW TEST:8.146 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:01:48.446: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 22 18:01:48.490: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:01:48.501: INFO: Number of nodes with available pods: 0
Jun 22 18:01:48.501: INFO: Node ip-10-0-15-125 is running more than one daemon pod
Jun 22 18:01:49.507: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:01:49.509: INFO: Number of nodes with available pods: 0
Jun 22 18:01:49.509: INFO: Node ip-10-0-15-125 is running more than one daemon pod
Jun 22 18:01:50.505: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:01:50.508: INFO: Number of nodes with available pods: 2
Jun 22 18:01:50.508: INFO: Node ip-10-0-42-243 is running more than one daemon pod
Jun 22 18:01:51.505: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:01:51.508: INFO: Number of nodes with available pods: 3
Jun 22 18:01:51.508: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jun 22 18:01:51.530: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:01:51.532: INFO: Number of nodes with available pods: 2
Jun 22 18:01:51.532: INFO: Node ip-10-0-20-165 is running more than one daemon pod
Jun 22 18:01:52.536: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:01:52.538: INFO: Number of nodes with available pods: 2
Jun 22 18:01:52.538: INFO: Node ip-10-0-20-165 is running more than one daemon pod
Jun 22 18:01:53.539: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:01:53.541: INFO: Number of nodes with available pods: 2
Jun 22 18:01:53.541: INFO: Node ip-10-0-20-165 is running more than one daemon pod
Jun 22 18:01:54.536: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:01:54.540: INFO: Number of nodes with available pods: 2
Jun 22 18:01:54.540: INFO: Node ip-10-0-20-165 is running more than one daemon pod
Jun 22 18:01:55.535: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:01:55.538: INFO: Number of nodes with available pods: 3
Jun 22 18:01:55.538: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9297, will wait for the garbage collector to delete the pods
Jun 22 18:01:55.597: INFO: Deleting DaemonSet.extensions daemon-set took: 4.739534ms
Jun 22 18:01:55.698: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.277623ms
Jun 22 18:02:03.701: INFO: Number of nodes with available pods: 0
Jun 22 18:02:03.701: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 18:02:03.704: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9297/daemonsets","resourceVersion":"4399"},"items":null}

Jun 22 18:02:03.707: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9297/pods","resourceVersion":"4399"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:02:03.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9297" for this suite.
Jun 22 18:02:09.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:02:09.807: INFO: namespace daemonsets-9297 deletion completed in 6.088492108s

• [SLOW TEST:21.361 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:02:09.809: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jun 22 18:02:12.370: INFO: Successfully updated pod "labelsupdate2ce5a4ff-aa01-485a-ae4f-893b372b4f81"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:02:16.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5669" for this suite.
Jun 22 18:02:38.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:02:38.494: INFO: namespace downward-api-5669 deletion completed in 22.0998207s

• [SLOW TEST:28.685 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:02:38.494: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Jun 22 18:02:38.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 --namespace=kubectl-7288 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jun 22 18:02:40.365: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jun 22 18:02:40.365: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:02:42.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7288" for this suite.
Jun 22 18:02:48.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:02:48.462: INFO: namespace kubectl-7288 deletion completed in 6.088701612s

• [SLOW TEST:9.968 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:02:48.463: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jun 22 18:02:48.520: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:02:48.528: INFO: Number of nodes with available pods: 0
Jun 22 18:02:48.528: INFO: Node ip-10-0-15-125 is running more than one daemon pod
Jun 22 18:02:49.531: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:02:49.534: INFO: Number of nodes with available pods: 2
Jun 22 18:02:49.534: INFO: Node ip-10-0-20-165 is running more than one daemon pod
Jun 22 18:02:50.531: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:02:50.534: INFO: Number of nodes with available pods: 3
Jun 22 18:02:50.534: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jun 22 18:02:50.544: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:02:50.547: INFO: Number of nodes with available pods: 3
Jun 22 18:02:50.547: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2865, will wait for the garbage collector to delete the pods
Jun 22 18:02:51.626: INFO: Deleting DaemonSet.extensions daemon-set took: 16.413267ms
Jun 22 18:02:51.727: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.23258ms
Jun 22 18:03:02.829: INFO: Number of nodes with available pods: 0
Jun 22 18:03:02.829: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 18:03:02.831: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2865/daemonsets","resourceVersion":"4685"},"items":null}

Jun 22 18:03:02.833: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2865/pods","resourceVersion":"4685"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:03:02.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2865" for this suite.
Jun 22 18:03:08.857: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:03:08.938: INFO: namespace daemonsets-2865 deletion completed in 6.093538616s

• [SLOW TEST:20.476 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:03:08.939: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 22 18:03:09.028: INFO: (0) /api/v1/nodes/ip-10-0-15-125:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 14.691367ms)
Jun 22 18:03:09.032: INFO: (1) /api/v1/nodes/ip-10-0-15-125:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.289789ms)
Jun 22 18:03:09.035: INFO: (2) /api/v1/nodes/ip-10-0-15-125:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.849685ms)
Jun 22 18:03:09.038: INFO: (3) /api/v1/nodes/ip-10-0-15-125:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.451939ms)
Jun 22 18:03:09.042: INFO: (4) /api/v1/nodes/ip-10-0-15-125:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.249887ms)
Jun 22 18:03:09.044: INFO: (5) /api/v1/nodes/ip-10-0-15-125:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.891698ms)
Jun 22 18:03:09.047: INFO: (6) /api/v1/nodes/ip-10-0-15-125:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.837169ms)
Jun 22 18:03:09.050: INFO: (7) /api/v1/nodes/ip-10-0-15-125:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.697633ms)
Jun 22 18:03:09.053: INFO: (8) /api/v1/nodes/ip-10-0-15-125:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.712466ms)
Jun 22 18:03:09.055: INFO: (9) /api/v1/nodes/ip-10-0-15-125:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.502558ms)
Jun 22 18:03:09.058: INFO: (10) /api/v1/nodes/ip-10-0-15-125:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.542185ms)
Jun 22 18:03:09.061: INFO: (11) /api/v1/nodes/ip-10-0-15-125:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.830041ms)
Jun 22 18:03:09.063: INFO: (12) /api/v1/nodes/ip-10-0-15-125:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.598103ms)
Jun 22 18:03:09.066: INFO: (13) /api/v1/nodes/ip-10-0-15-125:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.631125ms)
Jun 22 18:03:09.069: INFO: (14) /api/v1/nodes/ip-10-0-15-125:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.664827ms)
Jun 22 18:03:09.071: INFO: (15) /api/v1/nodes/ip-10-0-15-125:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.512279ms)
Jun 22 18:03:09.074: INFO: (16) /api/v1/nodes/ip-10-0-15-125:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.461128ms)
Jun 22 18:03:09.076: INFO: (17) /api/v1/nodes/ip-10-0-15-125:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.577412ms)
Jun 22 18:03:09.079: INFO: (18) /api/v1/nodes/ip-10-0-15-125:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.731315ms)
Jun 22 18:03:09.082: INFO: (19) /api/v1/nodes/ip-10-0-15-125:10250/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.722131ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:03:09.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5766" for this suite.
Jun 22 18:03:15.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:03:15.176: INFO: namespace proxy-5766 deletion completed in 6.091030445s

• [SLOW TEST:6.237 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:03:15.177: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:03:19.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8504" for this suite.
Jun 22 18:03:25.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:03:25.304: INFO: namespace kubelet-test-8504 deletion completed in 6.089288371s

• [SLOW TEST:10.128 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:03:25.304: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 22 18:03:25.386: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a2422b2e-85d7-42cc-bdcf-edbbfb382af2" in namespace "downward-api-3175" to be "success or failure"
Jun 22 18:03:25.390: INFO: Pod "downwardapi-volume-a2422b2e-85d7-42cc-bdcf-edbbfb382af2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.013494ms
Jun 22 18:03:27.393: INFO: Pod "downwardapi-volume-a2422b2e-85d7-42cc-bdcf-edbbfb382af2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006272663s
STEP: Saw pod success
Jun 22 18:03:27.393: INFO: Pod "downwardapi-volume-a2422b2e-85d7-42cc-bdcf-edbbfb382af2" satisfied condition "success or failure"
Jun 22 18:03:27.395: INFO: Trying to get logs from node ip-10-0-20-165 pod downwardapi-volume-a2422b2e-85d7-42cc-bdcf-edbbfb382af2 container client-container: <nil>
STEP: delete the pod
Jun 22 18:03:27.411: INFO: Waiting for pod downwardapi-volume-a2422b2e-85d7-42cc-bdcf-edbbfb382af2 to disappear
Jun 22 18:03:27.419: INFO: Pod downwardapi-volume-a2422b2e-85d7-42cc-bdcf-edbbfb382af2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:03:27.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3175" for this suite.
Jun 22 18:03:33.430: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:03:33.509: INFO: namespace downward-api-3175 deletion completed in 6.087308462s

• [SLOW TEST:8.205 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:03:33.510: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 22 18:03:33.532: INFO: Creating deployment "test-recreate-deployment"
Jun 22 18:03:33.536: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun 22 18:03:33.545: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jun 22 18:03:35.551: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun 22 18:03:35.554: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun 22 18:03:35.559: INFO: Updating deployment test-recreate-deployment
Jun 22 18:03:35.559: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 22 18:03:35.609: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-1023,SelfLink:/apis/apps/v1/namespaces/deployment-1023/deployments/test-recreate-deployment,UID:2523909e-7ce7-4e8c-a9a7-ca4e4429b10a,ResourceVersion:4877,Generation:2,CreationTimestamp:2019-06-22 18:03:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-06-22 18:03:35 +0000 UTC 2019-06-22 18:03:35 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-06-22 18:03:35 +0000 UTC 2019-06-22 18:03:33 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jun 22 18:03:35.614: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-1023,SelfLink:/apis/apps/v1/namespaces/deployment-1023/replicasets/test-recreate-deployment-5c8c9cc69d,UID:40cd5729-4754-4060-bfff-1281006d2b45,ResourceVersion:4875,Generation:1,CreationTimestamp:2019-06-22 18:03:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 2523909e-7ce7-4e8c-a9a7-ca4e4429b10a 0xc000aef287 0xc000aef288}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 22 18:03:35.614: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun 22 18:03:35.614: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-1023,SelfLink:/apis/apps/v1/namespaces/deployment-1023/replicasets/test-recreate-deployment-6df85df6b9,UID:d6ee3fbc-672d-4c42-8523-b11a2de2b7b0,ResourceVersion:4867,Generation:2,CreationTimestamp:2019-06-22 18:03:33 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 2523909e-7ce7-4e8c-a9a7-ca4e4429b10a 0xc000aef357 0xc000aef358}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 22 18:03:35.617: INFO: Pod "test-recreate-deployment-5c8c9cc69d-kw985" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-kw985,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-1023,SelfLink:/api/v1/namespaces/deployment-1023/pods/test-recreate-deployment-5c8c9cc69d-kw985,UID:46b4e5e3-8b24-4bd7-85ec-bada4d20af7d,ResourceVersion:4878,Generation:0,CreationTimestamp:2019-06-22 18:03:35 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 40cd5729-4754-4060-bfff-1281006d2b45 0xc000aefc27 0xc000aefc28}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-xdr8t {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-xdr8t,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-xdr8t true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-20-165,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000aefca0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000aefcc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:03:35 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:03:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:03:35 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:03:35 +0000 UTC  }],Message:,Reason:,HostIP:10.0.20.165,PodIP:,StartTime:2019-06-22 18:03:35 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:03:35.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1023" for this suite.
Jun 22 18:03:41.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:03:41.709: INFO: namespace deployment-1023 deletion completed in 6.089924881s

• [SLOW TEST:8.200 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:03:41.709: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jun 22 18:03:45.747: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-fd98c08c-03b1-4afd-bfd0-244f8d7355e9,GenerateName:,Namespace:events-9918,SelfLink:/api/v1/namespaces/events-9918/pods/send-events-fd98c08c-03b1-4afd-bfd0-244f8d7355e9,UID:b800a4c2-2e02-41d3-b1c2-7dd264901a97,ResourceVersion:4942,Generation:0,CreationTimestamp:2019-06-22 18:03:41 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 731217704,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.104.31/32,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-pxzfx {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-pxzfx,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-pxzfx true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-20-165,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002922d30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002922d50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:03:41 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:03:44 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:03:44 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:03:41 +0000 UTC  }],Message:,Reason:,HostIP:10.0.20.165,PodIP:10.2.104.31,StartTime:2019-06-22 18:03:41 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-06-22 18:03:43 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://f7a30326ea8080c88c06520b176ccea9ff3a8275e387163abc05dd33ca6f4168}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jun 22 18:03:47.750: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jun 22 18:03:49.753: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:03:49.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9918" for this suite.
Jun 22 18:04:27.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:04:27.861: INFO: namespace events-9918 deletion completed in 38.095385732s

• [SLOW TEST:46.152 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:04:27.861: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-19d3add9-1396-4dca-a93d-7c2ee8a96fd0
STEP: Creating a pod to test consume configMaps
Jun 22 18:04:27.895: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2f0cc75e-25df-4d3d-8147-eccd3a4b27eb" in namespace "projected-9252" to be "success or failure"
Jun 22 18:04:27.899: INFO: Pod "pod-projected-configmaps-2f0cc75e-25df-4d3d-8147-eccd3a4b27eb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.853306ms
Jun 22 18:04:29.901: INFO: Pod "pod-projected-configmaps-2f0cc75e-25df-4d3d-8147-eccd3a4b27eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006766446s
STEP: Saw pod success
Jun 22 18:04:29.902: INFO: Pod "pod-projected-configmaps-2f0cc75e-25df-4d3d-8147-eccd3a4b27eb" satisfied condition "success or failure"
Jun 22 18:04:29.904: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-projected-configmaps-2f0cc75e-25df-4d3d-8147-eccd3a4b27eb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 18:04:29.921: INFO: Waiting for pod pod-projected-configmaps-2f0cc75e-25df-4d3d-8147-eccd3a4b27eb to disappear
Jun 22 18:04:29.924: INFO: Pod pod-projected-configmaps-2f0cc75e-25df-4d3d-8147-eccd3a4b27eb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:04:29.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9252" for this suite.
Jun 22 18:04:35.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:04:36.019: INFO: namespace projected-9252 deletion completed in 6.092282253s

• [SLOW TEST:8.158 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:04:36.019: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 22 18:04:36.042: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:04:38.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7340" for this suite.
Jun 22 18:05:28.160: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:05:28.246: INFO: namespace pods-7340 deletion completed in 50.095467222s

• [SLOW TEST:52.226 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:05:28.246: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-e977f7c3-42ea-4118-b7b0-353ded24f0b9
STEP: Creating a pod to test consume secrets
Jun 22 18:05:28.278: INFO: Waiting up to 5m0s for pod "pod-secrets-7bd538b2-03b1-47eb-bdac-24276cc766f6" in namespace "secrets-2220" to be "success or failure"
Jun 22 18:05:28.281: INFO: Pod "pod-secrets-7bd538b2-03b1-47eb-bdac-24276cc766f6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.829726ms
Jun 22 18:05:30.285: INFO: Pod "pod-secrets-7bd538b2-03b1-47eb-bdac-24276cc766f6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006885396s
STEP: Saw pod success
Jun 22 18:05:30.285: INFO: Pod "pod-secrets-7bd538b2-03b1-47eb-bdac-24276cc766f6" satisfied condition "success or failure"
Jun 22 18:05:30.288: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-secrets-7bd538b2-03b1-47eb-bdac-24276cc766f6 container secret-env-test: <nil>
STEP: delete the pod
Jun 22 18:05:30.304: INFO: Waiting for pod pod-secrets-7bd538b2-03b1-47eb-bdac-24276cc766f6 to disappear
Jun 22 18:05:30.306: INFO: Pod pod-secrets-7bd538b2-03b1-47eb-bdac-24276cc766f6 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:05:30.306: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2220" for this suite.
Jun 22 18:05:36.319: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:05:36.409: INFO: namespace secrets-2220 deletion completed in 6.100055588s

• [SLOW TEST:8.163 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:05:36.410: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 22 18:05:36.436: INFO: Waiting up to 5m0s for pod "pod-2d65630b-394f-4bf1-a6f3-73ed1cd937e0" in namespace "emptydir-4765" to be "success or failure"
Jun 22 18:05:36.440: INFO: Pod "pod-2d65630b-394f-4bf1-a6f3-73ed1cd937e0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.768568ms
Jun 22 18:05:38.443: INFO: Pod "pod-2d65630b-394f-4bf1-a6f3-73ed1cd937e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00780905s
Jun 22 18:05:40.447: INFO: Pod "pod-2d65630b-394f-4bf1-a6f3-73ed1cd937e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010934358s
STEP: Saw pod success
Jun 22 18:05:40.447: INFO: Pod "pod-2d65630b-394f-4bf1-a6f3-73ed1cd937e0" satisfied condition "success or failure"
Jun 22 18:05:40.449: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-2d65630b-394f-4bf1-a6f3-73ed1cd937e0 container test-container: <nil>
STEP: delete the pod
Jun 22 18:05:40.466: INFO: Waiting for pod pod-2d65630b-394f-4bf1-a6f3-73ed1cd937e0 to disappear
Jun 22 18:05:40.468: INFO: Pod pod-2d65630b-394f-4bf1-a6f3-73ed1cd937e0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:05:40.468: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4765" for this suite.
Jun 22 18:05:46.481: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:05:46.567: INFO: namespace emptydir-4765 deletion completed in 6.095519882s

• [SLOW TEST:10.157 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:05:46.567: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:06:10.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3503" for this suite.
Jun 22 18:06:16.776: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:06:16.863: INFO: namespace container-runtime-3503 deletion completed in 6.096655286s

• [SLOW TEST:30.296 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:06:16.865: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jun 22 18:06:26.935: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0622 18:06:26.935256      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:06:26.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8091" for this suite.
Jun 22 18:06:32.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:06:33.031: INFO: namespace gc-8091 deletion completed in 6.093268318s

• [SLOW TEST:16.166 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:06:33.031: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Jun 22 18:06:33.058: INFO: Waiting up to 5m0s for pod "pod-86cee0d5-fd6a-490e-9452-a40e6e894130" in namespace "emptydir-4723" to be "success or failure"
Jun 22 18:06:33.061: INFO: Pod "pod-86cee0d5-fd6a-490e-9452-a40e6e894130": Phase="Pending", Reason="", readiness=false. Elapsed: 2.856749ms
Jun 22 18:06:35.065: INFO: Pod "pod-86cee0d5-fd6a-490e-9452-a40e6e894130": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006084178s
STEP: Saw pod success
Jun 22 18:06:35.065: INFO: Pod "pod-86cee0d5-fd6a-490e-9452-a40e6e894130" satisfied condition "success or failure"
Jun 22 18:06:35.067: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-86cee0d5-fd6a-490e-9452-a40e6e894130 container test-container: <nil>
STEP: delete the pod
Jun 22 18:06:35.081: INFO: Waiting for pod pod-86cee0d5-fd6a-490e-9452-a40e6e894130 to disappear
Jun 22 18:06:35.083: INFO: Pod pod-86cee0d5-fd6a-490e-9452-a40e6e894130 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:06:35.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4723" for this suite.
Jun 22 18:06:41.099: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:06:41.181: INFO: namespace emptydir-4723 deletion completed in 6.092689489s

• [SLOW TEST:8.150 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:06:41.181: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 22 18:06:41.201: INFO: Creating ReplicaSet my-hostname-basic-21f3a30a-be73-46ab-98a2-962ee26b76e0
Jun 22 18:06:41.208: INFO: Pod name my-hostname-basic-21f3a30a-be73-46ab-98a2-962ee26b76e0: Found 0 pods out of 1
Jun 22 18:06:46.211: INFO: Pod name my-hostname-basic-21f3a30a-be73-46ab-98a2-962ee26b76e0: Found 1 pods out of 1
Jun 22 18:06:46.211: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-21f3a30a-be73-46ab-98a2-962ee26b76e0" is running
Jun 22 18:06:46.213: INFO: Pod "my-hostname-basic-21f3a30a-be73-46ab-98a2-962ee26b76e0-h2tlh" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-22 18:06:41 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-22 18:06:42 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-22 18:06:42 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-22 18:06:41 +0000 UTC Reason: Message:}])
Jun 22 18:06:46.213: INFO: Trying to dial the pod
Jun 22 18:06:51.223: INFO: Controller my-hostname-basic-21f3a30a-be73-46ab-98a2-962ee26b76e0: Got expected result from replica 1 [my-hostname-basic-21f3a30a-be73-46ab-98a2-962ee26b76e0-h2tlh]: "my-hostname-basic-21f3a30a-be73-46ab-98a2-962ee26b76e0-h2tlh", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:06:51.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-929" for this suite.
Jun 22 18:06:57.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:06:57.354: INFO: namespace replicaset-929 deletion completed in 6.128491928s

• [SLOW TEST:16.173 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:06:57.355: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 22 18:06:57.392: INFO: Waiting up to 5m0s for pod "downwardapi-volume-972820b9-a50c-4e45-9a9b-eab9d0e633b9" in namespace "projected-2793" to be "success or failure"
Jun 22 18:06:57.397: INFO: Pod "downwardapi-volume-972820b9-a50c-4e45-9a9b-eab9d0e633b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.992546ms
Jun 22 18:06:59.401: INFO: Pod "downwardapi-volume-972820b9-a50c-4e45-9a9b-eab9d0e633b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008204097s
STEP: Saw pod success
Jun 22 18:06:59.401: INFO: Pod "downwardapi-volume-972820b9-a50c-4e45-9a9b-eab9d0e633b9" satisfied condition "success or failure"
Jun 22 18:06:59.403: INFO: Trying to get logs from node ip-10-0-20-165 pod downwardapi-volume-972820b9-a50c-4e45-9a9b-eab9d0e633b9 container client-container: <nil>
STEP: delete the pod
Jun 22 18:06:59.425: INFO: Waiting for pod downwardapi-volume-972820b9-a50c-4e45-9a9b-eab9d0e633b9 to disappear
Jun 22 18:06:59.427: INFO: Pod downwardapi-volume-972820b9-a50c-4e45-9a9b-eab9d0e633b9 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:06:59.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2793" for this suite.
Jun 22 18:07:05.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:07:05.526: INFO: namespace projected-2793 deletion completed in 6.093788923s

• [SLOW TEST:8.172 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:07:05.527: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-c424674d-b9d3-492a-ac51-94600bb0b4eb
STEP: Creating a pod to test consume secrets
Jun 22 18:07:05.558: INFO: Waiting up to 5m0s for pod "pod-secrets-cb7e15bd-ab14-45cf-ac2d-d01d280bf865" in namespace "secrets-1655" to be "success or failure"
Jun 22 18:07:05.560: INFO: Pod "pod-secrets-cb7e15bd-ab14-45cf-ac2d-d01d280bf865": Phase="Pending", Reason="", readiness=false. Elapsed: 2.211217ms
Jun 22 18:07:07.563: INFO: Pod "pod-secrets-cb7e15bd-ab14-45cf-ac2d-d01d280bf865": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005329356s
STEP: Saw pod success
Jun 22 18:07:07.564: INFO: Pod "pod-secrets-cb7e15bd-ab14-45cf-ac2d-d01d280bf865" satisfied condition "success or failure"
Jun 22 18:07:07.566: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-secrets-cb7e15bd-ab14-45cf-ac2d-d01d280bf865 container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 18:07:07.582: INFO: Waiting for pod pod-secrets-cb7e15bd-ab14-45cf-ac2d-d01d280bf865 to disappear
Jun 22 18:07:07.584: INFO: Pod pod-secrets-cb7e15bd-ab14-45cf-ac2d-d01d280bf865 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:07:07.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1655" for this suite.
Jun 22 18:07:13.597: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:07:13.678: INFO: namespace secrets-1655 deletion completed in 6.090477055s

• [SLOW TEST:8.151 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:07:13.678: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jun 22 18:07:13.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 create -f - --namespace=kubectl-3028'
Jun 22 18:07:13.926: INFO: stderr: ""
Jun 22 18:07:13.926: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 18:07:13.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3028'
Jun 22 18:07:13.997: INFO: stderr: ""
Jun 22 18:07:13.997: INFO: stdout: "update-demo-nautilus-2zlcz update-demo-nautilus-sqd7z "
Jun 22 18:07:13.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-2zlcz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3028'
Jun 22 18:07:14.067: INFO: stderr: ""
Jun 22 18:07:14.067: INFO: stdout: ""
Jun 22 18:07:14.067: INFO: update-demo-nautilus-2zlcz is created but not running
Jun 22 18:07:19.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3028'
Jun 22 18:07:19.138: INFO: stderr: ""
Jun 22 18:07:19.139: INFO: stdout: "update-demo-nautilus-2zlcz update-demo-nautilus-sqd7z "
Jun 22 18:07:19.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-2zlcz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3028'
Jun 22 18:07:19.209: INFO: stderr: ""
Jun 22 18:07:19.209: INFO: stdout: "true"
Jun 22 18:07:19.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-2zlcz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3028'
Jun 22 18:07:19.273: INFO: stderr: ""
Jun 22 18:07:19.273: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 18:07:19.273: INFO: validating pod update-demo-nautilus-2zlcz
Jun 22 18:07:19.278: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 18:07:19.278: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 18:07:19.278: INFO: update-demo-nautilus-2zlcz is verified up and running
Jun 22 18:07:19.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-sqd7z -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3028'
Jun 22 18:07:19.353: INFO: stderr: ""
Jun 22 18:07:19.353: INFO: stdout: "true"
Jun 22 18:07:19.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-sqd7z -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3028'
Jun 22 18:07:19.426: INFO: stderr: ""
Jun 22 18:07:19.426: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 18:07:19.426: INFO: validating pod update-demo-nautilus-sqd7z
Jun 22 18:07:19.431: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 18:07:19.431: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 18:07:19.431: INFO: update-demo-nautilus-sqd7z is verified up and running
STEP: scaling down the replication controller
Jun 22 18:07:19.433: INFO: scanned /root for discovery docs: <nil>
Jun 22 18:07:19.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-3028'
Jun 22 18:07:20.531: INFO: stderr: ""
Jun 22 18:07:20.532: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 18:07:20.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3028'
Jun 22 18:07:20.605: INFO: stderr: ""
Jun 22 18:07:20.605: INFO: stdout: "update-demo-nautilus-2zlcz update-demo-nautilus-sqd7z "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jun 22 18:07:25.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3028'
Jun 22 18:07:25.678: INFO: stderr: ""
Jun 22 18:07:25.678: INFO: stdout: "update-demo-nautilus-2zlcz "
Jun 22 18:07:25.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-2zlcz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3028'
Jun 22 18:07:25.743: INFO: stderr: ""
Jun 22 18:07:25.743: INFO: stdout: "true"
Jun 22 18:07:25.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-2zlcz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3028'
Jun 22 18:07:25.810: INFO: stderr: ""
Jun 22 18:07:25.810: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 18:07:25.810: INFO: validating pod update-demo-nautilus-2zlcz
Jun 22 18:07:25.813: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 18:07:25.813: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 18:07:25.813: INFO: update-demo-nautilus-2zlcz is verified up and running
STEP: scaling up the replication controller
Jun 22 18:07:25.815: INFO: scanned /root for discovery docs: <nil>
Jun 22 18:07:25.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-3028'
Jun 22 18:07:26.907: INFO: stderr: ""
Jun 22 18:07:26.907: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 18:07:26.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3028'
Jun 22 18:07:26.981: INFO: stderr: ""
Jun 22 18:07:26.981: INFO: stdout: "update-demo-nautilus-2zlcz update-demo-nautilus-5swb6 "
Jun 22 18:07:26.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-2zlcz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3028'
Jun 22 18:07:27.050: INFO: stderr: ""
Jun 22 18:07:27.050: INFO: stdout: "true"
Jun 22 18:07:27.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-2zlcz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3028'
Jun 22 18:07:27.115: INFO: stderr: ""
Jun 22 18:07:27.115: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 18:07:27.115: INFO: validating pod update-demo-nautilus-2zlcz
Jun 22 18:07:27.119: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 18:07:27.119: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 18:07:27.119: INFO: update-demo-nautilus-2zlcz is verified up and running
Jun 22 18:07:27.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-5swb6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3028'
Jun 22 18:07:27.190: INFO: stderr: ""
Jun 22 18:07:27.190: INFO: stdout: ""
Jun 22 18:07:27.190: INFO: update-demo-nautilus-5swb6 is created but not running
Jun 22 18:07:32.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-3028'
Jun 22 18:07:32.266: INFO: stderr: ""
Jun 22 18:07:32.266: INFO: stdout: "update-demo-nautilus-2zlcz update-demo-nautilus-5swb6 "
Jun 22 18:07:32.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-2zlcz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3028'
Jun 22 18:07:32.339: INFO: stderr: ""
Jun 22 18:07:32.339: INFO: stdout: "true"
Jun 22 18:07:32.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-2zlcz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3028'
Jun 22 18:07:32.421: INFO: stderr: ""
Jun 22 18:07:32.421: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 18:07:32.421: INFO: validating pod update-demo-nautilus-2zlcz
Jun 22 18:07:32.424: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 18:07:32.424: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 18:07:32.424: INFO: update-demo-nautilus-2zlcz is verified up and running
Jun 22 18:07:32.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-5swb6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3028'
Jun 22 18:07:32.488: INFO: stderr: ""
Jun 22 18:07:32.488: INFO: stdout: "true"
Jun 22 18:07:32.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-5swb6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3028'
Jun 22 18:07:32.555: INFO: stderr: ""
Jun 22 18:07:32.555: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 18:07:32.555: INFO: validating pod update-demo-nautilus-5swb6
Jun 22 18:07:32.560: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 18:07:32.560: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 18:07:32.560: INFO: update-demo-nautilus-5swb6 is verified up and running
STEP: using delete to clean up resources
Jun 22 18:07:32.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 delete --grace-period=0 --force -f - --namespace=kubectl-3028'
Jun 22 18:07:32.625: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 18:07:32.625: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 22 18:07:32.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3028'
Jun 22 18:07:32.699: INFO: stderr: "No resources found.\n"
Jun 22 18:07:32.699: INFO: stdout: ""
Jun 22 18:07:32.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods -l name=update-demo --namespace=kubectl-3028 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 18:07:32.766: INFO: stderr: ""
Jun 22 18:07:32.766: INFO: stdout: "update-demo-nautilus-2zlcz\nupdate-demo-nautilus-5swb6\n"
Jun 22 18:07:33.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-3028'
Jun 22 18:07:33.344: INFO: stderr: "No resources found.\n"
Jun 22 18:07:33.344: INFO: stdout: ""
Jun 22 18:07:33.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods -l name=update-demo --namespace=kubectl-3028 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 18:07:33.410: INFO: stderr: ""
Jun 22 18:07:33.410: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:07:33.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3028" for this suite.
Jun 22 18:07:55.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:07:55.506: INFO: namespace kubectl-3028 deletion completed in 22.092600942s

• [SLOW TEST:41.828 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:07:55.507: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-332d16ac-02ef-412b-a3f2-3ca4f01f8b9a
STEP: Creating a pod to test consume secrets
Jun 22 18:07:55.544: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-3059c533-08d8-498b-8e5d-106f5f823018" in namespace "projected-950" to be "success or failure"
Jun 22 18:07:55.547: INFO: Pod "pod-projected-secrets-3059c533-08d8-498b-8e5d-106f5f823018": Phase="Pending", Reason="", readiness=false. Elapsed: 2.683572ms
Jun 22 18:07:57.551: INFO: Pod "pod-projected-secrets-3059c533-08d8-498b-8e5d-106f5f823018": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007187292s
STEP: Saw pod success
Jun 22 18:07:57.552: INFO: Pod "pod-projected-secrets-3059c533-08d8-498b-8e5d-106f5f823018" satisfied condition "success or failure"
Jun 22 18:07:57.554: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-projected-secrets-3059c533-08d8-498b-8e5d-106f5f823018 container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 18:07:57.568: INFO: Waiting for pod pod-projected-secrets-3059c533-08d8-498b-8e5d-106f5f823018 to disappear
Jun 22 18:07:57.570: INFO: Pod pod-projected-secrets-3059c533-08d8-498b-8e5d-106f5f823018 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:07:57.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-950" for this suite.
Jun 22 18:08:03.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:08:03.659: INFO: namespace projected-950 deletion completed in 6.086173803s

• [SLOW TEST:8.152 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:08:03.660: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 22 18:08:03.687: INFO: Waiting up to 5m0s for pod "pod-82dbe4db-0143-47a3-a57f-6cd005b1e458" in namespace "emptydir-1283" to be "success or failure"
Jun 22 18:08:03.693: INFO: Pod "pod-82dbe4db-0143-47a3-a57f-6cd005b1e458": Phase="Pending", Reason="", readiness=false. Elapsed: 5.831783ms
Jun 22 18:08:05.696: INFO: Pod "pod-82dbe4db-0143-47a3-a57f-6cd005b1e458": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00897406s
STEP: Saw pod success
Jun 22 18:08:05.696: INFO: Pod "pod-82dbe4db-0143-47a3-a57f-6cd005b1e458" satisfied condition "success or failure"
Jun 22 18:08:05.699: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-82dbe4db-0143-47a3-a57f-6cd005b1e458 container test-container: <nil>
STEP: delete the pod
Jun 22 18:08:05.717: INFO: Waiting for pod pod-82dbe4db-0143-47a3-a57f-6cd005b1e458 to disappear
Jun 22 18:08:05.719: INFO: Pod pod-82dbe4db-0143-47a3-a57f-6cd005b1e458 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:08:05.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1283" for this suite.
Jun 22 18:08:11.731: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:08:11.815: INFO: namespace emptydir-1283 deletion completed in 6.092837532s

• [SLOW TEST:8.156 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:08:11.816: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 22 18:08:11.836: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:08:13.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8336" for this suite.
Jun 22 18:08:55.877: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:08:55.984: INFO: namespace pods-8336 deletion completed in 42.117828236s

• [SLOW TEST:44.168 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:08:55.985: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 22 18:08:56.018: INFO: Waiting up to 5m0s for pod "downwardapi-volume-da37b026-b555-477a-ad78-9e61041b836b" in namespace "projected-4343" to be "success or failure"
Jun 22 18:08:56.023: INFO: Pod "downwardapi-volume-da37b026-b555-477a-ad78-9e61041b836b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.865921ms
Jun 22 18:08:58.026: INFO: Pod "downwardapi-volume-da37b026-b555-477a-ad78-9e61041b836b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007763608s
STEP: Saw pod success
Jun 22 18:08:58.026: INFO: Pod "downwardapi-volume-da37b026-b555-477a-ad78-9e61041b836b" satisfied condition "success or failure"
Jun 22 18:08:58.029: INFO: Trying to get logs from node ip-10-0-20-165 pod downwardapi-volume-da37b026-b555-477a-ad78-9e61041b836b container client-container: <nil>
STEP: delete the pod
Jun 22 18:08:58.046: INFO: Waiting for pod downwardapi-volume-da37b026-b555-477a-ad78-9e61041b836b to disappear
Jun 22 18:08:58.048: INFO: Pod downwardapi-volume-da37b026-b555-477a-ad78-9e61041b836b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:08:58.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4343" for this suite.
Jun 22 18:09:04.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:09:04.152: INFO: namespace projected-4343 deletion completed in 6.099690862s

• [SLOW TEST:8.167 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:09:04.153: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-a77aa4f6-de68-4172-870d-995d51ae08de
STEP: Creating a pod to test consume configMaps
Jun 22 18:09:04.182: INFO: Waiting up to 5m0s for pod "pod-configmaps-5a346de4-af5e-4144-b054-899255e2a5ab" in namespace "configmap-6336" to be "success or failure"
Jun 22 18:09:04.184: INFO: Pod "pod-configmaps-5a346de4-af5e-4144-b054-899255e2a5ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.205662ms
Jun 22 18:09:06.187: INFO: Pod "pod-configmaps-5a346de4-af5e-4144-b054-899255e2a5ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00481702s
STEP: Saw pod success
Jun 22 18:09:06.187: INFO: Pod "pod-configmaps-5a346de4-af5e-4144-b054-899255e2a5ab" satisfied condition "success or failure"
Jun 22 18:09:06.189: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-configmaps-5a346de4-af5e-4144-b054-899255e2a5ab container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 18:09:06.203: INFO: Waiting for pod pod-configmaps-5a346de4-af5e-4144-b054-899255e2a5ab to disappear
Jun 22 18:09:06.206: INFO: Pod pod-configmaps-5a346de4-af5e-4144-b054-899255e2a5ab no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:09:06.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6336" for this suite.
Jun 22 18:09:12.220: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:09:12.301: INFO: namespace configmap-6336 deletion completed in 6.092323933s

• [SLOW TEST:8.148 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:09:12.302: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-6x4p
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 18:09:12.335: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-6x4p" in namespace "subpath-2801" to be "success or failure"
Jun 22 18:09:12.340: INFO: Pod "pod-subpath-test-configmap-6x4p": Phase="Pending", Reason="", readiness=false. Elapsed: 5.30457ms
Jun 22 18:09:14.345: INFO: Pod "pod-subpath-test-configmap-6x4p": Phase="Running", Reason="", readiness=true. Elapsed: 2.010326611s
Jun 22 18:09:16.356: INFO: Pod "pod-subpath-test-configmap-6x4p": Phase="Running", Reason="", readiness=true. Elapsed: 4.02075903s
Jun 22 18:09:18.362: INFO: Pod "pod-subpath-test-configmap-6x4p": Phase="Running", Reason="", readiness=true. Elapsed: 6.026849069s
Jun 22 18:09:20.365: INFO: Pod "pod-subpath-test-configmap-6x4p": Phase="Running", Reason="", readiness=true. Elapsed: 8.029936042s
Jun 22 18:09:22.368: INFO: Pod "pod-subpath-test-configmap-6x4p": Phase="Running", Reason="", readiness=true. Elapsed: 10.03292013s
Jun 22 18:09:24.372: INFO: Pod "pod-subpath-test-configmap-6x4p": Phase="Running", Reason="", readiness=true. Elapsed: 12.036409261s
Jun 22 18:09:26.375: INFO: Pod "pod-subpath-test-configmap-6x4p": Phase="Running", Reason="", readiness=true. Elapsed: 14.039725028s
Jun 22 18:09:28.378: INFO: Pod "pod-subpath-test-configmap-6x4p": Phase="Running", Reason="", readiness=true. Elapsed: 16.04274079s
Jun 22 18:09:30.381: INFO: Pod "pod-subpath-test-configmap-6x4p": Phase="Running", Reason="", readiness=true. Elapsed: 18.045752751s
Jun 22 18:09:32.384: INFO: Pod "pod-subpath-test-configmap-6x4p": Phase="Running", Reason="", readiness=true. Elapsed: 20.048669848s
Jun 22 18:09:34.387: INFO: Pod "pod-subpath-test-configmap-6x4p": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.051679642s
STEP: Saw pod success
Jun 22 18:09:34.387: INFO: Pod "pod-subpath-test-configmap-6x4p" satisfied condition "success or failure"
Jun 22 18:09:34.389: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-subpath-test-configmap-6x4p container test-container-subpath-configmap-6x4p: <nil>
STEP: delete the pod
Jun 22 18:09:34.406: INFO: Waiting for pod pod-subpath-test-configmap-6x4p to disappear
Jun 22 18:09:34.408: INFO: Pod pod-subpath-test-configmap-6x4p no longer exists
STEP: Deleting pod pod-subpath-test-configmap-6x4p
Jun 22 18:09:34.408: INFO: Deleting pod "pod-subpath-test-configmap-6x4p" in namespace "subpath-2801"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:09:34.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-2801" for this suite.
Jun 22 18:09:40.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:09:40.501: INFO: namespace subpath-2801 deletion completed in 6.088764967s

• [SLOW TEST:28.199 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:09:40.501: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 22 18:09:40.551: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"73ed77ec-c26b-47f5-a16d-34545a36e25f", Controller:(*bool)(0xc00304f57a), BlockOwnerDeletion:(*bool)(0xc00304f57b)}}
Jun 22 18:09:40.558: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"57e0f758-2a8d-4990-94ed-f43b99625c70", Controller:(*bool)(0xc002c558c6), BlockOwnerDeletion:(*bool)(0xc002c558c7)}}
Jun 22 18:09:40.562: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"8624edb4-1d5e-4055-b35e-e46258b9301e", Controller:(*bool)(0xc00304f786), BlockOwnerDeletion:(*bool)(0xc00304f787)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:09:45.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9007" for this suite.
Jun 22 18:09:51.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:09:51.663: INFO: namespace gc-9007 deletion completed in 6.090160031s

• [SLOW TEST:11.162 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:09:51.663: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Jun 22 18:09:53.702: INFO: Pod pod-hostip-26df8f93-8a18-4c25-bfc6-7665cf541475 has hostIP: 10.0.20.165
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:09:53.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9504" for this suite.
Jun 22 18:10:15.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:10:15.800: INFO: namespace pods-9504 deletion completed in 22.095619356s

• [SLOW TEST:24.137 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:10:15.800: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-xz44s in namespace proxy-3767
I0622 18:10:15.836123      15 runners.go:180] Created replication controller with name: proxy-service-xz44s, namespace: proxy-3767, replica count: 1
I0622 18:10:16.886648      15 runners.go:180] proxy-service-xz44s Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 18:10:17.886890      15 runners.go:180] proxy-service-xz44s Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 18:10:18.887150      15 runners.go:180] proxy-service-xz44s Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 18:10:19.887380      15 runners.go:180] proxy-service-xz44s Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0622 18:10:20.887542      15 runners.go:180] proxy-service-xz44s Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0622 18:10:21.887723      15 runners.go:180] proxy-service-xz44s Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0622 18:10:22.887951      15 runners.go:180] proxy-service-xz44s Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 18:10:22.890: INFO: setup took 7.068867864s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jun 22 18:10:22.900: INFO: (0) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 9.964493ms)
Jun 22 18:10:22.902: INFO: (0) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">... (200; 11.198453ms)
Jun 22 18:10:22.905: INFO: (0) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname2/proxy/: bar (200; 14.466524ms)
Jun 22 18:10:22.906: INFO: (0) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 15.268039ms)
Jun 22 18:10:22.906: INFO: (0) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 15.153605ms)
Jun 22 18:10:22.906: INFO: (0) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname2/proxy/: bar (200; 15.213549ms)
Jun 22 18:10:22.906: INFO: (0) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/rewriteme">test</a> (200; 15.35027ms)
Jun 22 18:10:22.906: INFO: (0) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">test<... (200; 15.404312ms)
Jun 22 18:10:22.909: INFO: (0) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 18.375162ms)
Jun 22 18:10:22.909: INFO: (0) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname1/proxy/: foo (200; 18.446288ms)
Jun 22 18:10:22.909: INFO: (0) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname1/proxy/: foo (200; 18.225273ms)
Jun 22 18:10:22.914: INFO: (0) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:460/proxy/: tls baz (200; 23.512517ms)
Jun 22 18:10:22.915: INFO: (0) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/tlsrewritem... (200; 23.812657ms)
Jun 22 18:10:22.915: INFO: (0) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:462/proxy/: tls qux (200; 24.470393ms)
Jun 22 18:10:22.916: INFO: (0) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname2/proxy/: tls qux (200; 25.412774ms)
Jun 22 18:10:22.920: INFO: (0) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname1/proxy/: tls baz (200; 29.041053ms)
Jun 22 18:10:22.924: INFO: (1) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 4.016153ms)
Jun 22 18:10:22.927: INFO: (1) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:460/proxy/: tls baz (200; 7.004581ms)
Jun 22 18:10:22.928: INFO: (1) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname1/proxy/: foo (200; 7.622722ms)
Jun 22 18:10:22.935: INFO: (1) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname2/proxy/: bar (200; 14.973278ms)
Jun 22 18:10:22.936: INFO: (1) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/tlsrewritem... (200; 16.135802ms)
Jun 22 18:10:22.937: INFO: (1) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 16.330565ms)
Jun 22 18:10:22.937: INFO: (1) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">test<... (200; 16.312758ms)
Jun 22 18:10:22.937: INFO: (1) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname2/proxy/: bar (200; 16.442505ms)
Jun 22 18:10:22.937: INFO: (1) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:462/proxy/: tls qux (200; 16.386071ms)
Jun 22 18:10:22.937: INFO: (1) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">... (200; 16.320068ms)
Jun 22 18:10:22.937: INFO: (1) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname2/proxy/: tls qux (200; 16.611807ms)
Jun 22 18:10:22.937: INFO: (1) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/rewriteme">test</a> (200; 16.28145ms)
Jun 22 18:10:22.938: INFO: (1) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 17.436296ms)
Jun 22 18:10:22.938: INFO: (1) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 17.504982ms)
Jun 22 18:10:22.938: INFO: (1) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname1/proxy/: tls baz (200; 17.666111ms)
Jun 22 18:10:22.938: INFO: (1) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname1/proxy/: foo (200; 17.964805ms)
Jun 22 18:10:22.943: INFO: (2) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:460/proxy/: tls baz (200; 5.037474ms)
Jun 22 18:10:22.948: INFO: (2) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname2/proxy/: bar (200; 9.310637ms)
Jun 22 18:10:22.949: INFO: (2) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">test<... (200; 9.751615ms)
Jun 22 18:10:22.949: INFO: (2) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 9.823816ms)
Jun 22 18:10:22.949: INFO: (2) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 10.208029ms)
Jun 22 18:10:22.949: INFO: (2) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname1/proxy/: foo (200; 10.781157ms)
Jun 22 18:10:22.949: INFO: (2) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname1/proxy/: tls baz (200; 10.266365ms)
Jun 22 18:10:22.950: INFO: (2) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 10.967138ms)
Jun 22 18:10:22.950: INFO: (2) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">... (200; 11.25802ms)
Jun 22 18:10:22.950: INFO: (2) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname1/proxy/: foo (200; 11.314111ms)
Jun 22 18:10:22.950: INFO: (2) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 11.770812ms)
Jun 22 18:10:22.951: INFO: (2) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:462/proxy/: tls qux (200; 11.842673ms)
Jun 22 18:10:22.951: INFO: (2) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/rewriteme">test</a> (200; 11.851822ms)
Jun 22 18:10:22.951: INFO: (2) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname2/proxy/: tls qux (200; 12.200599ms)
Jun 22 18:10:22.951: INFO: (2) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/tlsrewritem... (200; 11.905595ms)
Jun 22 18:10:22.951: INFO: (2) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname2/proxy/: bar (200; 11.867052ms)
Jun 22 18:10:22.956: INFO: (3) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:462/proxy/: tls qux (200; 4.918854ms)
Jun 22 18:10:22.960: INFO: (3) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 8.510242ms)
Jun 22 18:10:22.960: INFO: (3) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/rewriteme">test</a> (200; 8.77388ms)
Jun 22 18:10:22.960: INFO: (3) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">... (200; 8.983868ms)
Jun 22 18:10:22.962: INFO: (3) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 10.090296ms)
Jun 22 18:10:22.962: INFO: (3) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 10.692051ms)
Jun 22 18:10:22.962: INFO: (3) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/tlsrewritem... (200; 10.518508ms)
Jun 22 18:10:22.962: INFO: (3) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">test<... (200; 11.105331ms)
Jun 22 18:10:22.964: INFO: (3) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:460/proxy/: tls baz (200; 12.408323ms)
Jun 22 18:10:22.964: INFO: (3) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname1/proxy/: foo (200; 12.743615ms)
Jun 22 18:10:22.964: INFO: (3) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname2/proxy/: bar (200; 12.833323ms)
Jun 22 18:10:22.964: INFO: (3) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname2/proxy/: tls qux (200; 12.697323ms)
Jun 22 18:10:22.965: INFO: (3) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname2/proxy/: bar (200; 12.855053ms)
Jun 22 18:10:22.965: INFO: (3) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname1/proxy/: foo (200; 12.820168ms)
Jun 22 18:10:22.965: INFO: (3) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname1/proxy/: tls baz (200; 13.249484ms)
Jun 22 18:10:22.966: INFO: (3) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 13.904488ms)
Jun 22 18:10:22.973: INFO: (4) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/tlsrewritem... (200; 7.114829ms)
Jun 22 18:10:22.973: INFO: (4) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">test<... (200; 7.468067ms)
Jun 22 18:10:22.974: INFO: (4) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 8.214198ms)
Jun 22 18:10:22.977: INFO: (4) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 11.395157ms)
Jun 22 18:10:22.977: INFO: (4) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname1/proxy/: tls baz (200; 11.492757ms)
Jun 22 18:10:22.978: INFO: (4) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:460/proxy/: tls baz (200; 11.61928ms)
Jun 22 18:10:22.978: INFO: (4) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/rewriteme">test</a> (200; 11.875735ms)
Jun 22 18:10:22.978: INFO: (4) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname1/proxy/: foo (200; 12.269461ms)
Jun 22 18:10:22.978: INFO: (4) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname2/proxy/: bar (200; 12.256831ms)
Jun 22 18:10:22.978: INFO: (4) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 12.179477ms)
Jun 22 18:10:22.979: INFO: (4) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">... (200; 12.435764ms)
Jun 22 18:10:22.979: INFO: (4) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 12.520921ms)
Jun 22 18:10:22.979: INFO: (4) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname2/proxy/: tls qux (200; 13.070372ms)
Jun 22 18:10:22.980: INFO: (4) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname2/proxy/: bar (200; 13.899754ms)
Jun 22 18:10:22.980: INFO: (4) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname1/proxy/: foo (200; 13.762983ms)
Jun 22 18:10:22.980: INFO: (4) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:462/proxy/: tls qux (200; 13.731097ms)
Jun 22 18:10:22.992: INFO: (5) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">test<... (200; 11.306359ms)
Jun 22 18:10:22.992: INFO: (5) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname1/proxy/: foo (200; 11.519352ms)
Jun 22 18:10:22.992: INFO: (5) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:460/proxy/: tls baz (200; 11.406099ms)
Jun 22 18:10:22.992: INFO: (5) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 11.892467ms)
Jun 22 18:10:22.992: INFO: (5) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">... (200; 11.532303ms)
Jun 22 18:10:22.992: INFO: (5) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/tlsrewritem... (200; 11.896568ms)
Jun 22 18:10:22.992: INFO: (5) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 11.870225ms)
Jun 22 18:10:22.992: INFO: (5) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 11.84814ms)
Jun 22 18:10:22.992: INFO: (5) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname2/proxy/: tls qux (200; 12.158134ms)
Jun 22 18:10:22.992: INFO: (5) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:462/proxy/: tls qux (200; 12.059126ms)
Jun 22 18:10:22.992: INFO: (5) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/rewriteme">test</a> (200; 12.067403ms)
Jun 22 18:10:22.992: INFO: (5) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname2/proxy/: bar (200; 11.978832ms)
Jun 22 18:10:22.992: INFO: (5) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 12.30764ms)
Jun 22 18:10:22.992: INFO: (5) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname2/proxy/: bar (200; 12.127097ms)
Jun 22 18:10:22.992: INFO: (5) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname1/proxy/: foo (200; 12.016355ms)
Jun 22 18:10:22.993: INFO: (5) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname1/proxy/: tls baz (200; 12.18453ms)
Jun 22 18:10:23.001: INFO: (6) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 8.237483ms)
Jun 22 18:10:23.001: INFO: (6) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 8.511723ms)
Jun 22 18:10:23.001: INFO: (6) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">... (200; 8.335446ms)
Jun 22 18:10:23.002: INFO: (6) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">test<... (200; 8.87417ms)
Jun 22 18:10:23.002: INFO: (6) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:460/proxy/: tls baz (200; 9.085679ms)
Jun 22 18:10:23.002: INFO: (6) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/tlsrewritem... (200; 9.27649ms)
Jun 22 18:10:23.002: INFO: (6) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/rewriteme">test</a> (200; 9.563811ms)
Jun 22 18:10:23.003: INFO: (6) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 9.921357ms)
Jun 22 18:10:23.003: INFO: (6) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 9.69376ms)
Jun 22 18:10:23.003: INFO: (6) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:462/proxy/: tls qux (200; 9.469674ms)
Jun 22 18:10:23.003: INFO: (6) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname2/proxy/: tls qux (200; 10.203287ms)
Jun 22 18:10:23.004: INFO: (6) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname1/proxy/: foo (200; 10.981693ms)
Jun 22 18:10:23.004: INFO: (6) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname1/proxy/: tls baz (200; 11.331196ms)
Jun 22 18:10:23.007: INFO: (6) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname1/proxy/: foo (200; 13.983833ms)
Jun 22 18:10:23.007: INFO: (6) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname2/proxy/: bar (200; 14.253674ms)
Jun 22 18:10:23.007: INFO: (6) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname2/proxy/: bar (200; 14.179127ms)
Jun 22 18:10:23.014: INFO: (7) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:460/proxy/: tls baz (200; 6.361585ms)
Jun 22 18:10:23.017: INFO: (7) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 9.52425ms)
Jun 22 18:10:23.018: INFO: (7) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 9.835645ms)
Jun 22 18:10:23.018: INFO: (7) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 10.2058ms)
Jun 22 18:10:23.018: INFO: (7) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:462/proxy/: tls qux (200; 10.765109ms)
Jun 22 18:10:23.018: INFO: (7) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 10.800111ms)
Jun 22 18:10:23.018: INFO: (7) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/tlsrewritem... (200; 10.82064ms)
Jun 22 18:10:23.018: INFO: (7) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">... (200; 10.633177ms)
Jun 22 18:10:23.018: INFO: (7) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">test<... (200; 10.702493ms)
Jun 22 18:10:23.020: INFO: (7) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/rewriteme">test</a> (200; 12.488992ms)
Jun 22 18:10:23.022: INFO: (7) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname1/proxy/: foo (200; 14.39713ms)
Jun 22 18:10:23.022: INFO: (7) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname2/proxy/: bar (200; 14.181848ms)
Jun 22 18:10:23.022: INFO: (7) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname2/proxy/: tls qux (200; 14.477826ms)
Jun 22 18:10:23.022: INFO: (7) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname1/proxy/: foo (200; 14.037751ms)
Jun 22 18:10:23.022: INFO: (7) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname1/proxy/: tls baz (200; 14.321078ms)
Jun 22 18:10:23.022: INFO: (7) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname2/proxy/: bar (200; 14.235607ms)
Jun 22 18:10:23.027: INFO: (8) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:462/proxy/: tls qux (200; 5.002038ms)
Jun 22 18:10:23.027: INFO: (8) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/tlsrewritem... (200; 5.043717ms)
Jun 22 18:10:23.031: INFO: (8) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 8.807106ms)
Jun 22 18:10:23.031: INFO: (8) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/rewriteme">test</a> (200; 8.641966ms)
Jun 22 18:10:23.032: INFO: (8) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">... (200; 9.110383ms)
Jun 22 18:10:23.032: INFO: (8) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">test<... (200; 9.220418ms)
Jun 22 18:10:23.032: INFO: (8) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 9.098196ms)
Jun 22 18:10:23.032: INFO: (8) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname2/proxy/: tls qux (200; 9.611861ms)
Jun 22 18:10:23.032: INFO: (8) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname2/proxy/: bar (200; 9.698191ms)
Jun 22 18:10:23.032: INFO: (8) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 9.547167ms)
Jun 22 18:10:23.032: INFO: (8) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:460/proxy/: tls baz (200; 9.805131ms)
Jun 22 18:10:23.033: INFO: (8) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 10.194595ms)
Jun 22 18:10:23.034: INFO: (8) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname1/proxy/: foo (200; 11.426567ms)
Jun 22 18:10:23.035: INFO: (8) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname2/proxy/: bar (200; 12.438425ms)
Jun 22 18:10:23.035: INFO: (8) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname1/proxy/: tls baz (200; 12.601068ms)
Jun 22 18:10:23.035: INFO: (8) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname1/proxy/: foo (200; 12.619176ms)
Jun 22 18:10:23.041: INFO: (9) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname1/proxy/: foo (200; 5.474645ms)
Jun 22 18:10:23.046: INFO: (9) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname1/proxy/: tls baz (200; 10.368086ms)
Jun 22 18:10:23.046: INFO: (9) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/rewriteme">test</a> (200; 10.55639ms)
Jun 22 18:10:23.046: INFO: (9) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/tlsrewritem... (200; 10.869209ms)
Jun 22 18:10:23.046: INFO: (9) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">test<... (200; 10.597945ms)
Jun 22 18:10:23.046: INFO: (9) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 10.519086ms)
Jun 22 18:10:23.047: INFO: (9) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname2/proxy/: bar (200; 10.810077ms)
Jun 22 18:10:23.047: INFO: (9) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname2/proxy/: tls qux (200; 10.451227ms)
Jun 22 18:10:23.047: INFO: (9) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname1/proxy/: foo (200; 10.635573ms)
Jun 22 18:10:23.047: INFO: (9) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname2/proxy/: bar (200; 11.062919ms)
Jun 22 18:10:23.047: INFO: (9) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">... (200; 10.900373ms)
Jun 22 18:10:23.047: INFO: (9) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 11.334569ms)
Jun 22 18:10:23.047: INFO: (9) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 11.01173ms)
Jun 22 18:10:23.047: INFO: (9) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:460/proxy/: tls baz (200; 10.916199ms)
Jun 22 18:10:23.047: INFO: (9) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 11.712071ms)
Jun 22 18:10:23.048: INFO: (9) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:462/proxy/: tls qux (200; 12.297229ms)
Jun 22 18:10:23.057: INFO: (10) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/rewriteme">test</a> (200; 8.40411ms)
Jun 22 18:10:23.057: INFO: (10) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 9.127975ms)
Jun 22 18:10:23.057: INFO: (10) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/tlsrewritem... (200; 8.9365ms)
Jun 22 18:10:23.057: INFO: (10) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 9.050719ms)
Jun 22 18:10:23.057: INFO: (10) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 9.39247ms)
Jun 22 18:10:23.058: INFO: (10) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:462/proxy/: tls qux (200; 9.429573ms)
Jun 22 18:10:23.058: INFO: (10) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 9.378218ms)
Jun 22 18:10:23.058: INFO: (10) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">... (200; 9.290259ms)
Jun 22 18:10:23.061: INFO: (10) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname2/proxy/: tls qux (200; 13.140443ms)
Jun 22 18:10:23.062: INFO: (10) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">test<... (200; 13.698179ms)
Jun 22 18:10:23.062: INFO: (10) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname1/proxy/: foo (200; 14.269479ms)
Jun 22 18:10:23.063: INFO: (10) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:460/proxy/: tls baz (200; 14.548553ms)
Jun 22 18:10:23.063: INFO: (10) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname2/proxy/: bar (200; 14.379941ms)
Jun 22 18:10:23.063: INFO: (10) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname2/proxy/: bar (200; 14.64053ms)
Jun 22 18:10:23.063: INFO: (10) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname1/proxy/: foo (200; 14.743345ms)
Jun 22 18:10:23.063: INFO: (10) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname1/proxy/: tls baz (200; 15.027313ms)
Jun 22 18:10:23.068: INFO: (11) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:460/proxy/: tls baz (200; 4.285658ms)
Jun 22 18:10:23.072: INFO: (11) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 8.225098ms)
Jun 22 18:10:23.072: INFO: (11) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/tlsrewritem... (200; 8.603122ms)
Jun 22 18:10:23.073: INFO: (11) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 8.801187ms)
Jun 22 18:10:23.072: INFO: (11) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 8.378106ms)
Jun 22 18:10:23.072: INFO: (11) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">test<... (200; 8.872747ms)
Jun 22 18:10:23.073: INFO: (11) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">... (200; 9.125094ms)
Jun 22 18:10:23.073: INFO: (11) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 9.044828ms)
Jun 22 18:10:23.073: INFO: (11) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:462/proxy/: tls qux (200; 8.864469ms)
Jun 22 18:10:23.073: INFO: (11) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/rewriteme">test</a> (200; 8.982075ms)
Jun 22 18:10:23.075: INFO: (11) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname1/proxy/: tls baz (200; 11.707938ms)
Jun 22 18:10:23.075: INFO: (11) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname2/proxy/: bar (200; 11.995967ms)
Jun 22 18:10:23.075: INFO: (11) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname2/proxy/: tls qux (200; 11.585422ms)
Jun 22 18:10:23.076: INFO: (11) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname1/proxy/: foo (200; 12.146579ms)
Jun 22 18:10:23.076: INFO: (11) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname1/proxy/: foo (200; 11.704386ms)
Jun 22 18:10:23.076: INFO: (11) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname2/proxy/: bar (200; 12.065817ms)
Jun 22 18:10:23.082: INFO: (12) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:460/proxy/: tls baz (200; 6.850059ms)
Jun 22 18:10:23.083: INFO: (12) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 6.665766ms)
Jun 22 18:10:23.083: INFO: (12) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/tlsrewritem... (200; 6.982732ms)
Jun 22 18:10:23.083: INFO: (12) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 7.033965ms)
Jun 22 18:10:23.083: INFO: (12) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">... (200; 6.982433ms)
Jun 22 18:10:23.083: INFO: (12) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/rewriteme">test</a> (200; 7.359784ms)
Jun 22 18:10:23.083: INFO: (12) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 7.469125ms)
Jun 22 18:10:23.083: INFO: (12) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">test<... (200; 7.317534ms)
Jun 22 18:10:23.083: INFO: (12) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:462/proxy/: tls qux (200; 7.452952ms)
Jun 22 18:10:23.086: INFO: (12) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 10.347977ms)
Jun 22 18:10:23.086: INFO: (12) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname2/proxy/: bar (200; 10.243314ms)
Jun 22 18:10:23.086: INFO: (12) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname1/proxy/: foo (200; 10.396093ms)
Jun 22 18:10:23.087: INFO: (12) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname1/proxy/: tls baz (200; 10.57685ms)
Jun 22 18:10:23.087: INFO: (12) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname2/proxy/: bar (200; 10.639592ms)
Jun 22 18:10:23.087: INFO: (12) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname2/proxy/: tls qux (200; 11.025707ms)
Jun 22 18:10:23.087: INFO: (12) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname1/proxy/: foo (200; 11.308445ms)
Jun 22 18:10:23.091: INFO: (13) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">test<... (200; 4.090565ms)
Jun 22 18:10:23.098: INFO: (13) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 10.873491ms)
Jun 22 18:10:23.098: INFO: (13) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 11.400863ms)
Jun 22 18:10:23.099: INFO: (13) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/tlsrewritem... (200; 11.031082ms)
Jun 22 18:10:23.099: INFO: (13) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:460/proxy/: tls baz (200; 11.749738ms)
Jun 22 18:10:23.099: INFO: (13) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/rewriteme">test</a> (200; 11.690899ms)
Jun 22 18:10:23.099: INFO: (13) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 12.090912ms)
Jun 22 18:10:23.101: INFO: (13) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:462/proxy/: tls qux (200; 13.486027ms)
Jun 22 18:10:23.101: INFO: (13) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">... (200; 14.110816ms)
Jun 22 18:10:23.101: INFO: (13) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 13.990837ms)
Jun 22 18:10:23.102: INFO: (13) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname2/proxy/: bar (200; 14.687165ms)
Jun 22 18:10:23.102: INFO: (13) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname2/proxy/: bar (200; 14.90518ms)
Jun 22 18:10:23.103: INFO: (13) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname2/proxy/: tls qux (200; 15.431213ms)
Jun 22 18:10:23.103: INFO: (13) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname1/proxy/: foo (200; 15.666944ms)
Jun 22 18:10:23.103: INFO: (13) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname1/proxy/: foo (200; 15.588055ms)
Jun 22 18:10:23.104: INFO: (13) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname1/proxy/: tls baz (200; 16.359861ms)
Jun 22 18:10:23.108: INFO: (14) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">test<... (200; 3.978491ms)
Jun 22 18:10:23.108: INFO: (14) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">... (200; 4.078418ms)
Jun 22 18:10:23.111: INFO: (14) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 7.162921ms)
Jun 22 18:10:23.114: INFO: (14) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 10.028414ms)
Jun 22 18:10:23.115: INFO: (14) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 10.485152ms)
Jun 22 18:10:23.115: INFO: (14) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:462/proxy/: tls qux (200; 10.105434ms)
Jun 22 18:10:23.115: INFO: (14) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 10.714127ms)
Jun 22 18:10:23.116: INFO: (14) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:460/proxy/: tls baz (200; 11.56845ms)
Jun 22 18:10:23.116: INFO: (14) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/rewriteme">test</a> (200; 11.53418ms)
Jun 22 18:10:23.116: INFO: (14) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/tlsrewritem... (200; 11.563482ms)
Jun 22 18:10:23.117: INFO: (14) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname1/proxy/: foo (200; 13.173745ms)
Jun 22 18:10:23.118: INFO: (14) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname2/proxy/: bar (200; 13.161473ms)
Jun 22 18:10:23.118: INFO: (14) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname2/proxy/: bar (200; 13.493541ms)
Jun 22 18:10:23.118: INFO: (14) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname1/proxy/: foo (200; 13.319378ms)
Jun 22 18:10:23.118: INFO: (14) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname2/proxy/: tls qux (200; 13.289081ms)
Jun 22 18:10:23.118: INFO: (14) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname1/proxy/: tls baz (200; 13.602327ms)
Jun 22 18:10:23.127: INFO: (15) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:460/proxy/: tls baz (200; 8.077295ms)
Jun 22 18:10:23.127: INFO: (15) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 9.035547ms)
Jun 22 18:10:23.128: INFO: (15) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 9.767159ms)
Jun 22 18:10:23.128: INFO: (15) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/rewriteme">test</a> (200; 9.85908ms)
Jun 22 18:10:23.128: INFO: (15) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">... (200; 10.006906ms)
Jun 22 18:10:23.128: INFO: (15) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:462/proxy/: tls qux (200; 10.247388ms)
Jun 22 18:10:23.129: INFO: (15) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">test<... (200; 10.167042ms)
Jun 22 18:10:23.129: INFO: (15) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 10.455693ms)
Jun 22 18:10:23.129: INFO: (15) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 10.336215ms)
Jun 22 18:10:23.130: INFO: (15) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname2/proxy/: bar (200; 11.466583ms)
Jun 22 18:10:23.130: INFO: (15) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname1/proxy/: foo (200; 12.289053ms)
Jun 22 18:10:23.131: INFO: (15) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/tlsrewritem... (200; 12.472545ms)
Jun 22 18:10:23.131: INFO: (15) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname1/proxy/: tls baz (200; 12.780042ms)
Jun 22 18:10:23.131: INFO: (15) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname1/proxy/: foo (200; 12.896884ms)
Jun 22 18:10:23.131: INFO: (15) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname2/proxy/: tls qux (200; 12.954441ms)
Jun 22 18:10:23.132: INFO: (15) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname2/proxy/: bar (200; 13.560082ms)
Jun 22 18:10:23.136: INFO: (16) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:460/proxy/: tls baz (200; 4.460868ms)
Jun 22 18:10:23.142: INFO: (16) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/tlsrewritem... (200; 9.639045ms)
Jun 22 18:10:23.142: INFO: (16) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname1/proxy/: tls baz (200; 9.405291ms)
Jun 22 18:10:23.144: INFO: (16) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 11.260237ms)
Jun 22 18:10:23.144: INFO: (16) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 11.6346ms)
Jun 22 18:10:23.144: INFO: (16) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname2/proxy/: tls qux (200; 12.089509ms)
Jun 22 18:10:23.144: INFO: (16) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 12.253948ms)
Jun 22 18:10:23.145: INFO: (16) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname2/proxy/: bar (200; 12.069687ms)
Jun 22 18:10:23.145: INFO: (16) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname1/proxy/: foo (200; 12.894677ms)
Jun 22 18:10:23.146: INFO: (16) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname2/proxy/: bar (200; 13.003115ms)
Jun 22 18:10:23.146: INFO: (16) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">... (200; 13.28424ms)
Jun 22 18:10:23.146: INFO: (16) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 13.810877ms)
Jun 22 18:10:23.146: INFO: (16) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname1/proxy/: foo (200; 14.279293ms)
Jun 22 18:10:23.147: INFO: (16) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:462/proxy/: tls qux (200; 14.24873ms)
Jun 22 18:10:23.147: INFO: (16) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/rewriteme">test</a> (200; 14.222433ms)
Jun 22 18:10:23.147: INFO: (16) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">test<... (200; 13.350248ms)
Jun 22 18:10:23.156: INFO: (17) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:460/proxy/: tls baz (200; 8.693822ms)
Jun 22 18:10:23.156: INFO: (17) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 9.130682ms)
Jun 22 18:10:23.156: INFO: (17) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:462/proxy/: tls qux (200; 8.992099ms)
Jun 22 18:10:23.156: INFO: (17) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">test<... (200; 9.552599ms)
Jun 22 18:10:23.157: INFO: (17) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 9.607399ms)
Jun 22 18:10:23.157: INFO: (17) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">... (200; 9.726647ms)
Jun 22 18:10:23.157: INFO: (17) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 9.534355ms)
Jun 22 18:10:23.158: INFO: (17) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname1/proxy/: foo (200; 10.33286ms)
Jun 22 18:10:23.158: INFO: (17) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/tlsrewritem... (200; 10.308804ms)
Jun 22 18:10:23.158: INFO: (17) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 11.133402ms)
Jun 22 18:10:23.158: INFO: (17) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/rewriteme">test</a> (200; 10.770217ms)
Jun 22 18:10:23.158: INFO: (17) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname2/proxy/: bar (200; 11.243275ms)
Jun 22 18:10:23.158: INFO: (17) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname2/proxy/: tls qux (200; 11.128349ms)
Jun 22 18:10:23.159: INFO: (17) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname1/proxy/: tls baz (200; 11.851972ms)
Jun 22 18:10:23.159: INFO: (17) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname2/proxy/: bar (200; 11.539202ms)
Jun 22 18:10:23.159: INFO: (17) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname1/proxy/: foo (200; 11.766132ms)
Jun 22 18:10:23.166: INFO: (18) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 6.937072ms)
Jun 22 18:10:23.168: INFO: (18) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 8.80887ms)
Jun 22 18:10:23.168: INFO: (18) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 9.020618ms)
Jun 22 18:10:23.168: INFO: (18) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">... (200; 9.258427ms)
Jun 22 18:10:23.169: INFO: (18) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/tlsrewritem... (200; 9.287777ms)
Jun 22 18:10:23.169: INFO: (18) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">test<... (200; 9.75036ms)
Jun 22 18:10:23.169: INFO: (18) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 9.737893ms)
Jun 22 18:10:23.169: INFO: (18) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname2/proxy/: bar (200; 9.944731ms)
Jun 22 18:10:23.169: INFO: (18) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname1/proxy/: foo (200; 10.106176ms)
Jun 22 18:10:23.169: INFO: (18) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:460/proxy/: tls baz (200; 9.780499ms)
Jun 22 18:10:23.171: INFO: (18) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/rewriteme">test</a> (200; 12.029444ms)
Jun 22 18:10:23.172: INFO: (18) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname1/proxy/: foo (200; 12.456455ms)
Jun 22 18:10:23.172: INFO: (18) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:462/proxy/: tls qux (200; 12.602767ms)
Jun 22 18:10:23.172: INFO: (18) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname1/proxy/: tls baz (200; 12.88958ms)
Jun 22 18:10:23.172: INFO: (18) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname2/proxy/: bar (200; 13.246564ms)
Jun 22 18:10:23.172: INFO: (18) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname2/proxy/: tls qux (200; 12.926596ms)
Jun 22 18:10:23.178: INFO: (19) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n/proxy/rewriteme">test</a> (200; 5.799241ms)
Jun 22 18:10:23.182: INFO: (19) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 8.672835ms)
Jun 22 18:10:23.182: INFO: (19) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 9.451445ms)
Jun 22 18:10:23.182: INFO: (19) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:160/proxy/: foo (200; 9.661623ms)
Jun 22 18:10:23.182: INFO: (19) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname2/proxy/: bar (200; 10.00297ms)
Jun 22 18:10:23.183: INFO: (19) /api/v1/namespaces/proxy-3767/services/proxy-service-xz44s:portname1/proxy/: foo (200; 10.371827ms)
Jun 22 18:10:23.184: INFO: (19) /api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/http:proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">... (200; 11.264716ms)
Jun 22 18:10:23.184: INFO: (19) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:162/proxy/: bar (200; 11.740433ms)
Jun 22 18:10:23.184: INFO: (19) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:462/proxy/: tls qux (200; 11.269642ms)
Jun 22 18:10:23.185: INFO: (19) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:460/proxy/: tls baz (200; 11.978981ms)
Jun 22 18:10:23.185: INFO: (19) /api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/https:proxy-service-xz44s-zqv6n:443/proxy/tlsrewritem... (200; 12.073869ms)
Jun 22 18:10:23.185: INFO: (19) /api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/: <a href="/api/v1/namespaces/proxy-3767/pods/proxy-service-xz44s-zqv6n:1080/proxy/rewriteme">test<... (200; 12.423541ms)
Jun 22 18:10:23.185: INFO: (19) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname2/proxy/: bar (200; 12.670465ms)
Jun 22 18:10:23.186: INFO: (19) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname2/proxy/: tls qux (200; 12.776027ms)
Jun 22 18:10:23.186: INFO: (19) /api/v1/namespaces/proxy-3767/services/http:proxy-service-xz44s:portname1/proxy/: foo (200; 12.712378ms)
Jun 22 18:10:23.186: INFO: (19) /api/v1/namespaces/proxy-3767/services/https:proxy-service-xz44s:tlsportname1/proxy/: tls baz (200; 13.117961ms)
STEP: deleting ReplicationController proxy-service-xz44s in namespace proxy-3767, will wait for the garbage collector to delete the pods
Jun 22 18:10:23.244: INFO: Deleting ReplicationController proxy-service-xz44s took: 5.289061ms
Jun 22 18:10:23.544: INFO: Terminating ReplicationController proxy-service-xz44s pods took: 300.15667ms
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:10:26.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3767" for this suite.
Jun 22 18:10:32.058: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:10:32.162: INFO: namespace proxy-3767 deletion completed in 6.114707722s

• [SLOW TEST:16.362 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:10:32.163: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 22 18:10:32.185: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 create -f - --namespace=kubectl-7354'
Jun 22 18:10:32.484: INFO: stderr: ""
Jun 22 18:10:32.484: INFO: stdout: "replicationcontroller/redis-master created\n"
Jun 22 18:10:32.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 create -f - --namespace=kubectl-7354'
Jun 22 18:10:32.636: INFO: stderr: ""
Jun 22 18:10:32.636: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 22 18:10:33.639: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 18:10:33.639: INFO: Found 0 / 1
Jun 22 18:10:34.639: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 18:10:34.639: INFO: Found 1 / 1
Jun 22 18:10:34.639: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 22 18:10:34.642: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 18:10:34.642: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 22 18:10:34.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 describe pod redis-master-tsj7h --namespace=kubectl-7354'
Jun 22 18:10:34.729: INFO: stderr: ""
Jun 22 18:10:34.729: INFO: stdout: "Name:           redis-master-tsj7h\nNamespace:      kubectl-7354\nPriority:       0\nNode:           ip-10-0-20-165/10.0.20.165\nStart Time:     Sat, 22 Jun 2019 18:10:32 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    cni.projectcalico.org/podIP: 10.2.104.57/32\nStatus:         Running\nIP:             10.2.104.57\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://706aa982de8c33af79bef8c97394ef7d7842bc655fd8fa04d8165c4d7f61998a\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 22 Jun 2019 18:10:33 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-54xrb (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-54xrb:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-54xrb\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                     Message\n  ----    ------     ----  ----                     -------\n  Normal  Scheduled  2s    default-scheduler        Successfully assigned kubectl-7354/redis-master-tsj7h to ip-10-0-20-165\n  Normal  Pulled     1s    kubelet, ip-10-0-20-165  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    1s    kubelet, ip-10-0-20-165  Created container redis-master\n  Normal  Started    1s    kubelet, ip-10-0-20-165  Started container redis-master\n"
Jun 22 18:10:34.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 describe rc redis-master --namespace=kubectl-7354'
Jun 22 18:10:34.811: INFO: stderr: ""
Jun 22 18:10:34.811: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-7354\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-tsj7h\n"
Jun 22 18:10:34.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 describe service redis-master --namespace=kubectl-7354'
Jun 22 18:10:34.885: INFO: stderr: ""
Jun 22 18:10:34.885: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-7354\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.3.179.233\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.2.104.57:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun 22 18:10:34.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 describe node ip-10-0-15-125'
Jun 22 18:10:34.988: INFO: stderr: ""
Jun 22 18:10:34.988: INFO: stdout: "Name:               ip-10-0-15-125\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ip-10-0-15-125\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/node=\nAnnotations:        node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.0.15.125/20\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.2.133.0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 22 Jun 2019 17:45:59 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sat, 22 Jun 2019 17:46:45 +0000   Sat, 22 Jun 2019 17:46:45 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Sat, 22 Jun 2019 18:10:31 +0000   Sat, 22 Jun 2019 17:45:59 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sat, 22 Jun 2019 18:10:31 +0000   Sat, 22 Jun 2019 17:45:59 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sat, 22 Jun 2019 18:10:31 +0000   Sat, 22 Jun 2019 17:45:59 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sat, 22 Jun 2019 18:10:31 +0000   Sat, 22 Jun 2019 17:46:50 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.0.15.125\n  Hostname:    ip-10-0-15-125\nCapacity:\n cpu:                2\n ephemeral-storage:  17897500Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             2002168Ki\n pods:               110\nAllocatable:\n cpu:                2\n ephemeral-storage:  16494335973\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             1899768Ki\n pods:               110\nSystem Info:\n Machine ID:                 ec2d137649d178d2f2db46b52e997571\n System UUID:                ec2d1376-49d1-78d2-f2db-46b52e997571\n Boot ID:                    17758057-9603-4543-b640-2a868d1f5f9a\n Kernel Version:             4.19.43-coreos-r1\n OS Image:                   Container Linux by CoreOS 2079.6.0 (Rhyolite)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.3\n Kubelet Version:            v1.15.0\n Kube-Proxy Version:         v1.15.0\nPodCIDR:                     10.2.0.0/24\nNon-terminated Pods:         (4 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         19m\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-2e6d603853cb48ce-6dsjh    0 (0%)        0 (0%)      0 (0%)           0 (0%)         19m\n  kube-system                calico-node-n5n7z                                          150m (7%)     0 (0%)      0 (0%)           0 (0%)         24m\n  kube-system                kube-proxy-89t9g                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         24m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests   Limits\n  --------           --------   ------\n  cpu                150m (7%)  0 (0%)\n  memory             0 (0%)     0 (0%)\n  ephemeral-storage  0 (0%)     0 (0%)\nEvents:\n  Type    Reason    Age   From                        Message\n  ----    ------    ----  ----                        -------\n  Normal  Starting  24m   kube-proxy, ip-10-0-15-125  Starting kube-proxy.\n"
Jun 22 18:10:34.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 describe namespace kubectl-7354'
Jun 22 18:10:35.065: INFO: stderr: ""
Jun 22 18:10:35.065: INFO: stdout: "Name:         kubectl-7354\nLabels:       e2e-framework=kubectl\n              e2e-run=333810d3-9a5c-4520-896c-f92b8062d9e7\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:10:35.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7354" for this suite.
Jun 22 18:10:57.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:10:57.159: INFO: namespace kubectl-7354 deletion completed in 22.091664988s

• [SLOW TEST:24.997 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:10:57.160: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:11:57.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-929" for this suite.
Jun 22 18:12:19.200: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:12:19.284: INFO: namespace container-probe-929 deletion completed in 22.093664409s

• [SLOW TEST:82.125 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:12:19.286: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 22 18:12:19.323: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jun 22 18:12:19.330: INFO: Number of nodes with available pods: 0
Jun 22 18:12:19.330: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jun 22 18:12:19.345: INFO: Number of nodes with available pods: 0
Jun 22 18:12:19.345: INFO: Node ip-10-0-15-125 is running more than one daemon pod
Jun 22 18:12:20.349: INFO: Number of nodes with available pods: 0
Jun 22 18:12:20.349: INFO: Node ip-10-0-15-125 is running more than one daemon pod
Jun 22 18:12:21.349: INFO: Number of nodes with available pods: 1
Jun 22 18:12:21.349: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jun 22 18:12:21.367: INFO: Number of nodes with available pods: 1
Jun 22 18:12:21.367: INFO: Number of running nodes: 0, number of available pods: 1
Jun 22 18:12:22.371: INFO: Number of nodes with available pods: 0
Jun 22 18:12:22.371: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jun 22 18:12:22.381: INFO: Number of nodes with available pods: 0
Jun 22 18:12:22.381: INFO: Node ip-10-0-15-125 is running more than one daemon pod
Jun 22 18:12:23.384: INFO: Number of nodes with available pods: 0
Jun 22 18:12:23.384: INFO: Node ip-10-0-15-125 is running more than one daemon pod
Jun 22 18:12:24.385: INFO: Number of nodes with available pods: 0
Jun 22 18:12:24.385: INFO: Node ip-10-0-15-125 is running more than one daemon pod
Jun 22 18:12:25.384: INFO: Number of nodes with available pods: 0
Jun 22 18:12:25.384: INFO: Node ip-10-0-15-125 is running more than one daemon pod
Jun 22 18:12:26.385: INFO: Number of nodes with available pods: 1
Jun 22 18:12:26.385: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9059, will wait for the garbage collector to delete the pods
Jun 22 18:12:26.447: INFO: Deleting DaemonSet.extensions daemon-set took: 5.257378ms
Jun 22 18:12:26.748: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.406234ms
Jun 22 18:12:32.851: INFO: Number of nodes with available pods: 0
Jun 22 18:12:32.851: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 18:12:32.854: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9059/daemonsets","resourceVersion":"6974"},"items":null}

Jun 22 18:12:32.856: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9059/pods","resourceVersion":"6974"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:12:32.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9059" for this suite.
Jun 22 18:12:38.887: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:12:38.967: INFO: namespace daemonsets-9059 deletion completed in 6.089146663s

• [SLOW TEST:19.682 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:12:38.968: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jun 22 18:12:41.022: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-689426708 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jun 22 18:12:46.095: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:12:46.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7910" for this suite.
Jun 22 18:12:52.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:12:52.211: INFO: namespace pods-7910 deletion completed in 6.109855838s

• [SLOW TEST:13.243 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:12:52.211: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Jun 22 18:12:52.248: INFO: Waiting up to 5m0s for pod "var-expansion-dcf8f4f0-17b8-4e9f-a54b-42b46e391488" in namespace "var-expansion-7394" to be "success or failure"
Jun 22 18:12:52.252: INFO: Pod "var-expansion-dcf8f4f0-17b8-4e9f-a54b-42b46e391488": Phase="Pending", Reason="", readiness=false. Elapsed: 4.353934ms
Jun 22 18:12:54.255: INFO: Pod "var-expansion-dcf8f4f0-17b8-4e9f-a54b-42b46e391488": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007336357s
STEP: Saw pod success
Jun 22 18:12:54.255: INFO: Pod "var-expansion-dcf8f4f0-17b8-4e9f-a54b-42b46e391488" satisfied condition "success or failure"
Jun 22 18:12:54.258: INFO: Trying to get logs from node ip-10-0-20-165 pod var-expansion-dcf8f4f0-17b8-4e9f-a54b-42b46e391488 container dapi-container: <nil>
STEP: delete the pod
Jun 22 18:12:54.275: INFO: Waiting for pod var-expansion-dcf8f4f0-17b8-4e9f-a54b-42b46e391488 to disappear
Jun 22 18:12:54.278: INFO: Pod var-expansion-dcf8f4f0-17b8-4e9f-a54b-42b46e391488 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:12:54.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7394" for this suite.
Jun 22 18:13:00.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:13:00.386: INFO: namespace var-expansion-7394 deletion completed in 6.103014862s

• [SLOW TEST:8.175 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:13:00.386: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 22 18:13:00.414: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aff2957a-8ebb-4744-b545-aa4efa001ed3" in namespace "projected-647" to be "success or failure"
Jun 22 18:13:00.426: INFO: Pod "downwardapi-volume-aff2957a-8ebb-4744-b545-aa4efa001ed3": Phase="Pending", Reason="", readiness=false. Elapsed: 12.17478ms
Jun 22 18:13:02.429: INFO: Pod "downwardapi-volume-aff2957a-8ebb-4744-b545-aa4efa001ed3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.015177486s
STEP: Saw pod success
Jun 22 18:13:02.429: INFO: Pod "downwardapi-volume-aff2957a-8ebb-4744-b545-aa4efa001ed3" satisfied condition "success or failure"
Jun 22 18:13:02.432: INFO: Trying to get logs from node ip-10-0-20-165 pod downwardapi-volume-aff2957a-8ebb-4744-b545-aa4efa001ed3 container client-container: <nil>
STEP: delete the pod
Jun 22 18:13:02.446: INFO: Waiting for pod downwardapi-volume-aff2957a-8ebb-4744-b545-aa4efa001ed3 to disappear
Jun 22 18:13:02.449: INFO: Pod downwardapi-volume-aff2957a-8ebb-4744-b545-aa4efa001ed3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:13:02.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-647" for this suite.
Jun 22 18:13:08.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:13:08.542: INFO: namespace projected-647 deletion completed in 6.090267237s

• [SLOW TEST:8.155 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:13:08.542: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-6cfe3edd-3db5-4053-b0ae-2f277f998c89
STEP: Creating a pod to test consume secrets
Jun 22 18:13:08.570: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-29c03a84-4519-4bf9-9774-95e5b3f63bd6" in namespace "projected-7395" to be "success or failure"
Jun 22 18:13:08.574: INFO: Pod "pod-projected-secrets-29c03a84-4519-4bf9-9774-95e5b3f63bd6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007873ms
Jun 22 18:13:10.578: INFO: Pod "pod-projected-secrets-29c03a84-4519-4bf9-9774-95e5b3f63bd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007156576s
STEP: Saw pod success
Jun 22 18:13:10.578: INFO: Pod "pod-projected-secrets-29c03a84-4519-4bf9-9774-95e5b3f63bd6" satisfied condition "success or failure"
Jun 22 18:13:10.580: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-projected-secrets-29c03a84-4519-4bf9-9774-95e5b3f63bd6 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 18:13:10.596: INFO: Waiting for pod pod-projected-secrets-29c03a84-4519-4bf9-9774-95e5b3f63bd6 to disappear
Jun 22 18:13:10.599: INFO: Pod pod-projected-secrets-29c03a84-4519-4bf9-9774-95e5b3f63bd6 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:13:10.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7395" for this suite.
Jun 22 18:13:16.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:13:16.702: INFO: namespace projected-7395 deletion completed in 6.100251689s

• [SLOW TEST:8.160 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:13:16.703: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-0b13d1cb-375c-46d6-b585-4db07abe000d
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:13:16.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2326" for this suite.
Jun 22 18:13:22.789: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:13:22.870: INFO: namespace configmap-2326 deletion completed in 6.091180829s

• [SLOW TEST:6.167 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:13:22.870: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 22 18:13:22.900: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3975d333-21dc-426e-9a61-5f82989dc91b" in namespace "projected-7747" to be "success or failure"
Jun 22 18:13:22.905: INFO: Pod "downwardapi-volume-3975d333-21dc-426e-9a61-5f82989dc91b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.23407ms
Jun 22 18:13:24.908: INFO: Pod "downwardapi-volume-3975d333-21dc-426e-9a61-5f82989dc91b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008577961s
STEP: Saw pod success
Jun 22 18:13:24.908: INFO: Pod "downwardapi-volume-3975d333-21dc-426e-9a61-5f82989dc91b" satisfied condition "success or failure"
Jun 22 18:13:24.911: INFO: Trying to get logs from node ip-10-0-20-165 pod downwardapi-volume-3975d333-21dc-426e-9a61-5f82989dc91b container client-container: <nil>
STEP: delete the pod
Jun 22 18:13:24.925: INFO: Waiting for pod downwardapi-volume-3975d333-21dc-426e-9a61-5f82989dc91b to disappear
Jun 22 18:13:24.928: INFO: Pod downwardapi-volume-3975d333-21dc-426e-9a61-5f82989dc91b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:13:24.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7747" for this suite.
Jun 22 18:13:30.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:13:31.019: INFO: namespace projected-7747 deletion completed in 6.088659329s

• [SLOW TEST:8.148 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:13:31.020: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 22 18:13:31.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-6017'
Jun 22 18:13:31.132: INFO: stderr: ""
Jun 22 18:13:31.132: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jun 22 18:13:36.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pod e2e-test-nginx-pod --namespace=kubectl-6017 -o json'
Jun 22 18:13:36.252: INFO: stderr: ""
Jun 22 18:13:36.252: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/podIP\": \"10.2.104.64/32\"\n        },\n        \"creationTimestamp\": \"2019-06-22T18:13:31Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-6017\",\n        \"resourceVersion\": \"7259\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-6017/pods/e2e-test-nginx-pod\",\n        \"uid\": \"9dfc55ce-326b-440a-8216-ff2157a68d05\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-8qrtc\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ip-10-0-20-165\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-8qrtc\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-8qrtc\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-22T18:13:31Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-22T18:13:32Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-22T18:13:32Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-06-22T18:13:31Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://9167953b784b242991ec27d7e62b742c4a4cbeb9eb8b0b0e267061bfabd03443\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-06-22T18:13:32Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.20.165\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.2.104.64\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-06-22T18:13:31Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jun 22 18:13:36.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 replace -f - --namespace=kubectl-6017'
Jun 22 18:13:36.415: INFO: stderr: ""
Jun 22 18:13:36.415: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Jun 22 18:13:36.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 delete pods e2e-test-nginx-pod --namespace=kubectl-6017'
Jun 22 18:13:37.800: INFO: stderr: ""
Jun 22 18:13:37.800: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:13:37.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6017" for this suite.
Jun 22 18:13:43.813: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:13:43.905: INFO: namespace kubectl-6017 deletion completed in 6.102353696s

• [SLOW TEST:12.885 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:13:43.905: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jun 22 18:13:43.936: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 18:13:43.941: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 18:13:43.943: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-15-125 before test
Jun 22 18:13:43.950: INFO: calico-node-n5n7z from kube-system started at 2019-06-22 17:46:06 +0000 UTC (1 container statuses recorded)
Jun 22 18:13:43.950: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 18:13:43.951: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-22 17:50:43 +0000 UTC (1 container statuses recorded)
Jun 22 18:13:43.951: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 18:13:43.951: INFO: sonobuoy-systemd-logs-daemon-set-2e6d603853cb48ce-6dsjh from heptio-sonobuoy started at 2019-06-22 17:50:48 +0000 UTC (2 container statuses recorded)
Jun 22 18:13:43.951: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 18:13:43.951: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 18:13:43.951: INFO: kube-proxy-89t9g from kube-system started at 2019-06-22 17:46:06 +0000 UTC (1 container statuses recorded)
Jun 22 18:13:43.951: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 18:13:43.951: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-20-165 before test
Jun 22 18:13:43.955: INFO: kube-proxy-ww9qf from kube-system started at 2019-06-22 17:46:06 +0000 UTC (1 container statuses recorded)
Jun 22 18:13:43.955: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 18:13:43.955: INFO: calico-node-zd2zz from kube-system started at 2019-06-22 17:46:06 +0000 UTC (1 container statuses recorded)
Jun 22 18:13:43.955: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 18:13:43.955: INFO: sonobuoy-systemd-logs-daemon-set-2e6d603853cb48ce-rxlqb from heptio-sonobuoy started at 2019-06-22 17:50:48 +0000 UTC (2 container statuses recorded)
Jun 22 18:13:43.955: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 18:13:43.955: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 18:13:43.955: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-42-243 before test
Jun 22 18:13:43.971: INFO: kube-proxy-gkjvx from kube-system started at 2019-06-22 17:46:06 +0000 UTC (1 container statuses recorded)
Jun 22 18:13:43.971: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 18:13:43.971: INFO: calico-node-688zt from kube-system started at 2019-06-22 17:46:06 +0000 UTC (1 container statuses recorded)
Jun 22 18:13:43.971: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 18:13:43.971: INFO: sonobuoy-e2e-job-5c726901000d4f40 from heptio-sonobuoy started at 2019-06-22 17:50:48 +0000 UTC (2 container statuses recorded)
Jun 22 18:13:43.971: INFO: 	Container e2e ready: true, restart count 0
Jun 22 18:13:43.971: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 18:13:43.971: INFO: sonobuoy-systemd-logs-daemon-set-2e6d603853cb48ce-7pf2f from heptio-sonobuoy started at 2019-06-22 17:50:48 +0000 UTC (2 container statuses recorded)
Jun 22 18:13:43.971: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 18:13:43.971: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node ip-10-0-15-125
STEP: verifying the node has the label node ip-10-0-20-165
STEP: verifying the node has the label node ip-10-0-42-243
Jun 22 18:13:44.015: INFO: Pod sonobuoy requesting resource cpu=0m on Node ip-10-0-15-125
Jun 22 18:13:44.015: INFO: Pod sonobuoy-e2e-job-5c726901000d4f40 requesting resource cpu=0m on Node ip-10-0-42-243
Jun 22 18:13:44.015: INFO: Pod sonobuoy-systemd-logs-daemon-set-2e6d603853cb48ce-6dsjh requesting resource cpu=0m on Node ip-10-0-15-125
Jun 22 18:13:44.015: INFO: Pod sonobuoy-systemd-logs-daemon-set-2e6d603853cb48ce-7pf2f requesting resource cpu=0m on Node ip-10-0-42-243
Jun 22 18:13:44.015: INFO: Pod sonobuoy-systemd-logs-daemon-set-2e6d603853cb48ce-rxlqb requesting resource cpu=0m on Node ip-10-0-20-165
Jun 22 18:13:44.015: INFO: Pod calico-node-688zt requesting resource cpu=150m on Node ip-10-0-42-243
Jun 22 18:13:44.015: INFO: Pod calico-node-n5n7z requesting resource cpu=150m on Node ip-10-0-15-125
Jun 22 18:13:44.015: INFO: Pod calico-node-zd2zz requesting resource cpu=150m on Node ip-10-0-20-165
Jun 22 18:13:44.015: INFO: Pod kube-proxy-89t9g requesting resource cpu=0m on Node ip-10-0-15-125
Jun 22 18:13:44.015: INFO: Pod kube-proxy-gkjvx requesting resource cpu=0m on Node ip-10-0-42-243
Jun 22 18:13:44.015: INFO: Pod kube-proxy-ww9qf requesting resource cpu=0m on Node ip-10-0-20-165
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6a38df5a-e7f1-4046-954a-82c3febb35e9.15aa97e375926f3a], Reason = [Scheduled], Message = [Successfully assigned sched-pred-444/filler-pod-6a38df5a-e7f1-4046-954a-82c3febb35e9 to ip-10-0-15-125]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6a38df5a-e7f1-4046-954a-82c3febb35e9.15aa97e3a1ebbd7f], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6a38df5a-e7f1-4046-954a-82c3febb35e9.15aa97e3a5c28003], Reason = [Created], Message = [Created container filler-pod-6a38df5a-e7f1-4046-954a-82c3febb35e9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6a38df5a-e7f1-4046-954a-82c3febb35e9.15aa97e3ac3b3c8c], Reason = [Started], Message = [Started container filler-pod-6a38df5a-e7f1-4046-954a-82c3febb35e9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6cd0cc7e-d961-4d69-95c6-8e4004ce5602.15aa97e376c9463f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-444/filler-pod-6cd0cc7e-d961-4d69-95c6-8e4004ce5602 to ip-10-0-42-243]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6cd0cc7e-d961-4d69-95c6-8e4004ce5602.15aa97e3a1ee1f33], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6cd0cc7e-d961-4d69-95c6-8e4004ce5602.15aa97e3a4dacb97], Reason = [Created], Message = [Created container filler-pod-6cd0cc7e-d961-4d69-95c6-8e4004ce5602]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6cd0cc7e-d961-4d69-95c6-8e4004ce5602.15aa97e3aad01fd3], Reason = [Started], Message = [Started container filler-pod-6cd0cc7e-d961-4d69-95c6-8e4004ce5602]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c621bd0f-acb2-423a-96af-ed4f29318b78.15aa97e3763902e1], Reason = [Scheduled], Message = [Successfully assigned sched-pred-444/filler-pod-c621bd0f-acb2-423a-96af-ed4f29318b78 to ip-10-0-20-165]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c621bd0f-acb2-423a-96af-ed4f29318b78.15aa97e39e995db0], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c621bd0f-acb2-423a-96af-ed4f29318b78.15aa97e3a290680c], Reason = [Created], Message = [Created container filler-pod-c621bd0f-acb2-423a-96af-ed4f29318b78]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c621bd0f-acb2-423a-96af-ed4f29318b78.15aa97e3a8f49ba2], Reason = [Started], Message = [Started container filler-pod-c621bd0f-acb2-423a-96af-ed4f29318b78]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15aa97e3ee9bfc12], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 3 Insufficient cpu.]
STEP: removing the label node off the node ip-10-0-15-125
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-20-165
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node ip-10-0-42-243
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:13:47.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-444" for this suite.
Jun 22 18:13:53.121: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:13:53.202: INFO: namespace sched-pred-444 deletion completed in 6.089816661s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:9.297 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:13:53.203: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-8189
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 22 18:13:53.230: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 22 18:14:13.297: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.2.133.31:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8189 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 18:14:13.297: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
Jun 22 18:14:13.414: INFO: Found all expected endpoints: [netserver-0]
Jun 22 18:14:13.417: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.2.104.66:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8189 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 18:14:13.417: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
Jun 22 18:14:13.540: INFO: Found all expected endpoints: [netserver-1]
Jun 22 18:14:13.543: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.2.145.10:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8189 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 18:14:13.543: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
Jun 22 18:14:13.640: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:14:13.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8189" for this suite.
Jun 22 18:14:35.652: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:14:35.739: INFO: namespace pod-network-test-8189 deletion completed in 22.095193693s

• [SLOW TEST:42.536 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:14:35.740: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Jun 22 18:14:35.774: INFO: Waiting up to 5m0s for pod "pod-338b584a-b7e1-4da7-acc2-d1fc04d1e49e" in namespace "emptydir-3896" to be "success or failure"
Jun 22 18:14:35.777: INFO: Pod "pod-338b584a-b7e1-4da7-acc2-d1fc04d1e49e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.700293ms
Jun 22 18:14:37.780: INFO: Pod "pod-338b584a-b7e1-4da7-acc2-d1fc04d1e49e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006753548s
STEP: Saw pod success
Jun 22 18:14:37.781: INFO: Pod "pod-338b584a-b7e1-4da7-acc2-d1fc04d1e49e" satisfied condition "success or failure"
Jun 22 18:14:37.783: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-338b584a-b7e1-4da7-acc2-d1fc04d1e49e container test-container: <nil>
STEP: delete the pod
Jun 22 18:14:37.805: INFO: Waiting for pod pod-338b584a-b7e1-4da7-acc2-d1fc04d1e49e to disappear
Jun 22 18:14:37.808: INFO: Pod pod-338b584a-b7e1-4da7-acc2-d1fc04d1e49e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:14:37.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3896" for this suite.
Jun 22 18:14:43.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:14:43.906: INFO: namespace emptydir-3896 deletion completed in 6.094541465s

• [SLOW TEST:8.166 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:14:43.906: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:14:45.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9801" for this suite.
Jun 22 18:15:35.961: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:15:36.044: INFO: namespace kubelet-test-9801 deletion completed in 50.092411996s

• [SLOW TEST:52.138 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:15:36.044: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-560636ae-18b3-4d72-bb15-1dcf50d1241c
STEP: Creating a pod to test consume secrets
Jun 22 18:15:36.077: INFO: Waiting up to 5m0s for pod "pod-secrets-ab904641-7786-48cb-88b5-ef00b5469f58" in namespace "secrets-9757" to be "success or failure"
Jun 22 18:15:36.081: INFO: Pod "pod-secrets-ab904641-7786-48cb-88b5-ef00b5469f58": Phase="Pending", Reason="", readiness=false. Elapsed: 4.576913ms
Jun 22 18:15:38.084: INFO: Pod "pod-secrets-ab904641-7786-48cb-88b5-ef00b5469f58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007667644s
STEP: Saw pod success
Jun 22 18:15:38.084: INFO: Pod "pod-secrets-ab904641-7786-48cb-88b5-ef00b5469f58" satisfied condition "success or failure"
Jun 22 18:15:38.087: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-secrets-ab904641-7786-48cb-88b5-ef00b5469f58 container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 18:15:38.162: INFO: Waiting for pod pod-secrets-ab904641-7786-48cb-88b5-ef00b5469f58 to disappear
Jun 22 18:15:38.164: INFO: Pod pod-secrets-ab904641-7786-48cb-88b5-ef00b5469f58 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:15:38.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9757" for this suite.
Jun 22 18:15:44.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:15:44.254: INFO: namespace secrets-9757 deletion completed in 6.087148667s

• [SLOW TEST:8.210 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:15:44.255: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-870
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-870 to expose endpoints map[]
Jun 22 18:15:44.286: INFO: successfully validated that service endpoint-test2 in namespace services-870 exposes endpoints map[] (2.975223ms elapsed)
STEP: Creating pod pod1 in namespace services-870
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-870 to expose endpoints map[pod1:[80]]
Jun 22 18:15:46.321: INFO: successfully validated that service endpoint-test2 in namespace services-870 exposes endpoints map[pod1:[80]] (2.030536968s elapsed)
STEP: Creating pod pod2 in namespace services-870
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-870 to expose endpoints map[pod1:[80] pod2:[80]]
Jun 22 18:15:48.352: INFO: successfully validated that service endpoint-test2 in namespace services-870 exposes endpoints map[pod1:[80] pod2:[80]] (2.024678406s elapsed)
STEP: Deleting pod pod1 in namespace services-870
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-870 to expose endpoints map[pod2:[80]]
Jun 22 18:15:49.371: INFO: successfully validated that service endpoint-test2 in namespace services-870 exposes endpoints map[pod2:[80]] (1.013364915s elapsed)
STEP: Deleting pod pod2 in namespace services-870
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-870 to expose endpoints map[]
Jun 22 18:15:49.383: INFO: successfully validated that service endpoint-test2 in namespace services-870 exposes endpoints map[] (4.610077ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:15:49.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-870" for this suite.
Jun 22 18:16:11.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:16:11.494: INFO: namespace services-870 deletion completed in 22.093363825s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:27.239 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:16:11.495: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-09f7eea1-509f-477a-8143-964a00f4c8fb
STEP: Creating a pod to test consume configMaps
Jun 22 18:16:11.577: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5f3c3f45-58f8-43a7-a297-be6985787114" in namespace "projected-9701" to be "success or failure"
Jun 22 18:16:11.581: INFO: Pod "pod-projected-configmaps-5f3c3f45-58f8-43a7-a297-be6985787114": Phase="Pending", Reason="", readiness=false. Elapsed: 3.819311ms
Jun 22 18:16:13.584: INFO: Pod "pod-projected-configmaps-5f3c3f45-58f8-43a7-a297-be6985787114": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006798627s
STEP: Saw pod success
Jun 22 18:16:13.584: INFO: Pod "pod-projected-configmaps-5f3c3f45-58f8-43a7-a297-be6985787114" satisfied condition "success or failure"
Jun 22 18:16:13.586: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-projected-configmaps-5f3c3f45-58f8-43a7-a297-be6985787114 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 18:16:13.606: INFO: Waiting for pod pod-projected-configmaps-5f3c3f45-58f8-43a7-a297-be6985787114 to disappear
Jun 22 18:16:13.609: INFO: Pod pod-projected-configmaps-5f3c3f45-58f8-43a7-a297-be6985787114 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:16:13.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9701" for this suite.
Jun 22 18:16:19.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:16:19.700: INFO: namespace projected-9701 deletion completed in 6.087647373s

• [SLOW TEST:8.206 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:16:19.701: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 22 18:16:19.727: INFO: Waiting up to 5m0s for pod "downward-api-b1efdc02-f37b-4422-839f-a1a231eff571" in namespace "downward-api-1560" to be "success or failure"
Jun 22 18:16:19.731: INFO: Pod "downward-api-b1efdc02-f37b-4422-839f-a1a231eff571": Phase="Pending", Reason="", readiness=false. Elapsed: 3.148072ms
Jun 22 18:16:21.734: INFO: Pod "downward-api-b1efdc02-f37b-4422-839f-a1a231eff571": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006885638s
STEP: Saw pod success
Jun 22 18:16:21.734: INFO: Pod "downward-api-b1efdc02-f37b-4422-839f-a1a231eff571" satisfied condition "success or failure"
Jun 22 18:16:21.737: INFO: Trying to get logs from node ip-10-0-20-165 pod downward-api-b1efdc02-f37b-4422-839f-a1a231eff571 container dapi-container: <nil>
STEP: delete the pod
Jun 22 18:16:21.758: INFO: Waiting for pod downward-api-b1efdc02-f37b-4422-839f-a1a231eff571 to disappear
Jun 22 18:16:21.762: INFO: Pod downward-api-b1efdc02-f37b-4422-839f-a1a231eff571 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:16:21.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1560" for this suite.
Jun 22 18:16:27.774: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:16:27.860: INFO: namespace downward-api-1560 deletion completed in 6.09471281s

• [SLOW TEST:8.159 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:16:27.860: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 22 18:16:27.884: INFO: Waiting up to 5m0s for pod "downwardapi-volume-810e8eb9-fe36-427c-ae2b-0723c8d915d4" in namespace "downward-api-5002" to be "success or failure"
Jun 22 18:16:27.888: INFO: Pod "downwardapi-volume-810e8eb9-fe36-427c-ae2b-0723c8d915d4": Phase="Pending", Reason="", readiness=false. Elapsed: 4.590484ms
Jun 22 18:16:29.892: INFO: Pod "downwardapi-volume-810e8eb9-fe36-427c-ae2b-0723c8d915d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007865752s
STEP: Saw pod success
Jun 22 18:16:29.892: INFO: Pod "downwardapi-volume-810e8eb9-fe36-427c-ae2b-0723c8d915d4" satisfied condition "success or failure"
Jun 22 18:16:29.894: INFO: Trying to get logs from node ip-10-0-20-165 pod downwardapi-volume-810e8eb9-fe36-427c-ae2b-0723c8d915d4 container client-container: <nil>
STEP: delete the pod
Jun 22 18:16:29.909: INFO: Waiting for pod downwardapi-volume-810e8eb9-fe36-427c-ae2b-0723c8d915d4 to disappear
Jun 22 18:16:29.912: INFO: Pod downwardapi-volume-810e8eb9-fe36-427c-ae2b-0723c8d915d4 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:16:29.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5002" for this suite.
Jun 22 18:16:35.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:16:36.007: INFO: namespace downward-api-5002 deletion completed in 6.092798862s

• [SLOW TEST:8.147 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:16:36.007: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-151.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-151.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 18:16:46.070: INFO: DNS probes using dns-151/dns-test-783db592-bd6f-459f-9a8b-b90c8df5c94c succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:16:46.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-151" for this suite.
Jun 22 18:16:52.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:16:52.196: INFO: namespace dns-151 deletion completed in 6.113859277s

• [SLOW TEST:16.188 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:16:52.196: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jun 22 18:16:52.224: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-202,SelfLink:/api/v1/namespaces/watch-202/configmaps/e2e-watch-test-configmap-a,UID:75824832-f1f2-4604-abb9-c14a1238e374,ResourceVersion:8079,Generation:0,CreationTimestamp:2019-06-22 18:16:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 22 18:16:52.224: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-202,SelfLink:/api/v1/namespaces/watch-202/configmaps/e2e-watch-test-configmap-a,UID:75824832-f1f2-4604-abb9-c14a1238e374,ResourceVersion:8079,Generation:0,CreationTimestamp:2019-06-22 18:16:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jun 22 18:17:02.231: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-202,SelfLink:/api/v1/namespaces/watch-202/configmaps/e2e-watch-test-configmap-a,UID:75824832-f1f2-4604-abb9-c14a1238e374,ResourceVersion:8096,Generation:0,CreationTimestamp:2019-06-22 18:16:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 22 18:17:02.231: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-202,SelfLink:/api/v1/namespaces/watch-202/configmaps/e2e-watch-test-configmap-a,UID:75824832-f1f2-4604-abb9-c14a1238e374,ResourceVersion:8096,Generation:0,CreationTimestamp:2019-06-22 18:16:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jun 22 18:17:12.237: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-202,SelfLink:/api/v1/namespaces/watch-202/configmaps/e2e-watch-test-configmap-a,UID:75824832-f1f2-4604-abb9-c14a1238e374,ResourceVersion:8113,Generation:0,CreationTimestamp:2019-06-22 18:16:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 22 18:17:12.237: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-202,SelfLink:/api/v1/namespaces/watch-202/configmaps/e2e-watch-test-configmap-a,UID:75824832-f1f2-4604-abb9-c14a1238e374,ResourceVersion:8113,Generation:0,CreationTimestamp:2019-06-22 18:16:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jun 22 18:17:22.242: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-202,SelfLink:/api/v1/namespaces/watch-202/configmaps/e2e-watch-test-configmap-a,UID:75824832-f1f2-4604-abb9-c14a1238e374,ResourceVersion:8130,Generation:0,CreationTimestamp:2019-06-22 18:16:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 22 18:17:22.243: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-202,SelfLink:/api/v1/namespaces/watch-202/configmaps/e2e-watch-test-configmap-a,UID:75824832-f1f2-4604-abb9-c14a1238e374,ResourceVersion:8130,Generation:0,CreationTimestamp:2019-06-22 18:16:52 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jun 22 18:17:32.248: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-202,SelfLink:/api/v1/namespaces/watch-202/configmaps/e2e-watch-test-configmap-b,UID:9283f9c8-e428-432e-a80a-cc508371d846,ResourceVersion:8147,Generation:0,CreationTimestamp:2019-06-22 18:17:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 22 18:17:32.248: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-202,SelfLink:/api/v1/namespaces/watch-202/configmaps/e2e-watch-test-configmap-b,UID:9283f9c8-e428-432e-a80a-cc508371d846,ResourceVersion:8147,Generation:0,CreationTimestamp:2019-06-22 18:17:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jun 22 18:17:42.253: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-202,SelfLink:/api/v1/namespaces/watch-202/configmaps/e2e-watch-test-configmap-b,UID:9283f9c8-e428-432e-a80a-cc508371d846,ResourceVersion:8163,Generation:0,CreationTimestamp:2019-06-22 18:17:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 22 18:17:42.253: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-202,SelfLink:/api/v1/namespaces/watch-202/configmaps/e2e-watch-test-configmap-b,UID:9283f9c8-e428-432e-a80a-cc508371d846,ResourceVersion:8163,Generation:0,CreationTimestamp:2019-06-22 18:17:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:17:52.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-202" for this suite.
Jun 22 18:17:58.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:17:58.348: INFO: namespace watch-202 deletion completed in 6.091638295s

• [SLOW TEST:66.152 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:17:58.350: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 22 18:17:58.382: INFO: Waiting up to 5m0s for pod "downward-api-cd54f89c-b816-4723-8294-2d7831b243a9" in namespace "downward-api-2796" to be "success or failure"
Jun 22 18:17:58.386: INFO: Pod "downward-api-cd54f89c-b816-4723-8294-2d7831b243a9": Phase="Pending", Reason="", readiness=false. Elapsed: 3.332868ms
Jun 22 18:18:00.389: INFO: Pod "downward-api-cd54f89c-b816-4723-8294-2d7831b243a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006645769s
STEP: Saw pod success
Jun 22 18:18:00.389: INFO: Pod "downward-api-cd54f89c-b816-4723-8294-2d7831b243a9" satisfied condition "success or failure"
Jun 22 18:18:00.392: INFO: Trying to get logs from node ip-10-0-20-165 pod downward-api-cd54f89c-b816-4723-8294-2d7831b243a9 container dapi-container: <nil>
STEP: delete the pod
Jun 22 18:18:00.408: INFO: Waiting for pod downward-api-cd54f89c-b816-4723-8294-2d7831b243a9 to disappear
Jun 22 18:18:00.410: INFO: Pod downward-api-cd54f89c-b816-4723-8294-2d7831b243a9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:18:00.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2796" for this suite.
Jun 22 18:18:06.423: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:18:06.501: INFO: namespace downward-api-2796 deletion completed in 6.088553598s

• [SLOW TEST:8.151 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:18:06.502: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Jun 22 18:18:06.539: INFO: Waiting up to 5m0s for pod "var-expansion-aa16579a-b0e8-4c1c-af9e-db3581bd9725" in namespace "var-expansion-5749" to be "success or failure"
Jun 22 18:18:06.541: INFO: Pod "var-expansion-aa16579a-b0e8-4c1c-af9e-db3581bd9725": Phase="Pending", Reason="", readiness=false. Elapsed: 2.158014ms
Jun 22 18:18:08.544: INFO: Pod "var-expansion-aa16579a-b0e8-4c1c-af9e-db3581bd9725": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005013108s
STEP: Saw pod success
Jun 22 18:18:08.544: INFO: Pod "var-expansion-aa16579a-b0e8-4c1c-af9e-db3581bd9725" satisfied condition "success or failure"
Jun 22 18:18:08.546: INFO: Trying to get logs from node ip-10-0-20-165 pod var-expansion-aa16579a-b0e8-4c1c-af9e-db3581bd9725 container dapi-container: <nil>
STEP: delete the pod
Jun 22 18:18:08.559: INFO: Waiting for pod var-expansion-aa16579a-b0e8-4c1c-af9e-db3581bd9725 to disappear
Jun 22 18:18:08.562: INFO: Pod var-expansion-aa16579a-b0e8-4c1c-af9e-db3581bd9725 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:18:08.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5749" for this suite.
Jun 22 18:18:14.573: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:18:14.656: INFO: namespace var-expansion-5749 deletion completed in 6.091089967s

• [SLOW TEST:8.154 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:18:14.656: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-564/configmap-test-e6bfe091-2a28-4e83-97cb-378f922631bc
STEP: Creating a pod to test consume configMaps
Jun 22 18:18:14.736: INFO: Waiting up to 5m0s for pod "pod-configmaps-35a5c552-c545-4b59-ac9b-5f153c554cd8" in namespace "configmap-564" to be "success or failure"
Jun 22 18:18:14.739: INFO: Pod "pod-configmaps-35a5c552-c545-4b59-ac9b-5f153c554cd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.763744ms
Jun 22 18:18:16.742: INFO: Pod "pod-configmaps-35a5c552-c545-4b59-ac9b-5f153c554cd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005890065s
STEP: Saw pod success
Jun 22 18:18:16.742: INFO: Pod "pod-configmaps-35a5c552-c545-4b59-ac9b-5f153c554cd8" satisfied condition "success or failure"
Jun 22 18:18:16.744: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-configmaps-35a5c552-c545-4b59-ac9b-5f153c554cd8 container env-test: <nil>
STEP: delete the pod
Jun 22 18:18:16.759: INFO: Waiting for pod pod-configmaps-35a5c552-c545-4b59-ac9b-5f153c554cd8 to disappear
Jun 22 18:18:16.762: INFO: Pod pod-configmaps-35a5c552-c545-4b59-ac9b-5f153c554cd8 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:18:16.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-564" for this suite.
Jun 22 18:18:22.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:18:22.856: INFO: namespace configmap-564 deletion completed in 6.091200242s

• [SLOW TEST:8.200 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:18:22.857: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-d4f5b3ae-8cfd-410d-a801-8a2f60b24a3b
STEP: Creating a pod to test consume secrets
Jun 22 18:18:22.884: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-cff7867f-f1d9-4361-8e86-838e678973c8" in namespace "projected-9119" to be "success or failure"
Jun 22 18:18:22.886: INFO: Pod "pod-projected-secrets-cff7867f-f1d9-4361-8e86-838e678973c8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.295677ms
Jun 22 18:18:24.889: INFO: Pod "pod-projected-secrets-cff7867f-f1d9-4361-8e86-838e678973c8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005243964s
STEP: Saw pod success
Jun 22 18:18:24.889: INFO: Pod "pod-projected-secrets-cff7867f-f1d9-4361-8e86-838e678973c8" satisfied condition "success or failure"
Jun 22 18:18:24.892: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-projected-secrets-cff7867f-f1d9-4361-8e86-838e678973c8 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 18:18:24.905: INFO: Waiting for pod pod-projected-secrets-cff7867f-f1d9-4361-8e86-838e678973c8 to disappear
Jun 22 18:18:24.908: INFO: Pod pod-projected-secrets-cff7867f-f1d9-4361-8e86-838e678973c8 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:18:24.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9119" for this suite.
Jun 22 18:18:30.919: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:18:31.007: INFO: namespace projected-9119 deletion completed in 6.096298975s

• [SLOW TEST:8.150 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:18:31.008: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 22 18:18:31.042: INFO: Waiting up to 5m0s for pod "pod-2283f5f1-129c-48b9-a7ef-36e1f62bfbc6" in namespace "emptydir-76" to be "success or failure"
Jun 22 18:18:31.045: INFO: Pod "pod-2283f5f1-129c-48b9-a7ef-36e1f62bfbc6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.149509ms
Jun 22 18:18:33.048: INFO: Pod "pod-2283f5f1-129c-48b9-a7ef-36e1f62bfbc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006694204s
STEP: Saw pod success
Jun 22 18:18:33.049: INFO: Pod "pod-2283f5f1-129c-48b9-a7ef-36e1f62bfbc6" satisfied condition "success or failure"
Jun 22 18:18:33.051: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-2283f5f1-129c-48b9-a7ef-36e1f62bfbc6 container test-container: <nil>
STEP: delete the pod
Jun 22 18:18:33.064: INFO: Waiting for pod pod-2283f5f1-129c-48b9-a7ef-36e1f62bfbc6 to disappear
Jun 22 18:18:33.067: INFO: Pod pod-2283f5f1-129c-48b9-a7ef-36e1f62bfbc6 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:18:33.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-76" for this suite.
Jun 22 18:18:39.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:18:39.158: INFO: namespace emptydir-76 deletion completed in 6.088167232s

• [SLOW TEST:8.150 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:18:39.158: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 22 18:18:39.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-6842'
Jun 22 18:18:39.272: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 22 18:18:39.272: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Jun 22 18:18:39.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 delete deployment e2e-test-nginx-deployment --namespace=kubectl-6842'
Jun 22 18:18:39.354: INFO: stderr: ""
Jun 22 18:18:39.354: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:18:39.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6842" for this suite.
Jun 22 18:18:45.367: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:18:45.449: INFO: namespace kubectl-6842 deletion completed in 6.091282208s

• [SLOW TEST:6.291 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:18:45.450: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 22 18:18:47.487: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:18:47.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-933" for this suite.
Jun 22 18:18:53.515: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:18:53.597: INFO: namespace container-runtime-933 deletion completed in 6.0903035s

• [SLOW TEST:8.147 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:18:53.598: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6252.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6252.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6252.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6252.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6252.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6252.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6252.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6252.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6252.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6252.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6252.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6252.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6252.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 74.0.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.0.74_udp@PTR;check="$$(dig +tcp +noall +answer +search 74.0.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.0.74_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6252.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6252.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6252.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6252.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6252.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6252.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6252.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6252.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6252.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6252.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6252.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6252.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6252.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 74.0.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.0.74_udp@PTR;check="$$(dig +tcp +noall +answer +search 74.0.3.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.3.0.74_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 18:18:57.651: INFO: Unable to read wheezy_udp@dns-test-service.dns-6252.svc.cluster.local from pod dns-6252/dns-test-fd1c1e64-30b7-4ae5-88a7-abdd3ca2ef57: the server could not find the requested resource (get pods dns-test-fd1c1e64-30b7-4ae5-88a7-abdd3ca2ef57)
Jun 22 18:18:57.654: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6252.svc.cluster.local from pod dns-6252/dns-test-fd1c1e64-30b7-4ae5-88a7-abdd3ca2ef57: the server could not find the requested resource (get pods dns-test-fd1c1e64-30b7-4ae5-88a7-abdd3ca2ef57)
Jun 22 18:18:57.658: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6252.svc.cluster.local from pod dns-6252/dns-test-fd1c1e64-30b7-4ae5-88a7-abdd3ca2ef57: the server could not find the requested resource (get pods dns-test-fd1c1e64-30b7-4ae5-88a7-abdd3ca2ef57)
Jun 22 18:18:57.661: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6252.svc.cluster.local from pod dns-6252/dns-test-fd1c1e64-30b7-4ae5-88a7-abdd3ca2ef57: the server could not find the requested resource (get pods dns-test-fd1c1e64-30b7-4ae5-88a7-abdd3ca2ef57)
Jun 22 18:18:57.693: INFO: Unable to read jessie_udp@dns-test-service.dns-6252.svc.cluster.local from pod dns-6252/dns-test-fd1c1e64-30b7-4ae5-88a7-abdd3ca2ef57: the server could not find the requested resource (get pods dns-test-fd1c1e64-30b7-4ae5-88a7-abdd3ca2ef57)
Jun 22 18:18:57.697: INFO: Unable to read jessie_tcp@dns-test-service.dns-6252.svc.cluster.local from pod dns-6252/dns-test-fd1c1e64-30b7-4ae5-88a7-abdd3ca2ef57: the server could not find the requested resource (get pods dns-test-fd1c1e64-30b7-4ae5-88a7-abdd3ca2ef57)
Jun 22 18:18:57.700: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6252.svc.cluster.local from pod dns-6252/dns-test-fd1c1e64-30b7-4ae5-88a7-abdd3ca2ef57: the server could not find the requested resource (get pods dns-test-fd1c1e64-30b7-4ae5-88a7-abdd3ca2ef57)
Jun 22 18:18:57.703: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6252.svc.cluster.local from pod dns-6252/dns-test-fd1c1e64-30b7-4ae5-88a7-abdd3ca2ef57: the server could not find the requested resource (get pods dns-test-fd1c1e64-30b7-4ae5-88a7-abdd3ca2ef57)
Jun 22 18:18:57.721: INFO: Lookups using dns-6252/dns-test-fd1c1e64-30b7-4ae5-88a7-abdd3ca2ef57 failed for: [wheezy_udp@dns-test-service.dns-6252.svc.cluster.local wheezy_tcp@dns-test-service.dns-6252.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6252.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6252.svc.cluster.local jessie_udp@dns-test-service.dns-6252.svc.cluster.local jessie_tcp@dns-test-service.dns-6252.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6252.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6252.svc.cluster.local]

Jun 22 18:19:02.787: INFO: DNS probes using dns-6252/dns-test-fd1c1e64-30b7-4ae5-88a7-abdd3ca2ef57 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:19:02.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6252" for this suite.
Jun 22 18:19:08.861: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:19:08.942: INFO: namespace dns-6252 deletion completed in 6.093193235s

• [SLOW TEST:15.344 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:19:08.943: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Jun 22 18:19:48.992: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0622 18:19:48.992033      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:19:48.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1444" for this suite.
Jun 22 18:19:55.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:19:55.098: INFO: namespace gc-1444 deletion completed in 6.103685874s

• [SLOW TEST:46.155 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:19:55.099: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-3db7d0f3-3e71-4ff3-9011-736cee945d97
STEP: Creating a pod to test consume configMaps
Jun 22 18:19:55.128: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2897de82-0b81-4891-8012-972ae2b02923" in namespace "projected-4106" to be "success or failure"
Jun 22 18:19:55.135: INFO: Pod "pod-projected-configmaps-2897de82-0b81-4891-8012-972ae2b02923": Phase="Pending", Reason="", readiness=false. Elapsed: 6.46819ms
Jun 22 18:19:57.138: INFO: Pod "pod-projected-configmaps-2897de82-0b81-4891-8012-972ae2b02923": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009704739s
Jun 22 18:19:59.141: INFO: Pod "pod-projected-configmaps-2897de82-0b81-4891-8012-972ae2b02923": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012968716s
STEP: Saw pod success
Jun 22 18:19:59.141: INFO: Pod "pod-projected-configmaps-2897de82-0b81-4891-8012-972ae2b02923" satisfied condition "success or failure"
Jun 22 18:19:59.144: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-projected-configmaps-2897de82-0b81-4891-8012-972ae2b02923 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 18:19:59.170: INFO: Waiting for pod pod-projected-configmaps-2897de82-0b81-4891-8012-972ae2b02923 to disappear
Jun 22 18:19:59.172: INFO: Pod pod-projected-configmaps-2897de82-0b81-4891-8012-972ae2b02923 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:19:59.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4106" for this suite.
Jun 22 18:20:05.189: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:20:05.284: INFO: namespace projected-4106 deletion completed in 6.108477955s

• [SLOW TEST:10.185 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:20:05.285: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-ksc6
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 18:20:05.315: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-ksc6" in namespace "subpath-4468" to be "success or failure"
Jun 22 18:20:05.319: INFO: Pod "pod-subpath-test-configmap-ksc6": Phase="Pending", Reason="", readiness=false. Elapsed: 3.79434ms
Jun 22 18:20:07.322: INFO: Pod "pod-subpath-test-configmap-ksc6": Phase="Running", Reason="", readiness=true. Elapsed: 2.006952052s
Jun 22 18:20:09.325: INFO: Pod "pod-subpath-test-configmap-ksc6": Phase="Running", Reason="", readiness=true. Elapsed: 4.009901989s
Jun 22 18:20:11.328: INFO: Pod "pod-subpath-test-configmap-ksc6": Phase="Running", Reason="", readiness=true. Elapsed: 6.012785414s
Jun 22 18:20:13.331: INFO: Pod "pod-subpath-test-configmap-ksc6": Phase="Running", Reason="", readiness=true. Elapsed: 8.015923372s
Jun 22 18:20:15.334: INFO: Pod "pod-subpath-test-configmap-ksc6": Phase="Running", Reason="", readiness=true. Elapsed: 10.019215635s
Jun 22 18:20:17.338: INFO: Pod "pod-subpath-test-configmap-ksc6": Phase="Running", Reason="", readiness=true. Elapsed: 12.022620796s
Jun 22 18:20:19.341: INFO: Pod "pod-subpath-test-configmap-ksc6": Phase="Running", Reason="", readiness=true. Elapsed: 14.02580388s
Jun 22 18:20:21.344: INFO: Pod "pod-subpath-test-configmap-ksc6": Phase="Running", Reason="", readiness=true. Elapsed: 16.029070817s
Jun 22 18:20:23.347: INFO: Pod "pod-subpath-test-configmap-ksc6": Phase="Running", Reason="", readiness=true. Elapsed: 18.032353824s
Jun 22 18:20:25.351: INFO: Pod "pod-subpath-test-configmap-ksc6": Phase="Running", Reason="", readiness=true. Elapsed: 20.035444941s
Jun 22 18:20:27.355: INFO: Pod "pod-subpath-test-configmap-ksc6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.040112289s
STEP: Saw pod success
Jun 22 18:20:27.355: INFO: Pod "pod-subpath-test-configmap-ksc6" satisfied condition "success or failure"
Jun 22 18:20:27.359: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-subpath-test-configmap-ksc6 container test-container-subpath-configmap-ksc6: <nil>
STEP: delete the pod
Jun 22 18:20:27.379: INFO: Waiting for pod pod-subpath-test-configmap-ksc6 to disappear
Jun 22 18:20:27.381: INFO: Pod pod-subpath-test-configmap-ksc6 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-ksc6
Jun 22 18:20:27.381: INFO: Deleting pod "pod-subpath-test-configmap-ksc6" in namespace "subpath-4468"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:20:27.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4468" for this suite.
Jun 22 18:20:33.396: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:20:33.476: INFO: namespace subpath-4468 deletion completed in 6.088412208s

• [SLOW TEST:28.191 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:20:33.476: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Jun 22 18:20:33.507: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-1869" to be "success or failure"
Jun 22 18:20:33.510: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 3.108124ms
Jun 22 18:20:35.513: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005825312s
STEP: Saw pod success
Jun 22 18:20:35.513: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jun 22 18:20:35.516: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jun 22 18:20:35.531: INFO: Waiting for pod pod-host-path-test to disappear
Jun 22 18:20:35.533: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:20:35.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-1869" for this suite.
Jun 22 18:20:41.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:20:41.628: INFO: namespace hostpath-1869 deletion completed in 6.092109718s

• [SLOW TEST:8.152 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:20:41.628: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 22 18:20:43.676: INFO: Waiting up to 5m0s for pod "client-envvars-82873fab-4d69-4c09-98c5-f748272a0ba8" in namespace "pods-6223" to be "success or failure"
Jun 22 18:20:43.682: INFO: Pod "client-envvars-82873fab-4d69-4c09-98c5-f748272a0ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.831965ms
Jun 22 18:20:45.685: INFO: Pod "client-envvars-82873fab-4d69-4c09-98c5-f748272a0ba8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009283378s
Jun 22 18:20:47.688: INFO: Pod "client-envvars-82873fab-4d69-4c09-98c5-f748272a0ba8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012239526s
STEP: Saw pod success
Jun 22 18:20:47.688: INFO: Pod "client-envvars-82873fab-4d69-4c09-98c5-f748272a0ba8" satisfied condition "success or failure"
Jun 22 18:20:47.691: INFO: Trying to get logs from node ip-10-0-42-243 pod client-envvars-82873fab-4d69-4c09-98c5-f748272a0ba8 container env3cont: <nil>
STEP: delete the pod
Jun 22 18:20:47.715: INFO: Waiting for pod client-envvars-82873fab-4d69-4c09-98c5-f748272a0ba8 to disappear
Jun 22 18:20:47.717: INFO: Pod client-envvars-82873fab-4d69-4c09-98c5-f748272a0ba8 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:20:47.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6223" for this suite.
Jun 22 18:21:37.732: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:21:37.811: INFO: namespace pods-6223 deletion completed in 50.090701256s

• [SLOW TEST:56.182 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:21:37.811: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 22 18:21:37.837: INFO: Waiting up to 5m0s for pod "pod-bd4b2e57-a635-43a2-aa8a-8c14dd5f34e0" in namespace "emptydir-6899" to be "success or failure"
Jun 22 18:21:37.841: INFO: Pod "pod-bd4b2e57-a635-43a2-aa8a-8c14dd5f34e0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.030792ms
Jun 22 18:21:39.845: INFO: Pod "pod-bd4b2e57-a635-43a2-aa8a-8c14dd5f34e0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00729509s
STEP: Saw pod success
Jun 22 18:21:39.845: INFO: Pod "pod-bd4b2e57-a635-43a2-aa8a-8c14dd5f34e0" satisfied condition "success or failure"
Jun 22 18:21:39.847: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-bd4b2e57-a635-43a2-aa8a-8c14dd5f34e0 container test-container: <nil>
STEP: delete the pod
Jun 22 18:21:39.861: INFO: Waiting for pod pod-bd4b2e57-a635-43a2-aa8a-8c14dd5f34e0 to disappear
Jun 22 18:21:39.864: INFO: Pod pod-bd4b2e57-a635-43a2-aa8a-8c14dd5f34e0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:21:39.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6899" for this suite.
Jun 22 18:21:45.886: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:21:45.969: INFO: namespace emptydir-6899 deletion completed in 6.095782402s

• [SLOW TEST:8.158 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:21:45.970: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jun 22 18:21:45.994: INFO: Waiting up to 5m0s for pod "pod-12f71419-8f8e-4a38-bee6-acc82ab03014" in namespace "emptydir-4270" to be "success or failure"
Jun 22 18:21:45.997: INFO: Pod "pod-12f71419-8f8e-4a38-bee6-acc82ab03014": Phase="Pending", Reason="", readiness=false. Elapsed: 3.267458ms
Jun 22 18:21:48.000: INFO: Pod "pod-12f71419-8f8e-4a38-bee6-acc82ab03014": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005857457s
STEP: Saw pod success
Jun 22 18:21:48.000: INFO: Pod "pod-12f71419-8f8e-4a38-bee6-acc82ab03014" satisfied condition "success or failure"
Jun 22 18:21:48.012: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-12f71419-8f8e-4a38-bee6-acc82ab03014 container test-container: <nil>
STEP: delete the pod
Jun 22 18:21:48.053: INFO: Waiting for pod pod-12f71419-8f8e-4a38-bee6-acc82ab03014 to disappear
Jun 22 18:21:48.057: INFO: Pod pod-12f71419-8f8e-4a38-bee6-acc82ab03014 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:21:48.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4270" for this suite.
Jun 22 18:21:54.077: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:21:54.168: INFO: namespace emptydir-4270 deletion completed in 6.105536506s

• [SLOW TEST:8.198 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:21:54.168: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-7906
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 22 18:21:54.188: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 22 18:22:14.269: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.104.95:8080/dial?request=hostName&protocol=http&host=10.2.145.16&port=8080&tries=1'] Namespace:pod-network-test-7906 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 18:22:14.269: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
Jun 22 18:22:14.363: INFO: Waiting for endpoints: map[]
Jun 22 18:22:14.365: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.104.95:8080/dial?request=hostName&protocol=http&host=10.2.104.94&port=8080&tries=1'] Namespace:pod-network-test-7906 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 18:22:14.366: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
Jun 22 18:22:14.468: INFO: Waiting for endpoints: map[]
Jun 22 18:22:14.471: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.104.95:8080/dial?request=hostName&protocol=http&host=10.2.133.35&port=8080&tries=1'] Namespace:pod-network-test-7906 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 18:22:14.471: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
Jun 22 18:22:14.570: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:22:14.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7906" for this suite.
Jun 22 18:22:36.583: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:22:36.665: INFO: namespace pod-network-test-7906 deletion completed in 22.090551181s

• [SLOW TEST:42.497 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:22:36.665: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Jun 22 18:22:46.713: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0622 18:22:46.713222      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:22:46.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5002" for this suite.
Jun 22 18:22:52.725: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:22:52.822: INFO: namespace gc-5002 deletion completed in 6.10678435s

• [SLOW TEST:16.158 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:22:52.823: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Jun 22 18:22:52.851: INFO: Waiting up to 5m0s for pod "var-expansion-37f0d238-b4c5-4a81-ad3b-859838c63396" in namespace "var-expansion-5408" to be "success or failure"
Jun 22 18:22:52.854: INFO: Pod "var-expansion-37f0d238-b4c5-4a81-ad3b-859838c63396": Phase="Pending", Reason="", readiness=false. Elapsed: 3.188903ms
Jun 22 18:22:54.858: INFO: Pod "var-expansion-37f0d238-b4c5-4a81-ad3b-859838c63396": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006511468s
STEP: Saw pod success
Jun 22 18:22:54.858: INFO: Pod "var-expansion-37f0d238-b4c5-4a81-ad3b-859838c63396" satisfied condition "success or failure"
Jun 22 18:22:54.860: INFO: Trying to get logs from node ip-10-0-20-165 pod var-expansion-37f0d238-b4c5-4a81-ad3b-859838c63396 container dapi-container: <nil>
STEP: delete the pod
Jun 22 18:22:54.877: INFO: Waiting for pod var-expansion-37f0d238-b4c5-4a81-ad3b-859838c63396 to disappear
Jun 22 18:22:54.879: INFO: Pod var-expansion-37f0d238-b4c5-4a81-ad3b-859838c63396 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:22:54.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5408" for this suite.
Jun 22 18:23:00.893: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:23:00.986: INFO: namespace var-expansion-5408 deletion completed in 6.103536267s

• [SLOW TEST:8.163 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:23:00.986: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:23:03.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1839" for this suite.
Jun 22 18:23:09.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:23:09.150: INFO: namespace emptydir-wrapper-1839 deletion completed in 6.094720987s

• [SLOW TEST:8.164 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:23:09.150: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-27ba8819-5966-4a37-b61e-3a9af21c6672
STEP: Creating a pod to test consume secrets
Jun 22 18:23:09.182: INFO: Waiting up to 5m0s for pod "pod-secrets-7cfc1235-6ec8-43a2-9c61-9f7406488112" in namespace "secrets-3097" to be "success or failure"
Jun 22 18:23:09.186: INFO: Pod "pod-secrets-7cfc1235-6ec8-43a2-9c61-9f7406488112": Phase="Pending", Reason="", readiness=false. Elapsed: 4.209974ms
Jun 22 18:23:11.190: INFO: Pod "pod-secrets-7cfc1235-6ec8-43a2-9c61-9f7406488112": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007564125s
STEP: Saw pod success
Jun 22 18:23:11.190: INFO: Pod "pod-secrets-7cfc1235-6ec8-43a2-9c61-9f7406488112" satisfied condition "success or failure"
Jun 22 18:23:11.192: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-secrets-7cfc1235-6ec8-43a2-9c61-9f7406488112 container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 18:23:11.207: INFO: Waiting for pod pod-secrets-7cfc1235-6ec8-43a2-9c61-9f7406488112 to disappear
Jun 22 18:23:11.210: INFO: Pod pod-secrets-7cfc1235-6ec8-43a2-9c61-9f7406488112 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:23:11.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3097" for this suite.
Jun 22 18:23:17.223: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:23:17.314: INFO: namespace secrets-3097 deletion completed in 6.10196999s

• [SLOW TEST:8.164 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:23:17.315: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jun 22 18:23:17.360: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 18:23:17.366: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 18:23:17.369: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-15-125 before test
Jun 22 18:23:17.376: INFO: calico-node-n5n7z from kube-system started at 2019-06-22 17:46:06 +0000 UTC (1 container statuses recorded)
Jun 22 18:23:17.376: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 18:23:17.376: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-22 17:50:43 +0000 UTC (1 container statuses recorded)
Jun 22 18:23:17.376: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 18:23:17.376: INFO: sonobuoy-systemd-logs-daemon-set-2e6d603853cb48ce-6dsjh from heptio-sonobuoy started at 2019-06-22 17:50:48 +0000 UTC (2 container statuses recorded)
Jun 22 18:23:17.376: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 18:23:17.376: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 18:23:17.376: INFO: kube-proxy-89t9g from kube-system started at 2019-06-22 17:46:06 +0000 UTC (1 container statuses recorded)
Jun 22 18:23:17.376: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 18:23:17.376: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-20-165 before test
Jun 22 18:23:17.380: INFO: kube-proxy-ww9qf from kube-system started at 2019-06-22 17:46:06 +0000 UTC (1 container statuses recorded)
Jun 22 18:23:17.381: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 18:23:17.381: INFO: calico-node-zd2zz from kube-system started at 2019-06-22 17:46:06 +0000 UTC (1 container statuses recorded)
Jun 22 18:23:17.381: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 18:23:17.381: INFO: sonobuoy-systemd-logs-daemon-set-2e6d603853cb48ce-rxlqb from heptio-sonobuoy started at 2019-06-22 17:50:48 +0000 UTC (2 container statuses recorded)
Jun 22 18:23:17.381: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 18:23:17.381: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 22 18:23:17.381: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-42-243 before test
Jun 22 18:23:17.387: INFO: kube-proxy-gkjvx from kube-system started at 2019-06-22 17:46:06 +0000 UTC (1 container statuses recorded)
Jun 22 18:23:17.387: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 18:23:17.387: INFO: calico-node-688zt from kube-system started at 2019-06-22 17:46:06 +0000 UTC (1 container statuses recorded)
Jun 22 18:23:17.387: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 18:23:17.387: INFO: sonobuoy-e2e-job-5c726901000d4f40 from heptio-sonobuoy started at 2019-06-22 17:50:48 +0000 UTC (2 container statuses recorded)
Jun 22 18:23:17.387: INFO: 	Container e2e ready: true, restart count 0
Jun 22 18:23:17.387: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 18:23:17.387: INFO: sonobuoy-systemd-logs-daemon-set-2e6d603853cb48ce-7pf2f from heptio-sonobuoy started at 2019-06-22 17:50:48 +0000 UTC (2 container statuses recorded)
Jun 22 18:23:17.387: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 18:23:17.387: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-43cd25b2-ba5e-4e1a-9aa9-a0a8c4acc513 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-43cd25b2-ba5e-4e1a-9aa9-a0a8c4acc513 off the node ip-10-0-20-165
STEP: verifying the node doesn't have the label kubernetes.io/e2e-43cd25b2-ba5e-4e1a-9aa9-a0a8c4acc513
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:23:21.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2041" for this suite.
Jun 22 18:23:29.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:23:29.541: INFO: namespace sched-pred-2041 deletion completed in 8.087331369s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:12.227 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:23:29.541: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 22 18:23:31.580: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:23:31.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2445" for this suite.
Jun 22 18:23:37.604: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:23:37.687: INFO: namespace container-runtime-2445 deletion completed in 6.091419115s

• [SLOW TEST:8.145 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:23:37.687: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jun 22 18:23:37.742: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-2354,SelfLink:/api/v1/namespaces/watch-2354/configmaps/e2e-watch-test-resource-version,UID:b61fe0c9-7fce-4573-bece-b5b790a026b3,ResourceVersion:9833,Generation:0,CreationTimestamp:2019-06-22 18:23:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 22 18:23:37.742: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-2354,SelfLink:/api/v1/namespaces/watch-2354/configmaps/e2e-watch-test-resource-version,UID:b61fe0c9-7fce-4573-bece-b5b790a026b3,ResourceVersion:9834,Generation:0,CreationTimestamp:2019-06-22 18:23:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:23:37.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2354" for this suite.
Jun 22 18:23:43.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:23:43.836: INFO: namespace watch-2354 deletion completed in 6.091694586s

• [SLOW TEST:6.150 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:23:43.837: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jun 22 18:23:43.865: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4687,SelfLink:/api/v1/namespaces/watch-4687/configmaps/e2e-watch-test-watch-closed,UID:a2463b95-70e6-4166-a487-111871c4ea8c,ResourceVersion:9851,Generation:0,CreationTimestamp:2019-06-22 18:23:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 22 18:23:43.865: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4687,SelfLink:/api/v1/namespaces/watch-4687/configmaps/e2e-watch-test-watch-closed,UID:a2463b95-70e6-4166-a487-111871c4ea8c,ResourceVersion:9852,Generation:0,CreationTimestamp:2019-06-22 18:23:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jun 22 18:23:43.875: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4687,SelfLink:/api/v1/namespaces/watch-4687/configmaps/e2e-watch-test-watch-closed,UID:a2463b95-70e6-4166-a487-111871c4ea8c,ResourceVersion:9853,Generation:0,CreationTimestamp:2019-06-22 18:23:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 22 18:23:43.875: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-4687,SelfLink:/api/v1/namespaces/watch-4687/configmaps/e2e-watch-test-watch-closed,UID:a2463b95-70e6-4166-a487-111871c4ea8c,ResourceVersion:9854,Generation:0,CreationTimestamp:2019-06-22 18:23:43 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:23:43.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-4687" for this suite.
Jun 22 18:23:49.888: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:23:49.969: INFO: namespace watch-4687 deletion completed in 6.090835847s

• [SLOW TEST:6.132 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:23:49.969: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-e543ed7f-853b-4928-86ef-075523df0e9f
STEP: Creating configMap with name cm-test-opt-upd-7fdbc0e5-a55b-4b2a-8d42-dfc3827cf9e7
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-e543ed7f-853b-4928-86ef-075523df0e9f
STEP: Updating configmap cm-test-opt-upd-7fdbc0e5-a55b-4b2a-8d42-dfc3827cf9e7
STEP: Creating configMap with name cm-test-opt-create-932d6151-ab4d-4b02-8bae-f3e74d900ce7
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:23:54.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3839" for this suite.
Jun 22 18:24:16.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:24:16.192: INFO: namespace projected-3839 deletion completed in 22.10403133s

• [SLOW TEST:26.223 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:24:16.193: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jun 22 18:24:16.266: INFO: PodSpec: initContainers in spec.initContainers
Jun 22 18:25:01.897: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-110dcc13-8f2c-489c-9ca8-4745054fe36f", GenerateName:"", Namespace:"init-container-7177", SelfLink:"/api/v1/namespaces/init-container-7177/pods/pod-init-110dcc13-8f2c-489c-9ca8-4745054fe36f", UID:"1906820b-af4d-4bfa-87a1-63f75163dc15", ResourceVersion:"10062", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63696824656, loc:(*time.Location)(0x80bb5c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"266534766"}, Annotations:map[string]string{"cni.projectcalico.org/podIP":"10.2.104.104/32"}, OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-4vxgt", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc002b26080), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4vxgt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4vxgt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-4vxgt", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc002a850d8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ip-10-0-20-165", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0025e5aa0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002a85160)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc002a85180)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc002a85188), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc002a8518c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696824656, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696824656, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696824656, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696824656, loc:(*time.Location)(0x80bb5c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.20.165", PodIP:"10.2.104.104", StartTime:(*v1.Time)(0xc00273d880), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c34150)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000c34380)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://27e507e46c5a8c4039c3c0a683a52fd57194e9b9d9facf12af71e57db8b84fe0"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00273d8c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00273d8a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:25:01.897: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7177" for this suite.
Jun 22 18:25:23.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:25:23.996: INFO: namespace init-container-7177 deletion completed in 22.095100387s

• [SLOW TEST:67.803 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:25:23.997: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-90a5fda8-35b3-4491-83ec-28f5556a00ee
STEP: Creating a pod to test consume configMaps
Jun 22 18:25:24.027: INFO: Waiting up to 5m0s for pod "pod-configmaps-6ab09128-c23f-44db-843b-22ac80a9b466" in namespace "configmap-5054" to be "success or failure"
Jun 22 18:25:24.032: INFO: Pod "pod-configmaps-6ab09128-c23f-44db-843b-22ac80a9b466": Phase="Pending", Reason="", readiness=false. Elapsed: 4.649195ms
Jun 22 18:25:26.035: INFO: Pod "pod-configmaps-6ab09128-c23f-44db-843b-22ac80a9b466": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007957346s
STEP: Saw pod success
Jun 22 18:25:26.035: INFO: Pod "pod-configmaps-6ab09128-c23f-44db-843b-22ac80a9b466" satisfied condition "success or failure"
Jun 22 18:25:26.038: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-configmaps-6ab09128-c23f-44db-843b-22ac80a9b466 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 18:25:26.059: INFO: Waiting for pod pod-configmaps-6ab09128-c23f-44db-843b-22ac80a9b466 to disappear
Jun 22 18:25:26.062: INFO: Pod pod-configmaps-6ab09128-c23f-44db-843b-22ac80a9b466 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:25:26.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5054" for this suite.
Jun 22 18:25:32.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:25:32.182: INFO: namespace configmap-5054 deletion completed in 6.117396614s

• [SLOW TEST:8.185 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:25:32.182: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 22 18:25:32.207: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a7566fd3-3b11-4208-8645-5d4c3710e496" in namespace "projected-1739" to be "success or failure"
Jun 22 18:25:32.211: INFO: Pod "downwardapi-volume-a7566fd3-3b11-4208-8645-5d4c3710e496": Phase="Pending", Reason="", readiness=false. Elapsed: 3.08153ms
Jun 22 18:25:34.214: INFO: Pod "downwardapi-volume-a7566fd3-3b11-4208-8645-5d4c3710e496": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006837859s
STEP: Saw pod success
Jun 22 18:25:34.214: INFO: Pod "downwardapi-volume-a7566fd3-3b11-4208-8645-5d4c3710e496" satisfied condition "success or failure"
Jun 22 18:25:34.218: INFO: Trying to get logs from node ip-10-0-20-165 pod downwardapi-volume-a7566fd3-3b11-4208-8645-5d4c3710e496 container client-container: <nil>
STEP: delete the pod
Jun 22 18:25:34.273: INFO: Waiting for pod downwardapi-volume-a7566fd3-3b11-4208-8645-5d4c3710e496 to disappear
Jun 22 18:25:34.276: INFO: Pod downwardapi-volume-a7566fd3-3b11-4208-8645-5d4c3710e496 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:25:34.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1739" for this suite.
Jun 22 18:25:40.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:25:40.376: INFO: namespace projected-1739 deletion completed in 6.096946326s

• [SLOW TEST:8.194 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:25:40.377: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 22 18:25:40.408: INFO: Waiting up to 5m0s for pod "downwardapi-volume-04c3c40e-c493-496a-bced-0170f3597c73" in namespace "downward-api-9702" to be "success or failure"
Jun 22 18:25:40.413: INFO: Pod "downwardapi-volume-04c3c40e-c493-496a-bced-0170f3597c73": Phase="Pending", Reason="", readiness=false. Elapsed: 5.443437ms
Jun 22 18:25:42.416: INFO: Pod "downwardapi-volume-04c3c40e-c493-496a-bced-0170f3597c73": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008342984s
STEP: Saw pod success
Jun 22 18:25:42.416: INFO: Pod "downwardapi-volume-04c3c40e-c493-496a-bced-0170f3597c73" satisfied condition "success or failure"
Jun 22 18:25:42.418: INFO: Trying to get logs from node ip-10-0-20-165 pod downwardapi-volume-04c3c40e-c493-496a-bced-0170f3597c73 container client-container: <nil>
STEP: delete the pod
Jun 22 18:25:42.432: INFO: Waiting for pod downwardapi-volume-04c3c40e-c493-496a-bced-0170f3597c73 to disappear
Jun 22 18:25:42.434: INFO: Pod downwardapi-volume-04c3c40e-c493-496a-bced-0170f3597c73 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:25:42.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9702" for this suite.
Jun 22 18:25:48.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:25:48.545: INFO: namespace downward-api-9702 deletion completed in 6.108337031s

• [SLOW TEST:8.168 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:25:48.546: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Jun 22 18:25:48.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 create -f - --namespace=kubectl-1366'
Jun 22 18:25:48.902: INFO: stderr: ""
Jun 22 18:25:48.902: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 18:25:48.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1366'
Jun 22 18:25:48.966: INFO: stderr: ""
Jun 22 18:25:48.966: INFO: stdout: "update-demo-nautilus-kw2qk update-demo-nautilus-qx2fx "
Jun 22 18:25:48.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-kw2qk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1366'
Jun 22 18:25:49.032: INFO: stderr: ""
Jun 22 18:25:49.032: INFO: stdout: ""
Jun 22 18:25:49.032: INFO: update-demo-nautilus-kw2qk is created but not running
Jun 22 18:25:54.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1366'
Jun 22 18:25:54.100: INFO: stderr: ""
Jun 22 18:25:54.100: INFO: stdout: "update-demo-nautilus-kw2qk update-demo-nautilus-qx2fx "
Jun 22 18:25:54.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-kw2qk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1366'
Jun 22 18:25:54.167: INFO: stderr: ""
Jun 22 18:25:54.167: INFO: stdout: "true"
Jun 22 18:25:54.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-kw2qk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1366'
Jun 22 18:25:54.241: INFO: stderr: ""
Jun 22 18:25:54.241: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 18:25:54.241: INFO: validating pod update-demo-nautilus-kw2qk
Jun 22 18:25:54.244: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 18:25:54.244: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 18:25:54.244: INFO: update-demo-nautilus-kw2qk is verified up and running
Jun 22 18:25:54.244: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-qx2fx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1366'
Jun 22 18:25:54.312: INFO: stderr: ""
Jun 22 18:25:54.313: INFO: stdout: "true"
Jun 22 18:25:54.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-qx2fx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1366'
Jun 22 18:25:54.383: INFO: stderr: ""
Jun 22 18:25:54.383: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 18:25:54.383: INFO: validating pod update-demo-nautilus-qx2fx
Jun 22 18:25:54.388: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 18:25:54.388: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 18:25:54.388: INFO: update-demo-nautilus-qx2fx is verified up and running
STEP: rolling-update to new replication controller
Jun 22 18:25:54.389: INFO: scanned /root for discovery docs: <nil>
Jun 22 18:25:54.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-1366'
Jun 22 18:26:16.734: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 22 18:26:16.734: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 18:26:16.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-1366'
Jun 22 18:26:16.809: INFO: stderr: ""
Jun 22 18:26:16.809: INFO: stdout: "update-demo-kitten-f68mt update-demo-kitten-hz876 "
Jun 22 18:26:16.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-kitten-f68mt -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1366'
Jun 22 18:26:16.880: INFO: stderr: ""
Jun 22 18:26:16.880: INFO: stdout: "true"
Jun 22 18:26:16.880: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-kitten-f68mt -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1366'
Jun 22 18:26:16.944: INFO: stderr: ""
Jun 22 18:26:16.944: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 22 18:26:16.944: INFO: validating pod update-demo-kitten-f68mt
Jun 22 18:26:16.949: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 22 18:26:16.949: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 22 18:26:16.950: INFO: update-demo-kitten-f68mt is verified up and running
Jun 22 18:26:16.950: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-kitten-hz876 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-1366'
Jun 22 18:26:17.018: INFO: stderr: ""
Jun 22 18:26:17.018: INFO: stdout: "true"
Jun 22 18:26:17.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-kitten-hz876 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-1366'
Jun 22 18:26:17.082: INFO: stderr: ""
Jun 22 18:26:17.082: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jun 22 18:26:17.083: INFO: validating pod update-demo-kitten-hz876
Jun 22 18:26:17.086: INFO: got data: {
  "image": "kitten.jpg"
}

Jun 22 18:26:17.086: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jun 22 18:26:17.086: INFO: update-demo-kitten-hz876 is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:26:17.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1366" for this suite.
Jun 22 18:26:39.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:26:39.184: INFO: namespace kubectl-1366 deletion completed in 22.095115436s

• [SLOW TEST:50.637 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:26:39.184: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Jun 22 18:26:39.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 create -f - --namespace=kubectl-2238'
Jun 22 18:26:39.408: INFO: stderr: ""
Jun 22 18:26:39.408: INFO: stdout: "pod/pause created\n"
Jun 22 18:26:39.408: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun 22 18:26:39.408: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-2238" to be "running and ready"
Jun 22 18:26:39.412: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025431ms
Jun 22 18:26:41.415: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006866151s
Jun 22 18:26:41.415: INFO: Pod "pause" satisfied condition "running and ready"
Jun 22 18:26:41.415: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Jun 22 18:26:41.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 label pods pause testing-label=testing-label-value --namespace=kubectl-2238'
Jun 22 18:26:41.492: INFO: stderr: ""
Jun 22 18:26:41.492: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jun 22 18:26:41.492: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pod pause -L testing-label --namespace=kubectl-2238'
Jun 22 18:26:41.555: INFO: stderr: ""
Jun 22 18:26:41.555: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jun 22 18:26:41.555: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 label pods pause testing-label- --namespace=kubectl-2238'
Jun 22 18:26:41.625: INFO: stderr: ""
Jun 22 18:26:41.625: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jun 22 18:26:41.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pod pause -L testing-label --namespace=kubectl-2238'
Jun 22 18:26:41.693: INFO: stderr: ""
Jun 22 18:26:41.693: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Jun 22 18:26:41.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 delete --grace-period=0 --force -f - --namespace=kubectl-2238'
Jun 22 18:26:41.761: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 18:26:41.761: INFO: stdout: "pod \"pause\" force deleted\n"
Jun 22 18:26:41.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get rc,svc -l name=pause --no-headers --namespace=kubectl-2238'
Jun 22 18:26:41.831: INFO: stderr: "No resources found.\n"
Jun 22 18:26:41.831: INFO: stdout: ""
Jun 22 18:26:41.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods -l name=pause --namespace=kubectl-2238 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 18:26:41.892: INFO: stderr: ""
Jun 22 18:26:41.892: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:26:41.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2238" for this suite.
Jun 22 18:26:47.904: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:26:47.984: INFO: namespace kubectl-2238 deletion completed in 6.088537339s

• [SLOW TEST:8.800 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:26:47.984: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-3332
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-3332
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-3332
Jun 22 18:26:48.023: INFO: Found 0 stateful pods, waiting for 1
Jun 22 18:26:58.028: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jun 22 18:26:58.031: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-3332 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 22 18:26:58.209: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 22 18:26:58.209: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 22 18:26:58.209: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 22 18:26:58.212: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 22 18:27:08.216: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 18:27:08.216: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 18:27:08.236: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999751s
Jun 22 18:27:09.239: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.995495822s
Jun 22 18:27:10.244: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992385977s
Jun 22 18:27:11.247: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.98795693s
Jun 22 18:27:12.250: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.984787289s
Jun 22 18:27:13.254: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.981603789s
Jun 22 18:27:14.257: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.977978244s
Jun 22 18:27:15.260: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.974687102s
Jun 22 18:27:16.264: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.971281783s
Jun 22 18:27:17.268: INFO: Verifying statefulset ss doesn't scale past 1 for another 967.38027ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-3332
Jun 22 18:27:18.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-3332 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:27:18.452: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 22 18:27:18.452: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 22 18:27:18.452: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 22 18:27:18.461: INFO: Found 2 stateful pods, waiting for 3
Jun 22 18:27:28.465: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 18:27:28.465: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 18:27:28.465: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jun 22 18:27:28.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-3332 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 22 18:27:28.634: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 22 18:27:28.634: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 22 18:27:28.634: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 22 18:27:28.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-3332 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 22 18:27:28.922: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 22 18:27:28.922: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 22 18:27:28.922: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 22 18:27:28.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-3332 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 22 18:27:29.154: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 22 18:27:29.154: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 22 18:27:29.154: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 22 18:27:29.154: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 18:27:29.156: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jun 22 18:27:39.162: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 18:27:39.162: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 18:27:39.162: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 18:27:39.171: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999771s
Jun 22 18:27:40.175: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996150285s
Jun 22 18:27:41.178: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992703815s
Jun 22 18:27:42.182: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.989386643s
Jun 22 18:27:43.185: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.985914722s
Jun 22 18:27:44.188: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.982734989s
Jun 22 18:27:45.192: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.979456286s
Jun 22 18:27:46.195: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.976078939s
Jun 22 18:27:47.199: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.972479427s
Jun 22 18:27:48.202: INFO: Verifying statefulset ss doesn't scale past 3 for another 969.030539ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-3332
Jun 22 18:27:49.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-3332 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:27:49.379: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 22 18:27:49.379: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 22 18:27:49.379: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 22 18:27:49.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-3332 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:27:49.572: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 22 18:27:49.572: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 22 18:27:49.572: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 22 18:27:49.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-3332 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:27:49.756: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 22 18:27:49.756: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 22 18:27:49.756: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 22 18:27:49.756: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 22 18:28:09.771: INFO: Deleting all statefulset in ns statefulset-3332
Jun 22 18:28:09.773: INFO: Scaling statefulset ss to 0
Jun 22 18:28:09.780: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 18:28:09.782: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:28:09.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3332" for this suite.
Jun 22 18:28:15.805: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:28:15.887: INFO: namespace statefulset-3332 deletion completed in 6.091485106s

• [SLOW TEST:87.904 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:28:15.892: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Jun 22 18:28:15.919: INFO: Waiting up to 5m0s for pod "client-containers-0d7d6ab3-04a5-4be7-b9d6-af16f5d7aba9" in namespace "containers-5725" to be "success or failure"
Jun 22 18:28:15.924: INFO: Pod "client-containers-0d7d6ab3-04a5-4be7-b9d6-af16f5d7aba9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.531448ms
Jun 22 18:28:17.927: INFO: Pod "client-containers-0d7d6ab3-04a5-4be7-b9d6-af16f5d7aba9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007336281s
STEP: Saw pod success
Jun 22 18:28:17.927: INFO: Pod "client-containers-0d7d6ab3-04a5-4be7-b9d6-af16f5d7aba9" satisfied condition "success or failure"
Jun 22 18:28:17.929: INFO: Trying to get logs from node ip-10-0-20-165 pod client-containers-0d7d6ab3-04a5-4be7-b9d6-af16f5d7aba9 container test-container: <nil>
STEP: delete the pod
Jun 22 18:28:17.944: INFO: Waiting for pod client-containers-0d7d6ab3-04a5-4be7-b9d6-af16f5d7aba9 to disappear
Jun 22 18:28:17.946: INFO: Pod client-containers-0d7d6ab3-04a5-4be7-b9d6-af16f5d7aba9 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:28:17.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-5725" for this suite.
Jun 22 18:28:23.970: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:28:24.055: INFO: namespace containers-5725 deletion completed in 6.105336493s

• [SLOW TEST:8.163 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:28:24.056: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Jun 22 18:28:24.137: INFO: Waiting up to 5m0s for pod "client-containers-3e9f8839-008d-48e7-8d21-dea147b6e90d" in namespace "containers-7548" to be "success or failure"
Jun 22 18:28:24.140: INFO: Pod "client-containers-3e9f8839-008d-48e7-8d21-dea147b6e90d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.318342ms
Jun 22 18:28:26.143: INFO: Pod "client-containers-3e9f8839-008d-48e7-8d21-dea147b6e90d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005357469s
STEP: Saw pod success
Jun 22 18:28:26.143: INFO: Pod "client-containers-3e9f8839-008d-48e7-8d21-dea147b6e90d" satisfied condition "success or failure"
Jun 22 18:28:26.145: INFO: Trying to get logs from node ip-10-0-20-165 pod client-containers-3e9f8839-008d-48e7-8d21-dea147b6e90d container test-container: <nil>
STEP: delete the pod
Jun 22 18:28:26.164: INFO: Waiting for pod client-containers-3e9f8839-008d-48e7-8d21-dea147b6e90d to disappear
Jun 22 18:28:26.166: INFO: Pod client-containers-3e9f8839-008d-48e7-8d21-dea147b6e90d no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:28:26.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7548" for this suite.
Jun 22 18:28:32.178: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:28:32.261: INFO: namespace containers-7548 deletion completed in 6.091961849s

• [SLOW TEST:8.205 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:28:32.262: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 22 18:28:36.377: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 18:28:36.380: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 18:28:38.380: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 18:28:38.383: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 18:28:40.380: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 18:28:40.383: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 18:28:42.380: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 18:28:42.383: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 18:28:44.380: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 18:28:44.383: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 18:28:46.380: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 18:28:46.383: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 18:28:48.380: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 18:28:48.383: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 18:28:50.380: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 18:28:50.383: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 18:28:52.380: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 18:28:52.383: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 18:28:54.380: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 18:28:54.383: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 18:28:56.380: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 18:28:56.383: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 18:28:58.380: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 18:28:58.383: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 18:29:00.380: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 18:29:00.383: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 18:29:02.380: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 18:29:02.383: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 22 18:29:04.380: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 22 18:29:04.383: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:29:04.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8362" for this suite.
Jun 22 18:29:26.394: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:29:26.475: INFO: namespace container-lifecycle-hook-8362 deletion completed in 22.088328308s

• [SLOW TEST:54.213 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:29:26.476: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jun 22 18:29:26.510: INFO: Waiting up to 5m0s for pod "pod-b8520b24-8818-4cb9-9afc-6e16376b6db5" in namespace "emptydir-803" to be "success or failure"
Jun 22 18:29:26.514: INFO: Pod "pod-b8520b24-8818-4cb9-9afc-6e16376b6db5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.067272ms
Jun 22 18:29:28.517: INFO: Pod "pod-b8520b24-8818-4cb9-9afc-6e16376b6db5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00726713s
STEP: Saw pod success
Jun 22 18:29:28.517: INFO: Pod "pod-b8520b24-8818-4cb9-9afc-6e16376b6db5" satisfied condition "success or failure"
Jun 22 18:29:28.520: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-b8520b24-8818-4cb9-9afc-6e16376b6db5 container test-container: <nil>
STEP: delete the pod
Jun 22 18:29:28.532: INFO: Waiting for pod pod-b8520b24-8818-4cb9-9afc-6e16376b6db5 to disappear
Jun 22 18:29:28.535: INFO: Pod pod-b8520b24-8818-4cb9-9afc-6e16376b6db5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:29:28.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-803" for this suite.
Jun 22 18:29:34.546: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:29:34.626: INFO: namespace emptydir-803 deletion completed in 6.088448542s

• [SLOW TEST:8.150 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:29:34.626: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jun 22 18:29:34.646: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Jun 22 18:29:35.232: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun 22 18:29:37.281: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696824975, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696824975, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696824975, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696824975, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 18:29:39.285: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696824975, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696824975, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696824975, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696824975, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 18:29:41.285: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696824975, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696824975, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696824975, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696824975, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 18:29:43.285: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696824975, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696824975, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63696824975, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63696824975, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 22 18:29:47.705: INFO: Waited 2.416948364s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:29:48.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-6108" for this suite.
Jun 22 18:29:54.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:29:54.385: INFO: namespace aggregator-6108 deletion completed in 6.194298858s

• [SLOW TEST:19.759 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:29:54.386: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 22 18:29:54.465: INFO: Waiting up to 5m0s for pod "downwardapi-volume-21701149-a8e8-4070-87cf-7f52156961d2" in namespace "projected-1669" to be "success or failure"
Jun 22 18:29:54.470: INFO: Pod "downwardapi-volume-21701149-a8e8-4070-87cf-7f52156961d2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.194048ms
Jun 22 18:29:56.472: INFO: Pod "downwardapi-volume-21701149-a8e8-4070-87cf-7f52156961d2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007037718s
STEP: Saw pod success
Jun 22 18:29:56.472: INFO: Pod "downwardapi-volume-21701149-a8e8-4070-87cf-7f52156961d2" satisfied condition "success or failure"
Jun 22 18:29:56.475: INFO: Trying to get logs from node ip-10-0-20-165 pod downwardapi-volume-21701149-a8e8-4070-87cf-7f52156961d2 container client-container: <nil>
STEP: delete the pod
Jun 22 18:29:56.493: INFO: Waiting for pod downwardapi-volume-21701149-a8e8-4070-87cf-7f52156961d2 to disappear
Jun 22 18:29:56.495: INFO: Pod downwardapi-volume-21701149-a8e8-4070-87cf-7f52156961d2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:29:56.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1669" for this suite.
Jun 22 18:30:02.512: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:30:02.601: INFO: namespace projected-1669 deletion completed in 6.102767265s

• [SLOW TEST:8.215 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:30:02.601: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 22 18:30:02.629: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7b89f551-d220-4648-a1d6-d625aaf5bb5e" in namespace "downward-api-588" to be "success or failure"
Jun 22 18:30:02.635: INFO: Pod "downwardapi-volume-7b89f551-d220-4648-a1d6-d625aaf5bb5e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.835604ms
Jun 22 18:30:04.639: INFO: Pod "downwardapi-volume-7b89f551-d220-4648-a1d6-d625aaf5bb5e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009131489s
STEP: Saw pod success
Jun 22 18:30:04.639: INFO: Pod "downwardapi-volume-7b89f551-d220-4648-a1d6-d625aaf5bb5e" satisfied condition "success or failure"
Jun 22 18:30:04.641: INFO: Trying to get logs from node ip-10-0-20-165 pod downwardapi-volume-7b89f551-d220-4648-a1d6-d625aaf5bb5e container client-container: <nil>
STEP: delete the pod
Jun 22 18:30:04.654: INFO: Waiting for pod downwardapi-volume-7b89f551-d220-4648-a1d6-d625aaf5bb5e to disappear
Jun 22 18:30:04.658: INFO: Pod downwardapi-volume-7b89f551-d220-4648-a1d6-d625aaf5bb5e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:30:04.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-588" for this suite.
Jun 22 18:30:10.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:30:10.755: INFO: namespace downward-api-588 deletion completed in 6.09398037s

• [SLOW TEST:8.154 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:30:10.756: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-4d7ec8e6-25c5-4595-94a9-f28618bedaa0
STEP: Creating a pod to test consume secrets
Jun 22 18:30:10.810: INFO: Waiting up to 5m0s for pod "pod-secrets-fe8e0c2c-662b-49b7-ab4f-1fe8b8b517b8" in namespace "secrets-4596" to be "success or failure"
Jun 22 18:30:10.812: INFO: Pod "pod-secrets-fe8e0c2c-662b-49b7-ab4f-1fe8b8b517b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.349141ms
Jun 22 18:30:12.815: INFO: Pod "pod-secrets-fe8e0c2c-662b-49b7-ab4f-1fe8b8b517b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005398339s
STEP: Saw pod success
Jun 22 18:30:12.815: INFO: Pod "pod-secrets-fe8e0c2c-662b-49b7-ab4f-1fe8b8b517b8" satisfied condition "success or failure"
Jun 22 18:30:12.817: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-secrets-fe8e0c2c-662b-49b7-ab4f-1fe8b8b517b8 container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 18:30:12.832: INFO: Waiting for pod pod-secrets-fe8e0c2c-662b-49b7-ab4f-1fe8b8b517b8 to disappear
Jun 22 18:30:12.835: INFO: Pod pod-secrets-fe8e0c2c-662b-49b7-ab4f-1fe8b8b517b8 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:30:12.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4596" for this suite.
Jun 22 18:30:18.847: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:30:18.928: INFO: namespace secrets-4596 deletion completed in 6.090568358s
STEP: Destroying namespace "secret-namespace-864" for this suite.
Jun 22 18:30:24.938: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:30:25.082: INFO: namespace secret-namespace-864 deletion completed in 6.15430505s

• [SLOW TEST:14.327 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:30:25.083: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jun 22 18:30:25.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 create -f - --namespace=kubectl-6934'
Jun 22 18:30:25.370: INFO: stderr: ""
Jun 22 18:30:25.370: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jun 22 18:30:25.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6934'
Jun 22 18:30:25.450: INFO: stderr: ""
Jun 22 18:30:25.450: INFO: stdout: "update-demo-nautilus-qqwxv update-demo-nautilus-t24jl "
Jun 22 18:30:25.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-qqwxv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6934'
Jun 22 18:30:25.517: INFO: stderr: ""
Jun 22 18:30:25.517: INFO: stdout: ""
Jun 22 18:30:25.517: INFO: update-demo-nautilus-qqwxv is created but not running
Jun 22 18:30:30.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-6934'
Jun 22 18:30:30.599: INFO: stderr: ""
Jun 22 18:30:30.599: INFO: stdout: "update-demo-nautilus-qqwxv update-demo-nautilus-t24jl "
Jun 22 18:30:30.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-qqwxv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6934'
Jun 22 18:30:30.671: INFO: stderr: ""
Jun 22 18:30:30.671: INFO: stdout: "true"
Jun 22 18:30:30.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-qqwxv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6934'
Jun 22 18:30:30.742: INFO: stderr: ""
Jun 22 18:30:30.742: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 18:30:30.742: INFO: validating pod update-demo-nautilus-qqwxv
Jun 22 18:30:30.747: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 18:30:30.747: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 18:30:30.747: INFO: update-demo-nautilus-qqwxv is verified up and running
Jun 22 18:30:30.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-t24jl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-6934'
Jun 22 18:30:30.828: INFO: stderr: ""
Jun 22 18:30:30.828: INFO: stdout: "true"
Jun 22 18:30:30.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods update-demo-nautilus-t24jl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-6934'
Jun 22 18:30:30.903: INFO: stderr: ""
Jun 22 18:30:30.903: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jun 22 18:30:30.903: INFO: validating pod update-demo-nautilus-t24jl
Jun 22 18:30:30.910: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 22 18:30:30.910: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 22 18:30:30.910: INFO: update-demo-nautilus-t24jl is verified up and running
STEP: using delete to clean up resources
Jun 22 18:30:30.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 delete --grace-period=0 --force -f - --namespace=kubectl-6934'
Jun 22 18:30:30.979: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 18:30:30.979: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 22 18:30:30.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6934'
Jun 22 18:30:31.055: INFO: stderr: "No resources found.\n"
Jun 22 18:30:31.055: INFO: stdout: ""
Jun 22 18:30:31.055: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods -l name=update-demo --namespace=kubectl-6934 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 18:30:31.131: INFO: stderr: ""
Jun 22 18:30:31.131: INFO: stdout: "update-demo-nautilus-qqwxv\nupdate-demo-nautilus-t24jl\n"
Jun 22 18:30:31.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-6934'
Jun 22 18:30:31.713: INFO: stderr: "No resources found.\n"
Jun 22 18:30:31.713: INFO: stdout: ""
Jun 22 18:30:31.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods -l name=update-demo --namespace=kubectl-6934 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 18:30:31.788: INFO: stderr: ""
Jun 22 18:30:31.788: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:30:31.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6934" for this suite.
Jun 22 18:30:53.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:30:53.884: INFO: namespace kubectl-6934 deletion completed in 22.092888953s

• [SLOW TEST:28.801 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:30:53.885: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-37e35ca9-96af-4678-a3ba-e66c42b2fc8e in namespace container-probe-4045
Jun 22 18:30:55.918: INFO: Started pod busybox-37e35ca9-96af-4678-a3ba-e66c42b2fc8e in namespace container-probe-4045
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 18:30:55.921: INFO: Initial restart count of pod busybox-37e35ca9-96af-4678-a3ba-e66c42b2fc8e is 0
Jun 22 18:31:46.001: INFO: Restart count of pod container-probe-4045/busybox-37e35ca9-96af-4678-a3ba-e66c42b2fc8e is now 1 (50.080070366s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:31:46.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4045" for this suite.
Jun 22 18:31:52.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:31:52.131: INFO: namespace container-probe-4045 deletion completed in 6.117301484s

• [SLOW TEST:58.246 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:31:52.132: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:31:52.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3098" for this suite.
Jun 22 18:32:14.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:32:14.286: INFO: namespace pods-3098 deletion completed in 22.114625376s

• [SLOW TEST:22.154 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:32:14.287: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 22 18:32:14.308: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jun 22 18:32:16.332: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:32:17.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1197" for this suite.
Jun 22 18:32:23.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:32:23.432: INFO: namespace replication-controller-1197 deletion completed in 6.090792258s

• [SLOW TEST:9.145 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:32:23.434: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 22 18:32:23.511: INFO: Waiting up to 5m0s for pod "pod-9d1e6206-6fbd-4c22-a78d-96c394880937" in namespace "emptydir-4018" to be "success or failure"
Jun 22 18:32:23.513: INFO: Pod "pod-9d1e6206-6fbd-4c22-a78d-96c394880937": Phase="Pending", Reason="", readiness=false. Elapsed: 2.892993ms
Jun 22 18:32:25.563: INFO: Pod "pod-9d1e6206-6fbd-4c22-a78d-96c394880937": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.052780972s
STEP: Saw pod success
Jun 22 18:32:25.563: INFO: Pod "pod-9d1e6206-6fbd-4c22-a78d-96c394880937" satisfied condition "success or failure"
Jun 22 18:32:25.567: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-9d1e6206-6fbd-4c22-a78d-96c394880937 container test-container: <nil>
STEP: delete the pod
Jun 22 18:32:25.584: INFO: Waiting for pod pod-9d1e6206-6fbd-4c22-a78d-96c394880937 to disappear
Jun 22 18:32:25.587: INFO: Pod pod-9d1e6206-6fbd-4c22-a78d-96c394880937 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:32:25.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4018" for this suite.
Jun 22 18:32:31.598: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:32:31.677: INFO: namespace emptydir-4018 deletion completed in 6.087614237s

• [SLOW TEST:8.243 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:32:31.677: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 22 18:32:31.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-9693'
Jun 22 18:32:31.822: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 22 18:32:31.822: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
Jun 22 18:32:31.830: INFO: Waiting for rc e2e-test-nginx-rc to stabilize, generation 1 observed generation 1 spec.replicas 1 status.replicas 0
STEP: rolling-update to same image controller
Jun 22 18:32:31.843: INFO: scanned /root for discovery docs: <nil>
Jun 22 18:32:31.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-9693'
Jun 22 18:32:47.595: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jun 22 18:32:47.595: INFO: stdout: "Created e2e-test-nginx-rc-c87059cce2f7ff9b1f53dc3d174259b8\nScaling up e2e-test-nginx-rc-c87059cce2f7ff9b1f53dc3d174259b8 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-c87059cce2f7ff9b1f53dc3d174259b8 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-c87059cce2f7ff9b1f53dc3d174259b8 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jun 22 18:32:47.595: INFO: stdout: "Created e2e-test-nginx-rc-c87059cce2f7ff9b1f53dc3d174259b8\nScaling up e2e-test-nginx-rc-c87059cce2f7ff9b1f53dc3d174259b8 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-c87059cce2f7ff9b1f53dc3d174259b8 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-c87059cce2f7ff9b1f53dc3d174259b8 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jun 22 18:32:47.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-9693'
Jun 22 18:32:47.664: INFO: stderr: ""
Jun 22 18:32:47.664: INFO: stdout: "e2e-test-nginx-rc-c87059cce2f7ff9b1f53dc3d174259b8-qtxsd "
Jun 22 18:32:47.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods e2e-test-nginx-rc-c87059cce2f7ff9b1f53dc3d174259b8-qtxsd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-9693'
Jun 22 18:32:47.731: INFO: stderr: ""
Jun 22 18:32:47.731: INFO: stdout: "true"
Jun 22 18:32:47.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods e2e-test-nginx-rc-c87059cce2f7ff9b1f53dc3d174259b8-qtxsd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-9693'
Jun 22 18:32:47.795: INFO: stderr: ""
Jun 22 18:32:47.795: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jun 22 18:32:47.795: INFO: e2e-test-nginx-rc-c87059cce2f7ff9b1f53dc3d174259b8-qtxsd is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Jun 22 18:32:47.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 delete rc e2e-test-nginx-rc --namespace=kubectl-9693'
Jun 22 18:32:47.865: INFO: stderr: ""
Jun 22 18:32:47.865: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:32:47.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9693" for this suite.
Jun 22 18:33:09.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:33:09.980: INFO: namespace kubectl-9693 deletion completed in 22.10768423s

• [SLOW TEST:38.303 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:33:09.980: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jun 22 18:33:10.010: INFO: Pod name pod-release: Found 0 pods out of 1
Jun 22 18:33:15.013: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:33:15.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1588" for this suite.
Jun 22 18:33:21.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:33:21.174: INFO: namespace replication-controller-1588 deletion completed in 6.119744079s

• [SLOW TEST:11.194 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:33:21.175: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 22 18:33:21.201: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c512b52e-3ecf-460e-a5f2-470a2a467657" in namespace "projected-7239" to be "success or failure"
Jun 22 18:33:21.203: INFO: Pod "downwardapi-volume-c512b52e-3ecf-460e-a5f2-470a2a467657": Phase="Pending", Reason="", readiness=false. Elapsed: 2.28516ms
Jun 22 18:33:23.206: INFO: Pod "downwardapi-volume-c512b52e-3ecf-460e-a5f2-470a2a467657": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005401068s
STEP: Saw pod success
Jun 22 18:33:23.206: INFO: Pod "downwardapi-volume-c512b52e-3ecf-460e-a5f2-470a2a467657" satisfied condition "success or failure"
Jun 22 18:33:23.209: INFO: Trying to get logs from node ip-10-0-20-165 pod downwardapi-volume-c512b52e-3ecf-460e-a5f2-470a2a467657 container client-container: <nil>
STEP: delete the pod
Jun 22 18:33:23.227: INFO: Waiting for pod downwardapi-volume-c512b52e-3ecf-460e-a5f2-470a2a467657 to disappear
Jun 22 18:33:23.229: INFO: Pod downwardapi-volume-c512b52e-3ecf-460e-a5f2-470a2a467657 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:33:23.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7239" for this suite.
Jun 22 18:33:29.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:33:29.332: INFO: namespace projected-7239 deletion completed in 6.100025487s

• [SLOW TEST:8.157 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:33:29.333: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-b26d6f36-74fc-4aff-85e0-bcc9acacdde8 in namespace container-probe-1881
Jun 22 18:33:33.417: INFO: Started pod liveness-b26d6f36-74fc-4aff-85e0-bcc9acacdde8 in namespace container-probe-1881
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 18:33:33.419: INFO: Initial restart count of pod liveness-b26d6f36-74fc-4aff-85e0-bcc9acacdde8 is 0
Jun 22 18:33:49.446: INFO: Restart count of pod container-probe-1881/liveness-b26d6f36-74fc-4aff-85e0-bcc9acacdde8 is now 1 (16.027289489s elapsed)
Jun 22 18:34:09.479: INFO: Restart count of pod container-probe-1881/liveness-b26d6f36-74fc-4aff-85e0-bcc9acacdde8 is now 2 (36.060041982s elapsed)
Jun 22 18:34:29.509: INFO: Restart count of pod container-probe-1881/liveness-b26d6f36-74fc-4aff-85e0-bcc9acacdde8 is now 3 (56.090438287s elapsed)
Jun 22 18:34:49.542: INFO: Restart count of pod container-probe-1881/liveness-b26d6f36-74fc-4aff-85e0-bcc9acacdde8 is now 4 (1m16.122837926s elapsed)
Jun 22 18:36:03.666: INFO: Restart count of pod container-probe-1881/liveness-b26d6f36-74fc-4aff-85e0-bcc9acacdde8 is now 5 (2m30.246474802s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:36:03.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1881" for this suite.
Jun 22 18:36:09.695: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:36:09.776: INFO: namespace container-probe-1881 deletion completed in 6.093457388s

• [SLOW TEST:160.443 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:36:09.777: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 22 18:36:09.815: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jun 22 18:36:09.823: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:09.827: INFO: Number of nodes with available pods: 0
Jun 22 18:36:09.827: INFO: Node ip-10-0-15-125 is running more than one daemon pod
Jun 22 18:36:10.831: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:10.834: INFO: Number of nodes with available pods: 1
Jun 22 18:36:10.834: INFO: Node ip-10-0-15-125 is running more than one daemon pod
Jun 22 18:36:11.831: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:11.834: INFO: Number of nodes with available pods: 3
Jun 22 18:36:11.834: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jun 22 18:36:11.853: INFO: Wrong image for pod: daemon-set-57rw6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:11.853: INFO: Wrong image for pod: daemon-set-crbt7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:11.853: INFO: Wrong image for pod: daemon-set-dxwsn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:11.857: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:12.860: INFO: Wrong image for pod: daemon-set-57rw6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:12.860: INFO: Wrong image for pod: daemon-set-crbt7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:12.860: INFO: Wrong image for pod: daemon-set-dxwsn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:12.863: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:13.860: INFO: Wrong image for pod: daemon-set-57rw6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:13.860: INFO: Wrong image for pod: daemon-set-crbt7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:13.860: INFO: Wrong image for pod: daemon-set-dxwsn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:13.863: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:14.860: INFO: Wrong image for pod: daemon-set-57rw6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:14.860: INFO: Wrong image for pod: daemon-set-crbt7. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:14.860: INFO: Pod daemon-set-crbt7 is not available
Jun 22 18:36:14.860: INFO: Wrong image for pod: daemon-set-dxwsn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:14.864: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:15.860: INFO: Wrong image for pod: daemon-set-57rw6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:15.860: INFO: Wrong image for pod: daemon-set-dxwsn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:15.860: INFO: Pod daemon-set-tdq6b is not available
Jun 22 18:36:15.863: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:16.860: INFO: Wrong image for pod: daemon-set-57rw6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:16.860: INFO: Wrong image for pod: daemon-set-dxwsn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:16.860: INFO: Pod daemon-set-tdq6b is not available
Jun 22 18:36:16.864: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:17.860: INFO: Wrong image for pod: daemon-set-57rw6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:17.860: INFO: Wrong image for pod: daemon-set-dxwsn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:17.860: INFO: Pod daemon-set-tdq6b is not available
Jun 22 18:36:17.864: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:18.861: INFO: Wrong image for pod: daemon-set-57rw6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:18.861: INFO: Wrong image for pod: daemon-set-dxwsn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:18.864: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:19.860: INFO: Wrong image for pod: daemon-set-57rw6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:19.860: INFO: Wrong image for pod: daemon-set-dxwsn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:19.863: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:20.860: INFO: Wrong image for pod: daemon-set-57rw6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:20.860: INFO: Wrong image for pod: daemon-set-dxwsn. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:20.860: INFO: Pod daemon-set-dxwsn is not available
Jun 22 18:36:20.863: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:21.860: INFO: Wrong image for pod: daemon-set-57rw6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:21.860: INFO: Pod daemon-set-9vcsk is not available
Jun 22 18:36:21.863: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:22.860: INFO: Wrong image for pod: daemon-set-57rw6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:22.860: INFO: Pod daemon-set-9vcsk is not available
Jun 22 18:36:22.863: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:23.860: INFO: Wrong image for pod: daemon-set-57rw6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:23.860: INFO: Pod daemon-set-57rw6 is not available
Jun 22 18:36:23.863: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:24.860: INFO: Wrong image for pod: daemon-set-57rw6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:24.860: INFO: Pod daemon-set-57rw6 is not available
Jun 22 18:36:24.862: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:25.860: INFO: Wrong image for pod: daemon-set-57rw6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:25.860: INFO: Pod daemon-set-57rw6 is not available
Jun 22 18:36:25.863: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:26.860: INFO: Wrong image for pod: daemon-set-57rw6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:26.860: INFO: Pod daemon-set-57rw6 is not available
Jun 22 18:36:26.863: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:27.860: INFO: Wrong image for pod: daemon-set-57rw6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:27.860: INFO: Pod daemon-set-57rw6 is not available
Jun 22 18:36:27.863: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:28.860: INFO: Wrong image for pod: daemon-set-57rw6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:28.860: INFO: Pod daemon-set-57rw6 is not available
Jun 22 18:36:28.863: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:29.860: INFO: Wrong image for pod: daemon-set-57rw6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:29.860: INFO: Pod daemon-set-57rw6 is not available
Jun 22 18:36:29.862: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:30.860: INFO: Wrong image for pod: daemon-set-57rw6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:30.860: INFO: Pod daemon-set-57rw6 is not available
Jun 22 18:36:30.863: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:31.860: INFO: Wrong image for pod: daemon-set-57rw6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:31.860: INFO: Pod daemon-set-57rw6 is not available
Jun 22 18:36:31.863: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:32.860: INFO: Wrong image for pod: daemon-set-57rw6. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jun 22 18:36:32.860: INFO: Pod daemon-set-57rw6 is not available
Jun 22 18:36:32.863: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:33.861: INFO: Pod daemon-set-bcgg5 is not available
Jun 22 18:36:33.864: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jun 22 18:36:33.867: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:33.869: INFO: Number of nodes with available pods: 2
Jun 22 18:36:33.869: INFO: Node ip-10-0-20-165 is running more than one daemon pod
Jun 22 18:36:34.874: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:36:34.878: INFO: Number of nodes with available pods: 3
Jun 22 18:36:34.878: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4808, will wait for the garbage collector to delete the pods
Jun 22 18:36:34.947: INFO: Deleting DaemonSet.extensions daemon-set took: 5.560584ms
Jun 22 18:36:35.248: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.189642ms
Jun 22 18:36:43.651: INFO: Number of nodes with available pods: 0
Jun 22 18:36:43.651: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 18:36:43.653: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-4808/daemonsets","resourceVersion":"12644"},"items":null}

Jun 22 18:36:43.655: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-4808/pods","resourceVersion":"12644"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:36:43.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4808" for this suite.
Jun 22 18:36:49.678: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:36:49.755: INFO: namespace daemonsets-4808 deletion completed in 6.087498906s

• [SLOW TEST:39.979 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:36:49.756: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-23e53141-a30d-4fbc-9325-8c3edf3ecf75
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-23e53141-a30d-4fbc-9325-8c3edf3ecf75
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:36:53.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7527" for this suite.
Jun 22 18:37:15.836: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:37:15.918: INFO: namespace configmap-7527 deletion completed in 22.093747068s

• [SLOW TEST:26.163 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:37:15.919: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-75bf1016-e53f-417e-be0c-7453fc86ebcf
STEP: Creating a pod to test consume configMaps
Jun 22 18:37:16.000: INFO: Waiting up to 5m0s for pod "pod-configmaps-ebabb5f6-ec8c-4762-8a4a-5ce041c0b12e" in namespace "configmap-9061" to be "success or failure"
Jun 22 18:37:16.006: INFO: Pod "pod-configmaps-ebabb5f6-ec8c-4762-8a4a-5ce041c0b12e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.959534ms
Jun 22 18:37:18.009: INFO: Pod "pod-configmaps-ebabb5f6-ec8c-4762-8a4a-5ce041c0b12e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009387256s
STEP: Saw pod success
Jun 22 18:37:18.009: INFO: Pod "pod-configmaps-ebabb5f6-ec8c-4762-8a4a-5ce041c0b12e" satisfied condition "success or failure"
Jun 22 18:37:18.012: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-configmaps-ebabb5f6-ec8c-4762-8a4a-5ce041c0b12e container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 18:37:18.027: INFO: Waiting for pod pod-configmaps-ebabb5f6-ec8c-4762-8a4a-5ce041c0b12e to disappear
Jun 22 18:37:18.029: INFO: Pod pod-configmaps-ebabb5f6-ec8c-4762-8a4a-5ce041c0b12e no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:37:18.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9061" for this suite.
Jun 22 18:37:24.042: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:37:24.140: INFO: namespace configmap-9061 deletion completed in 6.107624151s

• [SLOW TEST:8.221 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:37:24.140: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jun 22 18:37:24.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 create -f - --namespace=kubectl-7246'
Jun 22 18:37:24.449: INFO: stderr: ""
Jun 22 18:37:24.449: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 22 18:37:25.453: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 18:37:25.453: INFO: Found 0 / 1
Jun 22 18:37:26.453: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 18:37:26.453: INFO: Found 1 / 1
Jun 22 18:37:26.453: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jun 22 18:37:26.455: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 18:37:26.455: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 22 18:37:26.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 patch pod redis-master-6h4xq --namespace=kubectl-7246 -p {"metadata":{"annotations":{"x":"y"}}}'
Jun 22 18:37:26.527: INFO: stderr: ""
Jun 22 18:37:26.527: INFO: stdout: "pod/redis-master-6h4xq patched\n"
STEP: checking annotations
Jun 22 18:37:26.529: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 18:37:26.529: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:37:26.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7246" for this suite.
Jun 22 18:37:48.541: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:37:48.623: INFO: namespace kubectl-7246 deletion completed in 22.090101549s

• [SLOW TEST:24.482 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:37:48.623: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-5415
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-5415
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5415
Jun 22 18:37:48.710: INFO: Found 0 stateful pods, waiting for 1
Jun 22 18:37:58.714: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jun 22 18:37:58.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 22 18:37:58.878: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 22 18:37:58.878: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 22 18:37:58.878: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 22 18:37:58.881: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 22 18:38:08.884: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 18:38:08.884: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 18:38:08.894: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jun 22 18:38:08.894: INFO: ss-0  ip-10-0-20-165  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:59 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:48 +0000 UTC  }]
Jun 22 18:38:08.894: INFO: 
Jun 22 18:38:08.894: INFO: StatefulSet ss has not reached scale 3, at 1
Jun 22 18:38:09.898: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997192419s
Jun 22 18:38:10.901: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993646932s
Jun 22 18:38:11.904: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.990221491s
Jun 22 18:38:12.908: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.986994823s
Jun 22 18:38:13.911: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.983595223s
Jun 22 18:38:14.915: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.980040857s
Jun 22 18:38:15.918: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.977090964s
Jun 22 18:38:16.921: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.974045943s
Jun 22 18:38:17.924: INFO: Verifying statefulset ss doesn't scale past 3 for another 970.712174ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5415
Jun 22 18:38:18.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:38:19.095: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 22 18:38:19.095: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 22 18:38:19.095: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 22 18:38:19.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:38:19.271: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 22 18:38:19.271: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 22 18:38:19.271: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 22 18:38:19.271: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:38:19.473: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 22 18:38:19.473: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 22 18:38:19.473: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 22 18:38:19.476: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 18:38:19.476: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 18:38:19.476: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jun 22 18:38:19.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 22 18:38:19.673: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 22 18:38:19.673: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 22 18:38:19.674: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 22 18:38:19.674: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 22 18:38:19.841: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 22 18:38:19.841: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 22 18:38:19.841: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 22 18:38:19.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 22 18:38:20.022: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 22 18:38:20.022: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 22 18:38:20.022: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 22 18:38:20.022: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 18:38:20.025: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jun 22 18:38:30.033: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 18:38:30.033: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 18:38:30.033: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 22 18:38:30.060: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jun 22 18:38:30.060: INFO: ss-0  ip-10-0-20-165  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:48 +0000 UTC  }]
Jun 22 18:38:30.060: INFO: ss-1  ip-10-0-42-243  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  }]
Jun 22 18:38:30.060: INFO: ss-2  ip-10-0-15-125  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  }]
Jun 22 18:38:30.060: INFO: 
Jun 22 18:38:30.060: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 22 18:38:31.064: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jun 22 18:38:31.064: INFO: ss-0  ip-10-0-20-165  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:48 +0000 UTC  }]
Jun 22 18:38:31.064: INFO: ss-1  ip-10-0-42-243  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  }]
Jun 22 18:38:31.064: INFO: ss-2  ip-10-0-15-125  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  }]
Jun 22 18:38:31.064: INFO: 
Jun 22 18:38:31.064: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 22 18:38:32.068: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jun 22 18:38:32.068: INFO: ss-0  ip-10-0-20-165  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:48 +0000 UTC  }]
Jun 22 18:38:32.068: INFO: ss-1  ip-10-0-42-243  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  }]
Jun 22 18:38:32.068: INFO: 
Jun 22 18:38:32.068: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 22 18:38:33.071: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jun 22 18:38:33.071: INFO: ss-0  ip-10-0-20-165  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:48 +0000 UTC  }]
Jun 22 18:38:33.072: INFO: ss-1  ip-10-0-42-243  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  }]
Jun 22 18:38:33.072: INFO: 
Jun 22 18:38:33.072: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 22 18:38:34.075: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jun 22 18:38:34.075: INFO: ss-0  ip-10-0-20-165  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:48 +0000 UTC  }]
Jun 22 18:38:34.075: INFO: ss-1  ip-10-0-42-243  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  }]
Jun 22 18:38:34.075: INFO: 
Jun 22 18:38:34.075: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 22 18:38:35.078: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jun 22 18:38:35.078: INFO: ss-0  ip-10-0-20-165  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:48 +0000 UTC  }]
Jun 22 18:38:35.078: INFO: ss-1  ip-10-0-42-243  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  }]
Jun 22 18:38:35.078: INFO: 
Jun 22 18:38:35.078: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 22 18:38:36.082: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jun 22 18:38:36.082: INFO: ss-0  ip-10-0-20-165  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:48 +0000 UTC  }]
Jun 22 18:38:36.082: INFO: ss-1  ip-10-0-42-243  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  }]
Jun 22 18:38:36.082: INFO: 
Jun 22 18:38:36.082: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 22 18:38:37.085: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jun 22 18:38:37.085: INFO: ss-0  ip-10-0-20-165  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:48 +0000 UTC  }]
Jun 22 18:38:37.085: INFO: ss-1  ip-10-0-42-243  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  }]
Jun 22 18:38:37.085: INFO: 
Jun 22 18:38:37.085: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 22 18:38:38.089: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jun 22 18:38:38.089: INFO: ss-0  ip-10-0-20-165  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:48 +0000 UTC  }]
Jun 22 18:38:38.089: INFO: ss-1  ip-10-0-42-243  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  }]
Jun 22 18:38:38.089: INFO: 
Jun 22 18:38:38.089: INFO: StatefulSet ss has not reached scale 0, at 2
Jun 22 18:38:39.092: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
Jun 22 18:38:39.092: INFO: ss-0  ip-10-0-20-165  Pending  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:37:48 +0000 UTC  }]
Jun 22 18:38:39.092: INFO: ss-1  ip-10-0-42-243  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:20 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:38:08 +0000 UTC  }]
Jun 22 18:38:39.092: INFO: 
Jun 22 18:38:39.092: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5415
Jun 22 18:38:40.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:38:40.197: INFO: rc: 1
Jun 22 18:38:40.197: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc001819230 exit status 1 <nil> <nil> true [0xc0022e9d30 0xc0022e9d48 0xc0022e9d60] [0xc0022e9d30 0xc0022e9d48 0xc0022e9d60] [0xc0022e9d40 0xc0022e9d58] [0x9d17b0 0x9d17b0] 0xc00226e600 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Jun 22 18:38:50.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:38:50.272: INFO: rc: 1
Jun 22 18:38:50.272: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0024a5b60 exit status 1 <nil> <nil> true [0xc0038b01c8 0xc0038b01e0 0xc0038b01f8] [0xc0038b01c8 0xc0038b01e0 0xc0038b01f8] [0xc0038b01d8 0xc0038b01f0] [0x9d17b0 0x9d17b0] 0xc00314daa0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:39:00.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:39:00.346: INFO: rc: 1
Jun 22 18:39:00.346: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b56330 exit status 1 <nil> <nil> true [0xc0014980c0 0xc0014982b8 0xc0014984c8] [0xc0014980c0 0xc0014982b8 0xc0014984c8] [0xc001498238 0xc001498490] [0x9d17b0 0x9d17b0] 0xc0039182a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:39:10.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:39:10.427: INFO: rc: 1
Jun 22 18:39:10.428: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022aa390 exit status 1 <nil> <nil> true [0xc002aa8000 0xc002aa8030 0xc002aa8050] [0xc002aa8000 0xc002aa8030 0xc002aa8050] [0xc002aa8020 0xc002aa8040] [0x9d17b0 0x9d17b0] 0xc002680720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:39:20.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:39:20.492: INFO: rc: 1
Jun 22 18:39:20.492: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022aa750 exit status 1 <nil> <nil> true [0xc002aa8060 0xc002aa8080 0xc002aa80a8] [0xc002aa8060 0xc002aa8080 0xc002aa80a8] [0xc002aa8078 0xc002aa8098] [0x9d17b0 0x9d17b0] 0xc002680fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:39:30.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:39:30.562: INFO: rc: 1
Jun 22 18:39:30.562: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b56690 exit status 1 <nil> <nil> true [0xc0014984e8 0xc0014985e8 0xc001498788] [0xc0014984e8 0xc0014985e8 0xc001498788] [0xc0014985c8 0xc001498740] [0x9d17b0 0x9d17b0] 0xc003918660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:39:40.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:39:40.630: INFO: rc: 1
Jun 22 18:39:40.630: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022aaae0 exit status 1 <nil> <nil> true [0xc002aa80b8 0xc002aa80d0 0xc002aa80f8] [0xc002aa80b8 0xc002aa80d0 0xc002aa80f8] [0xc002aa80c8 0xc002aa80e8] [0x9d17b0 0x9d17b0] 0xc0026817a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:39:50.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:39:50.699: INFO: rc: 1
Jun 22 18:39:50.699: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b56a80 exit status 1 <nil> <nil> true [0xc0014987d0 0xc001498910 0xc0014989d0] [0xc0014987d0 0xc001498910 0xc0014989d0] [0xc001498908 0xc001498950] [0x9d17b0 0x9d17b0] 0xc003918ae0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:40:00.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:40:00.775: INFO: rc: 1
Jun 22 18:40:00.775: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b56e10 exit status 1 <nil> <nil> true [0xc0014989e8 0xc001498a30 0xc001498ad0] [0xc0014989e8 0xc001498a30 0xc001498ad0] [0xc001498a18 0xc001498ab0] [0x9d17b0 0x9d17b0] 0xc003918f00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:40:10.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:40:10.843: INFO: rc: 1
Jun 22 18:40:10.843: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b571d0 exit status 1 <nil> <nil> true [0xc001498b48 0xc001498b98 0xc001498bb8] [0xc001498b48 0xc001498b98 0xc001498bb8] [0xc001498b78 0xc001498bb0] [0x9d17b0 0x9d17b0] 0xc003919680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:40:20.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:40:20.922: INFO: rc: 1
Jun 22 18:40:20.922: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022aae40 exit status 1 <nil> <nil> true [0xc002aa8108 0xc002aa8128 0xc002aa8150] [0xc002aa8108 0xc002aa8128 0xc002aa8150] [0xc002aa8120 0xc002aa8140] [0x9d17b0 0x9d17b0] 0xc002681b00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:40:30.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:40:31.005: INFO: rc: 1
Jun 22 18:40:31.006: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022ab230 exit status 1 <nil> <nil> true [0xc002aa8160 0xc002aa8180 0xc002aa8198] [0xc002aa8160 0xc002aa8180 0xc002aa8198] [0xc002aa8178 0xc002aa8190] [0x9d17b0 0x9d17b0] 0xc002681e60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:40:41.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:40:41.085: INFO: rc: 1
Jun 22 18:40:41.085: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022ab5f0 exit status 1 <nil> <nil> true [0xc002aa81a8 0xc002aa81d0 0xc002aa81e8] [0xc002aa81a8 0xc002aa81d0 0xc002aa81e8] [0xc002aa81c8 0xc002aa81e0] [0x9d17b0 0x9d17b0] 0xc0028e81e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:40:51.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:40:51.162: INFO: rc: 1
Jun 22 18:40:51.162: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b57590 exit status 1 <nil> <nil> true [0xc001498bc8 0xc001498c30 0xc001498c58] [0xc001498bc8 0xc001498c30 0xc001498c58] [0xc001498c10 0xc001498c48] [0x9d17b0 0x9d17b0] 0xc003919da0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:41:01.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:41:01.241: INFO: rc: 1
Jun 22 18:41:01.241: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022aa330 exit status 1 <nil> <nil> true [0xc002aa8010 0xc002aa8038 0xc002aa8060] [0xc002aa8010 0xc002aa8038 0xc002aa8060] [0xc002aa8030 0xc002aa8050] [0x9d17b0 0x9d17b0] 0xc002680720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:41:11.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:41:11.316: INFO: rc: 1
Jun 22 18:41:11.316: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022aa6f0 exit status 1 <nil> <nil> true [0xc002aa8070 0xc002aa8088 0xc002aa80b8] [0xc002aa8070 0xc002aa8088 0xc002aa80b8] [0xc002aa8080 0xc002aa80a8] [0x9d17b0 0x9d17b0] 0xc002680fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:41:21.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:41:21.389: INFO: rc: 1
Jun 22 18:41:21.389: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022aaab0 exit status 1 <nil> <nil> true [0xc002aa80c0 0xc002aa80d8 0xc002aa8108] [0xc002aa80c0 0xc002aa80d8 0xc002aa8108] [0xc002aa80d0 0xc002aa80f8] [0x9d17b0 0x9d17b0] 0xc0026817a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:41:31.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:41:31.462: INFO: rc: 1
Jun 22 18:41:31.462: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b56360 exit status 1 <nil> <nil> true [0xc001498010 0xc001498238 0xc001498490] [0xc001498010 0xc001498238 0xc001498490] [0xc001498180 0xc001498390] [0x9d17b0 0x9d17b0] 0xc0028e82a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:41:41.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:41:41.531: INFO: rc: 1
Jun 22 18:41:41.532: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b56720 exit status 1 <nil> <nil> true [0xc0014984c8 0xc0014985c8 0xc001498740] [0xc0014984c8 0xc0014985c8 0xc001498740] [0xc001498540 0xc001498660] [0x9d17b0 0x9d17b0] 0xc0028e8600 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:41:51.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:41:51.596: INFO: rc: 1
Jun 22 18:41:51.596: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022aaf00 exit status 1 <nil> <nil> true [0xc002aa8118 0xc002aa8130 0xc002aa8160] [0xc002aa8118 0xc002aa8130 0xc002aa8160] [0xc002aa8128 0xc002aa8150] [0x9d17b0 0x9d17b0] 0xc002681b00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:42:01.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:42:01.664: INFO: rc: 1
Jun 22 18:42:01.664: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022ab2c0 exit status 1 <nil> <nil> true [0xc002aa8170 0xc002aa8188 0xc002aa81a8] [0xc002aa8170 0xc002aa8188 0xc002aa81a8] [0xc002aa8180 0xc002aa8198] [0x9d17b0 0x9d17b0] 0xc002681e60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:42:11.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:42:11.733: INFO: rc: 1
Jun 22 18:42:11.733: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b56ae0 exit status 1 <nil> <nil> true [0xc001498788 0xc001498908 0xc001498950] [0xc001498788 0xc001498908 0xc001498950] [0xc001498878 0xc001498918] [0x9d17b0 0x9d17b0] 0xc0028e8a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:42:21.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:42:21.797: INFO: rc: 1
Jun 22 18:42:21.797: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b56ed0 exit status 1 <nil> <nil> true [0xc0014989d0 0xc001498a18 0xc001498ab0] [0xc0014989d0 0xc001498a18 0xc001498ab0] [0xc001498a08 0xc001498a50] [0x9d17b0 0x9d17b0] 0xc0028e8ea0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:42:31.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:42:31.866: INFO: rc: 1
Jun 22 18:42:31.866: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b57290 exit status 1 <nil> <nil> true [0xc001498ad0 0xc001498b78 0xc001498bb0] [0xc001498ad0 0xc001498b78 0xc001498bb0] [0xc001498b58 0xc001498ba8] [0x9d17b0 0x9d17b0] 0xc0028e9200 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:42:41.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:42:41.944: INFO: rc: 1
Jun 22 18:42:41.944: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022ab680 exit status 1 <nil> <nil> true [0xc002aa81b8 0xc002aa81d8 0xc002aa81f0] [0xc002aa81b8 0xc002aa81d8 0xc002aa81f0] [0xc002aa81d0 0xc002aa81e8] [0x9d17b0 0x9d17b0] 0xc0039181e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:42:51.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:42:52.013: INFO: rc: 1
Jun 22 18:42:52.013: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022aba10 exit status 1 <nil> <nil> true [0xc002aa81f8 0xc002aa8228 0xc002aa8248] [0xc002aa81f8 0xc002aa8228 0xc002aa8248] [0xc002aa8218 0xc002aa8240] [0x9d17b0 0x9d17b0] 0xc003918540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:43:02.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:43:02.141: INFO: rc: 1
Jun 22 18:43:02.141: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b57650 exit status 1 <nil> <nil> true [0xc001498bc8 0xc001498c30 0xc001498c58] [0xc001498bc8 0xc001498c30 0xc001498c58] [0xc001498c10 0xc001498c48] [0x9d17b0 0x9d17b0] 0xc0028e9620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:43:12.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:43:12.226: INFO: rc: 1
Jun 22 18:43:12.226: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022aa390 exit status 1 <nil> <nil> true [0xc002aa8000 0xc002aa8030 0xc002aa8050] [0xc002aa8000 0xc002aa8030 0xc002aa8050] [0xc002aa8020 0xc002aa8040] [0x9d17b0 0x9d17b0] 0xc002680720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:43:22.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:43:22.299: INFO: rc: 1
Jun 22 18:43:22.299: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001b56330 exit status 1 <nil> <nil> true [0xc001498010 0xc001498238 0xc001498490] [0xc001498010 0xc001498238 0xc001498490] [0xc001498180 0xc001498390] [0x9d17b0 0x9d17b0] 0xc0039182a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:43:32.299: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:43:32.381: INFO: rc: 1
Jun 22 18:43:32.381: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0022aa780 exit status 1 <nil> <nil> true [0xc002aa8060 0xc002aa8080 0xc002aa80a8] [0xc002aa8060 0xc002aa8080 0xc002aa80a8] [0xc002aa8078 0xc002aa8098] [0x9d17b0 0x9d17b0] 0xc002680fc0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jun 22 18:43:42.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-5415 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:43:42.448: INFO: rc: 1
Jun 22 18:43:42.448: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Jun 22 18:43:42.448: INFO: Scaling statefulset ss to 0
Jun 22 18:43:42.455: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 22 18:43:42.457: INFO: Deleting all statefulset in ns statefulset-5415
Jun 22 18:43:42.460: INFO: Scaling statefulset ss to 0
Jun 22 18:43:42.467: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 18:43:42.469: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:43:42.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5415" for this suite.
Jun 22 18:43:48.496: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:43:48.583: INFO: namespace statefulset-5415 deletion completed in 6.098362955s

• [SLOW TEST:359.960 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:43:48.583: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:43:48.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9269" for this suite.
Jun 22 18:43:54.632: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:43:54.717: INFO: namespace kubelet-test-9269 deletion completed in 6.094029129s

• [SLOW TEST:6.134 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:43:54.717: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 22 18:43:54.745: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e95e67a9-58ee-48d7-a29e-7b28dab821c2" in namespace "downward-api-8414" to be "success or failure"
Jun 22 18:43:54.750: INFO: Pod "downwardapi-volume-e95e67a9-58ee-48d7-a29e-7b28dab821c2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.973649ms
Jun 22 18:43:56.753: INFO: Pod "downwardapi-volume-e95e67a9-58ee-48d7-a29e-7b28dab821c2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00812867s
STEP: Saw pod success
Jun 22 18:43:56.753: INFO: Pod "downwardapi-volume-e95e67a9-58ee-48d7-a29e-7b28dab821c2" satisfied condition "success or failure"
Jun 22 18:43:56.755: INFO: Trying to get logs from node ip-10-0-20-165 pod downwardapi-volume-e95e67a9-58ee-48d7-a29e-7b28dab821c2 container client-container: <nil>
STEP: delete the pod
Jun 22 18:43:56.770: INFO: Waiting for pod downwardapi-volume-e95e67a9-58ee-48d7-a29e-7b28dab821c2 to disappear
Jun 22 18:43:56.772: INFO: Pod downwardapi-volume-e95e67a9-58ee-48d7-a29e-7b28dab821c2 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:43:56.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8414" for this suite.
Jun 22 18:44:02.784: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:44:02.865: INFO: namespace downward-api-8414 deletion completed in 6.089048818s

• [SLOW TEST:8.147 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:44:02.865: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 22 18:44:02.902: INFO: Create a RollingUpdate DaemonSet
Jun 22 18:44:02.906: INFO: Check that daemon pods launch on every node of the cluster
Jun 22 18:44:02.909: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:44:02.912: INFO: Number of nodes with available pods: 0
Jun 22 18:44:02.912: INFO: Node ip-10-0-15-125 is running more than one daemon pod
Jun 22 18:44:03.915: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:44:03.917: INFO: Number of nodes with available pods: 0
Jun 22 18:44:03.917: INFO: Node ip-10-0-15-125 is running more than one daemon pod
Jun 22 18:44:04.915: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:44:04.918: INFO: Number of nodes with available pods: 3
Jun 22 18:44:04.918: INFO: Number of running nodes: 3, number of available pods: 3
Jun 22 18:44:04.918: INFO: Update the DaemonSet to trigger a rollout
Jun 22 18:44:04.924: INFO: Updating DaemonSet daemon-set
Jun 22 18:44:12.938: INFO: Roll back the DaemonSet before rollout is complete
Jun 22 18:44:12.943: INFO: Updating DaemonSet daemon-set
Jun 22 18:44:12.943: INFO: Make sure DaemonSet rollback is complete
Jun 22 18:44:12.948: INFO: Wrong image for pod: daemon-set-t652p. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 22 18:44:12.948: INFO: Pod daemon-set-t652p is not available
Jun 22 18:44:12.953: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:44:13.957: INFO: Wrong image for pod: daemon-set-t652p. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 22 18:44:13.957: INFO: Pod daemon-set-t652p is not available
Jun 22 18:44:13.960: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:44:14.957: INFO: Wrong image for pod: daemon-set-t652p. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 22 18:44:14.957: INFO: Pod daemon-set-t652p is not available
Jun 22 18:44:14.960: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:44:15.957: INFO: Wrong image for pod: daemon-set-t652p. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 22 18:44:15.957: INFO: Pod daemon-set-t652p is not available
Jun 22 18:44:15.960: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:44:16.956: INFO: Wrong image for pod: daemon-set-t652p. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jun 22 18:44:16.956: INFO: Pod daemon-set-t652p is not available
Jun 22 18:44:16.959: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jun 22 18:44:17.957: INFO: Pod daemon-set-b7g6w is not available
Jun 22 18:44:17.960: INFO: DaemonSet pods can't tolerate node ip-10-0-2-237 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3794, will wait for the garbage collector to delete the pods
Jun 22 18:44:18.022: INFO: Deleting DaemonSet.extensions daemon-set took: 5.091732ms
Jun 22 18:44:18.322: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.362505ms
Jun 22 18:44:22.925: INFO: Number of nodes with available pods: 0
Jun 22 18:44:22.926: INFO: Number of running nodes: 0, number of available pods: 0
Jun 22 18:44:22.929: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3794/daemonsets","resourceVersion":"13863"},"items":null}

Jun 22 18:44:22.931: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3794/pods","resourceVersion":"13863"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:44:22.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3794" for this suite.
Jun 22 18:44:28.953: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:44:29.033: INFO: namespace daemonsets-3794 deletion completed in 6.089294778s

• [SLOW TEST:26.168 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:44:29.034: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-826
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jun 22 18:44:29.073: INFO: Found 0 stateful pods, waiting for 3
Jun 22 18:44:39.077: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 18:44:39.077: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 18:44:39.077: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 18:44:39.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-826 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 22 18:44:39.257: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 22 18:44:39.257: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 22 18:44:39.257: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jun 22 18:44:49.285: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jun 22 18:44:59.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-826 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:44:59.478: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 22 18:44:59.478: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 22 18:44:59.478: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 22 18:45:09.494: INFO: Waiting for StatefulSet statefulset-826/ss2 to complete update
Jun 22 18:45:09.494: INFO: Waiting for Pod statefulset-826/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 22 18:45:09.494: INFO: Waiting for Pod statefulset-826/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 22 18:45:09.494: INFO: Waiting for Pod statefulset-826/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 22 18:45:19.500: INFO: Waiting for StatefulSet statefulset-826/ss2 to complete update
Jun 22 18:45:19.500: INFO: Waiting for Pod statefulset-826/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 22 18:45:19.500: INFO: Waiting for Pod statefulset-826/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 22 18:45:29.500: INFO: Waiting for StatefulSet statefulset-826/ss2 to complete update
Jun 22 18:45:29.500: INFO: Waiting for Pod statefulset-826/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Jun 22 18:45:39.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-826 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jun 22 18:45:39.669: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jun 22 18:45:39.669: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jun 22 18:45:39.669: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jun 22 18:45:49.696: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jun 22 18:45:59.713: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec --namespace=statefulset-826 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jun 22 18:45:59.878: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jun 22 18:45:59.878: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jun 22 18:45:59.878: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jun 22 18:46:09.894: INFO: Waiting for StatefulSet statefulset-826/ss2 to complete update
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 22 18:46:19.900: INFO: Deleting all statefulset in ns statefulset-826
Jun 22 18:46:19.902: INFO: Scaling statefulset ss2 to 0
Jun 22 18:46:49.915: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 18:46:49.917: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:46:49.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-826" for this suite.
Jun 22 18:46:55.939: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:46:56.024: INFO: namespace statefulset-826 deletion completed in 6.094699131s

• [SLOW TEST:146.990 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:46:56.025: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-3fa432ce-9120-4fca-91e3-df4e9d67d8a4
STEP: Creating a pod to test consume configMaps
Jun 22 18:46:56.057: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2dd5f867-5e78-4edd-810e-40ee6b869acb" in namespace "projected-6868" to be "success or failure"
Jun 22 18:46:56.065: INFO: Pod "pod-projected-configmaps-2dd5f867-5e78-4edd-810e-40ee6b869acb": Phase="Pending", Reason="", readiness=false. Elapsed: 7.822428ms
Jun 22 18:46:58.068: INFO: Pod "pod-projected-configmaps-2dd5f867-5e78-4edd-810e-40ee6b869acb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01078271s
STEP: Saw pod success
Jun 22 18:46:58.068: INFO: Pod "pod-projected-configmaps-2dd5f867-5e78-4edd-810e-40ee6b869acb" satisfied condition "success or failure"
Jun 22 18:46:58.070: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-projected-configmaps-2dd5f867-5e78-4edd-810e-40ee6b869acb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 18:46:58.092: INFO: Waiting for pod pod-projected-configmaps-2dd5f867-5e78-4edd-810e-40ee6b869acb to disappear
Jun 22 18:46:58.094: INFO: Pod pod-projected-configmaps-2dd5f867-5e78-4edd-810e-40ee6b869acb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:46:58.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6868" for this suite.
Jun 22 18:47:04.112: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:47:04.199: INFO: namespace projected-6868 deletion completed in 6.102120319s

• [SLOW TEST:8.174 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:47:04.199: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:47:06.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6884" for this suite.
Jun 22 18:48:00.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:48:00.354: INFO: namespace kubelet-test-6884 deletion completed in 54.105679775s

• [SLOW TEST:56.155 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:48:00.354: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-0a627153-fcf1-47be-979d-23e2a5efb6d3
Jun 22 18:48:00.387: INFO: Pod name my-hostname-basic-0a627153-fcf1-47be-979d-23e2a5efb6d3: Found 0 pods out of 1
Jun 22 18:48:05.391: INFO: Pod name my-hostname-basic-0a627153-fcf1-47be-979d-23e2a5efb6d3: Found 1 pods out of 1
Jun 22 18:48:05.391: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-0a627153-fcf1-47be-979d-23e2a5efb6d3" are running
Jun 22 18:48:05.393: INFO: Pod "my-hostname-basic-0a627153-fcf1-47be-979d-23e2a5efb6d3-rgzh9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-22 18:48:00 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-22 18:48:01 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-22 18:48:01 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-06-22 18:48:00 +0000 UTC Reason: Message:}])
Jun 22 18:48:05.393: INFO: Trying to dial the pod
Jun 22 18:48:10.404: INFO: Controller my-hostname-basic-0a627153-fcf1-47be-979d-23e2a5efb6d3: Got expected result from replica 1 [my-hostname-basic-0a627153-fcf1-47be-979d-23e2a5efb6d3-rgzh9]: "my-hostname-basic-0a627153-fcf1-47be-979d-23e2a5efb6d3-rgzh9", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:48:10.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6569" for this suite.
Jun 22 18:48:16.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:48:16.509: INFO: namespace replication-controller-6569 deletion completed in 6.098374906s

• [SLOW TEST:16.155 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:48:16.509: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Jun 22 18:48:16.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 cluster-info'
Jun 22 18:48:16.757: INFO: stderr: ""
Jun 22 18:48:16.757: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.3.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:48:16.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7912" for this suite.
Jun 22 18:48:22.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:48:22.851: INFO: namespace kubectl-7912 deletion completed in 6.091420753s

• [SLOW TEST:6.342 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:48:22.852: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jun 22 18:48:26.919: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 18:48:26.922: INFO: Pod pod-with-prestop-http-hook still exists
Jun 22 18:48:28.922: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 18:48:28.925: INFO: Pod pod-with-prestop-http-hook still exists
Jun 22 18:48:30.922: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 18:48:30.925: INFO: Pod pod-with-prestop-http-hook still exists
Jun 22 18:48:32.922: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 18:48:32.925: INFO: Pod pod-with-prestop-http-hook still exists
Jun 22 18:48:34.922: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 22 18:48:34.925: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:48:34.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8961" for this suite.
Jun 22 18:48:56.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:48:57.023: INFO: namespace container-lifecycle-hook-8961 deletion completed in 22.088372947s

• [SLOW TEST:34.171 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:48:57.024: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jun 22 18:48:57.049: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:49:13.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3489" for this suite.
Jun 22 18:49:19.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:49:19.721: INFO: namespace pods-3489 deletion completed in 6.092347484s

• [SLOW TEST:22.697 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:49:19.722: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jun 22 18:49:23.781: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 18:49:23.783: INFO: Pod pod-with-poststart-http-hook still exists
Jun 22 18:49:25.783: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 18:49:25.786: INFO: Pod pod-with-poststart-http-hook still exists
Jun 22 18:49:27.783: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 18:49:27.786: INFO: Pod pod-with-poststart-http-hook still exists
Jun 22 18:49:29.783: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 18:49:29.787: INFO: Pod pod-with-poststart-http-hook still exists
Jun 22 18:49:31.783: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 18:49:31.786: INFO: Pod pod-with-poststart-http-hook still exists
Jun 22 18:49:33.783: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 22 18:49:33.786: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:49:33.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6083" for this suite.
Jun 22 18:49:55.799: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:49:55.886: INFO: namespace container-lifecycle-hook-6083 deletion completed in 22.096311104s

• [SLOW TEST:36.164 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:49:55.886: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jun 22 18:50:25.945: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W0622 18:50:25.945015      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:50:25.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2203" for this suite.
Jun 22 18:50:31.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:50:32.039: INFO: namespace gc-2203 deletion completed in 6.091965806s

• [SLOW TEST:36.153 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:50:32.039: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Jun 22 18:50:32.063: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-689426708 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:50:32.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6547" for this suite.
Jun 22 18:50:38.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:50:38.266: INFO: namespace kubectl-6547 deletion completed in 6.087793992s

• [SLOW TEST:6.227 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:50:38.268: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jun 22 18:50:38.289: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:50:41.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5344" for this suite.
Jun 22 18:50:47.673: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:50:47.754: INFO: namespace init-container-5344 deletion completed in 6.092276814s

• [SLOW TEST:9.486 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:50:47.754: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-498d390d-2e7d-427b-ab94-d1f06354955a
STEP: Creating a pod to test consume configMaps
Jun 22 18:50:47.787: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-6125b21a-9018-4d8a-b581-ad6bf341927f" in namespace "projected-9774" to be "success or failure"
Jun 22 18:50:47.790: INFO: Pod "pod-projected-configmaps-6125b21a-9018-4d8a-b581-ad6bf341927f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.345427ms
Jun 22 18:50:49.793: INFO: Pod "pod-projected-configmaps-6125b21a-9018-4d8a-b581-ad6bf341927f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006312456s
STEP: Saw pod success
Jun 22 18:50:49.793: INFO: Pod "pod-projected-configmaps-6125b21a-9018-4d8a-b581-ad6bf341927f" satisfied condition "success or failure"
Jun 22 18:50:49.795: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-projected-configmaps-6125b21a-9018-4d8a-b581-ad6bf341927f container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 18:50:49.812: INFO: Waiting for pod pod-projected-configmaps-6125b21a-9018-4d8a-b581-ad6bf341927f to disappear
Jun 22 18:50:49.814: INFO: Pod pod-projected-configmaps-6125b21a-9018-4d8a-b581-ad6bf341927f no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:50:49.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9774" for this suite.
Jun 22 18:50:55.827: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:50:55.913: INFO: namespace projected-9774 deletion completed in 6.095463241s

• [SLOW TEST:8.159 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:50:55.913: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:51:01.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-3311" for this suite.
Jun 22 18:51:08.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:51:08.094: INFO: namespace namespaces-3311 deletion completed in 6.100876557s
STEP: Destroying namespace "nsdeletetest-4383" for this suite.
Jun 22 18:51:08.097: INFO: Namespace nsdeletetest-4383 was already deleted
STEP: Destroying namespace "nsdeletetest-8384" for this suite.
Jun 22 18:51:14.118: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:51:14.201: INFO: namespace nsdeletetest-8384 deletion completed in 6.104583874s

• [SLOW TEST:18.288 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:51:14.201: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-b5f644ed-9468-45e4-915d-118130b8744d
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-b5f644ed-9468-45e4-915d-118130b8744d
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:51:20.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9094" for this suite.
Jun 22 18:51:42.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:51:42.373: INFO: namespace projected-9094 deletion completed in 22.09737863s

• [SLOW TEST:28.172 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:51:42.373: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Jun 22 18:51:42.399: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jun 22 18:51:42.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 create -f - --namespace=kubectl-9440'
Jun 22 18:51:42.583: INFO: stderr: ""
Jun 22 18:51:42.583: INFO: stdout: "service/redis-slave created\n"
Jun 22 18:51:42.583: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jun 22 18:51:42.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 create -f - --namespace=kubectl-9440'
Jun 22 18:51:42.756: INFO: stderr: ""
Jun 22 18:51:42.756: INFO: stdout: "service/redis-master created\n"
Jun 22 18:51:42.756: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun 22 18:51:42.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 create -f - --namespace=kubectl-9440'
Jun 22 18:51:42.931: INFO: stderr: ""
Jun 22 18:51:42.931: INFO: stdout: "service/frontend created\n"
Jun 22 18:51:42.932: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jun 22 18:51:42.932: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 create -f - --namespace=kubectl-9440'
Jun 22 18:51:43.103: INFO: stderr: ""
Jun 22 18:51:43.103: INFO: stdout: "deployment.apps/frontend created\n"
Jun 22 18:51:43.103: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 22 18:51:43.103: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 create -f - --namespace=kubectl-9440'
Jun 22 18:51:43.270: INFO: stderr: ""
Jun 22 18:51:43.270: INFO: stdout: "deployment.apps/redis-master created\n"
Jun 22 18:51:43.270: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jun 22 18:51:43.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 create -f - --namespace=kubectl-9440'
Jun 22 18:51:43.427: INFO: stderr: ""
Jun 22 18:51:43.427: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jun 22 18:51:43.427: INFO: Waiting for all frontend pods to be Running.
Jun 22 18:51:58.478: INFO: Waiting for frontend to serve content.
Jun 22 18:52:03.492: INFO: Failed to get response from guestbook. err: <nil>, response: <br />
<b>Fatal error</b>:  Uncaught exception 'Predis\Connection\ConnectionException' with message 'Connection timed out [tcp://redis-slave:6379]' in /usr/local/lib/php/Predis/Connection/AbstractConnection.php:155
Stack trace:
#0 /usr/local/lib/php/Predis/Connection/StreamConnection.php(128): Predis\Connection\AbstractConnection-&gt;onConnectionError('Connection time...', 110)
#1 /usr/local/lib/php/Predis/Connection/StreamConnection.php(178): Predis\Connection\StreamConnection-&gt;createStreamSocket(Object(Predis\Connection\Parameters), 'tcp://redis-sla...', 4)
#2 /usr/local/lib/php/Predis/Connection/StreamConnection.php(100): Predis\Connection\StreamConnection-&gt;tcpStreamInitializer(Object(Predis\Connection\Parameters))
#3 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(81): Predis\Connection\StreamConnection-&gt;createResource()
#4 /usr/local/lib/php/Predis/Connection/StreamConnection.php(258): Predis\Connection\AbstractConnection-&gt;connect()
#5 /usr/local/lib/php/Predis/Connection/AbstractConnection.php(180): Predis\Connection\Stre in <b>/usr/local/lib/php/Predis/Connection/AbstractConnection.php</b> on line <b>155</b><br />

Jun 22 18:52:08.511: INFO: Trying to add a new entry to the guestbook.
Jun 22 18:52:08.534: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Jun 22 18:52:08.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 delete --grace-period=0 --force -f - --namespace=kubectl-9440'
Jun 22 18:52:08.653: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 18:52:08.653: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 18:52:08.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 delete --grace-period=0 --force -f - --namespace=kubectl-9440'
Jun 22 18:52:08.752: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 18:52:08.753: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 18:52:08.753: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 delete --grace-period=0 --force -f - --namespace=kubectl-9440'
Jun 22 18:52:08.854: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 18:52:08.854: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 18:52:08.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 delete --grace-period=0 --force -f - --namespace=kubectl-9440'
Jun 22 18:52:08.945: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 18:52:08.945: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 18:52:08.946: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 delete --grace-period=0 --force -f - --namespace=kubectl-9440'
Jun 22 18:52:09.018: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 18:52:09.018: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jun 22 18:52:09.018: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 delete --grace-period=0 --force -f - --namespace=kubectl-9440'
Jun 22 18:52:09.093: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 18:52:09.093: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:52:09.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9440" for this suite.
Jun 22 18:52:55.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:52:55.185: INFO: namespace kubectl-9440 deletion completed in 46.088918328s

• [SLOW TEST:72.812 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:52:55.185: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-468.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-468.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-468.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-468.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-468.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-468.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 18:52:57.247: INFO: DNS probes using dns-468/dns-test-9630588b-7ec5-4d6c-8199-929e0186345d succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:52:57.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-468" for this suite.
Jun 22 18:53:03.279: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:53:03.368: INFO: namespace dns-468 deletion completed in 6.100482076s

• [SLOW TEST:8.183 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:53:03.369: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-cc15b453-d835-4783-bed1-29e64c591262
STEP: Creating a pod to test consume secrets
Jun 22 18:53:03.397: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-e7cb5d03-b9d4-40fd-8bd6-919841fba89d" in namespace "projected-4912" to be "success or failure"
Jun 22 18:53:03.402: INFO: Pod "pod-projected-secrets-e7cb5d03-b9d4-40fd-8bd6-919841fba89d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.246543ms
Jun 22 18:53:05.405: INFO: Pod "pod-projected-secrets-e7cb5d03-b9d4-40fd-8bd6-919841fba89d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00732799s
STEP: Saw pod success
Jun 22 18:53:05.405: INFO: Pod "pod-projected-secrets-e7cb5d03-b9d4-40fd-8bd6-919841fba89d" satisfied condition "success or failure"
Jun 22 18:53:05.407: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-projected-secrets-e7cb5d03-b9d4-40fd-8bd6-919841fba89d container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 18:53:05.419: INFO: Waiting for pod pod-projected-secrets-e7cb5d03-b9d4-40fd-8bd6-919841fba89d to disappear
Jun 22 18:53:05.422: INFO: Pod pod-projected-secrets-e7cb5d03-b9d4-40fd-8bd6-919841fba89d no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:53:05.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4912" for this suite.
Jun 22 18:53:11.434: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:53:11.512: INFO: namespace projected-4912 deletion completed in 6.087894802s

• [SLOW TEST:8.144 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:53:11.513: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jun 22 18:53:13.551: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:53:13.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4447" for this suite.
Jun 22 18:53:19.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:53:19.654: INFO: namespace container-runtime-4447 deletion completed in 6.090220942s

• [SLOW TEST:8.141 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:53:19.654: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 22 18:53:19.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-4794'
Jun 22 18:53:19.751: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 22 18:53:19.751: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jun 22 18:53:19.767: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-fkr2n]
Jun 22 18:53:19.767: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-fkr2n" in namespace "kubectl-4794" to be "running and ready"
Jun 22 18:53:19.769: INFO: Pod "e2e-test-nginx-rc-fkr2n": Phase="Pending", Reason="", readiness=false. Elapsed: 2.443977ms
Jun 22 18:53:21.772: INFO: Pod "e2e-test-nginx-rc-fkr2n": Phase="Running", Reason="", readiness=true. Elapsed: 2.00554897s
Jun 22 18:53:21.772: INFO: Pod "e2e-test-nginx-rc-fkr2n" satisfied condition "running and ready"
Jun 22 18:53:21.772: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-fkr2n]
Jun 22 18:53:21.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 logs rc/e2e-test-nginx-rc --namespace=kubectl-4794'
Jun 22 18:53:21.874: INFO: stderr: ""
Jun 22 18:53:21.874: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Jun 22 18:53:21.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 delete rc e2e-test-nginx-rc --namespace=kubectl-4794'
Jun 22 18:53:21.940: INFO: stderr: ""
Jun 22 18:53:21.940: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:53:21.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4794" for this suite.
Jun 22 18:53:43.954: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:53:44.034: INFO: namespace kubectl-4794 deletion completed in 22.090240837s

• [SLOW TEST:24.380 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:53:44.034: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0622 18:53:45.150901      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 22 18:53:45.150: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:53:45.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7647" for this suite.
Jun 22 18:53:51.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:53:51.243: INFO: namespace gc-7647 deletion completed in 6.089842337s

• [SLOW TEST:7.209 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:53:51.243: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-469407b5-54a5-43e5-a707-7ddc699dfba4
STEP: Creating secret with name s-test-opt-upd-0bb475be-bd31-4c8c-a6b4-9efb74e73613
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-469407b5-54a5-43e5-a707-7ddc699dfba4
STEP: Updating secret s-test-opt-upd-0bb475be-bd31-4c8c-a6b4-9efb74e73613
STEP: Creating secret with name s-test-opt-create-d06a4f93-ec06-418e-b9fb-bbf1ccdb0125
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:53:55.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3710" for this suite.
Jun 22 18:54:17.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:54:17.433: INFO: namespace projected-3710 deletion completed in 22.091022004s

• [SLOW TEST:26.190 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:54:17.433: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 22 18:54:17.458: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun 22 18:54:22.461: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 22 18:54:22.461: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 22 18:54:24.482: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-1158,SelfLink:/apis/apps/v1/namespaces/deployment-1158/deployments/test-cleanup-deployment,UID:aed4f7ea-c7d3-4fd5-a376-4433c38f90a0,ResourceVersion:16285,Generation:1,CreationTimestamp:2019-06-22 18:54:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-06-22 18:54:22 +0000 UTC 2019-06-22 18:54:22 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-06-22 18:54:23 +0000 UTC 2019-06-22 18:54:22 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jun 22 18:54:24.484: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-1158,SelfLink:/apis/apps/v1/namespaces/deployment-1158/replicasets/test-cleanup-deployment-55bbcbc84c,UID:5df4bc6a-c18b-45dc-a247-8b90f66e9002,ResourceVersion:16275,Generation:1,CreationTimestamp:2019-06-22 18:54:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment aed4f7ea-c7d3-4fd5-a376-4433c38f90a0 0xc002aefd67 0xc002aefd68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 22 18:54:24.487: INFO: Pod "test-cleanup-deployment-55bbcbc84c-fbrtr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-fbrtr,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-1158,SelfLink:/api/v1/namespaces/deployment-1158/pods/test-cleanup-deployment-55bbcbc84c-fbrtr,UID:3dc0f92c-a704-4ecc-b573-871110c68b4c,ResourceVersion:16274,Generation:0,CreationTimestamp:2019-06-22 18:54:22 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.104.163/32,},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c 5df4bc6a-c18b-45dc-a247-8b90f66e9002 0xc002cec487 0xc002cec488}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-6bf6t {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-6bf6t,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-6bf6t true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-20-165,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002cec500} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002cec520}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:22 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:23 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:23 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:22 +0000 UTC  }],Message:,Reason:,HostIP:10.0.20.165,PodIP:10.2.104.163,StartTime:2019-06-22 18:54:22 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-06-22 18:54:23 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://6d07e52348eb74ddc6cdec1f24110b717c53a3eb610177e0ca41c0a1ba400b4b}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:54:24.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1158" for this suite.
Jun 22 18:54:30.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:54:30.575: INFO: namespace deployment-1158 deletion completed in 6.085415543s

• [SLOW TEST:13.142 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:54:30.575: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 22 18:54:30.647: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun 22 18:54:30.654: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jun 22 18:54:32.663: INFO: Creating deployment "test-rolling-update-deployment"
Jun 22 18:54:32.666: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun 22 18:54:32.673: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun 22 18:54:34.679: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun 22 18:54:34.681: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 22 18:54:34.688: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-2843,SelfLink:/apis/apps/v1/namespaces/deployment-2843/deployments/test-rolling-update-deployment,UID:b545426d-996a-4f34-8795-d2aa8c0ed195,ResourceVersion:16387,Generation:1,CreationTimestamp:2019-06-22 18:54:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-06-22 18:54:32 +0000 UTC 2019-06-22 18:54:32 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-06-22 18:54:33 +0000 UTC 2019-06-22 18:54:32 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jun 22 18:54:34.690: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-2843,SelfLink:/apis/apps/v1/namespaces/deployment-2843/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:0ae37a47-4c93-4a07-a4d1-47b8ea811519,ResourceVersion:16377,Generation:1,CreationTimestamp:2019-06-22 18:54:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment b545426d-996a-4f34-8795-d2aa8c0ed195 0xc002314557 0xc002314558}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jun 22 18:54:34.690: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun 22 18:54:34.690: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-2843,SelfLink:/apis/apps/v1/namespaces/deployment-2843/replicasets/test-rolling-update-controller,UID:56d0a620-1e2c-492f-bb9b-0102539ce784,ResourceVersion:16385,Generation:2,CreationTimestamp:2019-06-22 18:54:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment b545426d-996a-4f34-8795-d2aa8c0ed195 0xc002314487 0xc002314488}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 22 18:54:34.693: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-5d7hj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-5d7hj,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-2843,SelfLink:/api/v1/namespaces/deployment-2843/pods/test-rolling-update-deployment-79f6b9d75c-5d7hj,UID:27c54dc3-a6e0-4503-add0-259c1e638e61,ResourceVersion:16376,Generation:0,CreationTimestamp:2019-06-22 18:54:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.145.30/32,},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 0ae37a47-4c93-4a07-a4d1-47b8ea811519 0xc002314e57 0xc002314e58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-45znv {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-45znv,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-45znv true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-42-243,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002314ed0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002314ef0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:32 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:33 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:33 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:32 +0000 UTC  }],Message:,Reason:,HostIP:10.0.42.243,PodIP:10.2.145.30,StartTime:2019-06-22 18:54:32 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-06-22 18:54:33 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://0cab541ea14ef266278e1fdd7f15a2d6e7a618be8a2ab646fe679101e1aa4ea5}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:54:34.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2843" for this suite.
Jun 22 18:54:40.705: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:54:40.782: INFO: namespace deployment-2843 deletion completed in 6.086291777s

• [SLOW TEST:10.207 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:54:40.782: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 22 18:54:40.802: INFO: Creating deployment "nginx-deployment"
Jun 22 18:54:40.806: INFO: Waiting for observed generation 1
Jun 22 18:54:42.812: INFO: Waiting for all required pods to come up
Jun 22 18:54:42.815: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jun 22 18:54:44.825: INFO: Waiting for deployment "nginx-deployment" to complete
Jun 22 18:54:44.829: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jun 22 18:54:44.835: INFO: Updating deployment nginx-deployment
Jun 22 18:54:44.835: INFO: Waiting for observed generation 2
Jun 22 18:54:46.841: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun 22 18:54:46.843: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun 22 18:54:46.845: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jun 22 18:54:46.852: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun 22 18:54:46.852: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun 22 18:54:46.854: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jun 22 18:54:46.858: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jun 22 18:54:46.858: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jun 22 18:54:46.864: INFO: Updating deployment nginx-deployment
Jun 22 18:54:46.864: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jun 22 18:54:46.869: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun 22 18:54:48.882: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jun 22 18:54:48.888: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-9378,SelfLink:/apis/apps/v1/namespaces/deployment-9378/deployments/nginx-deployment,UID:de119296-9440-4eb0-9e61-2ecc7fb669ab,ResourceVersion:16770,Generation:3,CreationTimestamp:2019-06-22 18:54:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[{Available False 2019-06-22 18:54:46 +0000 UTC 2019-06-22 18:54:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-06-22 18:54:46 +0000 UTC 2019-06-22 18:54:40 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.}],ReadyReplicas:8,CollisionCount:nil,},}

Jun 22 18:54:48.891: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-9378,SelfLink:/apis/apps/v1/namespaces/deployment-9378/replicasets/nginx-deployment-55fb7cb77f,UID:3e7f4db9-7b7b-405c-a067-30156fb78da0,ResourceVersion:16756,Generation:3,CreationTimestamp:2019-06-22 18:54:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment de119296-9440-4eb0-9e61-2ecc7fb669ab 0xc002e73487 0xc002e73488}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jun 22 18:54:48.891: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jun 22 18:54:48.891: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-9378,SelfLink:/apis/apps/v1/namespaces/deployment-9378/replicasets/nginx-deployment-7b8c6f4498,UID:65549839-05c3-4146-ac6d-bf654c119c81,ResourceVersion:16761,Generation:3,CreationTimestamp:2019-06-22 18:54:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment de119296-9440-4eb0-9e61-2ecc7fb669ab 0xc002e73557 0xc002e73558}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jun 22 18:54:48.908: INFO: Pod "nginx-deployment-55fb7cb77f-29vf5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-29vf5,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-55fb7cb77f-29vf5,UID:c9038650-dcf9-424e-9003-8cba10faff7c,ResourceVersion:16774,Generation:0,CreationTimestamp:2019-06-22 18:54:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.133.56/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 3e7f4db9-7b7b-405c-a067-30156fb78da0 0xc002e73ef7 0xc002e73ef8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-15-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002e73f70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002e73f90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:44 +0000 UTC  }],Message:,Reason:,HostIP:10.0.15.125,PodIP:10.2.133.56,StartTime:2019-06-22 18:54:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "nginx:404",} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.913: INFO: Pod "nginx-deployment-55fb7cb77f-2pm4f" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-2pm4f,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-55fb7cb77f-2pm4f,UID:e7e3d7e9-7d47-452c-b0bc-4af382b6d12d,ResourceVersion:16691,Generation:0,CreationTimestamp:2019-06-22 18:54:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.145.34/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 3e7f4db9-7b7b-405c-a067-30156fb78da0 0xc002a04090 0xc002a04091}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-42-243,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a04110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a04130}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:44 +0000 UTC  }],Message:,Reason:,HostIP:10.0.42.243,PodIP:10.2.145.34,StartTime:2019-06-22 18:54:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.914: INFO: Pod "nginx-deployment-55fb7cb77f-7z7n6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-7z7n6,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-55fb7cb77f-7z7n6,UID:00e7740b-a6d9-4839-a574-b16cb8fada5e,ResourceVersion:16753,Generation:0,CreationTimestamp:2019-06-22 18:54:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 3e7f4db9-7b7b-405c-a067-30156fb78da0 0xc002a04220 0xc002a04221}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-20-165,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a042a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a042c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  }],Message:,Reason:,HostIP:10.0.20.165,PodIP:,StartTime:2019-06-22 18:54:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.917: INFO: Pod "nginx-deployment-55fb7cb77f-9j8jg" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-9j8jg,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-55fb7cb77f-9j8jg,UID:ea9f69c0-2644-402f-94dd-2fb1604ae1d5,ResourceVersion:16844,Generation:0,CreationTimestamp:2019-06-22 18:54:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.145.39/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 3e7f4db9-7b7b-405c-a067-30156fb78da0 0xc002a043a0 0xc002a043a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-42-243,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a04420} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a04440}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  }],Message:,Reason:,HostIP:10.0.42.243,PodIP:,StartTime:2019-06-22 18:54:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.918: INFO: Pod "nginx-deployment-55fb7cb77f-9p6l6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-9p6l6,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-55fb7cb77f-9p6l6,UID:9cd4c84d-23f0-428d-add4-1563a30e56e4,ResourceVersion:16841,Generation:0,CreationTimestamp:2019-06-22 18:54:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.104.169/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 3e7f4db9-7b7b-405c-a067-30156fb78da0 0xc002a04520 0xc002a04521}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-20-165,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a045a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a045c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:44 +0000 UTC  }],Message:,Reason:,HostIP:10.0.20.165,PodIP:10.2.104.169,StartTime:2019-06-22 18:54:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.923: INFO: Pod "nginx-deployment-55fb7cb77f-bzhc9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-bzhc9,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-55fb7cb77f-bzhc9,UID:c6387e61-a7af-4253-9c4a-0275e8308f39,ResourceVersion:16775,Generation:0,CreationTimestamp:2019-06-22 18:54:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 3e7f4db9-7b7b-405c-a067-30156fb78da0 0xc002a046b0 0xc002a046b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-20-165,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a04730} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a04750}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  }],Message:,Reason:,HostIP:10.0.20.165,PodIP:,StartTime:2019-06-22 18:54:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.925: INFO: Pod "nginx-deployment-55fb7cb77f-ff7tf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-ff7tf,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-55fb7cb77f-ff7tf,UID:21b7136f-b3a1-4a9c-adb1-71553c4af2b9,ResourceVersion:16781,Generation:0,CreationTimestamp:2019-06-22 18:54:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 3e7f4db9-7b7b-405c-a067-30156fb78da0 0xc002a04820 0xc002a04821}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-15-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a048a0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a048c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  }],Message:,Reason:,HostIP:10.0.15.125,PodIP:,StartTime:2019-06-22 18:54:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.926: INFO: Pod "nginx-deployment-55fb7cb77f-h2z7j" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-h2z7j,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-55fb7cb77f-h2z7j,UID:a032b670-85f5-4444-9098-50623efa6ea8,ResourceVersion:16800,Generation:0,CreationTimestamp:2019-06-22 18:54:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 3e7f4db9-7b7b-405c-a067-30156fb78da0 0xc002a04990 0xc002a04991}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-20-165,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a04a10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a04a30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  }],Message:,Reason:,HostIP:10.0.20.165,PodIP:,StartTime:2019-06-22 18:54:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.930: INFO: Pod "nginx-deployment-55fb7cb77f-ptcvm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-ptcvm,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-55fb7cb77f-ptcvm,UID:08f3a929-acd2-4a12-8e97-6d524486ea8b,ResourceVersion:16822,Generation:0,CreationTimestamp:2019-06-22 18:54:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.145.36/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 3e7f4db9-7b7b-405c-a067-30156fb78da0 0xc002a04b10 0xc002a04b11}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-42-243,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a04b90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a04bb0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:44 +0000 UTC  }],Message:,Reason:,HostIP:10.0.42.243,PodIP:10.2.145.36,StartTime:2019-06-22 18:54:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "nginx:404",} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.931: INFO: Pod "nginx-deployment-55fb7cb77f-q5s42" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-q5s42,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-55fb7cb77f-q5s42,UID:5fad5099-9499-408c-865d-782ae78c993c,ResourceVersion:16865,Generation:0,CreationTimestamp:2019-06-22 18:54:44 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.104.170/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 3e7f4db9-7b7b-405c-a067-30156fb78da0 0xc002a04cb0 0xc002a04cb1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-20-165,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a04d30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a04d50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:44 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:44 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:44 +0000 UTC  }],Message:,Reason:,HostIP:10.0.20.165,PodIP:10.2.104.170,StartTime:2019-06-22 18:54:44 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: manifest for nginx:404 not found,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.935: INFO: Pod "nginx-deployment-55fb7cb77f-vkxhd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-vkxhd,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-55fb7cb77f-vkxhd,UID:dfad4da9-d79d-4909-b649-1962775d2751,ResourceVersion:16717,Generation:0,CreationTimestamp:2019-06-22 18:54:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 3e7f4db9-7b7b-405c-a067-30156fb78da0 0xc002a04e40 0xc002a04e41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-15-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a04ec0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a04ee0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  }],Message:,Reason:,HostIP:10.0.15.125,PodIP:,StartTime:2019-06-22 18:54:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.940: INFO: Pod "nginx-deployment-55fb7cb77f-xfc46" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-xfc46,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-55fb7cb77f-xfc46,UID:fa0c0ef8-d691-489e-8bf7-e65c6e3e086a,ResourceVersion:16843,Generation:0,CreationTimestamp:2019-06-22 18:54:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.145.38/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 3e7f4db9-7b7b-405c-a067-30156fb78da0 0xc002a04fc0 0xc002a04fc1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-42-243,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a05040} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a05060}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  }],Message:,Reason:,HostIP:10.0.42.243,PodIP:,StartTime:2019-06-22 18:54:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.940: INFO: Pod "nginx-deployment-55fb7cb77f-xksgd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-xksgd,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-55fb7cb77f-xksgd,UID:9d3d79b4-a11c-4843-ad88-6ccb3d3259f0,ResourceVersion:16767,Generation:0,CreationTimestamp:2019-06-22 18:54:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 3e7f4db9-7b7b-405c-a067-30156fb78da0 0xc002a05130 0xc002a05131}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-15-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a051b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a051d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  }],Message:,Reason:,HostIP:10.0.15.125,PodIP:,StartTime:2019-06-22 18:54:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.943: INFO: Pod "nginx-deployment-7b8c6f4498-25dtj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-25dtj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-7b8c6f4498-25dtj,UID:c1aac276-8001-47bb-b2b5-92457cb80a94,ResourceVersion:16858,Generation:0,CreationTimestamp:2019-06-22 18:54:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.145.41/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 65549839-05c3-4146-ac6d-bf654c119c81 0xc002a052b0 0xc002a052b1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-42-243,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a05320} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a05340}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  }],Message:,Reason:,HostIP:10.0.42.243,PodIP:,StartTime:2019-06-22 18:54:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.943: INFO: Pod "nginx-deployment-7b8c6f4498-4s9d9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-4s9d9,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-7b8c6f4498-4s9d9,UID:550e279e-c54e-42b6-93b1-8a9b2182f5ec,ResourceVersion:16584,Generation:0,CreationTimestamp:2019-06-22 18:54:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.104.167/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 65549839-05c3-4146-ac6d-bf654c119c81 0xc002a05410 0xc002a05411}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-20-165,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a05480} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a054a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:40 +0000 UTC  }],Message:,Reason:,HostIP:10.0.20.165,PodIP:10.2.104.167,StartTime:2019-06-22 18:54:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-22 18:54:42 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://1dcd7c28432ddd6e0f217660b5952f224f8fcd6eb71ed44def4ccf6ebc35bded}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.943: INFO: Pod "nginx-deployment-7b8c6f4498-5crxl" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-5crxl,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-7b8c6f4498-5crxl,UID:dff7647b-a8a1-4dcd-a61f-732f65588a31,ResourceVersion:16575,Generation:0,CreationTimestamp:2019-06-22 18:54:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.133.55/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 65549839-05c3-4146-ac6d-bf654c119c81 0xc002a05580 0xc002a05581}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-15-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a055f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a05610}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:40 +0000 UTC  }],Message:,Reason:,HostIP:10.0.15.125,PodIP:10.2.133.55,StartTime:2019-06-22 18:54:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-22 18:54:42 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://17be7dd22754f19f9424acb27c2a90099341cab207d2443d8edcb349c890de9c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.945: INFO: Pod "nginx-deployment-7b8c6f4498-865bl" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-865bl,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-7b8c6f4498-865bl,UID:0ed299c7-470e-48a0-854c-15e45eb2e687,ResourceVersion:16821,Generation:0,CreationTimestamp:2019-06-22 18:54:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 65549839-05c3-4146-ac6d-bf654c119c81 0xc002a056e0 0xc002a056e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-20-165,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a05750} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a05770}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  }],Message:,Reason:,HostIP:10.0.20.165,PodIP:,StartTime:2019-06-22 18:54:47 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.946: INFO: Pod "nginx-deployment-7b8c6f4498-c2sqf" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-c2sqf,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-7b8c6f4498-c2sqf,UID:f113d741-a17b-4aa6-a5a0-ba1e24e6d6af,ResourceVersion:16832,Generation:0,CreationTimestamp:2019-06-22 18:54:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.104.171/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 65549839-05c3-4146-ac6d-bf654c119c81 0xc002a05840 0xc002a05841}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-20-165,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a058b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a058d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  }],Message:,Reason:,HostIP:10.0.20.165,PodIP:,StartTime:2019-06-22 18:54:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.947: INFO: Pod "nginx-deployment-7b8c6f4498-cs46z" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-cs46z,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-7b8c6f4498-cs46z,UID:5e83e59a-79eb-4ed8-8b12-df2d23da52ff,ResourceVersion:16562,Generation:0,CreationTimestamp:2019-06-22 18:54:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.145.31/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 65549839-05c3-4146-ac6d-bf654c119c81 0xc002a059a0 0xc002a059a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-42-243,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a05a10} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a05a30}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:40 +0000 UTC  }],Message:,Reason:,HostIP:10.0.42.243,PodIP:10.2.145.31,StartTime:2019-06-22 18:54:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-22 18:54:42 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://d9462b9355c0814cb0ed876a979d3a9c73ab72fd2c91df8dc5a672343c6c28a8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.948: INFO: Pod "nginx-deployment-7b8c6f4498-dt8dm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-dt8dm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-7b8c6f4498-dt8dm,UID:a3182cc8-1c47-49df-bb60-6ad45b490a47,ResourceVersion:16812,Generation:0,CreationTimestamp:2019-06-22 18:54:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 65549839-05c3-4146-ac6d-bf654c119c81 0xc002a05b00 0xc002a05b01}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-20-165,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a05b70} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a05b90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:47 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  }],Message:,Reason:,HostIP:10.0.20.165,PodIP:,StartTime:2019-06-22 18:54:47 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.952: INFO: Pod "nginx-deployment-7b8c6f4498-fhqgr" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fhqgr,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-7b8c6f4498-fhqgr,UID:c9435180-7934-419f-b88b-21c91b0a1f6c,ResourceVersion:16565,Generation:0,CreationTimestamp:2019-06-22 18:54:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.145.33/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 65549839-05c3-4146-ac6d-bf654c119c81 0xc002a05c60 0xc002a05c61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-42-243,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a05cd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a05cf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:40 +0000 UTC  }],Message:,Reason:,HostIP:10.0.42.243,PodIP:10.2.145.33,StartTime:2019-06-22 18:54:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-22 18:54:42 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://130f83db65c80c17d27e97f762e8479431a7e1d748ddc749d528fb24a26056b8}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.952: INFO: Pod "nginx-deployment-7b8c6f4498-npfzf" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-npfzf,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-7b8c6f4498-npfzf,UID:b1ec439a-4b6e-47ea-ac33-8ed2507748d6,ResourceVersion:16572,Generation:0,CreationTimestamp:2019-06-22 18:54:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.133.53/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 65549839-05c3-4146-ac6d-bf654c119c81 0xc002a05dd0 0xc002a05dd1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-15-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a05e40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a05e60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:40 +0000 UTC  }],Message:,Reason:,HostIP:10.0.15.125,PodIP:10.2.133.53,StartTime:2019-06-22 18:54:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-22 18:54:42 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://fbf0d0585112250f04dd88243be055057946b2639fa74eca7577959f198b840a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.952: INFO: Pod "nginx-deployment-7b8c6f4498-ntl4r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-ntl4r,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-7b8c6f4498-ntl4r,UID:992f66d5-3625-4877-978c-a2e0e6f2ff20,ResourceVersion:16860,Generation:0,CreationTimestamp:2019-06-22 18:54:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.133.58/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 65549839-05c3-4146-ac6d-bf654c119c81 0xc002a05f40 0xc002a05f41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-15-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002a05fb0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002a05fd0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  }],Message:,Reason:,HostIP:10.0.15.125,PodIP:,StartTime:2019-06-22 18:54:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.952: INFO: Pod "nginx-deployment-7b8c6f4498-r5tsw" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-r5tsw,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-7b8c6f4498-r5tsw,UID:545691b9-63e6-42e4-85d2-a48f61bfc4cb,ResourceVersion:16559,Generation:0,CreationTimestamp:2019-06-22 18:54:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.145.32/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 65549839-05c3-4146-ac6d-bf654c119c81 0xc002db80a0 0xc002db80a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-42-243,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002db8110} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002db8130}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:40 +0000 UTC  }],Message:,Reason:,HostIP:10.0.42.243,PodIP:10.2.145.32,StartTime:2019-06-22 18:54:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-22 18:54:42 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://4abfb59310e291f445c3c5c0dc246ec08161684449cfad16b7808fd371e9ae83}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.952: INFO: Pod "nginx-deployment-7b8c6f4498-r7nq9" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-r7nq9,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-7b8c6f4498-r7nq9,UID:b835c8a2-acdf-4d68-a1dc-bb4a5b088215,ResourceVersion:16569,Generation:0,CreationTimestamp:2019-06-22 18:54:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.133.54/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 65549839-05c3-4146-ac6d-bf654c119c81 0xc002db8210 0xc002db8211}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-15-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002db8280} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002db82a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:42 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:42 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:40 +0000 UTC  }],Message:,Reason:,HostIP:10.0.15.125,PodIP:10.2.133.54,StartTime:2019-06-22 18:54:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-22 18:54:42 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://030b55b69e74680f5beb5a239e384c473d798cf7ddffa70094054819097febfc}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.952: INFO: Pod "nginx-deployment-7b8c6f4498-tk89t" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-tk89t,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-7b8c6f4498-tk89t,UID:953f30c2-38be-4fc2-afdd-7257291f5871,ResourceVersion:16581,Generation:0,CreationTimestamp:2019-06-22 18:54:40 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.104.165/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 65549839-05c3-4146-ac6d-bf654c119c81 0xc002db8380 0xc002db8381}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-20-165,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002db83f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002db8410}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:40 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:43 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:43 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:40 +0000 UTC  }],Message:,Reason:,HostIP:10.0.20.165,PodIP:10.2.104.165,StartTime:2019-06-22 18:54:40 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-06-22 18:54:42 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 docker://c0c60343cfe4ca5bbb95adcebb364d027972d0e2b6620e22323c84424069b48f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.953: INFO: Pod "nginx-deployment-7b8c6f4498-v9rhk" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-v9rhk,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-7b8c6f4498-v9rhk,UID:1ee4a597-1ef2-419a-a81f-cb32dbeef7ea,ResourceVersion:16833,Generation:0,CreationTimestamp:2019-06-22 18:54:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.133.57/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 65549839-05c3-4146-ac6d-bf654c119c81 0xc002db84f0 0xc002db84f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-15-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002db8560} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002db8580}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  }],Message:,Reason:,HostIP:10.0.15.125,PodIP:,StartTime:2019-06-22 18:54:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.953: INFO: Pod "nginx-deployment-7b8c6f4498-wd8z5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-wd8z5,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-7b8c6f4498-wd8z5,UID:f3352fb0-5d33-4923-8ca7-a59668955b21,ResourceVersion:16783,Generation:0,CreationTimestamp:2019-06-22 18:54:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 65549839-05c3-4146-ac6d-bf654c119c81 0xc002db8640 0xc002db8641}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-20-165,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002db86b0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002db86d0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  }],Message:,Reason:,HostIP:10.0.20.165,PodIP:,StartTime:2019-06-22 18:54:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.953: INFO: Pod "nginx-deployment-7b8c6f4498-wp5kd" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-wp5kd,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-7b8c6f4498-wp5kd,UID:3fb05250-5cd9-4943-82ae-78ea7b196274,ResourceVersion:16732,Generation:0,CreationTimestamp:2019-06-22 18:54:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 65549839-05c3-4146-ac6d-bf654c119c81 0xc002db8790 0xc002db8791}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-20-165,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002db8800} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002db8820}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  }],Message:,Reason:,HostIP:10.0.20.165,PodIP:,StartTime:2019-06-22 18:54:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.953: INFO: Pod "nginx-deployment-7b8c6f4498-wttcp" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-wttcp,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-7b8c6f4498-wttcp,UID:91e647f5-633c-443c-845c-820fe2a53c76,ResourceVersion:16820,Generation:0,CreationTimestamp:2019-06-22 18:54:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 65549839-05c3-4146-ac6d-bf654c119c81 0xc002db88e0 0xc002db88e1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-15-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002db8950} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002db8970}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  }],Message:,Reason:,HostIP:10.0.15.125,PodIP:,StartTime:2019-06-22 18:54:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.953: INFO: Pod "nginx-deployment-7b8c6f4498-xwz4t" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xwz4t,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-7b8c6f4498-xwz4t,UID:aa10310b-86b3-4403-9335-cf209ce5c46f,ResourceVersion:16799,Generation:0,CreationTimestamp:2019-06-22 18:54:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 65549839-05c3-4146-ac6d-bf654c119c81 0xc002db8a30 0xc002db8a31}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-15-125,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002db8aa0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002db8ac0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  }],Message:,Reason:,HostIP:10.0.15.125,PodIP:,StartTime:2019-06-22 18:54:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.954: INFO: Pod "nginx-deployment-7b8c6f4498-xxjwb" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xxjwb,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-7b8c6f4498-xxjwb,UID:5635a84c-e342-4197-b361-a316480bd058,ResourceVersion:16857,Generation:0,CreationTimestamp:2019-06-22 18:54:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.145.40/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 65549839-05c3-4146-ac6d-bf654c119c81 0xc002db8b90 0xc002db8b91}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-42-243,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002db8c00} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002db8c20}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  }],Message:,Reason:,HostIP:10.0.42.243,PodIP:,StartTime:2019-06-22 18:54:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jun 22 18:54:48.954: INFO: Pod "nginx-deployment-7b8c6f4498-zmvx6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-zmvx6,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-9378,SelfLink:/api/v1/namespaces/deployment-9378/pods/nginx-deployment-7b8c6f4498-zmvx6,UID:fd5414af-486a-43ee-8acc-ce97dbbca61f,ResourceVersion:16831,Generation:0,CreationTimestamp:2019-06-22 18:54:46 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{cni.projectcalico.org/podIP: 10.2.145.37/32,},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 65549839-05c3-4146-ac6d-bf654c119c81 0xc002db8cf0 0xc002db8cf1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-56kmp {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-56kmp,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-56kmp true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ip-10-0-42-243,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002db8d60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002db8d80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-06-22 18:54:46 +0000 UTC  }],Message:,Reason:,HostIP:10.0.42.243,PodIP:,StartTime:2019-06-22 18:54:46 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:54:48.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9378" for this suite.
Jun 22 18:54:54.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:54:55.081: INFO: namespace deployment-9378 deletion completed in 6.119778945s

• [SLOW TEST:14.300 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:54:55.082: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-1a2527af-8f9f-48e4-b468-52a0180150dc
STEP: Creating configMap with name cm-test-opt-upd-69d10b3e-471e-4633-8686-4860b4a11a11
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-1a2527af-8f9f-48e4-b468-52a0180150dc
STEP: Updating configmap cm-test-opt-upd-69d10b3e-471e-4633-8686-4860b4a11a11
STEP: Creating configMap with name cm-test-opt-create-e7225607-81fa-4a47-85a1-75767f189ae5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:55:03.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7858" for this suite.
Jun 22 18:55:25.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:55:25.291: INFO: namespace configmap-7858 deletion completed in 22.088739005s

• [SLOW TEST:30.209 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:55:25.291: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 22 18:55:25.319: INFO: Waiting up to 5m0s for pod "downwardapi-volume-46b70708-e6e7-4979-af80-a1508b707aac" in namespace "downward-api-604" to be "success or failure"
Jun 22 18:55:25.323: INFO: Pod "downwardapi-volume-46b70708-e6e7-4979-af80-a1508b707aac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.431727ms
Jun 22 18:55:27.327: INFO: Pod "downwardapi-volume-46b70708-e6e7-4979-af80-a1508b707aac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008123649s
STEP: Saw pod success
Jun 22 18:55:27.327: INFO: Pod "downwardapi-volume-46b70708-e6e7-4979-af80-a1508b707aac" satisfied condition "success or failure"
Jun 22 18:55:27.329: INFO: Trying to get logs from node ip-10-0-20-165 pod downwardapi-volume-46b70708-e6e7-4979-af80-a1508b707aac container client-container: <nil>
STEP: delete the pod
Jun 22 18:55:27.357: INFO: Waiting for pod downwardapi-volume-46b70708-e6e7-4979-af80-a1508b707aac to disappear
Jun 22 18:55:27.359: INFO: Pod downwardapi-volume-46b70708-e6e7-4979-af80-a1508b707aac no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:55:27.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-604" for this suite.
Jun 22 18:55:33.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:55:33.453: INFO: namespace downward-api-604 deletion completed in 6.090186216s

• [SLOW TEST:8.162 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:55:33.454: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jun 22 18:55:33.475: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:55:36.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6883" for this suite.
Jun 22 18:55:42.137: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:55:42.221: INFO: namespace init-container-6883 deletion completed in 6.094428803s

• [SLOW TEST:8.767 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:55:42.222: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-4de4de49-b05f-461e-a47f-0bde2823391c
STEP: Creating a pod to test consume secrets
Jun 22 18:55:42.251: INFO: Waiting up to 5m0s for pod "pod-secrets-aafe5c14-131b-4f39-ba67-e91ecf615d31" in namespace "secrets-144" to be "success or failure"
Jun 22 18:55:42.256: INFO: Pod "pod-secrets-aafe5c14-131b-4f39-ba67-e91ecf615d31": Phase="Pending", Reason="", readiness=false. Elapsed: 4.522789ms
Jun 22 18:55:44.259: INFO: Pod "pod-secrets-aafe5c14-131b-4f39-ba67-e91ecf615d31": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007519923s
STEP: Saw pod success
Jun 22 18:55:44.259: INFO: Pod "pod-secrets-aafe5c14-131b-4f39-ba67-e91ecf615d31" satisfied condition "success or failure"
Jun 22 18:55:44.261: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-secrets-aafe5c14-131b-4f39-ba67-e91ecf615d31 container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 18:55:44.275: INFO: Waiting for pod pod-secrets-aafe5c14-131b-4f39-ba67-e91ecf615d31 to disappear
Jun 22 18:55:44.277: INFO: Pod pod-secrets-aafe5c14-131b-4f39-ba67-e91ecf615d31 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:55:44.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-144" for this suite.
Jun 22 18:55:50.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:55:50.373: INFO: namespace secrets-144 deletion completed in 6.093042772s

• [SLOW TEST:8.151 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:55:50.373: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jun 22 18:55:56.424: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
W0622 18:55:56.424246      15 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jun 22 18:55:56.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6789" for this suite.
Jun 22 18:56:02.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:56:02.520: INFO: namespace gc-6789 deletion completed in 6.093088008s

• [SLOW TEST:12.147 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:56:02.520: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jun 22 18:56:02.593: INFO: namespace kubectl-3620
Jun 22 18:56:02.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 create -f - --namespace=kubectl-3620'
Jun 22 18:56:02.755: INFO: stderr: ""
Jun 22 18:56:02.755: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jun 22 18:56:03.759: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 18:56:03.759: INFO: Found 0 / 1
Jun 22 18:56:04.759: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 18:56:04.759: INFO: Found 1 / 1
Jun 22 18:56:04.759: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 22 18:56:04.761: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 18:56:04.761: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 22 18:56:04.761: INFO: wait on redis-master startup in kubectl-3620 
Jun 22 18:56:04.761: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 logs redis-master-qq65q redis-master --namespace=kubectl-3620'
Jun 22 18:56:04.846: INFO: stderr: ""
Jun 22 18:56:04.846: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 Jun 18:56:03.642 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 Jun 18:56:03.642 # Server started, Redis version 3.2.12\n1:M 22 Jun 18:56:03.642 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 Jun 18:56:03.642 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jun 22 18:56:04.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-3620'
Jun 22 18:56:04.928: INFO: stderr: ""
Jun 22 18:56:04.928: INFO: stdout: "service/rm2 exposed\n"
Jun 22 18:56:04.932: INFO: Service rm2 in namespace kubectl-3620 found.
STEP: exposing service
Jun 22 18:56:06.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-3620'
Jun 22 18:56:07.017: INFO: stderr: ""
Jun 22 18:56:07.017: INFO: stdout: "service/rm3 exposed\n"
Jun 22 18:56:07.022: INFO: Service rm3 in namespace kubectl-3620 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:56:09.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3620" for this suite.
Jun 22 18:56:31.045: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:56:31.130: INFO: namespace kubectl-3620 deletion completed in 22.098750988s

• [SLOW TEST:28.610 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:56:31.130: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8863.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8863.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8863.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8863.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 18:56:33.179: INFO: DNS probes using dns-test-b58f88ad-b6ea-4c1a-8522-ea46d7564be5 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8863.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8863.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8863.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8863.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 18:56:35.210: INFO: File wheezy_udp@dns-test-service-3.dns-8863.svc.cluster.local from pod  dns-8863/dns-test-69c13d3f-3f0b-4ef5-af71-6b29bd27c254 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jun 22 18:56:35.213: INFO: Lookups using dns-8863/dns-test-69c13d3f-3f0b-4ef5-af71-6b29bd27c254 failed for: [wheezy_udp@dns-test-service-3.dns-8863.svc.cluster.local]

Jun 22 18:56:40.221: INFO: DNS probes using dns-test-69c13d3f-3f0b-4ef5-af71-6b29bd27c254 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8863.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8863.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8863.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8863.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jun 22 18:56:42.276: INFO: DNS probes using dns-test-7eab2b3b-094b-42aa-afb5-48d0458dcfd8 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:56:42.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8863" for this suite.
Jun 22 18:56:48.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:56:48.412: INFO: namespace dns-8863 deletion completed in 6.102537663s

• [SLOW TEST:17.282 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:56:48.413: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 22 18:56:48.445: INFO: Waiting up to 5m0s for pod "downward-api-5a409818-2c3c-409d-b551-6cb9876b1ec0" in namespace "downward-api-4042" to be "success or failure"
Jun 22 18:56:48.448: INFO: Pod "downward-api-5a409818-2c3c-409d-b551-6cb9876b1ec0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.194925ms
Jun 22 18:56:50.451: INFO: Pod "downward-api-5a409818-2c3c-409d-b551-6cb9876b1ec0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006028971s
STEP: Saw pod success
Jun 22 18:56:50.451: INFO: Pod "downward-api-5a409818-2c3c-409d-b551-6cb9876b1ec0" satisfied condition "success or failure"
Jun 22 18:56:50.453: INFO: Trying to get logs from node ip-10-0-20-165 pod downward-api-5a409818-2c3c-409d-b551-6cb9876b1ec0 container dapi-container: <nil>
STEP: delete the pod
Jun 22 18:56:50.469: INFO: Waiting for pod downward-api-5a409818-2c3c-409d-b551-6cb9876b1ec0 to disappear
Jun 22 18:56:50.471: INFO: Pod downward-api-5a409818-2c3c-409d-b551-6cb9876b1ec0 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:56:50.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4042" for this suite.
Jun 22 18:56:56.483: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:56:56.571: INFO: namespace downward-api-4042 deletion completed in 6.096864332s

• [SLOW TEST:8.158 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:56:56.571: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-a2889cf7-20e3-42b0-8999-9c8e2e985a9a
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 18:56:58.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2310" for this suite.
Jun 22 18:57:20.637: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 18:57:20.718: INFO: namespace configmap-2310 deletion completed in 22.094611658s

• [SLOW TEST:24.147 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 18:57:20.718: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-257bc1bf-0eb4-4952-adeb-278e9f5fde74 in namespace container-probe-1050
Jun 22 18:57:22.763: INFO: Started pod test-webserver-257bc1bf-0eb4-4952-adeb-278e9f5fde74 in namespace container-probe-1050
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 18:57:22.765: INFO: Initial restart count of pod test-webserver-257bc1bf-0eb4-4952-adeb-278e9f5fde74 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:01:23.177: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1050" for this suite.
Jun 22 19:01:29.195: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:01:29.277: INFO: namespace container-probe-1050 deletion completed in 6.092057272s

• [SLOW TEST:248.559 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:01:29.278: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jun 22 19:01:29.305: INFO: Waiting up to 5m0s for pod "downward-api-7057739a-8703-4096-ae00-7ba81897888a" in namespace "downward-api-5913" to be "success or failure"
Jun 22 19:01:29.308: INFO: Pod "downward-api-7057739a-8703-4096-ae00-7ba81897888a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.565122ms
Jun 22 19:01:31.311: INFO: Pod "downward-api-7057739a-8703-4096-ae00-7ba81897888a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005819044s
STEP: Saw pod success
Jun 22 19:01:31.311: INFO: Pod "downward-api-7057739a-8703-4096-ae00-7ba81897888a" satisfied condition "success or failure"
Jun 22 19:01:31.313: INFO: Trying to get logs from node ip-10-0-20-165 pod downward-api-7057739a-8703-4096-ae00-7ba81897888a container dapi-container: <nil>
STEP: delete the pod
Jun 22 19:01:31.330: INFO: Waiting for pod downward-api-7057739a-8703-4096-ae00-7ba81897888a to disappear
Jun 22 19:01:31.333: INFO: Pod downward-api-7057739a-8703-4096-ae00-7ba81897888a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:01:31.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5913" for this suite.
Jun 22 19:01:37.346: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:01:37.431: INFO: namespace downward-api-5913 deletion completed in 6.095919686s

• [SLOW TEST:8.153 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:01:37.433: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jun 22 19:01:37.457: INFO: Waiting up to 5m0s for pod "pod-908822ee-15ff-4597-a7c6-0d96ec3e6978" in namespace "emptydir-5857" to be "success or failure"
Jun 22 19:01:37.462: INFO: Pod "pod-908822ee-15ff-4597-a7c6-0d96ec3e6978": Phase="Pending", Reason="", readiness=false. Elapsed: 5.051781ms
Jun 22 19:01:39.465: INFO: Pod "pod-908822ee-15ff-4597-a7c6-0d96ec3e6978": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008301778s
STEP: Saw pod success
Jun 22 19:01:39.465: INFO: Pod "pod-908822ee-15ff-4597-a7c6-0d96ec3e6978" satisfied condition "success or failure"
Jun 22 19:01:39.467: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-908822ee-15ff-4597-a7c6-0d96ec3e6978 container test-container: <nil>
STEP: delete the pod
Jun 22 19:01:39.479: INFO: Waiting for pod pod-908822ee-15ff-4597-a7c6-0d96ec3e6978 to disappear
Jun 22 19:01:39.482: INFO: Pod pod-908822ee-15ff-4597-a7c6-0d96ec3e6978 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:01:39.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5857" for this suite.
Jun 22 19:01:45.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:01:45.580: INFO: namespace emptydir-5857 deletion completed in 6.094968856s

• [SLOW TEST:8.148 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:01:45.581: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 22 19:01:45.607: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d093b999-e065-497c-8e23-cf8322dbd89b" in namespace "projected-9753" to be "success or failure"
Jun 22 19:01:45.612: INFO: Pod "downwardapi-volume-d093b999-e065-497c-8e23-cf8322dbd89b": Phase="Pending", Reason="", readiness=false. Elapsed: 4.553695ms
Jun 22 19:01:47.615: INFO: Pod "downwardapi-volume-d093b999-e065-497c-8e23-cf8322dbd89b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007711892s
STEP: Saw pod success
Jun 22 19:01:47.615: INFO: Pod "downwardapi-volume-d093b999-e065-497c-8e23-cf8322dbd89b" satisfied condition "success or failure"
Jun 22 19:01:47.618: INFO: Trying to get logs from node ip-10-0-20-165 pod downwardapi-volume-d093b999-e065-497c-8e23-cf8322dbd89b container client-container: <nil>
STEP: delete the pod
Jun 22 19:01:47.632: INFO: Waiting for pod downwardapi-volume-d093b999-e065-497c-8e23-cf8322dbd89b to disappear
Jun 22 19:01:47.634: INFO: Pod downwardapi-volume-d093b999-e065-497c-8e23-cf8322dbd89b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:01:47.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9753" for this suite.
Jun 22 19:01:53.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:01:53.731: INFO: namespace projected-9753 deletion completed in 6.09336979s

• [SLOW TEST:8.150 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:01:53.731: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-5800038d-893e-4a9f-be38-95df661235bc in namespace container-probe-9463
Jun 22 19:01:55.762: INFO: Started pod liveness-5800038d-893e-4a9f-be38-95df661235bc in namespace container-probe-9463
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 19:01:55.765: INFO: Initial restart count of pod liveness-5800038d-893e-4a9f-be38-95df661235bc is 0
Jun 22 19:02:17.803: INFO: Restart count of pod container-probe-9463/liveness-5800038d-893e-4a9f-be38-95df661235bc is now 1 (22.037789957s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:02:17.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9463" for this suite.
Jun 22 19:02:23.823: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:02:23.905: INFO: namespace container-probe-9463 deletion completed in 6.091080577s

• [SLOW TEST:30.174 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:02:23.906: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-5195
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5195 to expose endpoints map[]
Jun 22 19:02:23.936: INFO: Get endpoints failed (4.448953ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jun 22 19:02:24.939: INFO: successfully validated that service multi-endpoint-test in namespace services-5195 exposes endpoints map[] (1.00753694s elapsed)
STEP: Creating pod pod1 in namespace services-5195
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5195 to expose endpoints map[pod1:[100]]
Jun 22 19:02:26.964: INFO: successfully validated that service multi-endpoint-test in namespace services-5195 exposes endpoints map[pod1:[100]] (2.017328168s elapsed)
STEP: Creating pod pod2 in namespace services-5195
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5195 to expose endpoints map[pod1:[100] pod2:[101]]
Jun 22 19:02:28.994: INFO: successfully validated that service multi-endpoint-test in namespace services-5195 exposes endpoints map[pod1:[100] pod2:[101]] (2.026806857s elapsed)
STEP: Deleting pod pod1 in namespace services-5195
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5195 to expose endpoints map[pod2:[101]]
Jun 22 19:02:29.009: INFO: successfully validated that service multi-endpoint-test in namespace services-5195 exposes endpoints map[pod2:[101]] (10.035238ms elapsed)
STEP: Deleting pod pod2 in namespace services-5195
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5195 to expose endpoints map[]
Jun 22 19:02:30.018: INFO: successfully validated that service multi-endpoint-test in namespace services-5195 exposes endpoints map[] (1.005688888s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:02:30.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5195" for this suite.
Jun 22 19:02:36.056: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:02:36.153: INFO: namespace services-5195 deletion completed in 6.109958571s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:12.248 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:02:36.154: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jun 22 19:02:36.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-7821'
Jun 22 19:02:36.396: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jun 22 19:02:36.396: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Jun 22 19:02:38.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 delete deployment e2e-test-nginx-deployment --namespace=kubectl-7821'
Jun 22 19:02:38.484: INFO: stderr: ""
Jun 22 19:02:38.484: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:02:38.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7821" for this suite.
Jun 22 19:03:00.498: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:03:00.580: INFO: namespace kubectl-7821 deletion completed in 22.091484233s

• [SLOW TEST:24.426 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:03:00.581: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1817
I0622 19:03:00.607830      15 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1817, replica count: 1
I0622 19:03:01.658283      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0622 19:03:02.658488      15 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 22 19:03:02.764: INFO: Created: latency-svc-xjc7z
Jun 22 19:03:02.773: INFO: Got endpoints: latency-svc-xjc7z [14.920563ms]
Jun 22 19:03:02.786: INFO: Created: latency-svc-fwljh
Jun 22 19:03:02.800: INFO: Created: latency-svc-9v4c9
Jun 22 19:03:02.812: INFO: Got endpoints: latency-svc-9v4c9 [38.049512ms]
Jun 22 19:03:02.812: INFO: Got endpoints: latency-svc-fwljh [38.826211ms]
Jun 22 19:03:02.820: INFO: Created: latency-svc-9wlrd
Jun 22 19:03:02.826: INFO: Created: latency-svc-jzlzx
Jun 22 19:03:02.828: INFO: Got endpoints: latency-svc-9wlrd [54.556127ms]
Jun 22 19:03:02.833: INFO: Got endpoints: latency-svc-jzlzx [58.78862ms]
Jun 22 19:03:02.844: INFO: Created: latency-svc-pjdwd
Jun 22 19:03:02.849: INFO: Created: latency-svc-kvpr8
Jun 22 19:03:02.854: INFO: Got endpoints: latency-svc-pjdwd [41.316081ms]
Jun 22 19:03:02.858: INFO: Got endpoints: latency-svc-kvpr8 [83.905737ms]
Jun 22 19:03:02.859: INFO: Created: latency-svc-sg58t
Jun 22 19:03:02.865: INFO: Created: latency-svc-l7zvw
Jun 22 19:03:02.877: INFO: Got endpoints: latency-svc-sg58t [102.119795ms]
Jun 22 19:03:02.877: INFO: Created: latency-svc-p2fwd
Jun 22 19:03:02.878: INFO: Got endpoints: latency-svc-l7zvw [103.87209ms]
Jun 22 19:03:02.887: INFO: Got endpoints: latency-svc-p2fwd [111.996473ms]
Jun 22 19:03:02.892: INFO: Created: latency-svc-pdjwx
Jun 22 19:03:02.896: INFO: Got endpoints: latency-svc-pdjwx [121.071104ms]
Jun 22 19:03:02.900: INFO: Created: latency-svc-ksrrq
Jun 22 19:03:02.910: INFO: Created: latency-svc-kw4hp
Jun 22 19:03:02.920: INFO: Got endpoints: latency-svc-ksrrq [145.800801ms]
Jun 22 19:03:02.922: INFO: Got endpoints: latency-svc-kw4hp [147.220766ms]
Jun 22 19:03:02.926: INFO: Created: latency-svc-7qkx9
Jun 22 19:03:02.926: INFO: Created: latency-svc-w64jb
Jun 22 19:03:02.938: INFO: Created: latency-svc-tj7g5
Jun 22 19:03:02.940: INFO: Got endpoints: latency-svc-w64jb [165.678679ms]
Jun 22 19:03:02.941: INFO: Got endpoints: latency-svc-7qkx9 [165.860454ms]
Jun 22 19:03:02.954: INFO: Created: latency-svc-mvjgl
Jun 22 19:03:02.954: INFO: Got endpoints: latency-svc-tj7g5 [178.828365ms]
Jun 22 19:03:02.958: INFO: Created: latency-svc-kw84h
Jun 22 19:03:02.959: INFO: Got endpoints: latency-svc-mvjgl [184.083474ms]
Jun 22 19:03:02.962: INFO: Created: latency-svc-9cnfw
Jun 22 19:03:02.966: INFO: Got endpoints: latency-svc-kw84h [153.554082ms]
Jun 22 19:03:02.974: INFO: Created: latency-svc-gkfb4
Jun 22 19:03:02.980: INFO: Created: latency-svc-2gsss
Jun 22 19:03:02.980: INFO: Got endpoints: latency-svc-9cnfw [151.716765ms]
Jun 22 19:03:02.984: INFO: Got endpoints: latency-svc-gkfb4 [150.711277ms]
Jun 22 19:03:02.988: INFO: Created: latency-svc-2npv2
Jun 22 19:03:02.997: INFO: Created: latency-svc-9lh7b
Jun 22 19:03:03.008: INFO: Created: latency-svc-xjdnv
Jun 22 19:03:03.017: INFO: Created: latency-svc-8rsnv
Jun 22 19:03:03.030: INFO: Created: latency-svc-2bfmn
Jun 22 19:03:03.038: INFO: Got endpoints: latency-svc-xjdnv [159.513498ms]
Jun 22 19:03:03.039: INFO: Got endpoints: latency-svc-2npv2 [180.101016ms]
Jun 22 19:03:03.039: INFO: Got endpoints: latency-svc-2gsss [185.221488ms]
Jun 22 19:03:03.046: INFO: Got endpoints: latency-svc-2bfmn [150.623473ms]
Jun 22 19:03:03.047: INFO: Got endpoints: latency-svc-9lh7b [169.877318ms]
Jun 22 19:03:03.047: INFO: Got endpoints: latency-svc-8rsnv [160.425352ms]
Jun 22 19:03:03.064: INFO: Created: latency-svc-jscb7
Jun 22 19:03:03.064: INFO: Created: latency-svc-k8795
Jun 22 19:03:03.064: INFO: Got endpoints: latency-svc-k8795 [143.548361ms]
Jun 22 19:03:03.068: INFO: Got endpoints: latency-svc-jscb7 [145.659947ms]
Jun 22 19:03:03.069: INFO: Created: latency-svc-rjbjc
Jun 22 19:03:03.079: INFO: Created: latency-svc-wc8ng
Jun 22 19:03:03.079: INFO: Created: latency-svc-xkl2s
Jun 22 19:03:03.081: INFO: Got endpoints: latency-svc-rjbjc [140.166099ms]
Jun 22 19:03:03.088: INFO: Got endpoints: latency-svc-wc8ng [147.669592ms]
Jun 22 19:03:03.090: INFO: Got endpoints: latency-svc-xkl2s [135.906794ms]
Jun 22 19:03:03.094: INFO: Created: latency-svc-gstr8
Jun 22 19:03:03.115: INFO: Created: latency-svc-f9tzs
Jun 22 19:03:03.117: INFO: Got endpoints: latency-svc-gstr8 [158.018722ms]
Jun 22 19:03:03.121: INFO: Got endpoints: latency-svc-f9tzs [155.125429ms]
Jun 22 19:03:03.125: INFO: Created: latency-svc-l2cjx
Jun 22 19:03:03.127: INFO: Got endpoints: latency-svc-l2cjx [147.015291ms]
Jun 22 19:03:03.132: INFO: Created: latency-svc-cmgws
Jun 22 19:03:03.136: INFO: Created: latency-svc-h8k54
Jun 22 19:03:03.137: INFO: Got endpoints: latency-svc-cmgws [152.889312ms]
Jun 22 19:03:03.144: INFO: Got endpoints: latency-svc-h8k54 [105.764954ms]
Jun 22 19:03:03.157: INFO: Created: latency-svc-lww6j
Jun 22 19:03:03.164: INFO: Created: latency-svc-fjspj
Jun 22 19:03:03.168: INFO: Got endpoints: latency-svc-lww6j [128.626726ms]
Jun 22 19:03:03.175: INFO: Created: latency-svc-zbcnb
Jun 22 19:03:03.181: INFO: Got endpoints: latency-svc-fjspj [134.364102ms]
Jun 22 19:03:03.192: INFO: Created: latency-svc-dpfzn
Jun 22 19:03:03.201: INFO: Created: latency-svc-v4ch6
Jun 22 19:03:03.213: INFO: Created: latency-svc-dt4cj
Jun 22 19:03:03.218: INFO: Created: latency-svc-4brhq
Jun 22 19:03:03.226: INFO: Got endpoints: latency-svc-zbcnb [179.085152ms]
Jun 22 19:03:03.236: INFO: Created: latency-svc-zgd7g
Jun 22 19:03:03.252: INFO: Created: latency-svc-lk6mk
Jun 22 19:03:03.252: INFO: Created: latency-svc-4vhkz
Jun 22 19:03:03.262: INFO: Created: latency-svc-2hfsv
Jun 22 19:03:03.273: INFO: Created: latency-svc-7lfvf
Jun 22 19:03:03.273: INFO: Got endpoints: latency-svc-dpfzn [226.10106ms]
Jun 22 19:03:03.279: INFO: Created: latency-svc-78mbq
Jun 22 19:03:03.289: INFO: Created: latency-svc-52lct
Jun 22 19:03:03.296: INFO: Created: latency-svc-zscwq
Jun 22 19:03:03.301: INFO: Created: latency-svc-zc2zl
Jun 22 19:03:03.314: INFO: Created: latency-svc-9cn22
Jun 22 19:03:03.325: INFO: Got endpoints: latency-svc-v4ch6 [286.447339ms]
Jun 22 19:03:03.332: INFO: Created: latency-svc-kg7xb
Jun 22 19:03:03.336: INFO: Created: latency-svc-njdwh
Jun 22 19:03:03.339: INFO: Created: latency-svc-fc76c
Jun 22 19:03:03.371: INFO: Got endpoints: latency-svc-dt4cj [306.894173ms]
Jun 22 19:03:03.379: INFO: Created: latency-svc-26rqt
Jun 22 19:03:03.419: INFO: Got endpoints: latency-svc-4brhq [351.378351ms]
Jun 22 19:03:03.432: INFO: Created: latency-svc-bn885
Jun 22 19:03:03.470: INFO: Got endpoints: latency-svc-zgd7g [389.74804ms]
Jun 22 19:03:03.480: INFO: Created: latency-svc-spkh2
Jun 22 19:03:03.521: INFO: Got endpoints: latency-svc-lk6mk [432.932429ms]
Jun 22 19:03:03.534: INFO: Created: latency-svc-bcvjk
Jun 22 19:03:03.570: INFO: Got endpoints: latency-svc-4vhkz [480.194683ms]
Jun 22 19:03:03.581: INFO: Created: latency-svc-qv9gl
Jun 22 19:03:03.619: INFO: Got endpoints: latency-svc-2hfsv [501.826392ms]
Jun 22 19:03:03.629: INFO: Created: latency-svc-lxm2q
Jun 22 19:03:03.670: INFO: Got endpoints: latency-svc-7lfvf [549.120653ms]
Jun 22 19:03:03.682: INFO: Created: latency-svc-ppxhj
Jun 22 19:03:03.718: INFO: Got endpoints: latency-svc-78mbq [591.117563ms]
Jun 22 19:03:03.725: INFO: Created: latency-svc-qq5l7
Jun 22 19:03:03.769: INFO: Got endpoints: latency-svc-52lct [632.499893ms]
Jun 22 19:03:03.775: INFO: Created: latency-svc-mxr7c
Jun 22 19:03:03.819: INFO: Got endpoints: latency-svc-zscwq [675.249048ms]
Jun 22 19:03:03.827: INFO: Created: latency-svc-vb7z8
Jun 22 19:03:03.869: INFO: Got endpoints: latency-svc-zc2zl [701.52598ms]
Jun 22 19:03:03.878: INFO: Created: latency-svc-zw5pv
Jun 22 19:03:03.919: INFO: Got endpoints: latency-svc-9cn22 [738.225823ms]
Jun 22 19:03:03.928: INFO: Created: latency-svc-2b7mz
Jun 22 19:03:03.969: INFO: Got endpoints: latency-svc-kg7xb [742.617654ms]
Jun 22 19:03:03.975: INFO: Created: latency-svc-sswlh
Jun 22 19:03:04.020: INFO: Got endpoints: latency-svc-njdwh [747.027971ms]
Jun 22 19:03:04.027: INFO: Created: latency-svc-lm5kz
Jun 22 19:03:04.069: INFO: Got endpoints: latency-svc-fc76c [744.110994ms]
Jun 22 19:03:04.080: INFO: Created: latency-svc-h9frc
Jun 22 19:03:04.125: INFO: Got endpoints: latency-svc-26rqt [753.671342ms]
Jun 22 19:03:04.143: INFO: Created: latency-svc-t8kth
Jun 22 19:03:04.169: INFO: Got endpoints: latency-svc-bn885 [750.148524ms]
Jun 22 19:03:04.175: INFO: Created: latency-svc-5lfjf
Jun 22 19:03:04.219: INFO: Got endpoints: latency-svc-spkh2 [748.326618ms]
Jun 22 19:03:04.227: INFO: Created: latency-svc-8d4w4
Jun 22 19:03:04.269: INFO: Got endpoints: latency-svc-bcvjk [747.64123ms]
Jun 22 19:03:04.278: INFO: Created: latency-svc-qhcww
Jun 22 19:03:04.320: INFO: Got endpoints: latency-svc-qv9gl [749.960644ms]
Jun 22 19:03:04.327: INFO: Created: latency-svc-kzfkt
Jun 22 19:03:04.369: INFO: Got endpoints: latency-svc-lxm2q [750.320032ms]
Jun 22 19:03:04.378: INFO: Created: latency-svc-zvw7w
Jun 22 19:03:04.419: INFO: Got endpoints: latency-svc-ppxhj [748.549434ms]
Jun 22 19:03:04.429: INFO: Created: latency-svc-f9ntg
Jun 22 19:03:04.470: INFO: Got endpoints: latency-svc-qq5l7 [751.323169ms]
Jun 22 19:03:04.480: INFO: Created: latency-svc-whc86
Jun 22 19:03:04.519: INFO: Got endpoints: latency-svc-mxr7c [750.09323ms]
Jun 22 19:03:04.526: INFO: Created: latency-svc-rc4j5
Jun 22 19:03:04.568: INFO: Got endpoints: latency-svc-vb7z8 [748.731288ms]
Jun 22 19:03:04.574: INFO: Created: latency-svc-6mjfr
Jun 22 19:03:04.620: INFO: Got endpoints: latency-svc-zw5pv [750.827148ms]
Jun 22 19:03:04.626: INFO: Created: latency-svc-thr9v
Jun 22 19:03:04.668: INFO: Got endpoints: latency-svc-2b7mz [748.953824ms]
Jun 22 19:03:04.676: INFO: Created: latency-svc-lkt6w
Jun 22 19:03:04.719: INFO: Got endpoints: latency-svc-sswlh [750.667643ms]
Jun 22 19:03:04.726: INFO: Created: latency-svc-4kgd2
Jun 22 19:03:04.770: INFO: Got endpoints: latency-svc-lm5kz [749.39467ms]
Jun 22 19:03:04.780: INFO: Created: latency-svc-tgt4t
Jun 22 19:03:04.819: INFO: Got endpoints: latency-svc-h9frc [748.007146ms]
Jun 22 19:03:04.829: INFO: Created: latency-svc-9pgz5
Jun 22 19:03:04.870: INFO: Got endpoints: latency-svc-t8kth [744.541185ms]
Jun 22 19:03:04.880: INFO: Created: latency-svc-whqzh
Jun 22 19:03:04.920: INFO: Got endpoints: latency-svc-5lfjf [750.899064ms]
Jun 22 19:03:04.926: INFO: Created: latency-svc-2dghh
Jun 22 19:03:04.968: INFO: Got endpoints: latency-svc-8d4w4 [749.274499ms]
Jun 22 19:03:04.977: INFO: Created: latency-svc-mw7rg
Jun 22 19:03:05.020: INFO: Got endpoints: latency-svc-qhcww [751.082986ms]
Jun 22 19:03:05.029: INFO: Created: latency-svc-cj74c
Jun 22 19:03:05.069: INFO: Got endpoints: latency-svc-kzfkt [748.438511ms]
Jun 22 19:03:05.078: INFO: Created: latency-svc-gtnzg
Jun 22 19:03:05.120: INFO: Got endpoints: latency-svc-zvw7w [750.657032ms]
Jun 22 19:03:05.127: INFO: Created: latency-svc-v8nvr
Jun 22 19:03:05.169: INFO: Got endpoints: latency-svc-f9ntg [750.412787ms]
Jun 22 19:03:05.181: INFO: Created: latency-svc-8rj4t
Jun 22 19:03:05.219: INFO: Got endpoints: latency-svc-whc86 [748.567477ms]
Jun 22 19:03:05.230: INFO: Created: latency-svc-4zd4s
Jun 22 19:03:05.270: INFO: Got endpoints: latency-svc-rc4j5 [750.019571ms]
Jun 22 19:03:05.276: INFO: Created: latency-svc-75rlf
Jun 22 19:03:05.320: INFO: Got endpoints: latency-svc-6mjfr [751.268773ms]
Jun 22 19:03:05.330: INFO: Created: latency-svc-rkxrs
Jun 22 19:03:05.370: INFO: Got endpoints: latency-svc-thr9v [749.513171ms]
Jun 22 19:03:05.377: INFO: Created: latency-svc-ds27s
Jun 22 19:03:05.420: INFO: Got endpoints: latency-svc-lkt6w [751.063109ms]
Jun 22 19:03:05.425: INFO: Created: latency-svc-zwbzr
Jun 22 19:03:05.469: INFO: Got endpoints: latency-svc-4kgd2 [749.426812ms]
Jun 22 19:03:05.478: INFO: Created: latency-svc-gdhvx
Jun 22 19:03:05.519: INFO: Got endpoints: latency-svc-tgt4t [748.689176ms]
Jun 22 19:03:05.527: INFO: Created: latency-svc-6pgxw
Jun 22 19:03:05.570: INFO: Got endpoints: latency-svc-9pgz5 [750.63017ms]
Jun 22 19:03:05.579: INFO: Created: latency-svc-dcft6
Jun 22 19:03:05.621: INFO: Got endpoints: latency-svc-whqzh [750.829881ms]
Jun 22 19:03:05.629: INFO: Created: latency-svc-42tz8
Jun 22 19:03:05.669: INFO: Got endpoints: latency-svc-2dghh [749.027762ms]
Jun 22 19:03:05.678: INFO: Created: latency-svc-tb54b
Jun 22 19:03:05.719: INFO: Got endpoints: latency-svc-mw7rg [750.66991ms]
Jun 22 19:03:05.728: INFO: Created: latency-svc-99vcv
Jun 22 19:03:05.769: INFO: Got endpoints: latency-svc-cj74c [748.904995ms]
Jun 22 19:03:05.779: INFO: Created: latency-svc-gzvp9
Jun 22 19:03:05.819: INFO: Got endpoints: latency-svc-gtnzg [750.276796ms]
Jun 22 19:03:05.827: INFO: Created: latency-svc-kk4zx
Jun 22 19:03:05.870: INFO: Got endpoints: latency-svc-v8nvr [750.07087ms]
Jun 22 19:03:05.886: INFO: Created: latency-svc-vq7wc
Jun 22 19:03:05.919: INFO: Got endpoints: latency-svc-8rj4t [749.658275ms]
Jun 22 19:03:05.930: INFO: Created: latency-svc-54gn2
Jun 22 19:03:05.971: INFO: Got endpoints: latency-svc-4zd4s [752.824689ms]
Jun 22 19:03:05.983: INFO: Created: latency-svc-rqnn8
Jun 22 19:03:06.019: INFO: Got endpoints: latency-svc-75rlf [749.620082ms]
Jun 22 19:03:06.026: INFO: Created: latency-svc-n987r
Jun 22 19:03:06.069: INFO: Got endpoints: latency-svc-rkxrs [749.34228ms]
Jun 22 19:03:06.077: INFO: Created: latency-svc-pcgst
Jun 22 19:03:06.126: INFO: Got endpoints: latency-svc-ds27s [756.170893ms]
Jun 22 19:03:06.135: INFO: Created: latency-svc-2h28t
Jun 22 19:03:06.168: INFO: Got endpoints: latency-svc-zwbzr [748.488354ms]
Jun 22 19:03:06.176: INFO: Created: latency-svc-6m5cg
Jun 22 19:03:06.219: INFO: Got endpoints: latency-svc-gdhvx [750.379981ms]
Jun 22 19:03:06.228: INFO: Created: latency-svc-xwncs
Jun 22 19:03:06.270: INFO: Got endpoints: latency-svc-6pgxw [751.118616ms]
Jun 22 19:03:06.280: INFO: Created: latency-svc-7tg7n
Jun 22 19:03:06.320: INFO: Got endpoints: latency-svc-dcft6 [750.434631ms]
Jun 22 19:03:06.328: INFO: Created: latency-svc-mmx7g
Jun 22 19:03:06.369: INFO: Got endpoints: latency-svc-42tz8 [748.197788ms]
Jun 22 19:03:06.377: INFO: Created: latency-svc-nvjbd
Jun 22 19:03:06.423: INFO: Got endpoints: latency-svc-tb54b [753.208653ms]
Jun 22 19:03:06.432: INFO: Created: latency-svc-7clx4
Jun 22 19:03:06.470: INFO: Got endpoints: latency-svc-99vcv [749.621938ms]
Jun 22 19:03:06.478: INFO: Created: latency-svc-wrfbf
Jun 22 19:03:06.518: INFO: Got endpoints: latency-svc-gzvp9 [748.902613ms]
Jun 22 19:03:06.526: INFO: Created: latency-svc-js2lf
Jun 22 19:03:06.568: INFO: Got endpoints: latency-svc-kk4zx [748.822023ms]
Jun 22 19:03:06.575: INFO: Created: latency-svc-rrxm2
Jun 22 19:03:06.619: INFO: Got endpoints: latency-svc-vq7wc [741.381516ms]
Jun 22 19:03:06.627: INFO: Created: latency-svc-gbbkg
Jun 22 19:03:06.668: INFO: Got endpoints: latency-svc-54gn2 [749.0365ms]
Jun 22 19:03:06.680: INFO: Created: latency-svc-9pjbm
Jun 22 19:03:06.718: INFO: Got endpoints: latency-svc-rqnn8 [746.774906ms]
Jun 22 19:03:06.727: INFO: Created: latency-svc-l2kt8
Jun 22 19:03:06.769: INFO: Got endpoints: latency-svc-n987r [749.84889ms]
Jun 22 19:03:06.776: INFO: Created: latency-svc-92hdf
Jun 22 19:03:06.820: INFO: Got endpoints: latency-svc-pcgst [750.270472ms]
Jun 22 19:03:06.828: INFO: Created: latency-svc-7t4xw
Jun 22 19:03:06.871: INFO: Got endpoints: latency-svc-2h28t [744.508525ms]
Jun 22 19:03:06.884: INFO: Created: latency-svc-7rplv
Jun 22 19:03:06.920: INFO: Got endpoints: latency-svc-6m5cg [750.893844ms]
Jun 22 19:03:06.927: INFO: Created: latency-svc-wkflp
Jun 22 19:03:06.971: INFO: Got endpoints: latency-svc-xwncs [751.00066ms]
Jun 22 19:03:06.979: INFO: Created: latency-svc-hqsqv
Jun 22 19:03:07.021: INFO: Got endpoints: latency-svc-7tg7n [751.450694ms]
Jun 22 19:03:07.028: INFO: Created: latency-svc-bcbhf
Jun 22 19:03:07.069: INFO: Got endpoints: latency-svc-mmx7g [748.659463ms]
Jun 22 19:03:07.078: INFO: Created: latency-svc-5trdh
Jun 22 19:03:07.118: INFO: Got endpoints: latency-svc-nvjbd [749.211627ms]
Jun 22 19:03:07.125: INFO: Created: latency-svc-98bvv
Jun 22 19:03:07.174: INFO: Got endpoints: latency-svc-7clx4 [750.847903ms]
Jun 22 19:03:07.182: INFO: Created: latency-svc-rz4qv
Jun 22 19:03:07.220: INFO: Got endpoints: latency-svc-wrfbf [749.421512ms]
Jun 22 19:03:07.227: INFO: Created: latency-svc-jgjkq
Jun 22 19:03:07.269: INFO: Got endpoints: latency-svc-js2lf [750.217493ms]
Jun 22 19:03:07.283: INFO: Created: latency-svc-665zn
Jun 22 19:03:07.345: INFO: Got endpoints: latency-svc-rrxm2 [776.698369ms]
Jun 22 19:03:07.365: INFO: Created: latency-svc-b2gfc
Jun 22 19:03:07.373: INFO: Got endpoints: latency-svc-gbbkg [754.494516ms]
Jun 22 19:03:07.385: INFO: Created: latency-svc-nsdhd
Jun 22 19:03:07.420: INFO: Got endpoints: latency-svc-9pjbm [751.697386ms]
Jun 22 19:03:07.428: INFO: Created: latency-svc-4rmv6
Jun 22 19:03:07.472: INFO: Got endpoints: latency-svc-l2kt8 [753.928648ms]
Jun 22 19:03:07.481: INFO: Created: latency-svc-wtqvs
Jun 22 19:03:07.520: INFO: Got endpoints: latency-svc-92hdf [750.072189ms]
Jun 22 19:03:07.526: INFO: Created: latency-svc-bx5ng
Jun 22 19:03:07.569: INFO: Got endpoints: latency-svc-7t4xw [749.21597ms]
Jun 22 19:03:07.577: INFO: Created: latency-svc-j9b2k
Jun 22 19:03:07.620: INFO: Got endpoints: latency-svc-7rplv [749.278634ms]
Jun 22 19:03:07.629: INFO: Created: latency-svc-5dhrd
Jun 22 19:03:07.669: INFO: Got endpoints: latency-svc-wkflp [749.40649ms]
Jun 22 19:03:07.678: INFO: Created: latency-svc-jwrh7
Jun 22 19:03:07.719: INFO: Got endpoints: latency-svc-hqsqv [748.240575ms]
Jun 22 19:03:07.726: INFO: Created: latency-svc-mgbtv
Jun 22 19:03:07.770: INFO: Got endpoints: latency-svc-bcbhf [748.738038ms]
Jun 22 19:03:07.778: INFO: Created: latency-svc-vq2ft
Jun 22 19:03:07.821: INFO: Got endpoints: latency-svc-5trdh [751.337985ms]
Jun 22 19:03:07.827: INFO: Created: latency-svc-m8wrr
Jun 22 19:03:07.869: INFO: Got endpoints: latency-svc-98bvv [750.644647ms]
Jun 22 19:03:07.875: INFO: Created: latency-svc-txh2k
Jun 22 19:03:07.920: INFO: Got endpoints: latency-svc-rz4qv [745.519642ms]
Jun 22 19:03:07.927: INFO: Created: latency-svc-thptg
Jun 22 19:03:07.969: INFO: Got endpoints: latency-svc-jgjkq [749.350596ms]
Jun 22 19:03:07.976: INFO: Created: latency-svc-lv875
Jun 22 19:03:08.020: INFO: Got endpoints: latency-svc-665zn [751.46534ms]
Jun 22 19:03:08.028: INFO: Created: latency-svc-g9wd5
Jun 22 19:03:08.069: INFO: Got endpoints: latency-svc-b2gfc [723.244862ms]
Jun 22 19:03:08.076: INFO: Created: latency-svc-qg8cv
Jun 22 19:03:08.122: INFO: Got endpoints: latency-svc-nsdhd [748.660254ms]
Jun 22 19:03:08.132: INFO: Created: latency-svc-8sgps
Jun 22 19:03:08.168: INFO: Got endpoints: latency-svc-4rmv6 [747.867828ms]
Jun 22 19:03:08.181: INFO: Created: latency-svc-k6dh9
Jun 22 19:03:08.218: INFO: Got endpoints: latency-svc-wtqvs [745.795552ms]
Jun 22 19:03:08.226: INFO: Created: latency-svc-qrlfs
Jun 22 19:03:08.269: INFO: Got endpoints: latency-svc-bx5ng [748.97093ms]
Jun 22 19:03:08.276: INFO: Created: latency-svc-gln2x
Jun 22 19:03:08.322: INFO: Got endpoints: latency-svc-j9b2k [753.36786ms]
Jun 22 19:03:08.342: INFO: Created: latency-svc-5xds5
Jun 22 19:03:08.370: INFO: Got endpoints: latency-svc-5dhrd [747.167509ms]
Jun 22 19:03:08.381: INFO: Created: latency-svc-gz7vq
Jun 22 19:03:08.420: INFO: Got endpoints: latency-svc-jwrh7 [750.704129ms]
Jun 22 19:03:08.431: INFO: Created: latency-svc-r57jz
Jun 22 19:03:08.468: INFO: Got endpoints: latency-svc-mgbtv [749.392429ms]
Jun 22 19:03:08.476: INFO: Created: latency-svc-smfqg
Jun 22 19:03:08.518: INFO: Got endpoints: latency-svc-vq2ft [747.946213ms]
Jun 22 19:03:08.527: INFO: Created: latency-svc-thvxb
Jun 22 19:03:08.569: INFO: Got endpoints: latency-svc-m8wrr [747.815185ms]
Jun 22 19:03:08.577: INFO: Created: latency-svc-h84ln
Jun 22 19:03:08.618: INFO: Got endpoints: latency-svc-txh2k [749.481734ms]
Jun 22 19:03:08.628: INFO: Created: latency-svc-7bcg9
Jun 22 19:03:08.668: INFO: Got endpoints: latency-svc-thptg [748.588829ms]
Jun 22 19:03:08.679: INFO: Created: latency-svc-ktpwd
Jun 22 19:03:08.719: INFO: Got endpoints: latency-svc-lv875 [750.346226ms]
Jun 22 19:03:08.727: INFO: Created: latency-svc-5jxz4
Jun 22 19:03:08.768: INFO: Got endpoints: latency-svc-g9wd5 [747.888844ms]
Jun 22 19:03:08.777: INFO: Created: latency-svc-7tktd
Jun 22 19:03:08.819: INFO: Got endpoints: latency-svc-qg8cv [750.848039ms]
Jun 22 19:03:08.827: INFO: Created: latency-svc-49rf8
Jun 22 19:03:08.868: INFO: Got endpoints: latency-svc-8sgps [746.144988ms]
Jun 22 19:03:08.877: INFO: Created: latency-svc-v6fpd
Jun 22 19:03:08.918: INFO: Got endpoints: latency-svc-k6dh9 [749.328619ms]
Jun 22 19:03:08.927: INFO: Created: latency-svc-bhvvb
Jun 22 19:03:08.971: INFO: Got endpoints: latency-svc-qrlfs [752.124686ms]
Jun 22 19:03:08.978: INFO: Created: latency-svc-h5jnb
Jun 22 19:03:09.019: INFO: Got endpoints: latency-svc-gln2x [750.359147ms]
Jun 22 19:03:09.028: INFO: Created: latency-svc-dc5wq
Jun 22 19:03:09.069: INFO: Got endpoints: latency-svc-5xds5 [746.90407ms]
Jun 22 19:03:09.079: INFO: Created: latency-svc-x7l7x
Jun 22 19:03:09.120: INFO: Got endpoints: latency-svc-gz7vq [750.211831ms]
Jun 22 19:03:09.129: INFO: Created: latency-svc-hfpbv
Jun 22 19:03:09.169: INFO: Got endpoints: latency-svc-r57jz [748.820751ms]
Jun 22 19:03:09.177: INFO: Created: latency-svc-jftrn
Jun 22 19:03:09.219: INFO: Got endpoints: latency-svc-smfqg [750.998211ms]
Jun 22 19:03:09.229: INFO: Created: latency-svc-c5xdr
Jun 22 19:03:09.269: INFO: Got endpoints: latency-svc-thvxb [750.641197ms]
Jun 22 19:03:09.276: INFO: Created: latency-svc-zn84r
Jun 22 19:03:09.320: INFO: Got endpoints: latency-svc-h84ln [751.5027ms]
Jun 22 19:03:09.333: INFO: Created: latency-svc-jtd2p
Jun 22 19:03:09.370: INFO: Got endpoints: latency-svc-7bcg9 [751.716108ms]
Jun 22 19:03:09.377: INFO: Created: latency-svc-9cs74
Jun 22 19:03:09.418: INFO: Got endpoints: latency-svc-ktpwd [748.904138ms]
Jun 22 19:03:09.426: INFO: Created: latency-svc-vt5kg
Jun 22 19:03:09.469: INFO: Got endpoints: latency-svc-5jxz4 [749.255807ms]
Jun 22 19:03:09.477: INFO: Created: latency-svc-hdd2t
Jun 22 19:03:09.519: INFO: Got endpoints: latency-svc-7tktd [750.317411ms]
Jun 22 19:03:09.528: INFO: Created: latency-svc-4rxhs
Jun 22 19:03:09.570: INFO: Got endpoints: latency-svc-49rf8 [750.558698ms]
Jun 22 19:03:09.578: INFO: Created: latency-svc-xgpvb
Jun 22 19:03:09.620: INFO: Got endpoints: latency-svc-v6fpd [751.318018ms]
Jun 22 19:03:09.627: INFO: Created: latency-svc-z4gkn
Jun 22 19:03:09.669: INFO: Got endpoints: latency-svc-bhvvb [751.438426ms]
Jun 22 19:03:09.676: INFO: Created: latency-svc-55vqb
Jun 22 19:03:09.720: INFO: Got endpoints: latency-svc-h5jnb [749.114585ms]
Jun 22 19:03:09.728: INFO: Created: latency-svc-bksqv
Jun 22 19:03:09.768: INFO: Got endpoints: latency-svc-dc5wq [749.209317ms]
Jun 22 19:03:09.775: INFO: Created: latency-svc-2nn7g
Jun 22 19:03:09.819: INFO: Got endpoints: latency-svc-x7l7x [749.202066ms]
Jun 22 19:03:09.825: INFO: Created: latency-svc-pv8hg
Jun 22 19:03:09.869: INFO: Got endpoints: latency-svc-hfpbv [749.691577ms]
Jun 22 19:03:09.878: INFO: Created: latency-svc-qmlg5
Jun 22 19:03:09.920: INFO: Got endpoints: latency-svc-jftrn [751.286116ms]
Jun 22 19:03:09.928: INFO: Created: latency-svc-grpnt
Jun 22 19:03:09.969: INFO: Got endpoints: latency-svc-c5xdr [748.801281ms]
Jun 22 19:03:09.975: INFO: Created: latency-svc-6lqds
Jun 22 19:03:10.019: INFO: Got endpoints: latency-svc-zn84r [750.001789ms]
Jun 22 19:03:10.027: INFO: Created: latency-svc-9t8wg
Jun 22 19:03:10.069: INFO: Got endpoints: latency-svc-jtd2p [748.506612ms]
Jun 22 19:03:10.078: INFO: Created: latency-svc-hkd66
Jun 22 19:03:10.122: INFO: Got endpoints: latency-svc-9cs74 [751.317152ms]
Jun 22 19:03:10.137: INFO: Created: latency-svc-tt986
Jun 22 19:03:10.169: INFO: Got endpoints: latency-svc-vt5kg [751.040628ms]
Jun 22 19:03:10.178: INFO: Created: latency-svc-b4cjh
Jun 22 19:03:10.220: INFO: Got endpoints: latency-svc-hdd2t [750.675442ms]
Jun 22 19:03:10.227: INFO: Created: latency-svc-6lzkw
Jun 22 19:03:10.270: INFO: Got endpoints: latency-svc-4rxhs [750.4533ms]
Jun 22 19:03:10.277: INFO: Created: latency-svc-c292r
Jun 22 19:03:10.323: INFO: Got endpoints: latency-svc-xgpvb [753.11225ms]
Jun 22 19:03:10.331: INFO: Created: latency-svc-npmj4
Jun 22 19:03:10.372: INFO: Got endpoints: latency-svc-z4gkn [752.695874ms]
Jun 22 19:03:10.380: INFO: Created: latency-svc-j2lpt
Jun 22 19:03:10.420: INFO: Got endpoints: latency-svc-55vqb [749.968745ms]
Jun 22 19:03:10.427: INFO: Created: latency-svc-hcpzf
Jun 22 19:03:10.470: INFO: Got endpoints: latency-svc-bksqv [749.581122ms]
Jun 22 19:03:10.478: INFO: Created: latency-svc-glfgl
Jun 22 19:03:10.520: INFO: Got endpoints: latency-svc-2nn7g [751.586222ms]
Jun 22 19:03:10.526: INFO: Created: latency-svc-r2kkb
Jun 22 19:03:10.570: INFO: Got endpoints: latency-svc-pv8hg [750.803277ms]
Jun 22 19:03:10.576: INFO: Created: latency-svc-vt8b2
Jun 22 19:03:10.620: INFO: Got endpoints: latency-svc-qmlg5 [749.990472ms]
Jun 22 19:03:10.670: INFO: Got endpoints: latency-svc-grpnt [749.585578ms]
Jun 22 19:03:10.721: INFO: Got endpoints: latency-svc-6lqds [751.986428ms]
Jun 22 19:03:10.770: INFO: Got endpoints: latency-svc-9t8wg [750.303172ms]
Jun 22 19:03:10.820: INFO: Got endpoints: latency-svc-hkd66 [751.062637ms]
Jun 22 19:03:10.873: INFO: Got endpoints: latency-svc-tt986 [751.175641ms]
Jun 22 19:03:10.919: INFO: Got endpoints: latency-svc-b4cjh [749.467105ms]
Jun 22 19:03:10.970: INFO: Got endpoints: latency-svc-6lzkw [749.804999ms]
Jun 22 19:03:11.021: INFO: Got endpoints: latency-svc-c292r [751.53859ms]
Jun 22 19:03:11.069: INFO: Got endpoints: latency-svc-npmj4 [744.981512ms]
Jun 22 19:03:11.120: INFO: Got endpoints: latency-svc-j2lpt [747.615127ms]
Jun 22 19:03:11.172: INFO: Got endpoints: latency-svc-hcpzf [751.750097ms]
Jun 22 19:03:11.222: INFO: Got endpoints: latency-svc-glfgl [751.553616ms]
Jun 22 19:03:11.270: INFO: Got endpoints: latency-svc-r2kkb [749.585719ms]
Jun 22 19:03:11.319: INFO: Got endpoints: latency-svc-vt8b2 [749.886127ms]
Jun 22 19:03:11.320: INFO: Latencies: [38.049512ms 38.826211ms 41.316081ms 54.556127ms 58.78862ms 83.905737ms 102.119795ms 103.87209ms 105.764954ms 111.996473ms 121.071104ms 128.626726ms 134.364102ms 135.906794ms 140.166099ms 143.548361ms 145.659947ms 145.800801ms 147.015291ms 147.220766ms 147.669592ms 150.623473ms 150.711277ms 151.716765ms 152.889312ms 153.554082ms 155.125429ms 158.018722ms 159.513498ms 160.425352ms 165.678679ms 165.860454ms 169.877318ms 178.828365ms 179.085152ms 180.101016ms 184.083474ms 185.221488ms 226.10106ms 286.447339ms 306.894173ms 351.378351ms 389.74804ms 432.932429ms 480.194683ms 501.826392ms 549.120653ms 591.117563ms 632.499893ms 675.249048ms 701.52598ms 723.244862ms 738.225823ms 741.381516ms 742.617654ms 744.110994ms 744.508525ms 744.541185ms 744.981512ms 745.519642ms 745.795552ms 746.144988ms 746.774906ms 746.90407ms 747.027971ms 747.167509ms 747.615127ms 747.64123ms 747.815185ms 747.867828ms 747.888844ms 747.946213ms 748.007146ms 748.197788ms 748.240575ms 748.326618ms 748.438511ms 748.488354ms 748.506612ms 748.549434ms 748.567477ms 748.588829ms 748.659463ms 748.660254ms 748.689176ms 748.731288ms 748.738038ms 748.801281ms 748.820751ms 748.822023ms 748.902613ms 748.904138ms 748.904995ms 748.953824ms 748.97093ms 749.027762ms 749.0365ms 749.114585ms 749.202066ms 749.209317ms 749.211627ms 749.21597ms 749.255807ms 749.274499ms 749.278634ms 749.328619ms 749.34228ms 749.350596ms 749.392429ms 749.39467ms 749.40649ms 749.421512ms 749.426812ms 749.467105ms 749.481734ms 749.513171ms 749.581122ms 749.585578ms 749.585719ms 749.620082ms 749.621938ms 749.658275ms 749.691577ms 749.804999ms 749.84889ms 749.886127ms 749.960644ms 749.968745ms 749.990472ms 750.001789ms 750.019571ms 750.07087ms 750.072189ms 750.09323ms 750.148524ms 750.211831ms 750.217493ms 750.270472ms 750.276796ms 750.303172ms 750.317411ms 750.320032ms 750.346226ms 750.359147ms 750.379981ms 750.412787ms 750.434631ms 750.4533ms 750.558698ms 750.63017ms 750.641197ms 750.644647ms 750.657032ms 750.667643ms 750.66991ms 750.675442ms 750.704129ms 750.803277ms 750.827148ms 750.829881ms 750.847903ms 750.848039ms 750.893844ms 750.899064ms 750.998211ms 751.00066ms 751.040628ms 751.062637ms 751.063109ms 751.082986ms 751.118616ms 751.175641ms 751.268773ms 751.286116ms 751.317152ms 751.318018ms 751.323169ms 751.337985ms 751.438426ms 751.450694ms 751.46534ms 751.5027ms 751.53859ms 751.553616ms 751.586222ms 751.697386ms 751.716108ms 751.750097ms 751.986428ms 752.124686ms 752.695874ms 752.824689ms 753.11225ms 753.208653ms 753.36786ms 753.671342ms 753.928648ms 754.494516ms 756.170893ms 776.698369ms]
Jun 22 19:03:11.320: INFO: 50 %ile: 749.211627ms
Jun 22 19:03:11.320: INFO: 90 %ile: 751.46534ms
Jun 22 19:03:11.320: INFO: 99 %ile: 756.170893ms
Jun 22 19:03:11.320: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:03:11.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1817" for this suite.
Jun 22 19:03:21.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:03:21.418: INFO: namespace svc-latency-1817 deletion completed in 10.093842698s

• [SLOW TEST:20.837 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:03:21.419: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-d87z
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 19:03:21.502: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-d87z" in namespace "subpath-5726" to be "success or failure"
Jun 22 19:03:21.507: INFO: Pod "pod-subpath-test-secret-d87z": Phase="Pending", Reason="", readiness=false. Elapsed: 4.997467ms
Jun 22 19:03:23.509: INFO: Pod "pod-subpath-test-secret-d87z": Phase="Running", Reason="", readiness=true. Elapsed: 2.007833236s
Jun 22 19:03:25.514: INFO: Pod "pod-subpath-test-secret-d87z": Phase="Running", Reason="", readiness=true. Elapsed: 4.011905171s
Jun 22 19:03:27.516: INFO: Pod "pod-subpath-test-secret-d87z": Phase="Running", Reason="", readiness=true. Elapsed: 6.014797158s
Jun 22 19:03:29.519: INFO: Pod "pod-subpath-test-secret-d87z": Phase="Running", Reason="", readiness=true. Elapsed: 8.017680941s
Jun 22 19:03:31.522: INFO: Pod "pod-subpath-test-secret-d87z": Phase="Running", Reason="", readiness=true. Elapsed: 10.020477586s
Jun 22 19:03:33.525: INFO: Pod "pod-subpath-test-secret-d87z": Phase="Running", Reason="", readiness=true. Elapsed: 12.023519519s
Jun 22 19:03:35.528: INFO: Pod "pod-subpath-test-secret-d87z": Phase="Running", Reason="", readiness=true. Elapsed: 14.026606134s
Jun 22 19:03:37.532: INFO: Pod "pod-subpath-test-secret-d87z": Phase="Running", Reason="", readiness=true. Elapsed: 16.029873494s
Jun 22 19:03:39.535: INFO: Pod "pod-subpath-test-secret-d87z": Phase="Running", Reason="", readiness=true. Elapsed: 18.033097315s
Jun 22 19:03:41.538: INFO: Pod "pod-subpath-test-secret-d87z": Phase="Running", Reason="", readiness=true. Elapsed: 20.03594111s
Jun 22 19:03:43.541: INFO: Pod "pod-subpath-test-secret-d87z": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.038866739s
STEP: Saw pod success
Jun 22 19:03:43.541: INFO: Pod "pod-subpath-test-secret-d87z" satisfied condition "success or failure"
Jun 22 19:03:43.543: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-subpath-test-secret-d87z container test-container-subpath-secret-d87z: <nil>
STEP: delete the pod
Jun 22 19:03:43.558: INFO: Waiting for pod pod-subpath-test-secret-d87z to disappear
Jun 22 19:03:43.560: INFO: Pod pod-subpath-test-secret-d87z no longer exists
STEP: Deleting pod pod-subpath-test-secret-d87z
Jun 22 19:03:43.561: INFO: Deleting pod "pod-subpath-test-secret-d87z" in namespace "subpath-5726"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:03:43.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5726" for this suite.
Jun 22 19:03:49.582: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:03:49.667: INFO: namespace subpath-5726 deletion completed in 6.10021664s

• [SLOW TEST:28.248 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:03:49.667: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Jun 22 19:03:49.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 create -f - --namespace=kubectl-821'
Jun 22 19:03:49.896: INFO: stderr: ""
Jun 22 19:03:49.896: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Jun 22 19:03:50.900: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 19:03:50.900: INFO: Found 0 / 1
Jun 22 19:03:51.899: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 19:03:51.899: INFO: Found 1 / 1
Jun 22 19:03:51.899: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 22 19:03:51.902: INFO: Selector matched 1 pods for map[app:redis]
Jun 22 19:03:51.902: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jun 22 19:03:51.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 logs redis-master-xqv69 redis-master --namespace=kubectl-821'
Jun 22 19:03:51.984: INFO: stderr: ""
Jun 22 19:03:51.984: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 Jun 19:03:50.726 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 Jun 19:03:50.726 # Server started, Redis version 3.2.12\n1:M 22 Jun 19:03:50.726 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 Jun 19:03:50.727 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jun 22 19:03:51.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 log redis-master-xqv69 redis-master --namespace=kubectl-821 --tail=1'
Jun 22 19:03:52.067: INFO: stderr: ""
Jun 22 19:03:52.067: INFO: stdout: "1:M 22 Jun 19:03:50.727 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jun 22 19:03:52.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 log redis-master-xqv69 redis-master --namespace=kubectl-821 --limit-bytes=1'
Jun 22 19:03:52.242: INFO: stderr: ""
Jun 22 19:03:52.242: INFO: stdout: " "
STEP: exposing timestamps
Jun 22 19:03:52.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 log redis-master-xqv69 redis-master --namespace=kubectl-821 --tail=1 --timestamps'
Jun 22 19:03:52.324: INFO: stderr: ""
Jun 22 19:03:52.324: INFO: stdout: "2019-06-22T19:03:50.727244318Z 1:M 22 Jun 19:03:50.727 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jun 22 19:03:54.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 log redis-master-xqv69 redis-master --namespace=kubectl-821 --since=1s'
Jun 22 19:03:54.908: INFO: stderr: ""
Jun 22 19:03:54.908: INFO: stdout: ""
Jun 22 19:03:54.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 log redis-master-xqv69 redis-master --namespace=kubectl-821 --since=24h'
Jun 22 19:03:54.988: INFO: stderr: ""
Jun 22 19:03:54.988: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 22 Jun 19:03:50.726 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 22 Jun 19:03:50.726 # Server started, Redis version 3.2.12\n1:M 22 Jun 19:03:50.726 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 22 Jun 19:03:50.727 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Jun 22 19:03:54.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 delete --grace-period=0 --force -f - --namespace=kubectl-821'
Jun 22 19:03:55.057: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 22 19:03:55.057: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jun 22 19:03:55.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get rc,svc -l name=nginx --no-headers --namespace=kubectl-821'
Jun 22 19:03:55.135: INFO: stderr: "No resources found.\n"
Jun 22 19:03:55.135: INFO: stdout: ""
Jun 22 19:03:55.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 get pods -l name=nginx --namespace=kubectl-821 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 22 19:03:55.204: INFO: stderr: ""
Jun 22 19:03:55.204: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:03:55.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-821" for this suite.
Jun 22 19:04:17.216: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:04:17.333: INFO: namespace kubectl-821 deletion completed in 22.125372699s

• [SLOW TEST:27.665 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:04:17.333: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 22 19:04:17.377: INFO: (0) /api/v1/nodes/ip-10-0-15-125/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 5.979369ms)
Jun 22 19:04:17.380: INFO: (1) /api/v1/nodes/ip-10-0-15-125/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.871995ms)
Jun 22 19:04:17.383: INFO: (2) /api/v1/nodes/ip-10-0-15-125/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.727992ms)
Jun 22 19:04:17.386: INFO: (3) /api/v1/nodes/ip-10-0-15-125/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.806026ms)
Jun 22 19:04:17.388: INFO: (4) /api/v1/nodes/ip-10-0-15-125/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.717901ms)
Jun 22 19:04:17.391: INFO: (5) /api/v1/nodes/ip-10-0-15-125/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.632363ms)
Jun 22 19:04:17.394: INFO: (6) /api/v1/nodes/ip-10-0-15-125/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.692688ms)
Jun 22 19:04:17.397: INFO: (7) /api/v1/nodes/ip-10-0-15-125/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 3.266532ms)
Jun 22 19:04:17.400: INFO: (8) /api/v1/nodes/ip-10-0-15-125/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.70281ms)
Jun 22 19:04:17.402: INFO: (9) /api/v1/nodes/ip-10-0-15-125/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.597258ms)
Jun 22 19:04:17.405: INFO: (10) /api/v1/nodes/ip-10-0-15-125/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.774776ms)
Jun 22 19:04:17.408: INFO: (11) /api/v1/nodes/ip-10-0-15-125/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.747598ms)
Jun 22 19:04:17.411: INFO: (12) /api/v1/nodes/ip-10-0-15-125/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.552415ms)
Jun 22 19:04:17.413: INFO: (13) /api/v1/nodes/ip-10-0-15-125/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.888497ms)
Jun 22 19:04:17.416: INFO: (14) /api/v1/nodes/ip-10-0-15-125/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.586546ms)
Jun 22 19:04:17.419: INFO: (15) /api/v1/nodes/ip-10-0-15-125/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.880303ms)
Jun 22 19:04:17.422: INFO: (16) /api/v1/nodes/ip-10-0-15-125/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.559118ms)
Jun 22 19:04:17.424: INFO: (17) /api/v1/nodes/ip-10-0-15-125/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.568206ms)
Jun 22 19:04:17.427: INFO: (18) /api/v1/nodes/ip-10-0-15-125/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.814615ms)
Jun 22 19:04:17.430: INFO: (19) /api/v1/nodes/ip-10-0-15-125/proxy/logs/: <pre>
<a href="btmp">btmp</a>
<a href="containers/">containers/</a>
<a href="faillog">faillog</a>... (200; 2.803748ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:04:17.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1293" for this suite.
Jun 22 19:04:23.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:04:23.522: INFO: namespace proxy-1293 deletion completed in 6.089324317s

• [SLOW TEST:6.189 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:04:23.523: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-7429
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-7429
STEP: Creating statefulset with conflicting port in namespace statefulset-7429
STEP: Waiting until pod test-pod will start running in namespace statefulset-7429
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-7429
Jun 22 19:04:25.586: INFO: Observed stateful pod in namespace: statefulset-7429, name: ss-0, uid: 2b0ba053-a5f5-4b42-9a28-bf4489880850, status phase: Pending. Waiting for statefulset controller to delete.
Jun 22 19:04:26.162: INFO: Observed stateful pod in namespace: statefulset-7429, name: ss-0, uid: 2b0ba053-a5f5-4b42-9a28-bf4489880850, status phase: Failed. Waiting for statefulset controller to delete.
Jun 22 19:04:26.170: INFO: Observed stateful pod in namespace: statefulset-7429, name: ss-0, uid: 2b0ba053-a5f5-4b42-9a28-bf4489880850, status phase: Failed. Waiting for statefulset controller to delete.
Jun 22 19:04:26.172: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-7429
STEP: Removing pod with conflicting port in namespace statefulset-7429
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-7429 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 22 19:04:30.199: INFO: Deleting all statefulset in ns statefulset-7429
Jun 22 19:04:30.201: INFO: Scaling statefulset ss to 0
Jun 22 19:04:40.213: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 19:04:40.216: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:04:40.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7429" for this suite.
Jun 22 19:04:46.244: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:04:46.325: INFO: namespace statefulset-7429 deletion completed in 6.089956697s

• [SLOW TEST:22.802 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:04:46.326: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:05:10.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-1861" for this suite.
Jun 22 19:05:16.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:05:16.508: INFO: namespace namespaces-1861 deletion completed in 6.094946195s
STEP: Destroying namespace "nsdeletetest-5863" for this suite.
Jun 22 19:05:16.510: INFO: Namespace nsdeletetest-5863 was already deleted
STEP: Destroying namespace "nsdeletetest-2295" for this suite.
Jun 22 19:05:22.520: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:05:22.605: INFO: namespace nsdeletetest-2295 deletion completed in 6.094397722s

• [SLOW TEST:36.279 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:05:22.606: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-b58f47e3-a967-42b6-a9a0-ca898009cdea in namespace container-probe-762
Jun 22 19:05:24.645: INFO: Started pod busybox-b58f47e3-a967-42b6-a9a0-ca898009cdea in namespace container-probe-762
STEP: checking the pod's current state and verifying that restartCount is present
Jun 22 19:05:24.648: INFO: Initial restart count of pod busybox-b58f47e3-a967-42b6-a9a0-ca898009cdea is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:09:25.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-762" for this suite.
Jun 22 19:09:31.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:09:31.193: INFO: namespace container-probe-762 deletion completed in 6.094163451s

• [SLOW TEST:248.587 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:09:31.193: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 22 19:09:31.220: INFO: Waiting up to 5m0s for pod "downwardapi-volume-70a9363f-ea44-4cd9-827d-93a631602d6b" in namespace "downward-api-4231" to be "success or failure"
Jun 22 19:09:31.224: INFO: Pod "downwardapi-volume-70a9363f-ea44-4cd9-827d-93a631602d6b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.50911ms
Jun 22 19:09:33.227: INFO: Pod "downwardapi-volume-70a9363f-ea44-4cd9-827d-93a631602d6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006842297s
STEP: Saw pod success
Jun 22 19:09:33.227: INFO: Pod "downwardapi-volume-70a9363f-ea44-4cd9-827d-93a631602d6b" satisfied condition "success or failure"
Jun 22 19:09:33.229: INFO: Trying to get logs from node ip-10-0-20-165 pod downwardapi-volume-70a9363f-ea44-4cd9-827d-93a631602d6b container client-container: <nil>
STEP: delete the pod
Jun 22 19:09:33.244: INFO: Waiting for pod downwardapi-volume-70a9363f-ea44-4cd9-827d-93a631602d6b to disappear
Jun 22 19:09:33.247: INFO: Pod downwardapi-volume-70a9363f-ea44-4cd9-827d-93a631602d6b no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:09:33.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4231" for this suite.
Jun 22 19:09:39.259: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:09:39.342: INFO: namespace downward-api-4231 deletion completed in 6.091766023s

• [SLOW TEST:8.149 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:09:39.342: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-2b5a9cd0-2fee-4b98-839a-05c206f7d964
STEP: Creating a pod to test consume configMaps
Jun 22 19:09:39.370: INFO: Waiting up to 5m0s for pod "pod-configmaps-4526d80f-cab2-423e-9351-3f0dd6a6f2af" in namespace "configmap-311" to be "success or failure"
Jun 22 19:09:39.373: INFO: Pod "pod-configmaps-4526d80f-cab2-423e-9351-3f0dd6a6f2af": Phase="Pending", Reason="", readiness=false. Elapsed: 2.188404ms
Jun 22 19:09:41.376: INFO: Pod "pod-configmaps-4526d80f-cab2-423e-9351-3f0dd6a6f2af": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00538974s
STEP: Saw pod success
Jun 22 19:09:41.376: INFO: Pod "pod-configmaps-4526d80f-cab2-423e-9351-3f0dd6a6f2af" satisfied condition "success or failure"
Jun 22 19:09:41.379: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-configmaps-4526d80f-cab2-423e-9351-3f0dd6a6f2af container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 19:09:41.393: INFO: Waiting for pod pod-configmaps-4526d80f-cab2-423e-9351-3f0dd6a6f2af to disappear
Jun 22 19:09:41.395: INFO: Pod pod-configmaps-4526d80f-cab2-423e-9351-3f0dd6a6f2af no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:09:41.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-311" for this suite.
Jun 22 19:09:47.411: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:09:47.496: INFO: namespace configmap-311 deletion completed in 6.096377068s

• [SLOW TEST:8.153 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:09:47.497: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-dd2cc57f-1a40-42d5-9a72-814f9eff0e56
STEP: Creating a pod to test consume configMaps
Jun 22 19:09:47.576: INFO: Waiting up to 5m0s for pod "pod-configmaps-cef30c77-5e4b-44f4-8d70-3646fe2e49b2" in namespace "configmap-5087" to be "success or failure"
Jun 22 19:09:47.580: INFO: Pod "pod-configmaps-cef30c77-5e4b-44f4-8d70-3646fe2e49b2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.800698ms
Jun 22 19:09:49.583: INFO: Pod "pod-configmaps-cef30c77-5e4b-44f4-8d70-3646fe2e49b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006893704s
STEP: Saw pod success
Jun 22 19:09:49.583: INFO: Pod "pod-configmaps-cef30c77-5e4b-44f4-8d70-3646fe2e49b2" satisfied condition "success or failure"
Jun 22 19:09:49.585: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-configmaps-cef30c77-5e4b-44f4-8d70-3646fe2e49b2 container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 19:09:49.598: INFO: Waiting for pod pod-configmaps-cef30c77-5e4b-44f4-8d70-3646fe2e49b2 to disappear
Jun 22 19:09:49.602: INFO: Pod pod-configmaps-cef30c77-5e4b-44f4-8d70-3646fe2e49b2 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:09:49.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5087" for this suite.
Jun 22 19:09:55.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:09:55.696: INFO: namespace configmap-5087 deletion completed in 6.091231112s

• [SLOW TEST:8.200 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:09:55.697: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jun 22 19:09:55.717: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 22 19:09:55.722: INFO: Waiting for terminating namespaces to be deleted...
Jun 22 19:09:55.724: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-15-125 before test
Jun 22 19:09:55.730: INFO: kube-proxy-89t9g from kube-system started at 2019-06-22 17:46:06 +0000 UTC (1 container statuses recorded)
Jun 22 19:09:55.730: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 19:09:55.730: INFO: sonobuoy from heptio-sonobuoy started at 2019-06-22 17:50:43 +0000 UTC (1 container statuses recorded)
Jun 22 19:09:55.730: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 22 19:09:55.730: INFO: sonobuoy-systemd-logs-daemon-set-2e6d603853cb48ce-6dsjh from heptio-sonobuoy started at 2019-06-22 17:50:48 +0000 UTC (2 container statuses recorded)
Jun 22 19:09:55.730: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 22 19:09:55.730: INFO: 	Container systemd-logs ready: true, restart count 1
Jun 22 19:09:55.730: INFO: calico-node-n5n7z from kube-system started at 2019-06-22 17:46:06 +0000 UTC (1 container statuses recorded)
Jun 22 19:09:55.730: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 19:09:55.730: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-20-165 before test
Jun 22 19:09:55.734: INFO: sonobuoy-systemd-logs-daemon-set-2e6d603853cb48ce-rxlqb from heptio-sonobuoy started at 2019-06-22 17:50:48 +0000 UTC (2 container statuses recorded)
Jun 22 19:09:55.734: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 22 19:09:55.734: INFO: 	Container systemd-logs ready: true, restart count 1
Jun 22 19:09:55.734: INFO: calico-node-zd2zz from kube-system started at 2019-06-22 17:46:06 +0000 UTC (1 container statuses recorded)
Jun 22 19:09:55.734: INFO: 	Container calico-node ready: true, restart count 0
Jun 22 19:09:55.734: INFO: kube-proxy-ww9qf from kube-system started at 2019-06-22 17:46:06 +0000 UTC (1 container statuses recorded)
Jun 22 19:09:55.734: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 19:09:55.734: INFO: 
Logging pods the kubelet thinks is on node ip-10-0-42-243 before test
Jun 22 19:09:55.741: INFO: sonobuoy-e2e-job-5c726901000d4f40 from heptio-sonobuoy started at 2019-06-22 17:50:48 +0000 UTC (2 container statuses recorded)
Jun 22 19:09:55.741: INFO: 	Container e2e ready: true, restart count 0
Jun 22 19:09:55.741: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 22 19:09:55.741: INFO: sonobuoy-systemd-logs-daemon-set-2e6d603853cb48ce-7pf2f from heptio-sonobuoy started at 2019-06-22 17:50:48 +0000 UTC (2 container statuses recorded)
Jun 22 19:09:55.741: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Jun 22 19:09:55.741: INFO: 	Container systemd-logs ready: true, restart count 1
Jun 22 19:09:55.741: INFO: kube-proxy-gkjvx from kube-system started at 2019-06-22 17:46:06 +0000 UTC (1 container statuses recorded)
Jun 22 19:09:55.742: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 22 19:09:55.742: INFO: calico-node-688zt from kube-system started at 2019-06-22 17:46:06 +0000 UTC (1 container statuses recorded)
Jun 22 19:09:55.742: INFO: 	Container calico-node ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15aa9af480839a95], Reason = [FailedScheduling], Message = [0/4 nodes are available: 4 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:09:56.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1685" for this suite.
Jun 22 19:10:02.769: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:10:02.863: INFO: namespace sched-pred-1685 deletion completed in 6.102263021s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.166 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:10:02.863: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-d0972f2a-091d-4a11-85c2-aedcd654f559
STEP: Creating a pod to test consume configMaps
Jun 22 19:10:02.892: INFO: Waiting up to 5m0s for pod "pod-configmaps-f3a35819-0a34-4037-8483-c2e9951d6d0d" in namespace "configmap-1311" to be "success or failure"
Jun 22 19:10:02.897: INFO: Pod "pod-configmaps-f3a35819-0a34-4037-8483-c2e9951d6d0d": Phase="Pending", Reason="", readiness=false. Elapsed: 5.044662ms
Jun 22 19:10:04.900: INFO: Pod "pod-configmaps-f3a35819-0a34-4037-8483-c2e9951d6d0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008031801s
STEP: Saw pod success
Jun 22 19:10:04.900: INFO: Pod "pod-configmaps-f3a35819-0a34-4037-8483-c2e9951d6d0d" satisfied condition "success or failure"
Jun 22 19:10:04.902: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-configmaps-f3a35819-0a34-4037-8483-c2e9951d6d0d container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 19:10:04.915: INFO: Waiting for pod pod-configmaps-f3a35819-0a34-4037-8483-c2e9951d6d0d to disappear
Jun 22 19:10:04.918: INFO: Pod pod-configmaps-f3a35819-0a34-4037-8483-c2e9951d6d0d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:10:04.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1311" for this suite.
Jun 22 19:10:10.929: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:10:11.009: INFO: namespace configmap-1311 deletion completed in 6.088951333s

• [SLOW TEST:8.146 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:10:11.010: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-1567
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jun 22 19:10:11.031: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jun 22 19:10:27.110: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.133.70:8080/dial?request=hostName&protocol=udp&host=10.2.145.46&port=8081&tries=1'] Namespace:pod-network-test-1567 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 19:10:27.110: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
Jun 22 19:10:27.222: INFO: Waiting for endpoints: map[]
Jun 22 19:10:27.225: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.133.70:8080/dial?request=hostName&protocol=udp&host=10.2.133.69&port=8081&tries=1'] Namespace:pod-network-test-1567 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 19:10:27.225: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
Jun 22 19:10:27.320: INFO: Waiting for endpoints: map[]
Jun 22 19:10:27.323: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.2.133.70:8080/dial?request=hostName&protocol=udp&host=10.2.104.209&port=8081&tries=1'] Namespace:pod-network-test-1567 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 19:10:27.323: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
Jun 22 19:10:27.429: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:10:27.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1567" for this suite.
Jun 22 19:10:49.449: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:10:49.533: INFO: namespace pod-network-test-1567 deletion completed in 22.101231732s

• [SLOW TEST:38.523 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:10:49.535: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jun 22 19:10:49.613: INFO: Waiting up to 5m0s for pod "pod-1e8f03b0-a4f5-44d5-9118-11a04b3f7057" in namespace "emptydir-8273" to be "success or failure"
Jun 22 19:10:49.620: INFO: Pod "pod-1e8f03b0-a4f5-44d5-9118-11a04b3f7057": Phase="Pending", Reason="", readiness=false. Elapsed: 6.453024ms
Jun 22 19:10:51.623: INFO: Pod "pod-1e8f03b0-a4f5-44d5-9118-11a04b3f7057": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009461763s
STEP: Saw pod success
Jun 22 19:10:51.623: INFO: Pod "pod-1e8f03b0-a4f5-44d5-9118-11a04b3f7057" satisfied condition "success or failure"
Jun 22 19:10:51.625: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-1e8f03b0-a4f5-44d5-9118-11a04b3f7057 container test-container: <nil>
STEP: delete the pod
Jun 22 19:10:51.640: INFO: Waiting for pod pod-1e8f03b0-a4f5-44d5-9118-11a04b3f7057 to disappear
Jun 22 19:10:51.643: INFO: Pod pod-1e8f03b0-a4f5-44d5-9118-11a04b3f7057 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:10:51.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8273" for this suite.
Jun 22 19:10:57.654: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:10:57.738: INFO: namespace emptydir-8273 deletion completed in 6.092742091s

• [SLOW TEST:8.204 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:10:57.738: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-7721, will wait for the garbage collector to delete the pods
Jun 22 19:11:01.824: INFO: Deleting Job.batch foo took: 5.052499ms
Jun 22 19:11:02.124: INFO: Terminating Job.batch foo pods took: 300.634401ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:11:43.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7721" for this suite.
Jun 22 19:11:49.638: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:11:49.718: INFO: namespace job-7721 deletion completed in 6.08756765s

• [SLOW TEST:51.980 seconds]
[sig-apps] Job
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:11:49.719: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jun 22 19:11:49.750: INFO: Waiting up to 5m0s for pod "pod-16d16aa8-4115-48b8-b09b-9d91aea20f2a" in namespace "emptydir-3595" to be "success or failure"
Jun 22 19:11:49.755: INFO: Pod "pod-16d16aa8-4115-48b8-b09b-9d91aea20f2a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.382959ms
Jun 22 19:11:51.758: INFO: Pod "pod-16d16aa8-4115-48b8-b09b-9d91aea20f2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007691014s
STEP: Saw pod success
Jun 22 19:11:51.758: INFO: Pod "pod-16d16aa8-4115-48b8-b09b-9d91aea20f2a" satisfied condition "success or failure"
Jun 22 19:11:51.761: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-16d16aa8-4115-48b8-b09b-9d91aea20f2a container test-container: <nil>
STEP: delete the pod
Jun 22 19:11:51.786: INFO: Waiting for pod pod-16d16aa8-4115-48b8-b09b-9d91aea20f2a to disappear
Jun 22 19:11:51.788: INFO: Pod pod-16d16aa8-4115-48b8-b09b-9d91aea20f2a no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:11:51.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3595" for this suite.
Jun 22 19:11:57.801: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:11:57.883: INFO: namespace emptydir-3595 deletion completed in 6.091097598s

• [SLOW TEST:8.164 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:11:57.883: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Jun 22 19:12:00.429: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9339 pod-service-account-936d49b1-b6a4-41bc-8638-3c3f77d94b27 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jun 22 19:12:00.591: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9339 pod-service-account-936d49b1-b6a4-41bc-8638-3c3f77d94b27 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jun 22 19:12:00.756: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9339 pod-service-account-936d49b1-b6a4-41bc-8638-3c3f77d94b27 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:12:00.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9339" for this suite.
Jun 22 19:12:06.935: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:12:07.014: INFO: namespace svcaccounts-9339 deletion completed in 6.087812958s

• [SLOW TEST:9.131 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:12:07.015: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-274daf1b-47d6-431e-b32f-217e316353ac
STEP: Creating a pod to test consume configMaps
Jun 22 19:12:07.093: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7fc0e916-58da-4233-bf0d-d804fc5b3adf" in namespace "projected-6076" to be "success or failure"
Jun 22 19:12:07.095: INFO: Pod "pod-projected-configmaps-7fc0e916-58da-4233-bf0d-d804fc5b3adf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.201681ms
Jun 22 19:12:09.099: INFO: Pod "pod-projected-configmaps-7fc0e916-58da-4233-bf0d-d804fc5b3adf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005528448s
STEP: Saw pod success
Jun 22 19:12:09.099: INFO: Pod "pod-projected-configmaps-7fc0e916-58da-4233-bf0d-d804fc5b3adf" satisfied condition "success or failure"
Jun 22 19:12:09.101: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-projected-configmaps-7fc0e916-58da-4233-bf0d-d804fc5b3adf container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 19:12:09.116: INFO: Waiting for pod pod-projected-configmaps-7fc0e916-58da-4233-bf0d-d804fc5b3adf to disappear
Jun 22 19:12:09.120: INFO: Pod pod-projected-configmaps-7fc0e916-58da-4233-bf0d-d804fc5b3adf no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:12:09.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6076" for this suite.
Jun 22 19:12:15.136: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:12:15.225: INFO: namespace projected-6076 deletion completed in 6.098778669s

• [SLOW TEST:8.210 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:12:15.225: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-6c5324df-5d4f-4fd5-8e0c-6d74155475e0
STEP: Creating a pod to test consume secrets
Jun 22 19:12:15.253: INFO: Waiting up to 5m0s for pod "pod-secrets-63ace2d2-730e-4106-adb5-546cc5a60988" in namespace "secrets-1453" to be "success or failure"
Jun 22 19:12:15.256: INFO: Pod "pod-secrets-63ace2d2-730e-4106-adb5-546cc5a60988": Phase="Pending", Reason="", readiness=false. Elapsed: 3.018599ms
Jun 22 19:12:17.260: INFO: Pod "pod-secrets-63ace2d2-730e-4106-adb5-546cc5a60988": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006441148s
STEP: Saw pod success
Jun 22 19:12:17.260: INFO: Pod "pod-secrets-63ace2d2-730e-4106-adb5-546cc5a60988" satisfied condition "success or failure"
Jun 22 19:12:17.263: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-secrets-63ace2d2-730e-4106-adb5-546cc5a60988 container secret-volume-test: <nil>
STEP: delete the pod
Jun 22 19:12:17.279: INFO: Waiting for pod pod-secrets-63ace2d2-730e-4106-adb5-546cc5a60988 to disappear
Jun 22 19:12:17.282: INFO: Pod pod-secrets-63ace2d2-730e-4106-adb5-546cc5a60988 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:12:17.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1453" for this suite.
Jun 22 19:12:23.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:12:23.380: INFO: namespace secrets-1453 deletion completed in 6.095121824s

• [SLOW TEST:8.155 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:12:23.381: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-b8cfd8c1-eb2e-4f9b-b21e-5887717c5c8c
STEP: Creating a pod to test consume secrets
Jun 22 19:12:23.410: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-43511fd1-b2db-4b8f-873f-5953a90d68b0" in namespace "projected-1167" to be "success or failure"
Jun 22 19:12:23.413: INFO: Pod "pod-projected-secrets-43511fd1-b2db-4b8f-873f-5953a90d68b0": Phase="Pending", Reason="", readiness=false. Elapsed: 3.539291ms
Jun 22 19:12:25.416: INFO: Pod "pod-projected-secrets-43511fd1-b2db-4b8f-873f-5953a90d68b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006773024s
STEP: Saw pod success
Jun 22 19:12:25.416: INFO: Pod "pod-projected-secrets-43511fd1-b2db-4b8f-873f-5953a90d68b0" satisfied condition "success or failure"
Jun 22 19:12:25.419: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-projected-secrets-43511fd1-b2db-4b8f-873f-5953a90d68b0 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jun 22 19:12:25.434: INFO: Waiting for pod pod-projected-secrets-43511fd1-b2db-4b8f-873f-5953a90d68b0 to disappear
Jun 22 19:12:25.436: INFO: Pod pod-projected-secrets-43511fd1-b2db-4b8f-873f-5953a90d68b0 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:12:25.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1167" for this suite.
Jun 22 19:12:31.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:12:31.528: INFO: namespace projected-1167 deletion completed in 6.089690195s

• [SLOW TEST:8.148 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:12:31.528: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jun 22 19:12:33.568: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-689426708 exec pod-sharedvolume-13d5a76a-1c8d-42fe-abb3-6a34c5cdf16e -c busybox-main-container --namespace=emptydir-9052 -- cat /usr/share/volumeshare/shareddata.txt'
Jun 22 19:12:33.798: INFO: stderr: ""
Jun 22 19:12:33.798: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:12:33.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9052" for this suite.
Jun 22 19:12:39.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:12:39.891: INFO: namespace emptydir-9052 deletion completed in 6.090070326s

• [SLOW TEST:8.363 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:12:39.893: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-1143
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jun 22 19:12:39.935: INFO: Found 0 stateful pods, waiting for 3
Jun 22 19:12:49.939: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 19:12:49.939: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 19:12:49.939: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jun 22 19:12:49.962: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jun 22 19:12:59.992: INFO: Updating stateful set ss2
Jun 22 19:13:00.001: INFO: Waiting for Pod statefulset-1143/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 22 19:13:10.006: INFO: Waiting for Pod statefulset-1143/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Jun 22 19:13:20.065: INFO: Found 2 stateful pods, waiting for 3
Jun 22 19:13:30.069: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 19:13:30.069: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 22 19:13:30.069: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jun 22 19:13:30.090: INFO: Updating stateful set ss2
Jun 22 19:13:30.096: INFO: Waiting for Pod statefulset-1143/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 22 19:13:40.104: INFO: Waiting for Pod statefulset-1143/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jun 22 19:13:50.126: INFO: Updating stateful set ss2
Jun 22 19:13:50.133: INFO: Waiting for StatefulSet statefulset-1143/ss2 to complete update
Jun 22 19:13:50.133: INFO: Waiting for Pod statefulset-1143/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jun 22 19:14:00.149: INFO: Deleting all statefulset in ns statefulset-1143
Jun 22 19:14:00.155: INFO: Scaling statefulset ss2 to 0
Jun 22 19:14:40.188: INFO: Waiting for statefulset status.replicas updated to 0
Jun 22 19:14:40.191: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:14:40.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1143" for this suite.
Jun 22 19:14:46.213: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:14:46.298: INFO: namespace statefulset-1143 deletion completed in 6.094063077s

• [SLOW TEST:126.405 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:14:46.299: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jun 22 19:14:50.351: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5645 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 19:14:50.351: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
Jun 22 19:14:50.433: INFO: Exec stderr: ""
Jun 22 19:14:50.433: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5645 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 19:14:50.433: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
Jun 22 19:14:50.517: INFO: Exec stderr: ""
Jun 22 19:14:50.517: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5645 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 19:14:50.517: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
Jun 22 19:14:50.602: INFO: Exec stderr: ""
Jun 22 19:14:50.602: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5645 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 19:14:50.602: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
Jun 22 19:14:50.690: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jun 22 19:14:50.690: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5645 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 19:14:50.690: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
Jun 22 19:14:50.784: INFO: Exec stderr: ""
Jun 22 19:14:50.785: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5645 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 19:14:50.785: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
Jun 22 19:14:50.875: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jun 22 19:14:50.875: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5645 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 19:14:50.875: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
Jun 22 19:14:50.980: INFO: Exec stderr: ""
Jun 22 19:14:50.980: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5645 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 19:14:50.980: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
Jun 22 19:14:51.090: INFO: Exec stderr: ""
Jun 22 19:14:51.090: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5645 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 19:14:51.090: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
Jun 22 19:14:51.187: INFO: Exec stderr: ""
Jun 22 19:14:51.187: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5645 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jun 22 19:14:51.187: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
Jun 22 19:14:51.288: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:14:51.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-5645" for this suite.
Jun 22 19:15:35.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:15:35.384: INFO: namespace e2e-kubelet-etc-hosts-5645 deletion completed in 44.092043283s

• [SLOW TEST:49.085 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:15:35.385: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jun 22 19:15:37.984: INFO: Successfully updated pod "pod-update-27963062-84a1-4ae5-9707-bd89e5f6dbad"
STEP: verifying the updated pod is in kubernetes
Jun 22 19:15:37.990: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:15:37.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7519" for this suite.
Jun 22 19:16:00.017: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:16:00.192: INFO: namespace pods-7519 deletion completed in 22.198926373s

• [SLOW TEST:24.807 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:16:00.193: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-1280e081-9df9-4fb0-b79d-a1bfbeeb704f
STEP: Creating secret with name secret-projected-all-test-volume-aae08056-a905-41eb-8e1c-86cf4ae5e983
STEP: Creating a pod to test Check all projections for projected volume plugin
Jun 22 19:16:00.237: INFO: Waiting up to 5m0s for pod "projected-volume-7bcf362e-c6e5-4cd3-b5ea-c2a004e063ba" in namespace "projected-1358" to be "success or failure"
Jun 22 19:16:00.244: INFO: Pod "projected-volume-7bcf362e-c6e5-4cd3-b5ea-c2a004e063ba": Phase="Pending", Reason="", readiness=false. Elapsed: 7.286124ms
Jun 22 19:16:02.247: INFO: Pod "projected-volume-7bcf362e-c6e5-4cd3-b5ea-c2a004e063ba": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010428178s
STEP: Saw pod success
Jun 22 19:16:02.247: INFO: Pod "projected-volume-7bcf362e-c6e5-4cd3-b5ea-c2a004e063ba" satisfied condition "success or failure"
Jun 22 19:16:02.250: INFO: Trying to get logs from node ip-10-0-20-165 pod projected-volume-7bcf362e-c6e5-4cd3-b5ea-c2a004e063ba container projected-all-volume-test: <nil>
STEP: delete the pod
Jun 22 19:16:02.267: INFO: Waiting for pod projected-volume-7bcf362e-c6e5-4cd3-b5ea-c2a004e063ba to disappear
Jun 22 19:16:02.272: INFO: Pod projected-volume-7bcf362e-c6e5-4cd3-b5ea-c2a004e063ba no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:16:02.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1358" for this suite.
Jun 22 19:16:08.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:16:08.366: INFO: namespace projected-1358 deletion completed in 6.091822692s

• [SLOW TEST:8.174 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:16:08.367: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Jun 22 19:16:08.393: INFO: Waiting up to 5m0s for pod "client-containers-87420a1b-349a-45d1-a8be-5a99f9477f26" in namespace "containers-6643" to be "success or failure"
Jun 22 19:16:08.400: INFO: Pod "client-containers-87420a1b-349a-45d1-a8be-5a99f9477f26": Phase="Pending", Reason="", readiness=false. Elapsed: 6.573744ms
Jun 22 19:16:10.403: INFO: Pod "client-containers-87420a1b-349a-45d1-a8be-5a99f9477f26": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00965402s
STEP: Saw pod success
Jun 22 19:16:10.403: INFO: Pod "client-containers-87420a1b-349a-45d1-a8be-5a99f9477f26" satisfied condition "success or failure"
Jun 22 19:16:10.405: INFO: Trying to get logs from node ip-10-0-20-165 pod client-containers-87420a1b-349a-45d1-a8be-5a99f9477f26 container test-container: <nil>
STEP: delete the pod
Jun 22 19:16:10.421: INFO: Waiting for pod client-containers-87420a1b-349a-45d1-a8be-5a99f9477f26 to disappear
Jun 22 19:16:10.423: INFO: Pod client-containers-87420a1b-349a-45d1-a8be-5a99f9477f26 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:16:10.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6643" for this suite.
Jun 22 19:16:16.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:16:16.519: INFO: namespace containers-6643 deletion completed in 6.09343072s

• [SLOW TEST:8.152 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:16:16.520: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Jun 22 19:16:16.541: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-689426708 proxy --unix-socket=/tmp/kubectl-proxy-unix018539847/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:16:16.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-752" for this suite.
Jun 22 19:16:22.617: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:16:22.697: INFO: namespace kubectl-752 deletion completed in 6.090195566s

• [SLOW TEST:6.177 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:16:22.697: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 22 19:16:22.776: INFO: Waiting up to 5m0s for pod "downwardapi-volume-dd0634da-ec4f-4a82-bba7-dd5bfc6d5d60" in namespace "downward-api-2102" to be "success or failure"
Jun 22 19:16:22.780: INFO: Pod "downwardapi-volume-dd0634da-ec4f-4a82-bba7-dd5bfc6d5d60": Phase="Pending", Reason="", readiness=false. Elapsed: 4.165767ms
Jun 22 19:16:24.783: INFO: Pod "downwardapi-volume-dd0634da-ec4f-4a82-bba7-dd5bfc6d5d60": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007299548s
STEP: Saw pod success
Jun 22 19:16:24.783: INFO: Pod "downwardapi-volume-dd0634da-ec4f-4a82-bba7-dd5bfc6d5d60" satisfied condition "success or failure"
Jun 22 19:16:24.786: INFO: Trying to get logs from node ip-10-0-20-165 pod downwardapi-volume-dd0634da-ec4f-4a82-bba7-dd5bfc6d5d60 container client-container: <nil>
STEP: delete the pod
Jun 22 19:16:24.801: INFO: Waiting for pod downwardapi-volume-dd0634da-ec4f-4a82-bba7-dd5bfc6d5d60 to disappear
Jun 22 19:16:24.803: INFO: Pod downwardapi-volume-dd0634da-ec4f-4a82-bba7-dd5bfc6d5d60 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:16:24.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2102" for this suite.
Jun 22 19:16:30.814: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:16:30.898: INFO: namespace downward-api-2102 deletion completed in 6.092278377s

• [SLOW TEST:8.201 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:16:30.898: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jun 22 19:16:46.933: INFO: Container started at 2019-06-22 19:16:31 +0000 UTC, pod became ready at 2019-06-22 19:16:46 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:16:46.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5312" for this suite.
Jun 22 19:17:08.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:17:09.035: INFO: namespace container-probe-5312 deletion completed in 22.099110303s

• [SLOW TEST:38.137 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:17:09.036: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-640b74fa-84dd-43f6-bef1-2b69d1ecd208
STEP: Creating secret with name s-test-opt-upd-0460a3a8-1495-45a3-b916-3ab36b677ab8
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-640b74fa-84dd-43f6-bef1-2b69d1ecd208
STEP: Updating secret s-test-opt-upd-0460a3a8-1495-45a3-b916-3ab36b677ab8
STEP: Creating secret with name s-test-opt-create-42f4e002-4c6b-47b4-a189-30084ec73dcf
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:17:13.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1549" for this suite.
Jun 22 19:17:35.146: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:17:35.226: INFO: namespace secrets-1549 deletion completed in 22.092365526s

• [SLOW TEST:26.191 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:17:35.227: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jun 22 19:17:35.253: INFO: Waiting up to 5m0s for pod "downwardapi-volume-53f34cf5-ca27-471d-accd-d610d2e70b20" in namespace "downward-api-2788" to be "success or failure"
Jun 22 19:17:35.256: INFO: Pod "downwardapi-volume-53f34cf5-ca27-471d-accd-d610d2e70b20": Phase="Pending", Reason="", readiness=false. Elapsed: 3.253711ms
Jun 22 19:17:37.260: INFO: Pod "downwardapi-volume-53f34cf5-ca27-471d-accd-d610d2e70b20": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006596608s
STEP: Saw pod success
Jun 22 19:17:37.260: INFO: Pod "downwardapi-volume-53f34cf5-ca27-471d-accd-d610d2e70b20" satisfied condition "success or failure"
Jun 22 19:17:37.262: INFO: Trying to get logs from node ip-10-0-20-165 pod downwardapi-volume-53f34cf5-ca27-471d-accd-d610d2e70b20 container client-container: <nil>
STEP: delete the pod
Jun 22 19:17:37.281: INFO: Waiting for pod downwardapi-volume-53f34cf5-ca27-471d-accd-d610d2e70b20 to disappear
Jun 22 19:17:37.284: INFO: Pod downwardapi-volume-53f34cf5-ca27-471d-accd-d610d2e70b20 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:17:37.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2788" for this suite.
Jun 22 19:17:43.303: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:17:43.380: INFO: namespace downward-api-2788 deletion completed in 6.093063848s

• [SLOW TEST:8.153 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:17:43.381: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-9139/configmap-test-2117413f-da5c-4bd1-b773-2e81f8ebdf58
STEP: Creating a pod to test consume configMaps
Jun 22 19:17:43.408: INFO: Waiting up to 5m0s for pod "pod-configmaps-b84261bb-3ec9-43d8-9433-33bad1f3a4ca" in namespace "configmap-9139" to be "success or failure"
Jun 22 19:17:43.411: INFO: Pod "pod-configmaps-b84261bb-3ec9-43d8-9433-33bad1f3a4ca": Phase="Pending", Reason="", readiness=false. Elapsed: 2.773305ms
Jun 22 19:17:45.413: INFO: Pod "pod-configmaps-b84261bb-3ec9-43d8-9433-33bad1f3a4ca": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005542293s
STEP: Saw pod success
Jun 22 19:17:45.413: INFO: Pod "pod-configmaps-b84261bb-3ec9-43d8-9433-33bad1f3a4ca" satisfied condition "success or failure"
Jun 22 19:17:45.416: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-configmaps-b84261bb-3ec9-43d8-9433-33bad1f3a4ca container env-test: <nil>
STEP: delete the pod
Jun 22 19:17:45.430: INFO: Waiting for pod pod-configmaps-b84261bb-3ec9-43d8-9433-33bad1f3a4ca to disappear
Jun 22 19:17:45.433: INFO: Pod pod-configmaps-b84261bb-3ec9-43d8-9433-33bad1f3a4ca no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:17:45.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9139" for this suite.
Jun 22 19:17:51.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:17:51.524: INFO: namespace configmap-9139 deletion completed in 6.088430093s

• [SLOW TEST:8.143 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:17:51.524: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:17:53.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-9294" for this suite.
Jun 22 19:18:31.581: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:18:31.661: INFO: namespace kubelet-test-9294 deletion completed in 38.089683662s

• [SLOW TEST:40.136 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:18:31.661: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jun 22 19:18:31.690: INFO: Waiting up to 5m0s for pod "pod-e8b93161-b7cc-48e0-a8f7-3fd1cee7fe01" in namespace "emptydir-9683" to be "success or failure"
Jun 22 19:18:31.692: INFO: Pod "pod-e8b93161-b7cc-48e0-a8f7-3fd1cee7fe01": Phase="Pending", Reason="", readiness=false. Elapsed: 2.827789ms
Jun 22 19:18:33.696: INFO: Pod "pod-e8b93161-b7cc-48e0-a8f7-3fd1cee7fe01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005934398s
STEP: Saw pod success
Jun 22 19:18:33.696: INFO: Pod "pod-e8b93161-b7cc-48e0-a8f7-3fd1cee7fe01" satisfied condition "success or failure"
Jun 22 19:18:33.698: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-e8b93161-b7cc-48e0-a8f7-3fd1cee7fe01 container test-container: <nil>
STEP: delete the pod
Jun 22 19:18:33.712: INFO: Waiting for pod pod-e8b93161-b7cc-48e0-a8f7-3fd1cee7fe01 to disappear
Jun 22 19:18:33.714: INFO: Pod pod-e8b93161-b7cc-48e0-a8f7-3fd1cee7fe01 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:18:33.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9683" for this suite.
Jun 22 19:18:39.727: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:18:39.809: INFO: namespace emptydir-9683 deletion completed in 6.092264663s

• [SLOW TEST:8.148 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:18:39.812: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Jun 22 19:18:40.353: INFO: created pod pod-service-account-defaultsa
Jun 22 19:18:40.353: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun 22 19:18:40.363: INFO: created pod pod-service-account-mountsa
Jun 22 19:18:40.363: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun 22 19:18:40.378: INFO: created pod pod-service-account-nomountsa
Jun 22 19:18:40.378: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun 22 19:18:40.382: INFO: created pod pod-service-account-defaultsa-mountspec
Jun 22 19:18:40.382: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun 22 19:18:40.386: INFO: created pod pod-service-account-mountsa-mountspec
Jun 22 19:18:40.386: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun 22 19:18:40.394: INFO: created pod pod-service-account-nomountsa-mountspec
Jun 22 19:18:40.394: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun 22 19:18:40.407: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun 22 19:18:40.407: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun 22 19:18:40.414: INFO: created pod pod-service-account-mountsa-nomountspec
Jun 22 19:18:40.414: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun 22 19:18:40.420: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun 22 19:18:40.420: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:18:40.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9137" for this suite.
Jun 22 19:18:46.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:18:46.538: INFO: namespace svcaccounts-9137 deletion completed in 6.108251015s

• [SLOW TEST:6.726 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:18:46.539: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-w9jb
STEP: Creating a pod to test atomic-volume-subpath
Jun 22 19:18:46.572: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-w9jb" in namespace "subpath-6082" to be "success or failure"
Jun 22 19:18:46.574: INFO: Pod "pod-subpath-test-projected-w9jb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.302637ms
Jun 22 19:18:48.577: INFO: Pod "pod-subpath-test-projected-w9jb": Phase="Running", Reason="", readiness=true. Elapsed: 2.005291562s
Jun 22 19:18:50.580: INFO: Pod "pod-subpath-test-projected-w9jb": Phase="Running", Reason="", readiness=true. Elapsed: 4.008304856s
Jun 22 19:18:52.583: INFO: Pod "pod-subpath-test-projected-w9jb": Phase="Running", Reason="", readiness=true. Elapsed: 6.011569335s
Jun 22 19:18:54.589: INFO: Pod "pod-subpath-test-projected-w9jb": Phase="Running", Reason="", readiness=true. Elapsed: 8.017792805s
Jun 22 19:18:56.593: INFO: Pod "pod-subpath-test-projected-w9jb": Phase="Running", Reason="", readiness=true. Elapsed: 10.020921319s
Jun 22 19:18:58.596: INFO: Pod "pod-subpath-test-projected-w9jb": Phase="Running", Reason="", readiness=true. Elapsed: 12.024044654s
Jun 22 19:19:00.599: INFO: Pod "pod-subpath-test-projected-w9jb": Phase="Running", Reason="", readiness=true. Elapsed: 14.027221437s
Jun 22 19:19:02.602: INFO: Pod "pod-subpath-test-projected-w9jb": Phase="Running", Reason="", readiness=true. Elapsed: 16.030105308s
Jun 22 19:19:04.606: INFO: Pod "pod-subpath-test-projected-w9jb": Phase="Running", Reason="", readiness=true. Elapsed: 18.03445317s
Jun 22 19:19:06.609: INFO: Pod "pod-subpath-test-projected-w9jb": Phase="Running", Reason="", readiness=true. Elapsed: 20.037647061s
Jun 22 19:19:08.612: INFO: Pod "pod-subpath-test-projected-w9jb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.040613153s
STEP: Saw pod success
Jun 22 19:19:08.612: INFO: Pod "pod-subpath-test-projected-w9jb" satisfied condition "success or failure"
Jun 22 19:19:08.615: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-subpath-test-projected-w9jb container test-container-subpath-projected-w9jb: <nil>
STEP: delete the pod
Jun 22 19:19:08.629: INFO: Waiting for pod pod-subpath-test-projected-w9jb to disappear
Jun 22 19:19:08.631: INFO: Pod pod-subpath-test-projected-w9jb no longer exists
STEP: Deleting pod pod-subpath-test-projected-w9jb
Jun 22 19:19:08.631: INFO: Deleting pod "pod-subpath-test-projected-w9jb" in namespace "subpath-6082"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:19:08.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6082" for this suite.
Jun 22 19:19:14.644: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:19:14.725: INFO: namespace subpath-6082 deletion completed in 6.089578011s

• [SLOW TEST:28.186 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:19:14.726: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jun 22 19:19:14.800: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:19:18.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-6160" for this suite.
Jun 22 19:19:40.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:19:40.277: INFO: namespace init-container-6160 deletion completed in 22.093316245s

• [SLOW TEST:25.551 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:19:40.277: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:19:40.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-379" for this suite.
Jun 22 19:19:46.321: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:19:46.402: INFO: namespace services-379 deletion completed in 6.094410555s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.125 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:19:46.402: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-63ec9959-50ff-4b35-901a-538dc42b835b
STEP: Creating a pod to test consume configMaps
Jun 22 19:19:46.432: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b993de89-6d20-4dbf-83bc-6ae7a0875a2b" in namespace "projected-1528" to be "success or failure"
Jun 22 19:19:46.436: INFO: Pod "pod-projected-configmaps-b993de89-6d20-4dbf-83bc-6ae7a0875a2b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.396832ms
Jun 22 19:19:48.439: INFO: Pod "pod-projected-configmaps-b993de89-6d20-4dbf-83bc-6ae7a0875a2b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006678655s
STEP: Saw pod success
Jun 22 19:19:48.439: INFO: Pod "pod-projected-configmaps-b993de89-6d20-4dbf-83bc-6ae7a0875a2b" satisfied condition "success or failure"
Jun 22 19:19:48.442: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-projected-configmaps-b993de89-6d20-4dbf-83bc-6ae7a0875a2b container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 19:19:48.457: INFO: Waiting for pod pod-projected-configmaps-b993de89-6d20-4dbf-83bc-6ae7a0875a2b to disappear
Jun 22 19:19:48.459: INFO: Pod pod-projected-configmaps-b993de89-6d20-4dbf-83bc-6ae7a0875a2b no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:19:48.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1528" for this suite.
Jun 22 19:19:54.475: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:19:54.555: INFO: namespace projected-1528 deletion completed in 6.092747257s

• [SLOW TEST:8.153 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:19:54.555: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jun 22 19:19:54.591: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7896,SelfLink:/api/v1/namespaces/watch-7896/configmaps/e2e-watch-test-label-changed,UID:ee6b8280-f4bc-42b5-8d97-3ccee5738c4e,ResourceVersion:23627,Generation:0,CreationTimestamp:2019-06-22 19:19:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jun 22 19:19:54.591: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7896,SelfLink:/api/v1/namespaces/watch-7896/configmaps/e2e-watch-test-label-changed,UID:ee6b8280-f4bc-42b5-8d97-3ccee5738c4e,ResourceVersion:23628,Generation:0,CreationTimestamp:2019-06-22 19:19:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jun 22 19:19:54.591: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7896,SelfLink:/api/v1/namespaces/watch-7896/configmaps/e2e-watch-test-label-changed,UID:ee6b8280-f4bc-42b5-8d97-3ccee5738c4e,ResourceVersion:23629,Generation:0,CreationTimestamp:2019-06-22 19:19:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jun 22 19:20:04.611: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7896,SelfLink:/api/v1/namespaces/watch-7896/configmaps/e2e-watch-test-label-changed,UID:ee6b8280-f4bc-42b5-8d97-3ccee5738c4e,ResourceVersion:23647,Generation:0,CreationTimestamp:2019-06-22 19:19:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jun 22 19:20:04.611: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7896,SelfLink:/api/v1/namespaces/watch-7896/configmaps/e2e-watch-test-label-changed,UID:ee6b8280-f4bc-42b5-8d97-3ccee5738c4e,ResourceVersion:23648,Generation:0,CreationTimestamp:2019-06-22 19:19:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jun 22 19:20:04.612: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-7896,SelfLink:/api/v1/namespaces/watch-7896/configmaps/e2e-watch-test-label-changed,UID:ee6b8280-f4bc-42b5-8d97-3ccee5738c4e,ResourceVersion:23649,Generation:0,CreationTimestamp:2019-06-22 19:19:54 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:20:04.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7896" for this suite.
Jun 22 19:20:10.623: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:20:10.708: INFO: namespace watch-7896 deletion completed in 6.093302147s

• [SLOW TEST:16.153 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:20:10.709: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jun 22 19:20:13.259: INFO: Successfully updated pod "annotationupdate47f9ae4f-faad-4cfe-a0ae-233cf08cd735"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:20:17.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-157" for this suite.
Jun 22 19:20:39.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:20:39.387: INFO: namespace downward-api-157 deletion completed in 22.097225661s

• [SLOW TEST:28.678 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:20:39.387: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-6613/secret-test-234a45bb-99fe-4080-90bf-8703e04a449e
STEP: Creating a pod to test consume secrets
Jun 22 19:20:39.430: INFO: Waiting up to 5m0s for pod "pod-configmaps-7d5c7002-0ce1-438b-b6fa-b18b95d0ec42" in namespace "secrets-6613" to be "success or failure"
Jun 22 19:20:39.435: INFO: Pod "pod-configmaps-7d5c7002-0ce1-438b-b6fa-b18b95d0ec42": Phase="Pending", Reason="", readiness=false. Elapsed: 4.39895ms
Jun 22 19:20:41.438: INFO: Pod "pod-configmaps-7d5c7002-0ce1-438b-b6fa-b18b95d0ec42": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007578247s
STEP: Saw pod success
Jun 22 19:20:41.438: INFO: Pod "pod-configmaps-7d5c7002-0ce1-438b-b6fa-b18b95d0ec42" satisfied condition "success or failure"
Jun 22 19:20:41.441: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-configmaps-7d5c7002-0ce1-438b-b6fa-b18b95d0ec42 container env-test: <nil>
STEP: delete the pod
Jun 22 19:20:41.455: INFO: Waiting for pod pod-configmaps-7d5c7002-0ce1-438b-b6fa-b18b95d0ec42 to disappear
Jun 22 19:20:41.457: INFO: Pod pod-configmaps-7d5c7002-0ce1-438b-b6fa-b18b95d0ec42 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:20:41.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6613" for this suite.
Jun 22 19:20:47.471: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:20:47.552: INFO: namespace secrets-6613 deletion completed in 6.091918344s

• [SLOW TEST:8.165 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jun 22 19:20:47.554: INFO: >>> kubeConfig: /tmp/kubeconfig-689426708
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-17786a09-6889-4746-8ac5-327354b01d4a
STEP: Creating a pod to test consume configMaps
Jun 22 19:20:47.582: INFO: Waiting up to 5m0s for pod "pod-configmaps-1ef73beb-004d-45da-80a2-6e415ce6acae" in namespace "configmap-5788" to be "success or failure"
Jun 22 19:20:47.585: INFO: Pod "pod-configmaps-1ef73beb-004d-45da-80a2-6e415ce6acae": Phase="Pending", Reason="", readiness=false. Elapsed: 2.340948ms
Jun 22 19:20:49.588: INFO: Pod "pod-configmaps-1ef73beb-004d-45da-80a2-6e415ce6acae": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005498946s
STEP: Saw pod success
Jun 22 19:20:49.588: INFO: Pod "pod-configmaps-1ef73beb-004d-45da-80a2-6e415ce6acae" satisfied condition "success or failure"
Jun 22 19:20:49.590: INFO: Trying to get logs from node ip-10-0-20-165 pod pod-configmaps-1ef73beb-004d-45da-80a2-6e415ce6acae container configmap-volume-test: <nil>
STEP: delete the pod
Jun 22 19:20:49.606: INFO: Waiting for pod pod-configmaps-1ef73beb-004d-45da-80a2-6e415ce6acae to disappear
Jun 22 19:20:49.608: INFO: Pod pod-configmaps-1ef73beb-004d-45da-80a2-6e415ce6acae no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jun 22 19:20:49.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5788" for this suite.
Jun 22 19:20:55.619: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jun 22 19:20:55.700: INFO: namespace configmap-5788 deletion completed in 6.088930412s

• [SLOW TEST:8.145 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSJun 22 19:20:55.700: INFO: Running AfterSuite actions on all nodes
Jun 22 19:20:55.701: INFO: Running AfterSuite actions on node 1
Jun 22 19:20:55.701: INFO: Skipping dumping logs from cluster

Ran 215 of 4411 Specs in 5391.651 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4196 Skipped
PASS

Ginkgo ran 1 suite in 1h29m53.360638133s
Test Suite Passed
