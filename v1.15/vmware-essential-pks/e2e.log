I0725 08:49:10.877508      16 test_context.go:406] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-739010025
I0725 08:49:10.877941      16 e2e.go:241] Starting e2e run "40426138-2e3f-462c-85c1-7c4a344941db" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1564044549 - Will randomize all specs
Will run 215 of 4411 specs

Jul 25 08:49:11.106: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
Jul 25 08:49:11.108: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jul 25 08:49:11.126: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jul 25 08:49:11.157: INFO: 12 / 12 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jul 25 08:49:11.157: INFO: expected 2 pod replicas in namespace 'kube-system', 2 are Running and Ready.
Jul 25 08:49:11.157: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jul 25 08:49:11.166: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-amd64' (0 seconds elapsed)
Jul 25 08:49:11.167: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm' (0 seconds elapsed)
Jul 25 08:49:11.167: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-arm64' (0 seconds elapsed)
Jul 25 08:49:11.167: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-ppc64le' (0 seconds elapsed)
Jul 25 08:49:11.167: INFO: 0 / 0 pods ready in namespace 'kube-system' in daemonset 'kube-flannel-ds-s390x' (0 seconds elapsed)
Jul 25 08:49:11.167: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Jul 25 08:49:11.167: INFO: e2e test version: v1.15.0
Jul 25 08:49:11.168: INFO: kube-apiserver version: v1.15.1+vmware.1
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 08:49:11.168: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename container-lifecycle-hook
Jul 25 08:49:11.200: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 25 08:49:17.249: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 25 08:49:17.255: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 25 08:49:19.255: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 25 08:49:19.258: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 25 08:49:21.255: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 25 08:49:21.258: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 25 08:49:23.255: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 25 08:49:23.260: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 25 08:49:25.255: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 25 08:49:25.259: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 25 08:49:27.255: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 25 08:49:27.260: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 25 08:49:29.255: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 25 08:49:29.259: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 25 08:49:31.255: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 25 08:49:31.259: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 25 08:49:33.255: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 25 08:49:33.261: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 25 08:49:35.255: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 25 08:49:35.260: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 25 08:49:37.255: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 25 08:49:37.258: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 25 08:49:39.255: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 25 08:49:39.258: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 25 08:49:41.255: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 25 08:49:41.259: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 25 08:49:43.255: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 25 08:49:43.259: INFO: Pod pod-with-poststart-exec-hook still exists
Jul 25 08:49:45.255: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jul 25 08:49:45.259: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 08:49:45.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8402" for this suite.
Jul 25 08:50:07.273: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 08:50:07.373: INFO: namespace container-lifecycle-hook-8402 deletion completed in 22.111451981s

• [SLOW TEST:56.205 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 08:50:07.374: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 25 08:50:07.408: INFO: Waiting up to 5m0s for pod "downwardapi-volume-66d7bbf3-2ec4-461b-8402-7b50e3461651" in namespace "downward-api-6926" to be "success or failure"
Jul 25 08:50:07.413: INFO: Pod "downwardapi-volume-66d7bbf3-2ec4-461b-8402-7b50e3461651": Phase="Pending", Reason="", readiness=false. Elapsed: 5.204789ms
Jul 25 08:50:09.418: INFO: Pod "downwardapi-volume-66d7bbf3-2ec4-461b-8402-7b50e3461651": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010052853s
STEP: Saw pod success
Jul 25 08:50:09.418: INFO: Pod "downwardapi-volume-66d7bbf3-2ec4-461b-8402-7b50e3461651" satisfied condition "success or failure"
Jul 25 08:50:09.421: INFO: Trying to get logs from node essentialpks-conformance-3 pod downwardapi-volume-66d7bbf3-2ec4-461b-8402-7b50e3461651 container client-container: <nil>
STEP: delete the pod
Jul 25 08:50:09.438: INFO: Waiting for pod downwardapi-volume-66d7bbf3-2ec4-461b-8402-7b50e3461651 to disappear
Jul 25 08:50:09.441: INFO: Pod downwardapi-volume-66d7bbf3-2ec4-461b-8402-7b50e3461651 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 08:50:09.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6926" for this suite.
Jul 25 08:50:15.456: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 08:50:15.538: INFO: namespace downward-api-6926 deletion completed in 6.093625482s

• [SLOW TEST:8.165 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 08:50:15.542: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-2608
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 25 08:50:15.568: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 25 08:50:35.643: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.196:8080/dial?request=hostName&protocol=udp&host=10.244.2.195&port=8081&tries=1'] Namespace:pod-network-test-2608 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 25 08:50:35.644: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
Jul 25 08:50:35.784: INFO: Waiting for endpoints: map[]
Jul 25 08:50:35.786: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.196:8080/dial?request=hostName&protocol=udp&host=10.244.1.253&port=8081&tries=1'] Namespace:pod-network-test-2608 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 25 08:50:35.787: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
Jul 25 08:50:35.904: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 08:50:35.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2608" for this suite.
Jul 25 08:50:57.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 08:50:58.025: INFO: namespace pod-network-test-2608 deletion completed in 22.11423443s

• [SLOW TEST:42.483 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 08:50:58.026: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-33190aba-5a5a-4fb3-af9c-aa0640cf3dd2 in namespace container-probe-5509
Jul 25 08:51:04.079: INFO: Started pod liveness-33190aba-5a5a-4fb3-af9c-aa0640cf3dd2 in namespace container-probe-5509
STEP: checking the pod's current state and verifying that restartCount is present
Jul 25 08:51:04.082: INFO: Initial restart count of pod liveness-33190aba-5a5a-4fb3-af9c-aa0640cf3dd2 is 0
Jul 25 08:51:22.129: INFO: Restart count of pod container-probe-5509/liveness-33190aba-5a5a-4fb3-af9c-aa0640cf3dd2 is now 1 (18.046051835s elapsed)
Jul 25 08:51:42.168: INFO: Restart count of pod container-probe-5509/liveness-33190aba-5a5a-4fb3-af9c-aa0640cf3dd2 is now 2 (38.085935581s elapsed)
Jul 25 08:52:02.216: INFO: Restart count of pod container-probe-5509/liveness-33190aba-5a5a-4fb3-af9c-aa0640cf3dd2 is now 3 (58.133446837s elapsed)
Jul 25 08:52:22.266: INFO: Restart count of pod container-probe-5509/liveness-33190aba-5a5a-4fb3-af9c-aa0640cf3dd2 is now 4 (1m18.183356504s elapsed)
Jul 25 08:53:22.392: INFO: Restart count of pod container-probe-5509/liveness-33190aba-5a5a-4fb3-af9c-aa0640cf3dd2 is now 5 (2m18.309465381s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 08:53:22.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5509" for this suite.
Jul 25 08:53:28.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 08:53:28.492: INFO: namespace container-probe-5509 deletion completed in 6.085821478s

• [SLOW TEST:150.467 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 08:53:28.493: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 25 08:53:28.516: INFO: Creating deployment "nginx-deployment"
Jul 25 08:53:28.520: INFO: Waiting for observed generation 1
Jul 25 08:53:30.534: INFO: Waiting for all required pods to come up
Jul 25 08:53:30.548: INFO: Pod name nginx: Found 10 pods out of 10
STEP: ensuring each pod is running
Jul 25 08:53:32.563: INFO: Waiting for deployment "nginx-deployment" to complete
Jul 25 08:53:32.569: INFO: Updating deployment "nginx-deployment" with a non-existent image
Jul 25 08:53:32.577: INFO: Updating deployment nginx-deployment
Jul 25 08:53:32.577: INFO: Waiting for observed generation 2
Jul 25 08:53:34.596: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jul 25 08:53:34.598: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jul 25 08:53:34.600: INFO: Waiting for the first rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul 25 08:53:34.606: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jul 25 08:53:34.606: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jul 25 08:53:34.608: INFO: Waiting for the second rollout's replicaset of deployment "nginx-deployment" to have desired number of replicas
Jul 25 08:53:34.611: INFO: Verifying that deployment "nginx-deployment" has minimum required number of available replicas
Jul 25 08:53:34.611: INFO: Scaling up the deployment "nginx-deployment" from 10 to 30
Jul 25 08:53:34.617: INFO: Updating deployment nginx-deployment
Jul 25 08:53:34.617: INFO: Waiting for the replicasets of deployment "nginx-deployment" to have desired number of replicas
Jul 25 08:53:34.621: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jul 25 08:53:34.626: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 25 08:53:34.659: INFO: Deployment "nginx-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment,GenerateName:,Namespace:deployment-728,SelfLink:/apis/apps/v1/namespaces/deployment-728/deployments/nginx-deployment,UID:6c94267a-c6af-43c2-9497-0ce9810476eb,ResourceVersion:56630,Generation:3,CreationTimestamp:2019-07-25 08:53:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*30,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[{Progressing True 2019-07-25 08:53:32 +0000 UTC 2019-07-25 08:53:28 +0000 UTC ReplicaSetUpdated ReplicaSet "nginx-deployment-55fb7cb77f" is progressing.} {Available False 2019-07-25 08:53:34 +0000 UTC 2019-07-25 08:53:34 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}],ReadyReplicas:8,CollisionCount:nil,},}

Jul 25 08:53:34.683: INFO: New ReplicaSet "nginx-deployment-55fb7cb77f" of Deployment "nginx-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f,GenerateName:,Namespace:deployment-728,SelfLink:/apis/apps/v1/namespaces/deployment-728/replicasets/nginx-deployment-55fb7cb77f,UID:9183d45e-9164-4271-8776-62ead48889bb,ResourceVersion:56625,Generation:3,CreationTimestamp:2019-07-25 08:53:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 6c94267a-c6af-43c2-9497-0ce9810476eb 0xc000d6fa57 0xc000d6fa58}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*13,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 25 08:53:34.683: INFO: All old ReplicaSets of Deployment "nginx-deployment":
Jul 25 08:53:34.683: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498,GenerateName:,Namespace:deployment-728,SelfLink:/apis/apps/v1/namespaces/deployment-728/replicasets/nginx-deployment-7b8c6f4498,UID:f066502e-6136-4434-9524-fb9fef05d697,ResourceVersion:56622,Generation:3,CreationTimestamp:2019-07-25 08:53:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 30,deployment.kubernetes.io/max-replicas: 33,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment nginx-deployment 6c94267a-c6af-43c2-9497-0ce9810476eb 0xc000d6fb67 0xc000d6fb68}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*20,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[],},}
Jul 25 08:53:34.706: INFO: Pod "nginx-deployment-55fb7cb77f-5bvrw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-5bvrw,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-55fb7cb77f-5bvrw,UID:a66c47f4-b8bb-4c76-aef0-529446894878,ResourceVersion:56662,Generation:0,CreationTimestamp:2019-07-25 08:53:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 9183d45e-9164-4271-8776-62ead48889bb 0xc000af58b7 0xc000af58b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000af5920} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000af5950}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.706: INFO: Pod "nginx-deployment-55fb7cb77f-85ct5" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-85ct5,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-55fb7cb77f-85ct5,UID:c90ea301-00c3-483e-93c3-6dc8b1fe9ea2,ResourceVersion:56581,Generation:0,CreationTimestamp:2019-07-25 08:53:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 9183d45e-9164-4271-8776-62ead48889bb 0xc000af59d0 0xc000af59d1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000af5a60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000af5a80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:32 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.4,PodIP:,StartTime:2019-07-25 08:53:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.707: INFO: Pod "nginx-deployment-55fb7cb77f-9gdgc" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-9gdgc,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-55fb7cb77f-9gdgc,UID:26c15b72-1ba6-4f96-a7c0-9ce95c22ea4c,ResourceVersion:56665,Generation:0,CreationTimestamp:2019-07-25 08:53:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 9183d45e-9164-4271-8776-62ead48889bb 0xc000af5b60 0xc000af5b61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000af5be0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000af5c50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.707: INFO: Pod "nginx-deployment-55fb7cb77f-dh4k9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-dh4k9,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-55fb7cb77f-dh4k9,UID:1d15dbb0-5441-4559-902a-c49483e49fa9,ResourceVersion:56588,Generation:0,CreationTimestamp:2019-07-25 08:53:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 9183d45e-9164-4271-8776-62ead48889bb 0xc000af5cf0 0xc000af5cf1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000af5d60} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000af5d80}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:32 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:,StartTime:2019-07-25 08:53:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.707: INFO: Pod "nginx-deployment-55fb7cb77f-hjpbw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-hjpbw,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-55fb7cb77f-hjpbw,UID:a9305af1-9bdb-4e0a-8450-5b9df6748bc5,ResourceVersion:56647,Generation:0,CreationTimestamp:2019-07-25 08:53:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 9183d45e-9164-4271-8776-62ead48889bb 0xc000af5e80 0xc000af5e81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00096e040} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00096e060}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.707: INFO: Pod "nginx-deployment-55fb7cb77f-ks7dz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-ks7dz,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-55fb7cb77f-ks7dz,UID:e807ecd6-b788-43dc-9ac3-b3cab1aeb282,ResourceVersion:56607,Generation:0,CreationTimestamp:2019-07-25 08:53:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 9183d45e-9164-4271-8776-62ead48889bb 0xc00096e0f0 0xc00096e0f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00096e170} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00096e190}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:32 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.4,PodIP:,StartTime:2019-07-25 08:53:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.707: INFO: Pod "nginx-deployment-55fb7cb77f-p7mc2" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-p7mc2,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-55fb7cb77f-p7mc2,UID:4dd24d1d-bccc-44c8-8560-e75749f96abb,ResourceVersion:56666,Generation:0,CreationTimestamp:2019-07-25 08:53:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 9183d45e-9164-4271-8776-62ead48889bb 0xc00096e260 0xc00096e261}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00096e2d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00096e2f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.707: INFO: Pod "nginx-deployment-55fb7cb77f-qgvv8" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-qgvv8,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-55fb7cb77f-qgvv8,UID:316130c4-0c0c-4d54-9260-cfd0f481761c,ResourceVersion:56649,Generation:0,CreationTimestamp:2019-07-25 08:53:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 9183d45e-9164-4271-8776-62ead48889bb 0xc00096e370 0xc00096e371}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00096e3e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00096e400}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.708: INFO: Pod "nginx-deployment-55fb7cb77f-tm7bv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-tm7bv,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-55fb7cb77f-tm7bv,UID:d03257a5-de4e-415c-8d46-860c37d7214d,ResourceVersion:56609,Generation:0,CreationTimestamp:2019-07-25 08:53:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 9183d45e-9164-4271-8776-62ead48889bb 0xc00096e480 0xc00096e481}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00096e510} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00096e530}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:32 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.4,PodIP:,StartTime:2019-07-25 08:53:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.708: INFO: Pod "nginx-deployment-55fb7cb77f-tpr2r" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-tpr2r,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-55fb7cb77f-tpr2r,UID:2c37855a-bd92-4169-a22d-bb6a34593049,ResourceVersion:56582,Generation:0,CreationTimestamp:2019-07-25 08:53:32 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 9183d45e-9164-4271-8776-62ead48889bb 0xc00096e6a0 0xc00096e6a1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00096e7e0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00096e800}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:32 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:32 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:,StartTime:2019-07-25 08:53:32 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 nginx:404  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.708: INFO: Pod "nginx-deployment-55fb7cb77f-v9l72" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-v9l72,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-55fb7cb77f-v9l72,UID:4db8d989-50fa-48ed-a80b-e943e318feda,ResourceVersion:56639,Generation:0,CreationTimestamp:2019-07-25 08:53:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 9183d45e-9164-4271-8776-62ead48889bb 0xc00096e980 0xc00096e981}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00096eab0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00096eaf0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.708: INFO: Pod "nginx-deployment-55fb7cb77f-vlhlq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-55fb7cb77f-vlhlq,GenerateName:nginx-deployment-55fb7cb77f-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-55fb7cb77f-vlhlq,UID:261446e0-8141-4dfe-9e6c-42147f6a847a,ResourceVersion:56667,Generation:0,CreationTimestamp:2019-07-25 08:53:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 55fb7cb77f,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-55fb7cb77f 9183d45e-9164-4271-8776-62ead48889bb 0xc00096eb80 0xc00096eb81}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx nginx:404 [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00096ec90} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00096ece0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.708: INFO: Pod "nginx-deployment-7b8c6f4498-5n2bw" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-5n2bw,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-7b8c6f4498-5n2bw,UID:2a3eb3bf-f81c-4603-b069-15a6b6efaf7e,ResourceVersion:56635,Generation:0,CreationTimestamp:2019-07-25 08:53:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f066502e-6136-4434-9524-fb9fef05d697 0xc00096ed60 0xc00096ed61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00096edc0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00096ee00}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.708: INFO: Pod "nginx-deployment-7b8c6f4498-6ppkt" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-6ppkt,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-7b8c6f4498-6ppkt,UID:4e0d0def-0275-4a76-ac75-384500b2ac37,ResourceVersion:56659,Generation:0,CreationTimestamp:2019-07-25 08:53:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f066502e-6136-4434-9524-fb9fef05d697 0xc00096ef20 0xc00096ef21}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00096efd0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00096eff0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.709: INFO: Pod "nginx-deployment-7b8c6f4498-7pzfn" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-7pzfn,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-7b8c6f4498-7pzfn,UID:007178a6-f62b-4b90-a1b7-58a909c60493,ResourceVersion:56512,Generation:0,CreationTimestamp:2019-07-25 08:53:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f066502e-6136-4434-9524-fb9fef05d697 0xc00096f097 0xc00096f098}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00096f100} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00096f120}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:28 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:10.244.2.197,StartTime:2019-07-25 08:53:28 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-25 08:53:29 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://3ba3050991230484c4cd6f27cec10870d790e7a7be158326cc47e4010682d19c}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.709: INFO: Pod "nginx-deployment-7b8c6f4498-88zlz" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-88zlz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-7b8c6f4498-88zlz,UID:ec841698-18fc-4219-ad26-81ee20b60f28,ResourceVersion:56645,Generation:0,CreationTimestamp:2019-07-25 08:53:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f066502e-6136-4434-9524-fb9fef05d697 0xc00096f2b7 0xc00096f2b8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00096f3d0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00096f410}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.709: INFO: Pod "nginx-deployment-7b8c6f4498-bfn45" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-bfn45,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-7b8c6f4498-bfn45,UID:bbafcec9-eedb-4fa5-8c4d-d8680a0646b6,ResourceVersion:56656,Generation:0,CreationTimestamp:2019-07-25 08:53:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f066502e-6136-4434-9524-fb9fef05d697 0xc00096f540 0xc00096f541}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00096f620} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00096f6c0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.709: INFO: Pod "nginx-deployment-7b8c6f4498-dc5m2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-dc5m2,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-7b8c6f4498-dc5m2,UID:8ca234de-6464-4244-a477-f891fbf3222b,ResourceVersion:56533,Generation:0,CreationTimestamp:2019-07-25 08:53:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f066502e-6136-4434-9524-fb9fef05d697 0xc00096f9e7 0xc00096f9e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00096fb40} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00096fb60}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:28 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:10.244.2.200,StartTime:2019-07-25 08:53:28 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-25 08:53:30 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://b06d74563964a663438091ea5bb6762c38946dccf90ba3c31e7daee9e96b8167}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.709: INFO: Pod "nginx-deployment-7b8c6f4498-dh7qv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-dh7qv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-7b8c6f4498-dh7qv,UID:4b23db35-23c0-4611-bb51-67fc94a74e97,ResourceVersion:56521,Generation:0,CreationTimestamp:2019-07-25 08:53:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f066502e-6136-4434-9524-fb9fef05d697 0xc00096fc97 0xc00096fc98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00040c180} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00040c1b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:28 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:10.244.2.198,StartTime:2019-07-25 08:53:28 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-25 08:53:29 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://4244311e590eab6cda676d3f3a7b9f6d06e22703a22f6966aeb80d83105ab71d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.709: INFO: Pod "nginx-deployment-7b8c6f4498-drjdq" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-drjdq,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-7b8c6f4498-drjdq,UID:1bc61bbf-594d-4421-bfd0-222b152fce9b,ResourceVersion:56648,Generation:0,CreationTimestamp:2019-07-25 08:53:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f066502e-6136-4434-9524-fb9fef05d697 0xc00040c7e7 0xc00040c7e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00040c910} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00040c9b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.709: INFO: Pod "nginx-deployment-7b8c6f4498-fddlv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-fddlv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-7b8c6f4498-fddlv,UID:61da5de6-1d70-46eb-a375-8e5228a7d8ed,ResourceVersion:56545,Generation:0,CreationTimestamp:2019-07-25 08:53:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f066502e-6136-4434-9524-fb9fef05d697 0xc00040cb40 0xc00040cb41}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00040cc30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00040cc50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:31 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:31 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:28 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.4,PodIP:10.244.1.5,StartTime:2019-07-25 08:53:28 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-25 08:53:30 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://8ea7dbe57871684851fdcbb04b91f98d63c8b9239fe0eb5b66f792610a9648b6}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.710: INFO: Pod "nginx-deployment-7b8c6f4498-flz5t" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-flz5t,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-7b8c6f4498-flz5t,UID:45591014-d92e-4fa5-855e-326b662e241f,ResourceVersion:56515,Generation:0,CreationTimestamp:2019-07-25 08:53:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f066502e-6136-4434-9524-fb9fef05d697 0xc00040cf70 0xc00040cf71}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00040d120} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00040d1a0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:28 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:10.244.2.199,StartTime:2019-07-25 08:53:28 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-25 08:53:29 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://ddaa1713c9fc91edf9c84b4ca89b87e07ad21cadfceb8523e0ed8bef37a857dc}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.710: INFO: Pod "nginx-deployment-7b8c6f4498-gxwps" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-gxwps,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-7b8c6f4498-gxwps,UID:21513235-7653-4cad-a102-5d1e5459c7fb,ResourceVersion:56650,Generation:0,CreationTimestamp:2019-07-25 08:53:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f066502e-6136-4434-9524-fb9fef05d697 0xc00040d4e7 0xc00040d4e8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00040d630} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00040d6f0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.710: INFO: Pod "nginx-deployment-7b8c6f4498-lgqpm" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-lgqpm,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-7b8c6f4498-lgqpm,UID:f2234719-39c7-48cd-a028-7930c587d1fb,ResourceVersion:56657,Generation:0,CreationTimestamp:2019-07-25 08:53:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f066502e-6136-4434-9524-fb9fef05d697 0xc00040d7c0 0xc00040d7c1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00040d820} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00040d870}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.710: INFO: Pod "nginx-deployment-7b8c6f4498-n9pcz" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-n9pcz,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-7b8c6f4498-n9pcz,UID:4407571e-9915-414c-b617-1d626ea8b4ef,ResourceVersion:56518,Generation:0,CreationTimestamp:2019-07-25 08:53:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f066502e-6136-4434-9524-fb9fef05d697 0xc00040d9a7 0xc00040d9a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc00040db20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc00040dcc0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:30 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:30 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:28 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:10.244.2.201,StartTime:2019-07-25 08:53:28 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-25 08:53:30 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://6a9a25bdbec1099f29cd548ef4c55da7465b3266ece6f809e8229f6e15163d5f}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.710: INFO: Pod "nginx-deployment-7b8c6f4498-r9pv6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-r9pv6,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-7b8c6f4498-r9pv6,UID:84a27880-6eca-4c74-917e-b9935c7eb772,ResourceVersion:56631,Generation:0,CreationTimestamp:2019-07-25 08:53:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f066502e-6136-4434-9524-fb9fef05d697 0xc0004a80f7 0xc0004a80f8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0004a82f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0004a8420}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.710: INFO: Pod "nginx-deployment-7b8c6f4498-s45cv" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-s45cv,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-7b8c6f4498-s45cv,UID:79dce410-26db-4abd-9aa8-5b39c5f24419,ResourceVersion:56542,Generation:0,CreationTimestamp:2019-07-25 08:53:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f066502e-6136-4434-9524-fb9fef05d697 0xc0004a8720 0xc0004a8721}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0004a87c0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0004a8820}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:31 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:31 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:28 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.4,PodIP:10.244.1.4,StartTime:2019-07-25 08:53:28 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-25 08:53:30 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://b303db9dbc411e8472d53ace15fa1b5f11c5fef8394786d8666a23b90da2a452}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.710: INFO: Pod "nginx-deployment-7b8c6f4498-szzjj" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-szzjj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-7b8c6f4498-szzjj,UID:a53265eb-f4eb-4177-80e1-d27589d58ecf,ResourceVersion:56658,Generation:0,CreationTimestamp:2019-07-25 08:53:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f066502e-6136-4434-9524-fb9fef05d697 0xc0004a8ac0 0xc0004a8ac1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0004a8d20} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0004a8d90}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.710: INFO: Pod "nginx-deployment-7b8c6f4498-t9tzj" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-t9tzj,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-7b8c6f4498-t9tzj,UID:9af78e7e-9159-445c-93f1-cf47112d8f7d,ResourceVersion:56554,Generation:0,CreationTimestamp:2019-07-25 08:53:28 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f066502e-6136-4434-9524-fb9fef05d697 0xc0004a8f87 0xc0004a8f88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0004a90f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0004a9160}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:28 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:31 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:31 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:28 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.4,PodIP:10.244.1.3,StartTime:2019-07-25 08:53:28 +0000 UTC,ContainerStatuses:[{nginx {nil ContainerStateRunning{StartedAt:2019-07-25 08:53:30 +0000 UTC,} nil} {nil nil nil} true 0 nginx:1.14-alpine docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d docker://ce93c4b3dd0d8b9c9b4f165048984af1f4ebaea0d042f40c649f4c58cf258a06}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.710: INFO: Pod "nginx-deployment-7b8c6f4498-vx9q6" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-vx9q6,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-7b8c6f4498-vx9q6,UID:f016e779-b446-45ab-9c45-b010596e632b,ResourceVersion:56660,Generation:0,CreationTimestamp:2019-07-25 08:53:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f066502e-6136-4434-9524-fb9fef05d697 0xc0004a9430 0xc0004a9431}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0004a9500} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0004a9570}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:34 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:34 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.4,PodIP:,StartTime:2019-07-25 08:53:34 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.711: INFO: Pod "nginx-deployment-7b8c6f4498-wbrm9" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-wbrm9,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-7b8c6f4498-wbrm9,UID:c8aae84e-8659-4c9e-9381-cef85ccc5f46,ResourceVersion:56661,Generation:0,CreationTimestamp:2019-07-25 08:53:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f066502e-6136-4434-9524-fb9fef05d697 0xc0004a96a7 0xc0004a96a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0004a9730} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0004a9750}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
Jul 25 08:53:34.711: INFO: Pod "nginx-deployment-7b8c6f4498-xds6b" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:nginx-deployment-7b8c6f4498-xds6b,GenerateName:nginx-deployment-7b8c6f4498-,Namespace:deployment-728,SelfLink:/api/v1/namespaces/deployment-728/pods/nginx-deployment-7b8c6f4498-xds6b,UID:474598a7-6c6a-4622-ab19-569f814e3411,ResourceVersion:56651,Generation:0,CreationTimestamp:2019-07-25 08:53:34 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: nginx,pod-template-hash: 7b8c6f4498,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet nginx-deployment-7b8c6f4498 f066502e-6136-4434-9524-fb9fef05d697 0xc0004a97f0 0xc0004a97f1}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-95hl8 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-95hl8,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-95hl8 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0004a9870} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0004a9890}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 08:53:34 +0000 UTC  }],Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 08:53:34.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-728" for this suite.
Jul 25 08:53:40.756: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 08:53:40.832: INFO: namespace deployment-728 deletion completed in 6.110869378s

• [SLOW TEST:12.339 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 08:53:40.832: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-2771
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jul 25 08:53:40.876: INFO: Found 0 stateful pods, waiting for 3
Jul 25 08:53:50.880: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 25 08:53:50.880: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 25 08:53:50.880: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jul 25 08:53:50.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-2771 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 25 08:53:51.219: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 25 08:53:51.219: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 25 08:53:51.219: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul 25 08:54:01.250: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Jul 25 08:54:11.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-2771 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 08:54:11.519: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 25 08:54:11.519: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 25 08:54:11.519: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 25 08:54:31.545: INFO: Waiting for StatefulSet statefulset-2771/ss2 to complete update
Jul 25 08:54:31.545: INFO: Waiting for Pod statefulset-2771/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 25 08:54:41.551: INFO: Waiting for StatefulSet statefulset-2771/ss2 to complete update
Jul 25 08:54:41.551: INFO: Waiting for Pod statefulset-2771/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Rolling back to a previous revision
Jul 25 08:54:51.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-2771 ss2-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 25 08:54:51.759: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 25 08:54:51.759: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 25 08:54:51.759: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss2-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 25 08:55:01.789: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Jul 25 08:55:11.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-2771 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 08:55:11.990: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 25 08:55:11.990: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 25 08:55:11.990: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss2-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 25 08:55:32.010: INFO: Waiting for StatefulSet statefulset-2771/ss2 to complete update
Jul 25 08:55:32.010: INFO: Waiting for Pod statefulset-2771/ss2-0 to have revision ss2-7c9b54fd4c update revision ss2-6c5cd755cd
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 25 08:55:42.017: INFO: Deleting all statefulset in ns statefulset-2771
Jul 25 08:55:42.021: INFO: Scaling statefulset ss2 to 0
Jul 25 08:56:02.036: INFO: Waiting for statefulset status.replicas updated to 0
Jul 25 08:56:02.039: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 08:56:02.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2771" for this suite.
Jul 25 08:56:08.080: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 08:56:08.176: INFO: namespace statefulset-2771 deletion completed in 6.120383294s

• [SLOW TEST:147.344 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 08:56:08.176: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 08:56:32.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6802" for this suite.
Jul 25 08:56:38.299: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 08:56:38.372: INFO: namespace namespaces-6802 deletion completed in 6.082447626s
STEP: Destroying namespace "nsdeletetest-6049" for this suite.
Jul 25 08:56:38.374: INFO: Namespace nsdeletetest-6049 was already deleted
STEP: Destroying namespace "nsdeletetest-7836" for this suite.
Jul 25 08:56:44.382: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 08:56:44.446: INFO: namespace nsdeletetest-7836 deletion completed in 6.071621753s

• [SLOW TEST:36.270 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 08:56:44.446: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 25 08:56:44.479: INFO: Waiting up to 5m0s for pod "pod-dbc49345-1b21-4b76-a286-0e9915d69ea7" in namespace "emptydir-4273" to be "success or failure"
Jul 25 08:56:44.485: INFO: Pod "pod-dbc49345-1b21-4b76-a286-0e9915d69ea7": Phase="Pending", Reason="", readiness=false. Elapsed: 5.21699ms
Jul 25 08:56:46.488: INFO: Pod "pod-dbc49345-1b21-4b76-a286-0e9915d69ea7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008665022s
Jul 25 08:56:48.492: INFO: Pod "pod-dbc49345-1b21-4b76-a286-0e9915d69ea7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012273801s
Jul 25 08:56:50.495: INFO: Pod "pod-dbc49345-1b21-4b76-a286-0e9915d69ea7": Phase="Pending", Reason="", readiness=false. Elapsed: 6.015718213s
Jul 25 08:56:52.500: INFO: Pod "pod-dbc49345-1b21-4b76-a286-0e9915d69ea7": Phase="Pending", Reason="", readiness=false. Elapsed: 8.020445998s
Jul 25 08:56:54.505: INFO: Pod "pod-dbc49345-1b21-4b76-a286-0e9915d69ea7": Phase="Pending", Reason="", readiness=false. Elapsed: 10.025170544s
Jul 25 08:56:56.509: INFO: Pod "pod-dbc49345-1b21-4b76-a286-0e9915d69ea7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.029340702s
Jul 25 08:56:58.512: INFO: Pod "pod-dbc49345-1b21-4b76-a286-0e9915d69ea7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 14.032949876s
STEP: Saw pod success
Jul 25 08:56:58.512: INFO: Pod "pod-dbc49345-1b21-4b76-a286-0e9915d69ea7" satisfied condition "success or failure"
Jul 25 08:56:58.516: INFO: Trying to get logs from node essentialpks-conformance-3 pod pod-dbc49345-1b21-4b76-a286-0e9915d69ea7 container test-container: <nil>
STEP: delete the pod
Jul 25 08:56:58.542: INFO: Waiting for pod pod-dbc49345-1b21-4b76-a286-0e9915d69ea7 to disappear
Jul 25 08:56:58.544: INFO: Pod pod-dbc49345-1b21-4b76-a286-0e9915d69ea7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 08:56:58.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4273" for this suite.
Jul 25 08:57:04.556: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 08:57:04.640: INFO: namespace emptydir-4273 deletion completed in 6.093178165s

• [SLOW TEST:20.194 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 08:57:04.640: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-7bdd1ffd-d21c-445c-af5d-3ce83d8b7a1b
STEP: Creating a pod to test consume secrets
Jul 25 08:57:04.677: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-ef9acddd-b744-454c-a992-5d43407df2dc" in namespace "projected-5750" to be "success or failure"
Jul 25 08:57:04.680: INFO: Pod "pod-projected-secrets-ef9acddd-b744-454c-a992-5d43407df2dc": Phase="Pending", Reason="", readiness=false. Elapsed: 3.288556ms
Jul 25 08:57:06.686: INFO: Pod "pod-projected-secrets-ef9acddd-b744-454c-a992-5d43407df2dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00877158s
STEP: Saw pod success
Jul 25 08:57:06.686: INFO: Pod "pod-projected-secrets-ef9acddd-b744-454c-a992-5d43407df2dc" satisfied condition "success or failure"
Jul 25 08:57:06.690: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-projected-secrets-ef9acddd-b744-454c-a992-5d43407df2dc container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 25 08:57:06.721: INFO: Waiting for pod pod-projected-secrets-ef9acddd-b744-454c-a992-5d43407df2dc to disappear
Jul 25 08:57:06.725: INFO: Pod pod-projected-secrets-ef9acddd-b744-454c-a992-5d43407df2dc no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 08:57:06.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5750" for this suite.
Jul 25 08:57:12.740: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 08:57:12.832: INFO: namespace projected-5750 deletion completed in 6.102313734s

• [SLOW TEST:8.192 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 08:57:12.832: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
W0725 08:57:22.905345      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 25 08:57:22.905: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 08:57:22.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9834" for this suite.
Jul 25 08:57:28.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 08:57:29.018: INFO: namespace gc-9834 deletion completed in 6.106796287s

• [SLOW TEST:16.186 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 08:57:29.019: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-321990ae-d79c-4903-9ac5-cefd09e00d96
STEP: Creating a pod to test consume configMaps
Jul 25 08:57:29.054: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-49a17942-56ac-40f1-ab75-b74d18874c34" in namespace "projected-5079" to be "success or failure"
Jul 25 08:57:29.127: INFO: Pod "pod-projected-configmaps-49a17942-56ac-40f1-ab75-b74d18874c34": Phase="Pending", Reason="", readiness=false. Elapsed: 72.690876ms
Jul 25 08:57:31.133: INFO: Pod "pod-projected-configmaps-49a17942-56ac-40f1-ab75-b74d18874c34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.078630901s
STEP: Saw pod success
Jul 25 08:57:31.133: INFO: Pod "pod-projected-configmaps-49a17942-56ac-40f1-ab75-b74d18874c34" satisfied condition "success or failure"
Jul 25 08:57:31.137: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-projected-configmaps-49a17942-56ac-40f1-ab75-b74d18874c34 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 25 08:57:31.163: INFO: Waiting for pod pod-projected-configmaps-49a17942-56ac-40f1-ab75-b74d18874c34 to disappear
Jul 25 08:57:31.166: INFO: Pod pod-projected-configmaps-49a17942-56ac-40f1-ab75-b74d18874c34 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 08:57:31.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5079" for this suite.
Jul 25 08:57:37.181: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 08:57:37.267: INFO: namespace projected-5079 deletion completed in 6.097038269s

• [SLOW TEST:8.248 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 08:57:37.269: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Jul 25 08:57:41.324: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9763 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 25 08:57:41.324: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
Jul 25 08:57:41.435: INFO: Exec stderr: ""
Jul 25 08:57:41.435: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9763 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 25 08:57:41.435: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
Jul 25 08:57:41.531: INFO: Exec stderr: ""
Jul 25 08:57:41.531: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9763 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 25 08:57:41.531: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
Jul 25 08:57:41.631: INFO: Exec stderr: ""
Jul 25 08:57:41.631: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9763 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 25 08:57:41.631: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
Jul 25 08:57:41.729: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Jul 25 08:57:41.729: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9763 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 25 08:57:41.729: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
Jul 25 08:57:41.823: INFO: Exec stderr: ""
Jul 25 08:57:41.823: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9763 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 25 08:57:41.823: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
Jul 25 08:57:41.925: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Jul 25 08:57:41.925: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9763 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 25 08:57:41.925: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
Jul 25 08:57:42.050: INFO: Exec stderr: ""
Jul 25 08:57:42.050: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9763 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 25 08:57:42.050: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
Jul 25 08:57:42.152: INFO: Exec stderr: ""
Jul 25 08:57:42.152: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9763 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 25 08:57:42.152: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
Jul 25 08:57:42.258: INFO: Exec stderr: ""
Jul 25 08:57:42.259: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9763 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 25 08:57:42.259: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
Jul 25 08:57:42.359: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 08:57:42.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9763" for this suite.
Jul 25 08:58:32.372: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 08:58:32.441: INFO: namespace e2e-kubelet-etc-hosts-9763 deletion completed in 50.078105097s

• [SLOW TEST:55.172 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 08:58:32.442: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1722
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 25 08:58:32.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 run e2e-test-nginx-pod --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --labels=run=e2e-test-nginx-pod --namespace=kubectl-3346'
Jul 25 08:58:32.566: INFO: stderr: ""
Jul 25 08:58:32.566: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod is running
STEP: verifying the pod e2e-test-nginx-pod was created
Jul 25 08:58:37.616: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pod e2e-test-nginx-pod --namespace=kubectl-3346 -o json'
Jul 25 08:58:37.696: INFO: stderr: ""
Jul 25 08:58:37.696: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-07-25T08:58:32Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-nginx-pod\"\n        },\n        \"name\": \"e2e-test-nginx-pod\",\n        \"namespace\": \"kubectl-3346\",\n        \"resourceVersion\": \"57920\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-3346/pods/e2e-test-nginx-pod\",\n        \"uid\": \"84c4cd2f-beed-4f73-abfe-9dcad1e351d5\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/nginx:1.14-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-nginx-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-982w2\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"essentialpks-conformance-2\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-982w2\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-982w2\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-25T08:58:32Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-25T08:58:33Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-25T08:58:33Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-07-25T08:58:32Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://ea55cb3a2171915b923ae7bfc7ddd4377b13dbbcb22d0214e9ab2c0c81879e98\",\n                \"image\": \"nginx:1.14-alpine\",\n                \"imageID\": \"docker-pullable://nginx@sha256:b67e90a1d8088f0e205c77c793c271524773a6de163fb3855b1c1bedf979da7d\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-nginx-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-07-25T08:58:33Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.102.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.2.225\",\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-07-25T08:58:32Z\"\n    }\n}\n"
STEP: replace the image in the pod
Jul 25 08:58:37.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 replace -f - --namespace=kubectl-3346'
Jul 25 08:58:37.918: INFO: stderr: ""
Jul 25 08:58:37.918: INFO: stdout: "pod/e2e-test-nginx-pod replaced\n"
STEP: verifying the pod e2e-test-nginx-pod has the right image docker.io/library/busybox:1.29
[AfterEach] [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1727
Jul 25 08:58:37.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 delete pods e2e-test-nginx-pod --namespace=kubectl-3346'
Jul 25 08:58:49.841: INFO: stderr: ""
Jul 25 08:58:49.841: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 08:58:49.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3346" for this suite.
Jul 25 08:58:55.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 08:58:55.937: INFO: namespace kubectl-3346 deletion completed in 6.089252854s

• [SLOW TEST:23.495 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl replace
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 08:58:55.937: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jul 25 08:58:55.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 create -f - --namespace=kubectl-8456'
Jul 25 08:58:56.159: INFO: stderr: ""
Jul 25 08:58:56.160: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 25 08:58:56.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8456'
Jul 25 08:58:56.254: INFO: stderr: ""
Jul 25 08:58:56.254: INFO: stdout: "update-demo-nautilus-46gbd update-demo-nautilus-x22jh "
Jul 25 08:58:56.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-nautilus-46gbd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8456'
Jul 25 08:58:56.332: INFO: stderr: ""
Jul 25 08:58:56.332: INFO: stdout: ""
Jul 25 08:58:56.333: INFO: update-demo-nautilus-46gbd is created but not running
Jul 25 08:59:01.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8456'
Jul 25 08:59:01.420: INFO: stderr: ""
Jul 25 08:59:01.420: INFO: stdout: "update-demo-nautilus-46gbd update-demo-nautilus-x22jh "
Jul 25 08:59:01.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-nautilus-46gbd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8456'
Jul 25 08:59:01.498: INFO: stderr: ""
Jul 25 08:59:01.498: INFO: stdout: "true"
Jul 25 08:59:01.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-nautilus-46gbd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8456'
Jul 25 08:59:01.576: INFO: stderr: ""
Jul 25 08:59:01.576: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 25 08:59:01.576: INFO: validating pod update-demo-nautilus-46gbd
Jul 25 08:59:01.588: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 25 08:59:01.588: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 25 08:59:01.588: INFO: update-demo-nautilus-46gbd is verified up and running
Jul 25 08:59:01.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-nautilus-x22jh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8456'
Jul 25 08:59:01.669: INFO: stderr: ""
Jul 25 08:59:01.669: INFO: stdout: "true"
Jul 25 08:59:01.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-nautilus-x22jh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8456'
Jul 25 08:59:01.755: INFO: stderr: ""
Jul 25 08:59:01.755: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 25 08:59:01.755: INFO: validating pod update-demo-nautilus-x22jh
Jul 25 08:59:01.767: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 25 08:59:01.767: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 25 08:59:01.767: INFO: update-demo-nautilus-x22jh is verified up and running
STEP: using delete to clean up resources
Jul 25 08:59:01.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 delete --grace-period=0 --force -f - --namespace=kubectl-8456'
Jul 25 08:59:01.849: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 25 08:59:01.849: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 25 08:59:01.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8456'
Jul 25 08:59:01.937: INFO: stderr: "No resources found.\n"
Jul 25 08:59:01.937: INFO: stdout: ""
Jul 25 08:59:01.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods -l name=update-demo --namespace=kubectl-8456 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 25 08:59:02.017: INFO: stderr: ""
Jul 25 08:59:02.017: INFO: stdout: "update-demo-nautilus-46gbd\nupdate-demo-nautilus-x22jh\n"
Jul 25 08:59:02.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8456'
Jul 25 08:59:02.613: INFO: stderr: "No resources found.\n"
Jul 25 08:59:02.613: INFO: stdout: ""
Jul 25 08:59:02.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods -l name=update-demo --namespace=kubectl-8456 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 25 08:59:02.692: INFO: stderr: ""
Jul 25 08:59:02.692: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 08:59:02.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8456" for this suite.
Jul 25 08:59:24.713: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 08:59:24.799: INFO: namespace kubectl-8456 deletion completed in 22.103115074s

• [SLOW TEST:28.862 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 08:59:24.800: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 25 08:59:24.835: INFO: Waiting up to 5m0s for pod "pod-e6758bc1-af38-4c6e-ae27-7828e8685db5" in namespace "emptydir-3974" to be "success or failure"
Jul 25 08:59:24.839: INFO: Pod "pod-e6758bc1-af38-4c6e-ae27-7828e8685db5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.704698ms
Jul 25 08:59:26.842: INFO: Pod "pod-e6758bc1-af38-4c6e-ae27-7828e8685db5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006542033s
STEP: Saw pod success
Jul 25 08:59:26.842: INFO: Pod "pod-e6758bc1-af38-4c6e-ae27-7828e8685db5" satisfied condition "success or failure"
Jul 25 08:59:26.844: INFO: Trying to get logs from node essentialpks-conformance-3 pod pod-e6758bc1-af38-4c6e-ae27-7828e8685db5 container test-container: <nil>
STEP: delete the pod
Jul 25 08:59:26.860: INFO: Waiting for pod pod-e6758bc1-af38-4c6e-ae27-7828e8685db5 to disappear
Jul 25 08:59:26.862: INFO: Pod pod-e6758bc1-af38-4c6e-ae27-7828e8685db5 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 08:59:26.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3974" for this suite.
Jul 25 08:59:32.875: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 08:59:32.962: INFO: namespace emptydir-3974 deletion completed in 6.097084986s

• [SLOW TEST:8.163 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 08:59:32.967: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override arguments
Jul 25 08:59:33.012: INFO: Waiting up to 5m0s for pod "client-containers-8d18afc3-e948-4191-901d-85ec8e7f8632" in namespace "containers-3758" to be "success or failure"
Jul 25 08:59:33.017: INFO: Pod "client-containers-8d18afc3-e948-4191-901d-85ec8e7f8632": Phase="Pending", Reason="", readiness=false. Elapsed: 4.652319ms
Jul 25 08:59:35.022: INFO: Pod "client-containers-8d18afc3-e948-4191-901d-85ec8e7f8632": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008982666s
STEP: Saw pod success
Jul 25 08:59:35.022: INFO: Pod "client-containers-8d18afc3-e948-4191-901d-85ec8e7f8632" satisfied condition "success or failure"
Jul 25 08:59:35.027: INFO: Trying to get logs from node essentialpks-conformance-2 pod client-containers-8d18afc3-e948-4191-901d-85ec8e7f8632 container test-container: <nil>
STEP: delete the pod
Jul 25 08:59:35.044: INFO: Waiting for pod client-containers-8d18afc3-e948-4191-901d-85ec8e7f8632 to disappear
Jul 25 08:59:35.046: INFO: Pod client-containers-8d18afc3-e948-4191-901d-85ec8e7f8632 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 08:59:35.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3758" for this suite.
Jul 25 08:59:41.060: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 08:59:41.150: INFO: namespace containers-3758 deletion completed in 6.099087076s

• [SLOW TEST:8.183 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 08:59:41.151: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be submitted and removed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 08:59:41.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9975" for this suite.
Jul 25 09:00:03.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:00:03.294: INFO: namespace pods-9975 deletion completed in 22.101615617s

• [SLOW TEST:22.143 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:00:03.294: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 25 09:00:03.326: INFO: Waiting up to 5m0s for pod "downwardapi-volume-380d47a2-e05c-4deb-83a1-59443974e893" in namespace "downward-api-9302" to be "success or failure"
Jul 25 09:00:03.330: INFO: Pod "downwardapi-volume-380d47a2-e05c-4deb-83a1-59443974e893": Phase="Pending", Reason="", readiness=false. Elapsed: 3.344826ms
Jul 25 09:00:05.333: INFO: Pod "downwardapi-volume-380d47a2-e05c-4deb-83a1-59443974e893": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007085704s
STEP: Saw pod success
Jul 25 09:00:05.334: INFO: Pod "downwardapi-volume-380d47a2-e05c-4deb-83a1-59443974e893" satisfied condition "success or failure"
Jul 25 09:00:05.336: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-380d47a2-e05c-4deb-83a1-59443974e893 container client-container: <nil>
STEP: delete the pod
Jul 25 09:00:05.353: INFO: Waiting for pod downwardapi-volume-380d47a2-e05c-4deb-83a1-59443974e893 to disappear
Jul 25 09:00:05.356: INFO: Pod downwardapi-volume-380d47a2-e05c-4deb-83a1-59443974e893 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:00:05.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9302" for this suite.
Jul 25 09:00:11.371: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:00:11.440: INFO: namespace downward-api-9302 deletion completed in 6.079904375s

• [SLOW TEST:8.146 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:00:11.442: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 25 09:00:11.472: INFO: Waiting up to 5m0s for pod "downward-api-f6e41f03-dfea-4dd9-814d-f732b67038e9" in namespace "downward-api-1561" to be "success or failure"
Jul 25 09:00:11.474: INFO: Pod "downward-api-f6e41f03-dfea-4dd9-814d-f732b67038e9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.110733ms
Jul 25 09:00:13.478: INFO: Pod "downward-api-f6e41f03-dfea-4dd9-814d-f732b67038e9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005580032s
STEP: Saw pod success
Jul 25 09:00:13.478: INFO: Pod "downward-api-f6e41f03-dfea-4dd9-814d-f732b67038e9" satisfied condition "success or failure"
Jul 25 09:00:13.481: INFO: Trying to get logs from node essentialpks-conformance-2 pod downward-api-f6e41f03-dfea-4dd9-814d-f732b67038e9 container dapi-container: <nil>
STEP: delete the pod
Jul 25 09:00:13.502: INFO: Waiting for pod downward-api-f6e41f03-dfea-4dd9-814d-f732b67038e9 to disappear
Jul 25 09:00:13.505: INFO: Pod downward-api-f6e41f03-dfea-4dd9-814d-f732b67038e9 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:00:13.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1561" for this suite.
Jul 25 09:00:19.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:00:19.608: INFO: namespace downward-api-1561 deletion completed in 6.09891175s

• [SLOW TEST:8.166 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:00:19.616: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on tmpfs
Jul 25 09:00:19.646: INFO: Waiting up to 5m0s for pod "pod-932b697d-98de-4a8a-a060-67395fe1c4a7" in namespace "emptydir-2490" to be "success or failure"
Jul 25 09:00:19.654: INFO: Pod "pod-932b697d-98de-4a8a-a060-67395fe1c4a7": Phase="Pending", Reason="", readiness=false. Elapsed: 7.644869ms
Jul 25 09:00:21.658: INFO: Pod "pod-932b697d-98de-4a8a-a060-67395fe1c4a7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011594901s
STEP: Saw pod success
Jul 25 09:00:21.658: INFO: Pod "pod-932b697d-98de-4a8a-a060-67395fe1c4a7" satisfied condition "success or failure"
Jul 25 09:00:21.660: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-932b697d-98de-4a8a-a060-67395fe1c4a7 container test-container: <nil>
STEP: delete the pod
Jul 25 09:00:21.675: INFO: Waiting for pod pod-932b697d-98de-4a8a-a060-67395fe1c4a7 to disappear
Jul 25 09:00:21.677: INFO: Pod pod-932b697d-98de-4a8a-a060-67395fe1c4a7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:00:21.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2490" for this suite.
Jul 25 09:00:27.691: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:00:27.778: INFO: namespace emptydir-2490 deletion completed in 6.096615717s

• [SLOW TEST:8.162 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:00:27.778: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-6vxwz in namespace proxy-5382
I0725 09:00:27.825855      16 runners.go:180] Created replication controller with name: proxy-service-6vxwz, namespace: proxy-5382, replica count: 1
I0725 09:00:28.876438      16 runners.go:180] proxy-service-6vxwz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0725 09:00:29.876670      16 runners.go:180] proxy-service-6vxwz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0725 09:00:30.876945      16 runners.go:180] proxy-service-6vxwz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0725 09:00:31.877137      16 runners.go:180] proxy-service-6vxwz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0725 09:00:32.877491      16 runners.go:180] proxy-service-6vxwz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0725 09:00:33.877838      16 runners.go:180] proxy-service-6vxwz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0725 09:00:34.878153      16 runners.go:180] proxy-service-6vxwz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0725 09:00:35.878518      16 runners.go:180] proxy-service-6vxwz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0725 09:00:36.878953      16 runners.go:180] proxy-service-6vxwz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0725 09:00:37.879270      16 runners.go:180] proxy-service-6vxwz Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 25 09:00:37.883: INFO: setup took 10.075049675s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Jul 25 09:00:37.907: INFO: (0) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 23.233107ms)
Jul 25 09:00:37.907: INFO: (0) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">test<... (200; 23.450233ms)
Jul 25 09:00:37.907: INFO: (0) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 22.901938ms)
Jul 25 09:00:37.907: INFO: (0) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/rewriteme">test</a> (200; 23.175006ms)
Jul 25 09:00:37.907: INFO: (0) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname1/proxy/: foo (200; 23.168373ms)
Jul 25 09:00:37.907: INFO: (0) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 23.015482ms)
Jul 25 09:00:37.912: INFO: (0) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname2/proxy/: bar (200; 27.997061ms)
Jul 25 09:00:37.916: INFO: (0) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">... (200; 31.621526ms)
Jul 25 09:00:37.916: INFO: (0) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname1/proxy/: foo (200; 31.764595ms)
Jul 25 09:00:37.916: INFO: (0) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 32.230317ms)
Jul 25 09:00:37.920: INFO: (0) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:462/proxy/: tls qux (200; 36.181549ms)
Jul 25 09:00:37.925: INFO: (0) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname2/proxy/: bar (200; 40.44621ms)
Jul 25 09:00:37.925: INFO: (0) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:460/proxy/: tls baz (200; 40.67018ms)
Jul 25 09:00:37.925: INFO: (0) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname1/proxy/: tls baz (200; 41.296293ms)
Jul 25 09:00:37.926: INFO: (0) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname2/proxy/: tls qux (200; 41.743499ms)
Jul 25 09:00:37.926: INFO: (0) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/tlsrewritem... (200; 42.086337ms)
Jul 25 09:00:37.938: INFO: (1) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 11.197627ms)
Jul 25 09:00:37.938: INFO: (1) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:462/proxy/: tls qux (200; 11.965687ms)
Jul 25 09:00:37.939: INFO: (1) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:460/proxy/: tls baz (200; 11.978485ms)
Jul 25 09:00:37.939: INFO: (1) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 12.184032ms)
Jul 25 09:00:37.939: INFO: (1) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 12.160345ms)
Jul 25 09:00:37.939: INFO: (1) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/rewriteme">test</a> (200; 11.992913ms)
Jul 25 09:00:37.939: INFO: (1) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 13.341242ms)
Jul 25 09:00:37.939: INFO: (1) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/tlsrewritem... (200; 12.925667ms)
Jul 25 09:00:37.940: INFO: (1) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname1/proxy/: tls baz (200; 12.691995ms)
Jul 25 09:00:37.940: INFO: (1) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname2/proxy/: bar (200; 13.337619ms)
Jul 25 09:00:37.941: INFO: (1) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">... (200; 14.022197ms)
Jul 25 09:00:37.941: INFO: (1) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname1/proxy/: foo (200; 14.285815ms)
Jul 25 09:00:37.941: INFO: (1) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">test<... (200; 14.824378ms)
Jul 25 09:00:37.941: INFO: (1) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname2/proxy/: tls qux (200; 14.425351ms)
Jul 25 09:00:37.941: INFO: (1) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname2/proxy/: bar (200; 14.330475ms)
Jul 25 09:00:37.942: INFO: (1) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname1/proxy/: foo (200; 15.302872ms)
Jul 25 09:00:37.948: INFO: (2) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 5.333271ms)
Jul 25 09:00:37.948: INFO: (2) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/tlsrewritem... (200; 5.283317ms)
Jul 25 09:00:37.950: INFO: (2) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 6.801957ms)
Jul 25 09:00:37.951: INFO: (2) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:462/proxy/: tls qux (200; 7.739687ms)
Jul 25 09:00:37.953: INFO: (2) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:460/proxy/: tls baz (200; 9.652587ms)
Jul 25 09:00:37.953: INFO: (2) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname1/proxy/: tls baz (200; 10.270446ms)
Jul 25 09:00:37.953: INFO: (2) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 10.077337ms)
Jul 25 09:00:37.953: INFO: (2) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 10.019204ms)
Jul 25 09:00:37.953: INFO: (2) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">... (200; 10.066264ms)
Jul 25 09:00:37.953: INFO: (2) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname2/proxy/: tls qux (200; 10.494557ms)
Jul 25 09:00:37.954: INFO: (2) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/rewriteme">test</a> (200; 10.79479ms)
Jul 25 09:00:37.954: INFO: (2) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">test<... (200; 10.677593ms)
Jul 25 09:00:37.961: INFO: (2) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname2/proxy/: bar (200; 17.787499ms)
Jul 25 09:00:37.961: INFO: (2) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname1/proxy/: foo (200; 18.210025ms)
Jul 25 09:00:37.962: INFO: (2) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname1/proxy/: foo (200; 18.852503ms)
Jul 25 09:00:37.962: INFO: (2) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname2/proxy/: bar (200; 18.867508ms)
Jul 25 09:00:37.967: INFO: (3) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 4.715334ms)
Jul 25 09:00:37.967: INFO: (3) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:462/proxy/: tls qux (200; 4.797535ms)
Jul 25 09:00:37.969: INFO: (3) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/tlsrewritem... (200; 6.61463ms)
Jul 25 09:00:37.970: INFO: (3) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">... (200; 7.695163ms)
Jul 25 09:00:37.972: INFO: (3) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname1/proxy/: foo (200; 10.142803ms)
Jul 25 09:00:37.974: INFO: (3) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname2/proxy/: bar (200; 12.06542ms)
Jul 25 09:00:37.974: INFO: (3) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname1/proxy/: tls baz (200; 11.680312ms)
Jul 25 09:00:37.974: INFO: (3) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/rewriteme">test</a> (200; 12.117259ms)
Jul 25 09:00:37.975: INFO: (3) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">test<... (200; 12.452173ms)
Jul 25 09:00:37.975: INFO: (3) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:460/proxy/: tls baz (200; 12.425126ms)
Jul 25 09:00:37.975: INFO: (3) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 12.412893ms)
Jul 25 09:00:37.975: INFO: (3) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 12.389655ms)
Jul 25 09:00:37.975: INFO: (3) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 12.478998ms)
Jul 25 09:00:37.977: INFO: (3) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname2/proxy/: bar (200; 14.379645ms)
Jul 25 09:00:37.977: INFO: (3) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname1/proxy/: foo (200; 14.59502ms)
Jul 25 09:00:37.979: INFO: (3) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname2/proxy/: tls qux (200; 16.104332ms)
Jul 25 09:00:37.989: INFO: (4) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 10.475494ms)
Jul 25 09:00:37.990: INFO: (4) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 10.821469ms)
Jul 25 09:00:37.990: INFO: (4) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:460/proxy/: tls baz (200; 10.675206ms)
Jul 25 09:00:38.012: INFO: (4) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 32.347849ms)
Jul 25 09:00:38.012: INFO: (4) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 32.573488ms)
Jul 25 09:00:38.012: INFO: (4) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname2/proxy/: bar (200; 33.130822ms)
Jul 25 09:00:38.014: INFO: (4) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:462/proxy/: tls qux (200; 34.21209ms)
Jul 25 09:00:38.014: INFO: (4) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">... (200; 35.139961ms)
Jul 25 09:00:38.014: INFO: (4) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">test<... (200; 34.684499ms)
Jul 25 09:00:38.015: INFO: (4) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/tlsrewritem... (200; 34.732251ms)
Jul 25 09:00:38.015: INFO: (4) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/rewriteme">test</a> (200; 34.746392ms)
Jul 25 09:00:38.024: INFO: (4) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname1/proxy/: foo (200; 43.961806ms)
Jul 25 09:00:38.024: INFO: (4) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname2/proxy/: bar (200; 44.535745ms)
Jul 25 09:00:38.035: INFO: (4) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname1/proxy/: tls baz (200; 56.302655ms)
Jul 25 09:00:38.035: INFO: (4) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname1/proxy/: foo (200; 55.581403ms)
Jul 25 09:00:38.036: INFO: (4) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname2/proxy/: tls qux (200; 56.242892ms)
Jul 25 09:00:38.071: INFO: (5) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/tlsrewritem... (200; 34.285288ms)
Jul 25 09:00:38.071: INFO: (5) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 35.242381ms)
Jul 25 09:00:38.071: INFO: (5) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">... (200; 35.632579ms)
Jul 25 09:00:38.072: INFO: (5) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:460/proxy/: tls baz (200; 36.417789ms)
Jul 25 09:00:38.072: INFO: (5) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname1/proxy/: foo (200; 35.934175ms)
Jul 25 09:00:38.073: INFO: (5) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:462/proxy/: tls qux (200; 36.897717ms)
Jul 25 09:00:38.073: INFO: (5) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">test<... (200; 37.237211ms)
Jul 25 09:00:38.079: INFO: (5) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname2/proxy/: tls qux (200; 42.240035ms)
Jul 25 09:00:38.080: INFO: (5) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname2/proxy/: bar (200; 44.420088ms)
Jul 25 09:00:38.084: INFO: (5) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname1/proxy/: foo (200; 47.703796ms)
Jul 25 09:00:38.084: INFO: (5) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 47.736498ms)
Jul 25 09:00:38.085: INFO: (5) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 48.909449ms)
Jul 25 09:00:38.086: INFO: (5) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname2/proxy/: bar (200; 50.064689ms)
Jul 25 09:00:38.086: INFO: (5) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname1/proxy/: tls baz (200; 50.1732ms)
Jul 25 09:00:38.086: INFO: (5) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/rewriteme">test</a> (200; 50.079516ms)
Jul 25 09:00:38.116: INFO: (5) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 79.241374ms)
Jul 25 09:00:38.138: INFO: (6) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/tlsrewritem... (200; 21.881698ms)
Jul 25 09:00:38.141: INFO: (6) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname1/proxy/: foo (200; 24.104453ms)
Jul 25 09:00:38.141: INFO: (6) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/rewriteme">test</a> (200; 24.323081ms)
Jul 25 09:00:38.141: INFO: (6) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 25.191469ms)
Jul 25 09:00:38.141: INFO: (6) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 25.000185ms)
Jul 25 09:00:38.141: INFO: (6) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">... (200; 24.491966ms)
Jul 25 09:00:38.142: INFO: (6) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:462/proxy/: tls qux (200; 25.557468ms)
Jul 25 09:00:38.142: INFO: (6) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname2/proxy/: bar (200; 26.467733ms)
Jul 25 09:00:38.143: INFO: (6) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname1/proxy/: tls baz (200; 26.182074ms)
Jul 25 09:00:38.143: INFO: (6) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:460/proxy/: tls baz (200; 26.316368ms)
Jul 25 09:00:38.143: INFO: (6) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname2/proxy/: tls qux (200; 26.205224ms)
Jul 25 09:00:38.143: INFO: (6) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 26.840217ms)
Jul 25 09:00:38.143: INFO: (6) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 26.983066ms)
Jul 25 09:00:38.143: INFO: (6) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">test<... (200; 26.474912ms)
Jul 25 09:00:38.144: INFO: (6) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname2/proxy/: bar (200; 27.32973ms)
Jul 25 09:00:38.144: INFO: (6) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname1/proxy/: foo (200; 28.512991ms)
Jul 25 09:00:38.174: INFO: (7) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/rewriteme">test</a> (200; 29.17415ms)
Jul 25 09:00:38.174: INFO: (7) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">test<... (200; 28.438976ms)
Jul 25 09:00:38.174: INFO: (7) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 29.056078ms)
Jul 25 09:00:38.175: INFO: (7) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 29.581329ms)
Jul 25 09:00:38.175: INFO: (7) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">... (200; 30.148225ms)
Jul 25 09:00:38.175: INFO: (7) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 29.591148ms)
Jul 25 09:00:38.175: INFO: (7) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 29.148777ms)
Jul 25 09:00:38.175: INFO: (7) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:460/proxy/: tls baz (200; 29.501284ms)
Jul 25 09:00:38.175: INFO: (7) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:462/proxy/: tls qux (200; 29.238407ms)
Jul 25 09:00:38.175: INFO: (7) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/tlsrewritem... (200; 30.982193ms)
Jul 25 09:00:38.183: INFO: (7) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname2/proxy/: tls qux (200; 37.273414ms)
Jul 25 09:00:38.183: INFO: (7) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname1/proxy/: foo (200; 38.27271ms)
Jul 25 09:00:38.183: INFO: (7) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname1/proxy/: foo (200; 38.489665ms)
Jul 25 09:00:38.184: INFO: (7) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname1/proxy/: tls baz (200; 38.243501ms)
Jul 25 09:00:38.184: INFO: (7) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname2/proxy/: bar (200; 38.05897ms)
Jul 25 09:00:38.184: INFO: (7) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname2/proxy/: bar (200; 39.134951ms)
Jul 25 09:00:38.193: INFO: (8) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 8.375066ms)
Jul 25 09:00:38.193: INFO: (8) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 8.866504ms)
Jul 25 09:00:38.193: INFO: (8) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/tlsrewritem... (200; 9.307744ms)
Jul 25 09:00:38.194: INFO: (8) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:462/proxy/: tls qux (200; 9.538525ms)
Jul 25 09:00:38.194: INFO: (8) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname1/proxy/: tls baz (200; 9.192825ms)
Jul 25 09:00:38.194: INFO: (8) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/rewriteme">test</a> (200; 9.617997ms)
Jul 25 09:00:38.194: INFO: (8) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:460/proxy/: tls baz (200; 9.524404ms)
Jul 25 09:00:38.194: INFO: (8) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname1/proxy/: foo (200; 9.825964ms)
Jul 25 09:00:38.195: INFO: (8) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname2/proxy/: bar (200; 10.651007ms)
Jul 25 09:00:38.195: INFO: (8) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname2/proxy/: bar (200; 10.529063ms)
Jul 25 09:00:38.195: INFO: (8) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname1/proxy/: foo (200; 10.960489ms)
Jul 25 09:00:38.195: INFO: (8) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 10.759323ms)
Jul 25 09:00:38.196: INFO: (8) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">... (200; 11.212805ms)
Jul 25 09:00:38.196: INFO: (8) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">test<... (200; 11.517481ms)
Jul 25 09:00:38.196: INFO: (8) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 11.575334ms)
Jul 25 09:00:38.196: INFO: (8) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname2/proxy/: tls qux (200; 12.018138ms)
Jul 25 09:00:38.203: INFO: (9) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:462/proxy/: tls qux (200; 5.997488ms)
Jul 25 09:00:38.203: INFO: (9) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname1/proxy/: tls baz (200; 7.087963ms)
Jul 25 09:00:38.204: INFO: (9) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">test<... (200; 7.341895ms)
Jul 25 09:00:38.205: INFO: (9) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 8.306267ms)
Jul 25 09:00:38.208: INFO: (9) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">... (200; 10.916007ms)
Jul 25 09:00:38.208: INFO: (9) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname2/proxy/: tls qux (200; 11.427062ms)
Jul 25 09:00:38.208: INFO: (9) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname2/proxy/: bar (200; 11.061778ms)
Jul 25 09:00:38.208: INFO: (9) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname1/proxy/: foo (200; 11.275438ms)
Jul 25 09:00:38.209: INFO: (9) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname1/proxy/: foo (200; 11.585112ms)
Jul 25 09:00:38.209: INFO: (9) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 12.149462ms)
Jul 25 09:00:38.209: INFO: (9) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname2/proxy/: bar (200; 12.294965ms)
Jul 25 09:00:38.209: INFO: (9) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/rewriteme">test</a> (200; 12.11645ms)
Jul 25 09:00:38.209: INFO: (9) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:460/proxy/: tls baz (200; 12.624662ms)
Jul 25 09:00:38.209: INFO: (9) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 12.184227ms)
Jul 25 09:00:38.209: INFO: (9) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 12.647487ms)
Jul 25 09:00:38.209: INFO: (9) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/tlsrewritem... (200; 12.479917ms)
Jul 25 09:00:38.224: INFO: (10) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname2/proxy/: bar (200; 13.73979ms)
Jul 25 09:00:38.224: INFO: (10) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname2/proxy/: tls qux (200; 14.532585ms)
Jul 25 09:00:38.224: INFO: (10) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/rewriteme">test</a> (200; 13.329229ms)
Jul 25 09:00:38.225: INFO: (10) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname2/proxy/: bar (200; 16.134094ms)
Jul 25 09:00:38.226: INFO: (10) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname1/proxy/: tls baz (200; 15.938429ms)
Jul 25 09:00:38.226: INFO: (10) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname1/proxy/: foo (200; 15.342596ms)
Jul 25 09:00:38.226: INFO: (10) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 16.510522ms)
Jul 25 09:00:38.226: INFO: (10) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 16.383997ms)
Jul 25 09:00:38.226: INFO: (10) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 15.613846ms)
Jul 25 09:00:38.227: INFO: (10) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname1/proxy/: foo (200; 16.582967ms)
Jul 25 09:00:38.228: INFO: (10) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">test<... (200; 18.22447ms)
Jul 25 09:00:38.230: INFO: (10) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:462/proxy/: tls qux (200; 19.592054ms)
Jul 25 09:00:38.230: INFO: (10) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:460/proxy/: tls baz (200; 19.951525ms)
Jul 25 09:00:38.230: INFO: (10) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 20.164242ms)
Jul 25 09:00:38.230: INFO: (10) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/tlsrewritem... (200; 19.7791ms)
Jul 25 09:00:38.230: INFO: (10) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">... (200; 19.441384ms)
Jul 25 09:00:38.238: INFO: (11) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 7.545955ms)
Jul 25 09:00:38.238: INFO: (11) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">... (200; 6.442891ms)
Jul 25 09:00:38.238: INFO: (11) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/tlsrewritem... (200; 6.919022ms)
Jul 25 09:00:38.239: INFO: (11) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/rewriteme">test</a> (200; 7.773889ms)
Jul 25 09:00:38.239: INFO: (11) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 8.918447ms)
Jul 25 09:00:38.239: INFO: (11) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:462/proxy/: tls qux (200; 8.367464ms)
Jul 25 09:00:38.239: INFO: (11) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 9.075374ms)
Jul 25 09:00:38.240: INFO: (11) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:460/proxy/: tls baz (200; 9.057308ms)
Jul 25 09:00:38.240: INFO: (11) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">test<... (200; 8.964653ms)
Jul 25 09:00:38.240: INFO: (11) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 9.42342ms)
Jul 25 09:00:38.243: INFO: (11) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname2/proxy/: tls qux (200; 13.033404ms)
Jul 25 09:00:38.244: INFO: (11) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname2/proxy/: bar (200; 13.132947ms)
Jul 25 09:00:38.244: INFO: (11) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname2/proxy/: bar (200; 12.660449ms)
Jul 25 09:00:38.244: INFO: (11) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname1/proxy/: foo (200; 13.040746ms)
Jul 25 09:00:38.244: INFO: (11) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname1/proxy/: foo (200; 13.160585ms)
Jul 25 09:00:38.244: INFO: (11) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname1/proxy/: tls baz (200; 14.155643ms)
Jul 25 09:00:38.250: INFO: (12) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:462/proxy/: tls qux (200; 5.218594ms)
Jul 25 09:00:38.251: INFO: (12) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:460/proxy/: tls baz (200; 6.086781ms)
Jul 25 09:00:38.256: INFO: (12) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname1/proxy/: foo (200; 10.196778ms)
Jul 25 09:00:38.256: INFO: (12) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 11.076974ms)
Jul 25 09:00:38.256: INFO: (12) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">test<... (200; 10.843308ms)
Jul 25 09:00:38.256: INFO: (12) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/rewriteme">test</a> (200; 10.461581ms)
Jul 25 09:00:38.256: INFO: (12) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 11.276838ms)
Jul 25 09:00:38.256: INFO: (12) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname1/proxy/: tls baz (200; 11.023889ms)
Jul 25 09:00:38.256: INFO: (12) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname2/proxy/: tls qux (200; 11.703448ms)
Jul 25 09:00:38.256: INFO: (12) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/tlsrewritem... (200; 10.864042ms)
Jul 25 09:00:38.256: INFO: (12) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">... (200; 10.651785ms)
Jul 25 09:00:38.256: INFO: (12) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 11.548448ms)
Jul 25 09:00:38.256: INFO: (12) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 11.484734ms)
Jul 25 09:00:38.256: INFO: (12) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname1/proxy/: foo (200; 10.549449ms)
Jul 25 09:00:38.256: INFO: (12) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname2/proxy/: bar (200; 10.808465ms)
Jul 25 09:00:38.257: INFO: (12) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname2/proxy/: bar (200; 12.60818ms)
Jul 25 09:00:38.265: INFO: (13) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">test<... (200; 7.865505ms)
Jul 25 09:00:38.265: INFO: (13) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 7.086143ms)
Jul 25 09:00:38.267: INFO: (13) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname1/proxy/: tls baz (200; 9.284063ms)
Jul 25 09:00:38.268: INFO: (13) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 9.34089ms)
Jul 25 09:00:38.268: INFO: (13) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 9.282505ms)
Jul 25 09:00:38.269: INFO: (13) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:462/proxy/: tls qux (200; 11.650555ms)
Jul 25 09:00:38.270: INFO: (13) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 11.755913ms)
Jul 25 09:00:38.270: INFO: (13) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/rewriteme">test</a> (200; 12.081338ms)
Jul 25 09:00:38.270: INFO: (13) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">... (200; 12.061835ms)
Jul 25 09:00:38.270: INFO: (13) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/tlsrewritem... (200; 12.753102ms)
Jul 25 09:00:38.270: INFO: (13) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:460/proxy/: tls baz (200; 13.115636ms)
Jul 25 09:00:38.270: INFO: (13) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname2/proxy/: bar (200; 12.336392ms)
Jul 25 09:00:38.270: INFO: (13) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname2/proxy/: bar (200; 13.044023ms)
Jul 25 09:00:38.270: INFO: (13) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname1/proxy/: foo (200; 12.702579ms)
Jul 25 09:00:38.270: INFO: (13) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname1/proxy/: foo (200; 12.708209ms)
Jul 25 09:00:38.270: INFO: (13) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname2/proxy/: tls qux (200; 12.279704ms)
Jul 25 09:00:38.277: INFO: (14) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">test<... (200; 6.634645ms)
Jul 25 09:00:38.277: INFO: (14) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/rewriteme">test</a> (200; 6.532994ms)
Jul 25 09:00:38.279: INFO: (14) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:462/proxy/: tls qux (200; 7.731599ms)
Jul 25 09:00:38.279: INFO: (14) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">... (200; 8.783084ms)
Jul 25 09:00:38.279: INFO: (14) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 8.373636ms)
Jul 25 09:00:38.280: INFO: (14) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 8.756755ms)
Jul 25 09:00:38.280: INFO: (14) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:460/proxy/: tls baz (200; 8.689723ms)
Jul 25 09:00:38.280: INFO: (14) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/tlsrewritem... (200; 8.473449ms)
Jul 25 09:00:38.280: INFO: (14) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 9.08484ms)
Jul 25 09:00:38.280: INFO: (14) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 9.398384ms)
Jul 25 09:00:38.282: INFO: (14) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname1/proxy/: foo (200; 11.077105ms)
Jul 25 09:00:38.282: INFO: (14) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname2/proxy/: bar (200; 10.238111ms)
Jul 25 09:00:38.282: INFO: (14) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname2/proxy/: bar (200; 11.190391ms)
Jul 25 09:00:38.282: INFO: (14) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname1/proxy/: tls baz (200; 10.776973ms)
Jul 25 09:00:38.282: INFO: (14) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname1/proxy/: foo (200; 11.571004ms)
Jul 25 09:00:38.285: INFO: (14) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname2/proxy/: tls qux (200; 13.367413ms)
Jul 25 09:00:38.290: INFO: (15) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:460/proxy/: tls baz (200; 4.649793ms)
Jul 25 09:00:38.295: INFO: (15) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname2/proxy/: bar (200; 10.388809ms)
Jul 25 09:00:38.296: INFO: (15) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname1/proxy/: foo (200; 10.388977ms)
Jul 25 09:00:38.296: INFO: (15) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname1/proxy/: tls baz (200; 10.274364ms)
Jul 25 09:00:38.296: INFO: (15) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname2/proxy/: bar (200; 10.377215ms)
Jul 25 09:00:38.296: INFO: (15) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname2/proxy/: tls qux (200; 10.713368ms)
Jul 25 09:00:38.296: INFO: (15) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:462/proxy/: tls qux (200; 10.893481ms)
Jul 25 09:00:38.296: INFO: (15) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 10.753424ms)
Jul 25 09:00:38.297: INFO: (15) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">... (200; 12.428658ms)
Jul 25 09:00:38.297: INFO: (15) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">test<... (200; 12.41028ms)
Jul 25 09:00:38.299: INFO: (15) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname1/proxy/: foo (200; 13.391984ms)
Jul 25 09:00:38.300: INFO: (15) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 14.63729ms)
Jul 25 09:00:38.300: INFO: (15) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/rewriteme">test</a> (200; 14.88859ms)
Jul 25 09:00:38.300: INFO: (15) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 15.177357ms)
Jul 25 09:00:38.301: INFO: (15) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/tlsrewritem... (200; 15.718453ms)
Jul 25 09:00:38.301: INFO: (15) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 15.604305ms)
Jul 25 09:00:38.310: INFO: (16) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 7.699658ms)
Jul 25 09:00:38.311: INFO: (16) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/rewriteme">test</a> (200; 9.328299ms)
Jul 25 09:00:38.312: INFO: (16) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">test<... (200; 9.786047ms)
Jul 25 09:00:38.312: INFO: (16) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:460/proxy/: tls baz (200; 9.967394ms)
Jul 25 09:00:38.312: INFO: (16) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 10.782564ms)
Jul 25 09:00:38.313: INFO: (16) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">... (200; 11.029962ms)
Jul 25 09:00:38.313: INFO: (16) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:462/proxy/: tls qux (200; 11.53637ms)
Jul 25 09:00:38.313: INFO: (16) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 10.495053ms)
Jul 25 09:00:38.313: INFO: (16) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/tlsrewritem... (200; 10.195087ms)
Jul 25 09:00:38.313: INFO: (16) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 10.773098ms)
Jul 25 09:00:38.315: INFO: (16) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname2/proxy/: bar (200; 12.272021ms)
Jul 25 09:00:38.319: INFO: (16) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname1/proxy/: foo (200; 18.26038ms)
Jul 25 09:00:38.320: INFO: (16) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname1/proxy/: foo (200; 18.344981ms)
Jul 25 09:00:38.321: INFO: (16) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname2/proxy/: bar (200; 19.643565ms)
Jul 25 09:00:38.321: INFO: (16) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname2/proxy/: tls qux (200; 19.491063ms)
Jul 25 09:00:38.322: INFO: (16) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname1/proxy/: tls baz (200; 19.771472ms)
Jul 25 09:00:38.332: INFO: (17) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/rewriteme">test</a> (200; 9.679278ms)
Jul 25 09:00:38.332: INFO: (17) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:462/proxy/: tls qux (200; 9.838714ms)
Jul 25 09:00:38.332: INFO: (17) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname2/proxy/: bar (200; 10.366743ms)
Jul 25 09:00:38.332: INFO: (17) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 10.387648ms)
Jul 25 09:00:38.332: INFO: (17) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname1/proxy/: foo (200; 10.671298ms)
Jul 25 09:00:38.333: INFO: (17) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">test<... (200; 10.799213ms)
Jul 25 09:00:38.333: INFO: (17) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">... (200; 10.964279ms)
Jul 25 09:00:38.333: INFO: (17) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/tlsrewritem... (200; 11.304846ms)
Jul 25 09:00:38.336: INFO: (17) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:460/proxy/: tls baz (200; 14.403101ms)
Jul 25 09:00:38.336: INFO: (17) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname1/proxy/: tls baz (200; 14.541881ms)
Jul 25 09:00:38.337: INFO: (17) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname2/proxy/: bar (200; 14.892937ms)
Jul 25 09:00:38.337: INFO: (17) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname1/proxy/: foo (200; 14.818732ms)
Jul 25 09:00:38.337: INFO: (17) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 15.416074ms)
Jul 25 09:00:38.337: INFO: (17) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname2/proxy/: tls qux (200; 15.573583ms)
Jul 25 09:00:38.337: INFO: (17) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 15.606374ms)
Jul 25 09:00:38.338: INFO: (17) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 15.833495ms)
Jul 25 09:00:38.349: INFO: (18) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 9.818475ms)
Jul 25 09:00:38.349: INFO: (18) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname2/proxy/: bar (200; 9.682465ms)
Jul 25 09:00:38.352: INFO: (18) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">... (200; 12.977624ms)
Jul 25 09:00:38.352: INFO: (18) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/rewriteme">test</a> (200; 13.759174ms)
Jul 25 09:00:38.352: INFO: (18) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 13.572477ms)
Jul 25 09:00:38.352: INFO: (18) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">test<... (200; 14.584767ms)
Jul 25 09:00:38.352: INFO: (18) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:460/proxy/: tls baz (200; 14.434231ms)
Jul 25 09:00:38.352: INFO: (18) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname1/proxy/: foo (200; 14.126051ms)
Jul 25 09:00:38.353: INFO: (18) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/tlsrewritem... (200; 14.510064ms)
Jul 25 09:00:38.353: INFO: (18) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 14.884099ms)
Jul 25 09:00:38.353: INFO: (18) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:462/proxy/: tls qux (200; 14.449776ms)
Jul 25 09:00:38.353: INFO: (18) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname2/proxy/: tls qux (200; 13.785942ms)
Jul 25 09:00:38.353: INFO: (18) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 13.935634ms)
Jul 25 09:00:38.353: INFO: (18) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname1/proxy/: foo (200; 15.129086ms)
Jul 25 09:00:38.355: INFO: (18) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname2/proxy/: bar (200; 16.459029ms)
Jul 25 09:00:38.356: INFO: (18) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname1/proxy/: tls baz (200; 16.667308ms)
Jul 25 09:00:38.364: INFO: (19) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">test<... (200; 7.824712ms)
Jul 25 09:00:38.364: INFO: (19) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 8.24668ms)
Jul 25 09:00:38.364: INFO: (19) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g/proxy/rewriteme">test</a> (200; 8.48917ms)
Jul 25 09:00:38.365: INFO: (19) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:462/proxy/: tls qux (200; 9.538563ms)
Jul 25 09:00:38.366: INFO: (19) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:443/proxy/tlsrewritem... (200; 9.508469ms)
Jul 25 09:00:38.366: INFO: (19) /api/v1/namespaces/proxy-5382/pods/proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 9.642199ms)
Jul 25 09:00:38.366: INFO: (19) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:162/proxy/: bar (200; 10.057966ms)
Jul 25 09:00:38.366: INFO: (19) /api/v1/namespaces/proxy-5382/pods/https:proxy-service-6vxwz-svb6g:460/proxy/: tls baz (200; 10.445491ms)
Jul 25 09:00:38.367: INFO: (19) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:160/proxy/: foo (200; 10.881085ms)
Jul 25 09:00:38.367: INFO: (19) /api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/: <a href="/api/v1/namespaces/proxy-5382/pods/http:proxy-service-6vxwz-svb6g:1080/proxy/rewriteme">... (200; 10.946832ms)
Jul 25 09:00:38.370: INFO: (19) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname1/proxy/: foo (200; 14.415296ms)
Jul 25 09:00:38.370: INFO: (19) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname1/proxy/: foo (200; 14.554256ms)
Jul 25 09:00:38.370: INFO: (19) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname2/proxy/: tls qux (200; 14.34705ms)
Jul 25 09:00:38.370: INFO: (19) /api/v1/namespaces/proxy-5382/services/proxy-service-6vxwz:portname2/proxy/: bar (200; 14.409884ms)
Jul 25 09:00:38.370: INFO: (19) /api/v1/namespaces/proxy-5382/services/http:proxy-service-6vxwz:portname2/proxy/: bar (200; 14.649889ms)
Jul 25 09:00:38.373: INFO: (19) /api/v1/namespaces/proxy-5382/services/https:proxy-service-6vxwz:tlsportname1/proxy/: tls baz (200; 17.456616ms)
STEP: deleting ReplicationController proxy-service-6vxwz in namespace proxy-5382, will wait for the garbage collector to delete the pods
Jul 25 09:00:38.435: INFO: Deleting ReplicationController proxy-service-6vxwz took: 9.157543ms
Jul 25 09:00:38.736: INFO: Terminating ReplicationController proxy-service-6vxwz pods took: 300.362223ms
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:00:40.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-5382" for this suite.
Jul 25 09:00:46.952: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:00:47.056: INFO: namespace proxy-5382 deletion completed in 6.11597788s

• [SLOW TEST:19.278 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:00:47.059: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service endpoint-test2 in namespace services-6252
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6252 to expose endpoints map[]
Jul 25 09:00:47.105: INFO: successfully validated that service endpoint-test2 in namespace services-6252 exposes endpoints map[] (3.692804ms elapsed)
STEP: Creating pod pod1 in namespace services-6252
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6252 to expose endpoints map[pod1:[80]]
Jul 25 09:00:49.139: INFO: successfully validated that service endpoint-test2 in namespace services-6252 exposes endpoints map[pod1:[80]] (2.025737263s elapsed)
STEP: Creating pod pod2 in namespace services-6252
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6252 to expose endpoints map[pod1:[80] pod2:[80]]
Jul 25 09:00:51.174: INFO: successfully validated that service endpoint-test2 in namespace services-6252 exposes endpoints map[pod1:[80] pod2:[80]] (2.030086581s elapsed)
STEP: Deleting pod pod1 in namespace services-6252
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6252 to expose endpoints map[pod2:[80]]
Jul 25 09:00:51.198: INFO: successfully validated that service endpoint-test2 in namespace services-6252 exposes endpoints map[pod2:[80]] (11.368008ms elapsed)
STEP: Deleting pod pod2 in namespace services-6252
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6252 to expose endpoints map[]
Jul 25 09:00:51.208: INFO: successfully validated that service endpoint-test2 in namespace services-6252 exposes endpoints map[] (2.76121ms elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:00:51.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-6252" for this suite.
Jul 25 09:01:13.256: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:01:13.349: INFO: namespace services-6252 deletion completed in 22.10370331s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:26.291 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:01:13.350: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-2604
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 25 09:01:13.378: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 25 09:01:35.452: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.30:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2604 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 25 09:01:35.452: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
Jul 25 09:01:35.578: INFO: Found all expected endpoints: [netserver-0]
Jul 25 09:01:35.581: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.2.233:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-2604 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 25 09:01:35.581: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
Jul 25 09:01:35.689: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:01:35.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2604" for this suite.
Jul 25 09:01:57.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:01:57.790: INFO: namespace pod-network-test-2604 deletion completed in 22.094812264s

• [SLOW TEST:44.440 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:01:57.794: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-4545/configmap-test-df39903e-4d9c-43aa-832c-580f0a4f76e7
STEP: Creating a pod to test consume configMaps
Jul 25 09:01:57.831: INFO: Waiting up to 5m0s for pod "pod-configmaps-ff74f891-1793-404b-8bec-70eba1329a3f" in namespace "configmap-4545" to be "success or failure"
Jul 25 09:01:57.834: INFO: Pod "pod-configmaps-ff74f891-1793-404b-8bec-70eba1329a3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.991262ms
Jul 25 09:01:59.838: INFO: Pod "pod-configmaps-ff74f891-1793-404b-8bec-70eba1329a3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006367692s
STEP: Saw pod success
Jul 25 09:01:59.838: INFO: Pod "pod-configmaps-ff74f891-1793-404b-8bec-70eba1329a3f" satisfied condition "success or failure"
Jul 25 09:01:59.840: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-configmaps-ff74f891-1793-404b-8bec-70eba1329a3f container env-test: <nil>
STEP: delete the pod
Jul 25 09:01:59.855: INFO: Waiting for pod pod-configmaps-ff74f891-1793-404b-8bec-70eba1329a3f to disappear
Jul 25 09:01:59.858: INFO: Pod pod-configmaps-ff74f891-1793-404b-8bec-70eba1329a3f no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:01:59.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4545" for this suite.
Jul 25 09:02:05.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:02:05.942: INFO: namespace configmap-4545 deletion completed in 6.080154875s

• [SLOW TEST:8.148 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:02:05.947: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
STEP: reading a file in the container
Jul 25 09:02:08.495: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6397 pod-service-account-2fd1ac56-61fb-4262-9e9e-0fa93c532e42 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Jul 25 09:02:08.672: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6397 pod-service-account-2fd1ac56-61fb-4262-9e9e-0fa93c532e42 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Jul 25 09:02:08.879: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-6397 pod-service-account-2fd1ac56-61fb-4262-9e9e-0fa93c532e42 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:02:09.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6397" for this suite.
Jul 25 09:02:15.075: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:02:15.161: INFO: namespace svcaccounts-6397 deletion completed in 6.093880305s

• [SLOW TEST:9.214 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:02:15.166: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with configMap that has name projected-configmap-test-upd-4b5944f1-a758-4c62-86f9-1aa61329f20a
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-4b5944f1-a758-4c62-86f9-1aa61329f20a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:02:19.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4025" for this suite.
Jul 25 09:02:41.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:02:41.339: INFO: namespace projected-4025 deletion completed in 22.093013437s

• [SLOW TEST:26.173 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:02:41.347: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:02:44.407: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6155" for this suite.
Jul 25 09:03:06.419: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:03:06.495: INFO: namespace replication-controller-6155 deletion completed in 22.083821347s

• [SLOW TEST:25.148 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:03:06.497: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Jul 25 09:03:06.535: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3705,SelfLink:/api/v1/namespaces/watch-3705/configmaps/e2e-watch-test-watch-closed,UID:6172a56f-0dc2-4b74-9c79-60f18df4e17c,ResourceVersion:58796,Generation:0,CreationTimestamp:2019-07-25 09:03:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 25 09:03:06.535: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3705,SelfLink:/api/v1/namespaces/watch-3705/configmaps/e2e-watch-test-watch-closed,UID:6172a56f-0dc2-4b74-9c79-60f18df4e17c,ResourceVersion:58797,Generation:0,CreationTimestamp:2019-07-25 09:03:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Jul 25 09:03:06.545: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3705,SelfLink:/api/v1/namespaces/watch-3705/configmaps/e2e-watch-test-watch-closed,UID:6172a56f-0dc2-4b74-9c79-60f18df4e17c,ResourceVersion:58798,Generation:0,CreationTimestamp:2019-07-25 09:03:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 25 09:03:06.545: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-watch-closed,GenerateName:,Namespace:watch-3705,SelfLink:/api/v1/namespaces/watch-3705/configmaps/e2e-watch-test-watch-closed,UID:6172a56f-0dc2-4b74-9c79-60f18df4e17c,ResourceVersion:58799,Generation:0,CreationTimestamp:2019-07-25 09:03:06 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: watch-closed-and-restarted,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:03:06.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3705" for this suite.
Jul 25 09:03:12.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:03:12.654: INFO: namespace watch-3705 deletion completed in 6.104843675s

• [SLOW TEST:6.158 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:03:12.654: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 25 09:03:12.705: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 09:03:12.709: INFO: Number of nodes with available pods: 0
Jul 25 09:03:12.709: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 09:03:13.714: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 09:03:13.716: INFO: Number of nodes with available pods: 0
Jul 25 09:03:13.717: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 09:03:14.714: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 09:03:14.716: INFO: Number of nodes with available pods: 2
Jul 25 09:03:14.716: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Jul 25 09:03:14.734: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 09:03:14.740: INFO: Number of nodes with available pods: 1
Jul 25 09:03:14.740: INFO: Node essentialpks-conformance-3 is running more than one daemon pod
Jul 25 09:03:15.744: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 09:03:15.747: INFO: Number of nodes with available pods: 1
Jul 25 09:03:15.747: INFO: Node essentialpks-conformance-3 is running more than one daemon pod
Jul 25 09:03:16.744: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 09:03:16.747: INFO: Number of nodes with available pods: 2
Jul 25 09:03:16.747: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8287, will wait for the garbage collector to delete the pods
Jul 25 09:03:16.841: INFO: Deleting DaemonSet.extensions daemon-set took: 37.118591ms
Jul 25 09:03:17.041: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.300828ms
Jul 25 09:03:29.945: INFO: Number of nodes with available pods: 0
Jul 25 09:03:29.945: INFO: Number of running nodes: 0, number of available pods: 0
Jul 25 09:03:29.949: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8287/daemonsets","resourceVersion":"58897"},"items":null}

Jul 25 09:03:29.953: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8287/pods","resourceVersion":"58897"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:03:29.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8287" for this suite.
Jul 25 09:03:35.978: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:03:36.060: INFO: namespace daemonsets-8287 deletion completed in 6.09162244s

• [SLOW TEST:23.405 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:03:36.060: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
W0725 09:03:36.683620      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 25 09:03:36.683: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:03:36.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-375" for this suite.
Jul 25 09:03:42.699: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:03:42.787: INFO: namespace gc-375 deletion completed in 6.098163281s

• [SLOW TEST:6.728 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:03:42.788: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-34a962b6-86a6-481d-8c03-7636b3a3fa58
STEP: Creating secret with name s-test-opt-upd-150ed8e9-d528-4b13-8a97-42bc9972f423
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-34a962b6-86a6-481d-8c03-7636b3a3fa58
STEP: Updating secret s-test-opt-upd-150ed8e9-d528-4b13-8a97-42bc9972f423
STEP: Creating secret with name s-test-opt-create-b860b108-2d11-4eaa-a0b7-7a6c47bb9249
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:03:46.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1218" for this suite.
Jul 25 09:04:08.928: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:04:09.012: INFO: namespace secrets-1218 deletion completed in 22.094750528s

• [SLOW TEST:26.224 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:04:09.013: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Jul 25 09:04:09.051: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9475,SelfLink:/api/v1/namespaces/watch-9475/configmaps/e2e-watch-test-label-changed,UID:486b2095-3585-446f-805a-883f11090b6f,ResourceVersion:59064,Generation:0,CreationTimestamp:2019-07-25 09:04:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 25 09:04:09.052: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9475,SelfLink:/api/v1/namespaces/watch-9475/configmaps/e2e-watch-test-label-changed,UID:486b2095-3585-446f-805a-883f11090b6f,ResourceVersion:59065,Generation:0,CreationTimestamp:2019-07-25 09:04:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 25 09:04:09.052: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9475,SelfLink:/api/v1/namespaces/watch-9475/configmaps/e2e-watch-test-label-changed,UID:486b2095-3585-446f-805a-883f11090b6f,ResourceVersion:59066,Generation:0,CreationTimestamp:2019-07-25 09:04:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Jul 25 09:04:19.074: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9475,SelfLink:/api/v1/namespaces/watch-9475/configmaps/e2e-watch-test-label-changed,UID:486b2095-3585-446f-805a-883f11090b6f,ResourceVersion:59083,Generation:0,CreationTimestamp:2019-07-25 09:04:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 25 09:04:19.074: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9475,SelfLink:/api/v1/namespaces/watch-9475/configmaps/e2e-watch-test-label-changed,UID:486b2095-3585-446f-805a-883f11090b6f,ResourceVersion:59084,Generation:0,CreationTimestamp:2019-07-25 09:04:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Jul 25 09:04:19.074: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-label-changed,GenerateName:,Namespace:watch-9475,SelfLink:/api/v1/namespaces/watch-9475/configmaps/e2e-watch-test-label-changed,UID:486b2095-3585-446f-805a-883f11090b6f,ResourceVersion:59085,Generation:0,CreationTimestamp:2019-07-25 09:04:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: label-changed-and-restored,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:04:19.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-9475" for this suite.
Jul 25 09:04:25.089: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:04:25.194: INFO: namespace watch-9475 deletion completed in 6.116279169s

• [SLOW TEST:16.182 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:04:25.201: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 25 09:04:47.255: INFO: Container started at 2019-07-25 09:04:26 +0000 UTC, pod became ready at 2019-07-25 09:04:45 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:04:47.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5666" for this suite.
Jul 25 09:05:09.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:05:09.373: INFO: namespace container-probe-5666 deletion completed in 22.113586191s

• [SLOW TEST:44.173 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:05:09.377: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 25 09:05:09.418: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f176a9ad-29f7-429d-b273-190cbdb065b0" in namespace "downward-api-8915" to be "success or failure"
Jul 25 09:05:09.426: INFO: Pod "downwardapi-volume-f176a9ad-29f7-429d-b273-190cbdb065b0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.72207ms
Jul 25 09:05:11.431: INFO: Pod "downwardapi-volume-f176a9ad-29f7-429d-b273-190cbdb065b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013633645s
STEP: Saw pod success
Jul 25 09:05:11.432: INFO: Pod "downwardapi-volume-f176a9ad-29f7-429d-b273-190cbdb065b0" satisfied condition "success or failure"
Jul 25 09:05:11.435: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-f176a9ad-29f7-429d-b273-190cbdb065b0 container client-container: <nil>
STEP: delete the pod
Jul 25 09:05:11.453: INFO: Waiting for pod downwardapi-volume-f176a9ad-29f7-429d-b273-190cbdb065b0 to disappear
Jul 25 09:05:11.457: INFO: Pod downwardapi-volume-f176a9ad-29f7-429d-b273-190cbdb065b0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:05:11.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8915" for this suite.
Jul 25 09:05:17.470: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:05:17.552: INFO: namespace downward-api-8915 deletion completed in 6.090253693s

• [SLOW TEST:8.175 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:05:17.558: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 25 09:05:17.588: INFO: Waiting up to 5m0s for pod "downwardapi-volume-66957078-2261-4d0a-a008-ef201baea58e" in namespace "projected-5840" to be "success or failure"
Jul 25 09:05:17.596: INFO: Pod "downwardapi-volume-66957078-2261-4d0a-a008-ef201baea58e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.472017ms
Jul 25 09:05:19.599: INFO: Pod "downwardapi-volume-66957078-2261-4d0a-a008-ef201baea58e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011617589s
STEP: Saw pod success
Jul 25 09:05:19.599: INFO: Pod "downwardapi-volume-66957078-2261-4d0a-a008-ef201baea58e" satisfied condition "success or failure"
Jul 25 09:05:19.602: INFO: Trying to get logs from node essentialpks-conformance-3 pod downwardapi-volume-66957078-2261-4d0a-a008-ef201baea58e container client-container: <nil>
STEP: delete the pod
Jul 25 09:05:19.618: INFO: Waiting for pod downwardapi-volume-66957078-2261-4d0a-a008-ef201baea58e to disappear
Jul 25 09:05:19.620: INFO: Pod downwardapi-volume-66957078-2261-4d0a-a008-ef201baea58e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:05:19.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5840" for this suite.
Jul 25 09:05:25.631: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:05:25.731: INFO: namespace projected-5840 deletion completed in 6.108063323s

• [SLOW TEST:8.173 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:05:25.737: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 25 09:05:25.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 create -f - --namespace=kubectl-618'
Jul 25 09:05:26.130: INFO: stderr: ""
Jul 25 09:05:26.130: INFO: stdout: "replicationcontroller/redis-master created\n"
Jul 25 09:05:26.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 create -f - --namespace=kubectl-618'
Jul 25 09:05:26.301: INFO: stderr: ""
Jul 25 09:05:26.301: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 25 09:05:27.305: INFO: Selector matched 1 pods for map[app:redis]
Jul 25 09:05:27.305: INFO: Found 0 / 1
Jul 25 09:05:28.304: INFO: Selector matched 1 pods for map[app:redis]
Jul 25 09:05:28.304: INFO: Found 1 / 1
Jul 25 09:05:28.304: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 25 09:05:28.308: INFO: Selector matched 1 pods for map[app:redis]
Jul 25 09:05:28.308: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 25 09:05:28.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 describe pod redis-master-kqmdr --namespace=kubectl-618'
Jul 25 09:05:28.405: INFO: stderr: ""
Jul 25 09:05:28.405: INFO: stdout: "Name:           redis-master-kqmdr\nNamespace:      kubectl-618\nPriority:       0\nNode:           essentialpks-conformance-2/192.168.102.3\nStart Time:     Thu, 25 Jul 2019 09:05:26 +0000\nLabels:         app=redis\n                role=master\nAnnotations:    <none>\nStatus:         Running\nIP:             10.244.2.241\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://f51da419c0353694c5fd2152e4f27b8a6333135102cf98f5235587e88e18944e\n    Image:          gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Image ID:       docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 25 Jul 2019 09:05:27 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-rrmjw (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-rrmjw:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-rrmjw\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From                                 Message\n  ----    ------     ----  ----                                 -------\n  Normal  Scheduled  2s    default-scheduler                    Successfully assigned kubectl-618/redis-master-kqmdr to essentialpks-conformance-2\n  Normal  Pulled     2s    kubelet, essentialpks-conformance-2  Container image \"gcr.io/kubernetes-e2e-test-images/redis:1.0\" already present on machine\n  Normal  Created    2s    kubelet, essentialpks-conformance-2  Created container redis-master\n  Normal  Started    1s    kubelet, essentialpks-conformance-2  Started container redis-master\n"
Jul 25 09:05:28.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 describe rc redis-master --namespace=kubectl-618'
Jul 25 09:05:28.517: INFO: stderr: ""
Jul 25 09:05:28.517: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-618\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        gcr.io/kubernetes-e2e-test-images/redis:1.0\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: redis-master-kqmdr\n"
Jul 25 09:05:28.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 describe service redis-master --namespace=kubectl-618'
Jul 25 09:05:28.601: INFO: stderr: ""
Jul 25 09:05:28.601: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-618\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.101.203.95\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.244.2.241:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jul 25 09:05:28.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 describe node essentialpks-conformance-1'
Jul 25 09:05:28.707: INFO: stderr: ""
Jul 25 09:05:28.707: INFO: stdout: "Name:               essentialpks-conformance-1\nRoles:              master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=essentialpks-conformance-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"fa:ea:d5:77:6f:90\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 192.168.102.2\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Tue, 19 Mar 2019 13:23:36 +0000\nTaints:             node-role.kubernetes.io/master:NoSchedule\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Thu, 25 Jul 2019 09:05:15 +0000   Mon, 29 Apr 2019 09:54:38 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Thu, 25 Jul 2019 09:05:15 +0000   Mon, 29 Apr 2019 09:54:38 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Thu, 25 Jul 2019 09:05:15 +0000   Mon, 29 Apr 2019 09:54:38 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Thu, 25 Jul 2019 09:05:15 +0000   Thu, 25 Jul 2019 08:33:43 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.102.2\n  Hostname:    essentialpks-conformance-1\nCapacity:\n cpu:                2\n ephemeral-storage:  40168028Ki\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             8175132Ki\n pods:               110\nAllocatable:\n cpu:                2\n ephemeral-storage:  37018854544\n hugepages-1Gi:      0\n hugepages-2Mi:      0\n memory:             8072732Ki\n pods:               110\nSystem Info:\n Machine ID:                 a05031910e15e3003afad29e5c8f7b92\n System UUID:                B4732442-5A30-614F-A047-8F2023A2AFAD\n Boot ID:                    52bb9525-03ac-4a6e-80ee-9a9c3ce372f8\n Kernel Version:             4.4.0-116-generic\n OS Image:                   Ubuntu 16.04.4 LTS\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.6.3\n Kubelet Version:            v1.15.1+vmware.1\n Kube-Proxy Version:         v1.15.1+vmware.1\nPodCIDR:                     10.244.0.0/24\nNon-terminated Pods:         (9 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  heptio-sonobuoy            sonobuoy-systemd-logs-daemon-set-8c8b567fc9ec4bcc-2s6jh    0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m\n  kube-system                coredns-59d4dcfccd-4tmr7                                   100m (5%)     0 (0%)      70Mi (0%)        170Mi (2%)     127d\n  kube-system                coredns-59d4dcfccd-gjs6z                                   100m (5%)     0 (0%)      70Mi (0%)        170Mi (2%)     127d\n  kube-system                etcd-essentialpks-conformance-1                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         31m\n  kube-system                kube-apiserver-essentialpks-conformance-1                  250m (12%)    0 (0%)      0 (0%)           0 (0%)         21m\n  kube-system                kube-controller-manager-essentialpks-conformance-1         200m (10%)    0 (0%)      0 (0%)           0 (0%)         21m\n  kube-system                kube-flannel-ds-amd64-cs5jp                                100m (5%)     100m (5%)   50Mi (0%)        50Mi (0%)      127d\n  kube-system                kube-proxy-xbbzv                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         127d\n  kube-system                kube-scheduler-essentialpks-conformance-1                  100m (5%)     0 (0%)      0 (0%)           0 (0%)         21m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                850m (42%)  100m (5%)\n  memory             190Mi (2%)  390Mi (4%)\n  ephemeral-storage  0 (0%)      0 (0%)\nEvents:\n  Type    Reason                   Age                From                                    Message\n  ----    ------                   ----               ----                                    -------\n  Normal  Starting                 74m                kubelet, essentialpks-conformance-1     Starting kubelet.\n  Normal  NodeHasSufficientMemory  74m (x8 over 74m)  kubelet, essentialpks-conformance-1     Node essentialpks-conformance-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    74m (x8 over 74m)  kubelet, essentialpks-conformance-1     Node essentialpks-conformance-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     74m (x7 over 74m)  kubelet, essentialpks-conformance-1     Node essentialpks-conformance-1 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  74m                kubelet, essentialpks-conformance-1     Updated Node Allocatable limit across pods\n  Normal  Starting                 74m                kube-proxy, essentialpks-conformance-1  Starting kube-proxy.\n  Normal  Starting                 31m                kubelet, essentialpks-conformance-1     Starting kubelet.\n  Normal  NodeHasSufficientMemory  31m                kubelet, essentialpks-conformance-1     Node essentialpks-conformance-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    31m                kubelet, essentialpks-conformance-1     Node essentialpks-conformance-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     31m                kubelet, essentialpks-conformance-1     Node essentialpks-conformance-1 status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             31m                kubelet, essentialpks-conformance-1     Node essentialpks-conformance-1 status is now: NodeNotReady\n  Normal  NodeAllocatableEnforced  31m                kubelet, essentialpks-conformance-1     Updated Node Allocatable limit across pods\n  Normal  NodeReady                31m                kubelet, essentialpks-conformance-1     Node essentialpks-conformance-1 status is now: NodeReady\n  Normal  Starting                 31m                kube-proxy, essentialpks-conformance-1  Starting kube-proxy.\n"
Jul 25 09:05:28.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 describe namespace kubectl-618'
Jul 25 09:05:28.793: INFO: stderr: ""
Jul 25 09:05:28.793: INFO: stdout: "Name:         kubectl-618\nLabels:       e2e-framework=kubectl\n              e2e-run=40426138-2e3f-462c-85c1-7c4a344941db\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:05:28.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-618" for this suite.
Jul 25 09:05:50.806: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:05:50.883: INFO: namespace kubectl-618 deletion completed in 22.087043728s

• [SLOW TEST:25.147 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl describe
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:05:50.884: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 25 09:05:50.908: INFO: PodSpec: initContainers in spec.initContainers
Jul 25 09:06:35.672: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-c710bfa5-126c-492e-8840-3aa7b853bcfa", GenerateName:"", Namespace:"init-container-9370", SelfLink:"/api/v1/namespaces/init-container-9370/pods/pod-init-c710bfa5-126c-492e-8840-3aa7b853bcfa", UID:"cd7a8c75-dfbd-40cb-8696-2298acebddcb", ResourceVersion:"59425", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63699642350, loc:(*time.Location)(0x80bb5c0)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"908525013"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Initializers:(*v1.Initializers)(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-7f7fr", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc0027aa000), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7f7fr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7f7fr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-7f7fr", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00022a208), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"essentialpks-conformance-2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc001d7e5a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00022a380)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00022a400)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00022a408), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00022a40c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699642350, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699642350, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699642350, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699642350, loc:(*time.Location)(0x80bb5c0)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.102.3", PodIP:"10.244.2.242", StartTime:(*v1.Time)(0xc002878060), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00285c150)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc00285c1c0)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://f7d743ef519799b9b80699f3d1d76c57ca8c62a430e7947810617c37819e3467"}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0028780a0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:""}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc002878080), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:""}}, QOSClass:"Guaranteed"}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:06:35.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9370" for this suite.
Jul 25 09:06:57.701: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:06:57.793: INFO: namespace init-container-9370 deletion completed in 22.110131616s

• [SLOW TEST:66.909 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:06:57.793: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
W0725 09:07:07.891631      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 25 09:07:07.891: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:07:07.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1812" for this suite.
Jul 25 09:07:13.911: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:07:14.010: INFO: namespace gc-1812 deletion completed in 6.109001867s

• [SLOW TEST:16.224 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:07:14.018: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:07:16.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-298" for this suite.
Jul 25 09:07:22.105: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:07:22.182: INFO: namespace emptydir-wrapper-298 deletion completed in 6.087239644s

• [SLOW TEST:8.164 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not conflict [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:07:22.182: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-8a746e11-e589-4472-a5ad-d27834b28eab
STEP: Creating a pod to test consume configMaps
Jul 25 09:07:22.219: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-b723ec09-d612-4c46-82e9-dab935a339a2" in namespace "projected-8719" to be "success or failure"
Jul 25 09:07:22.223: INFO: Pod "pod-projected-configmaps-b723ec09-d612-4c46-82e9-dab935a339a2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.612218ms
Jul 25 09:07:24.227: INFO: Pod "pod-projected-configmaps-b723ec09-d612-4c46-82e9-dab935a339a2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007335871s
STEP: Saw pod success
Jul 25 09:07:24.227: INFO: Pod "pod-projected-configmaps-b723ec09-d612-4c46-82e9-dab935a339a2" satisfied condition "success or failure"
Jul 25 09:07:24.230: INFO: Trying to get logs from node essentialpks-conformance-3 pod pod-projected-configmaps-b723ec09-d612-4c46-82e9-dab935a339a2 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 25 09:07:24.248: INFO: Waiting for pod pod-projected-configmaps-b723ec09-d612-4c46-82e9-dab935a339a2 to disappear
Jul 25 09:07:24.250: INFO: Pod pod-projected-configmaps-b723ec09-d612-4c46-82e9-dab935a339a2 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:07:24.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8719" for this suite.
Jul 25 09:07:30.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:07:30.335: INFO: namespace projected-8719 deletion completed in 6.081091153s

• [SLOW TEST:8.153 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:07:30.336: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-3846, will wait for the garbage collector to delete the pods
Jul 25 09:07:32.428: INFO: Deleting Job.batch foo took: 6.782758ms
Jul 25 09:07:32.728: INFO: Terminating Job.batch foo pods took: 300.311799ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:08:14.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-3846" for this suite.
Jul 25 09:08:20.947: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:08:21.037: INFO: namespace job-3846 deletion completed in 6.099884572s

• [SLOW TEST:50.702 seconds]
[sig-apps] Job
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:08:21.038: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-96696418-6684-4ae3-8102-5f7148e411ca
STEP: Creating a pod to test consume configMaps
Jul 25 09:08:21.078: INFO: Waiting up to 5m0s for pod "pod-configmaps-92e996bb-332f-4b99-a9ed-48217b2c908b" in namespace "configmap-1509" to be "success or failure"
Jul 25 09:08:21.083: INFO: Pod "pod-configmaps-92e996bb-332f-4b99-a9ed-48217b2c908b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.558931ms
Jul 25 09:08:23.087: INFO: Pod "pod-configmaps-92e996bb-332f-4b99-a9ed-48217b2c908b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008818227s
STEP: Saw pod success
Jul 25 09:08:23.087: INFO: Pod "pod-configmaps-92e996bb-332f-4b99-a9ed-48217b2c908b" satisfied condition "success or failure"
Jul 25 09:08:23.089: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-configmaps-92e996bb-332f-4b99-a9ed-48217b2c908b container configmap-volume-test: <nil>
STEP: delete the pod
Jul 25 09:08:23.117: INFO: Waiting for pod pod-configmaps-92e996bb-332f-4b99-a9ed-48217b2c908b to disappear
Jul 25 09:08:23.120: INFO: Pod pod-configmaps-92e996bb-332f-4b99-a9ed-48217b2c908b no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:08:23.120: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1509" for this suite.
Jul 25 09:08:29.133: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:08:29.221: INFO: namespace configmap-1509 deletion completed in 6.098299712s

• [SLOW TEST:8.183 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:08:29.222: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 25 09:08:30.323: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:08:30.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-3846" for this suite.
Jul 25 09:08:36.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:08:36.434: INFO: namespace container-runtime-3846 deletion completed in 6.094983033s

• [SLOW TEST:7.212 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:08:36.439: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Jul 25 09:08:36.479: INFO: Pod name pod-release: Found 0 pods out of 1
Jul 25 09:08:41.483: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:08:42.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-1979" for this suite.
Jul 25 09:08:48.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:08:48.599: INFO: namespace replication-controller-1979 deletion completed in 6.095004379s

• [SLOW TEST:12.160 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:08:48.602: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-1cb2fcb7-2cda-421a-9146-fadbae74a61a
STEP: Creating a pod to test consume configMaps
Jul 25 09:08:48.651: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-341e8b3e-3457-4fb9-aced-476d473ebf0d" in namespace "projected-1246" to be "success or failure"
Jul 25 09:08:48.653: INFO: Pod "pod-projected-configmaps-341e8b3e-3457-4fb9-aced-476d473ebf0d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.085918ms
Jul 25 09:08:50.658: INFO: Pod "pod-projected-configmaps-341e8b3e-3457-4fb9-aced-476d473ebf0d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006166887s
STEP: Saw pod success
Jul 25 09:08:50.658: INFO: Pod "pod-projected-configmaps-341e8b3e-3457-4fb9-aced-476d473ebf0d" satisfied condition "success or failure"
Jul 25 09:08:50.661: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-projected-configmaps-341e8b3e-3457-4fb9-aced-476d473ebf0d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 25 09:08:50.678: INFO: Waiting for pod pod-projected-configmaps-341e8b3e-3457-4fb9-aced-476d473ebf0d to disappear
Jul 25 09:08:50.681: INFO: Pod pod-projected-configmaps-341e8b3e-3457-4fb9-aced-476d473ebf0d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:08:50.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1246" for this suite.
Jul 25 09:08:56.696: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:08:56.787: INFO: namespace projected-1246 deletion completed in 6.101206582s

• [SLOW TEST:8.185 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:08:56.788: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1211
STEP: creating the pod
Jul 25 09:08:56.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 create -f - --namespace=kubectl-4412'
Jul 25 09:08:57.007: INFO: stderr: ""
Jul 25 09:08:57.007: INFO: stdout: "pod/pause created\n"
Jul 25 09:08:57.007: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jul 25 09:08:57.007: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-4412" to be "running and ready"
Jul 25 09:08:57.014: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 6.644114ms
Jul 25 09:08:59.017: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.009786911s
Jul 25 09:08:59.017: INFO: Pod "pause" satisfied condition "running and ready"
Jul 25 09:08:59.017: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: adding the label testing-label with value testing-label-value to a pod
Jul 25 09:08:59.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 label pods pause testing-label=testing-label-value --namespace=kubectl-4412'
Jul 25 09:08:59.105: INFO: stderr: ""
Jul 25 09:08:59.105: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Jul 25 09:08:59.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pod pause -L testing-label --namespace=kubectl-4412'
Jul 25 09:08:59.189: INFO: stderr: ""
Jul 25 09:08:59.189: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Jul 25 09:08:59.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 label pods pause testing-label- --namespace=kubectl-4412'
Jul 25 09:08:59.319: INFO: stderr: ""
Jul 25 09:08:59.319: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Jul 25 09:08:59.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pod pause -L testing-label --namespace=kubectl-4412'
Jul 25 09:08:59.396: INFO: stderr: ""
Jul 25 09:08:59.396: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1218
STEP: using delete to clean up resources
Jul 25 09:08:59.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 delete --grace-period=0 --force -f - --namespace=kubectl-4412'
Jul 25 09:08:59.513: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 25 09:08:59.513: INFO: stdout: "pod \"pause\" force deleted\n"
Jul 25 09:08:59.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get rc,svc -l name=pause --no-headers --namespace=kubectl-4412'
Jul 25 09:08:59.756: INFO: stderr: "No resources found.\n"
Jul 25 09:08:59.756: INFO: stdout: ""
Jul 25 09:08:59.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods -l name=pause --namespace=kubectl-4412 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 25 09:08:59.837: INFO: stderr: ""
Jul 25 09:08:59.837: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:08:59.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4412" for this suite.
Jul 25 09:09:05.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:09:06.044: INFO: namespace kubectl-4412 deletion completed in 6.204259786s

• [SLOW TEST:9.257 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl label
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:09:06.044: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 25 09:09:06.237: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a24e7484-d150-4065-b522-04422f35db92" in namespace "downward-api-8953" to be "success or failure"
Jul 25 09:09:06.371: INFO: Pod "downwardapi-volume-a24e7484-d150-4065-b522-04422f35db92": Phase="Pending", Reason="", readiness=false. Elapsed: 134.666633ms
Jul 25 09:09:08.375: INFO: Pod "downwardapi-volume-a24e7484-d150-4065-b522-04422f35db92": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.138076852s
STEP: Saw pod success
Jul 25 09:09:08.375: INFO: Pod "downwardapi-volume-a24e7484-d150-4065-b522-04422f35db92" satisfied condition "success or failure"
Jul 25 09:09:08.377: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-a24e7484-d150-4065-b522-04422f35db92 container client-container: <nil>
STEP: delete the pod
Jul 25 09:09:08.400: INFO: Waiting for pod downwardapi-volume-a24e7484-d150-4065-b522-04422f35db92 to disappear
Jul 25 09:09:08.402: INFO: Pod downwardapi-volume-a24e7484-d150-4065-b522-04422f35db92 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:09:08.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8953" for this suite.
Jul 25 09:09:14.417: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:09:14.506: INFO: namespace downward-api-8953 deletion completed in 6.099174067s

• [SLOW TEST:8.462 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:09:14.509: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test use defaults
Jul 25 09:09:14.539: INFO: Waiting up to 5m0s for pod "client-containers-19cc4b5c-acda-494f-8a4f-5e8dc9f8fa48" in namespace "containers-4793" to be "success or failure"
Jul 25 09:09:14.546: INFO: Pod "client-containers-19cc4b5c-acda-494f-8a4f-5e8dc9f8fa48": Phase="Pending", Reason="", readiness=false. Elapsed: 6.881504ms
Jul 25 09:09:16.550: INFO: Pod "client-containers-19cc4b5c-acda-494f-8a4f-5e8dc9f8fa48": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011570486s
STEP: Saw pod success
Jul 25 09:09:16.550: INFO: Pod "client-containers-19cc4b5c-acda-494f-8a4f-5e8dc9f8fa48" satisfied condition "success or failure"
Jul 25 09:09:16.555: INFO: Trying to get logs from node essentialpks-conformance-3 pod client-containers-19cc4b5c-acda-494f-8a4f-5e8dc9f8fa48 container test-container: <nil>
STEP: delete the pod
Jul 25 09:09:16.574: INFO: Waiting for pod client-containers-19cc4b5c-acda-494f-8a4f-5e8dc9f8fa48 to disappear
Jul 25 09:09:16.576: INFO: Pod client-containers-19cc4b5c-acda-494f-8a4f-5e8dc9f8fa48 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:09:16.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4793" for this suite.
Jul 25 09:09:22.592: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:09:22.676: INFO: namespace containers-4793 deletion completed in 6.096848412s

• [SLOW TEST:8.167 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:09:22.676: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 25 09:09:25.300: INFO: Successfully updated pod "labelsupdate389339c0-1a70-422e-be7f-116d049a1d78"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:09:29.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6944" for this suite.
Jul 25 09:09:51.341: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:09:51.419: INFO: namespace projected-6944 deletion completed in 22.088671882s

• [SLOW TEST:28.743 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:09:51.421: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:167
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating server pod server in namespace prestop-4154
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-4154
STEP: Deleting pre-stop pod
Jul 25 09:10:00.493: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:10:00.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-4154" for this suite.
Jul 25 09:10:38.519: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:10:38.605: INFO: namespace prestop-4154 deletion completed in 38.097152511s

• [SLOW TEST:47.184 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:10:38.609: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-149e9e05-8a5c-4fe7-939c-09dc0dd8b2b9
STEP: Creating a pod to test consume configMaps
Jul 25 09:10:38.643: INFO: Waiting up to 5m0s for pod "pod-configmaps-82a297f2-892b-4515-a72f-243d598e7554" in namespace "configmap-3220" to be "success or failure"
Jul 25 09:10:38.647: INFO: Pod "pod-configmaps-82a297f2-892b-4515-a72f-243d598e7554": Phase="Pending", Reason="", readiness=false. Elapsed: 3.640382ms
Jul 25 09:10:40.650: INFO: Pod "pod-configmaps-82a297f2-892b-4515-a72f-243d598e7554": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006892977s
STEP: Saw pod success
Jul 25 09:10:40.650: INFO: Pod "pod-configmaps-82a297f2-892b-4515-a72f-243d598e7554" satisfied condition "success or failure"
Jul 25 09:10:40.653: INFO: Trying to get logs from node essentialpks-conformance-3 pod pod-configmaps-82a297f2-892b-4515-a72f-243d598e7554 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 25 09:10:40.670: INFO: Waiting for pod pod-configmaps-82a297f2-892b-4515-a72f-243d598e7554 to disappear
Jul 25 09:10:40.672: INFO: Pod pod-configmaps-82a297f2-892b-4515-a72f-243d598e7554 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:10:40.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3220" for this suite.
Jul 25 09:10:46.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:10:46.786: INFO: namespace configmap-3220 deletion completed in 6.110721843s

• [SLOW TEST:8.178 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:10:46.789: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 25 09:10:46.820: INFO: Creating ReplicaSet my-hostname-basic-7b064967-aab3-45c6-b0cb-ef0fb29876e8
Jul 25 09:10:46.827: INFO: Pod name my-hostname-basic-7b064967-aab3-45c6-b0cb-ef0fb29876e8: Found 0 pods out of 1
Jul 25 09:10:51.832: INFO: Pod name my-hostname-basic-7b064967-aab3-45c6-b0cb-ef0fb29876e8: Found 1 pods out of 1
Jul 25 09:10:51.832: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-7b064967-aab3-45c6-b0cb-ef0fb29876e8" is running
Jul 25 09:10:51.836: INFO: Pod "my-hostname-basic-7b064967-aab3-45c6-b0cb-ef0fb29876e8-rp69h" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-25 09:10:46 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-25 09:10:48 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-25 09:10:48 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-25 09:10:46 +0000 UTC Reason: Message:}])
Jul 25 09:10:51.836: INFO: Trying to dial the pod
Jul 25 09:10:56.848: INFO: Controller my-hostname-basic-7b064967-aab3-45c6-b0cb-ef0fb29876e8: Got expected result from replica 1 [my-hostname-basic-7b064967-aab3-45c6-b0cb-ef0fb29876e8-rp69h]: "my-hostname-basic-7b064967-aab3-45c6-b0cb-ef0fb29876e8-rp69h", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:10:56.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2185" for this suite.
Jul 25 09:11:02.865: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:11:02.951: INFO: namespace replicaset-2185 deletion completed in 6.097927471s

• [SLOW TEST:16.163 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:11:02.954: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-092f9f97-f0f0-445c-904f-0a09955d2705
STEP: Creating configMap with name cm-test-opt-upd-85a82cec-5017-46f9-9b4d-d054a946c51b
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-092f9f97-f0f0-445c-904f-0a09955d2705
STEP: Updating configmap cm-test-opt-upd-85a82cec-5017-46f9-9b4d-d054a946c51b
STEP: Creating configMap with name cm-test-opt-create-53e07c83-22ba-4a39-a12b-82f0620ef71c
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:11:07.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7987" for this suite.
Jul 25 09:11:29.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:11:29.183: INFO: namespace projected-7987 deletion completed in 22.093494026s

• [SLOW TEST:26.229 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:11:29.183: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-2676
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 25 09:11:29.208: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 25 09:11:53.284: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.52 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2676 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 25 09:11:53.285: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
Jul 25 09:11:54.409: INFO: Found all expected endpoints: [netserver-0]
Jul 25 09:11:54.414: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.2.6 8081 | grep -v '^\s*$'] Namespace:pod-network-test-2676 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 25 09:11:54.414: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
Jul 25 09:11:55.546: INFO: Found all expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:11:55.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-2676" for this suite.
Jul 25 09:12:17.566: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:12:17.652: INFO: namespace pod-network-test-2676 deletion completed in 22.096587664s

• [SLOW TEST:48.469 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:12:17.653: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
W0725 09:12:57.711391      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Jul 25 09:12:57.711: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:12:57.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3954" for this suite.
Jul 25 09:13:03.728: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:13:03.809: INFO: namespace gc-3954 deletion completed in 6.093215584s

• [SLOW TEST:46.156 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:13:03.814: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 25 09:13:03.848: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bb890472-38dd-4b2d-b10e-dcd0895cd8b2" in namespace "projected-1455" to be "success or failure"
Jul 25 09:13:03.851: INFO: Pod "downwardapi-volume-bb890472-38dd-4b2d-b10e-dcd0895cd8b2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.080043ms
Jul 25 09:13:05.855: INFO: Pod "downwardapi-volume-bb890472-38dd-4b2d-b10e-dcd0895cd8b2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00656288s
Jul 25 09:13:07.859: INFO: Pod "downwardapi-volume-bb890472-38dd-4b2d-b10e-dcd0895cd8b2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010349867s
STEP: Saw pod success
Jul 25 09:13:07.859: INFO: Pod "downwardapi-volume-bb890472-38dd-4b2d-b10e-dcd0895cd8b2" satisfied condition "success or failure"
Jul 25 09:13:07.862: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-bb890472-38dd-4b2d-b10e-dcd0895cd8b2 container client-container: <nil>
STEP: delete the pod
Jul 25 09:13:07.878: INFO: Waiting for pod downwardapi-volume-bb890472-38dd-4b2d-b10e-dcd0895cd8b2 to disappear
Jul 25 09:13:07.880: INFO: Pod downwardapi-volume-bb890472-38dd-4b2d-b10e-dcd0895cd8b2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:13:07.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1455" for this suite.
Jul 25 09:13:13.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:13:13.985: INFO: namespace projected-1455 deletion completed in 6.100818792s

• [SLOW TEST:10.171 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:13:13.986: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:13:18.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6572" for this suite.
Jul 25 09:13:24.043: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:13:24.126: INFO: namespace kubelet-test-6572 deletion completed in 6.093833045s

• [SLOW TEST:10.141 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:13:24.128: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-5a60467d-894b-4fbd-9d92-b1a14c0f59ad
STEP: Creating a pod to test consume configMaps
Jul 25 09:13:24.159: INFO: Waiting up to 5m0s for pod "pod-configmaps-7f52b8af-b921-40a1-b8d9-ae139bdeb168" in namespace "configmap-5818" to be "success or failure"
Jul 25 09:13:24.168: INFO: Pod "pod-configmaps-7f52b8af-b921-40a1-b8d9-ae139bdeb168": Phase="Pending", Reason="", readiness=false. Elapsed: 8.720586ms
Jul 25 09:13:26.171: INFO: Pod "pod-configmaps-7f52b8af-b921-40a1-b8d9-ae139bdeb168": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012034511s
STEP: Saw pod success
Jul 25 09:13:26.171: INFO: Pod "pod-configmaps-7f52b8af-b921-40a1-b8d9-ae139bdeb168" satisfied condition "success or failure"
Jul 25 09:13:26.174: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-configmaps-7f52b8af-b921-40a1-b8d9-ae139bdeb168 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 25 09:13:26.198: INFO: Waiting for pod pod-configmaps-7f52b8af-b921-40a1-b8d9-ae139bdeb168 to disappear
Jul 25 09:13:26.201: INFO: Pod pod-configmaps-7f52b8af-b921-40a1-b8d9-ae139bdeb168 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:13:26.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5818" for this suite.
Jul 25 09:13:32.214: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:13:32.307: INFO: namespace configmap-5818 deletion completed in 6.103518552s

• [SLOW TEST:8.181 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:13:32.312: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 25 09:13:32.346: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:13:34.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5436" for this suite.
Jul 25 09:14:16.502: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:14:16.590: INFO: namespace pods-5436 deletion completed in 42.097465865s

• [SLOW TEST:44.280 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:14:16.599: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4783.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4783.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4783.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4783.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4783.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4783.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 25 09:14:18.668: INFO: DNS probes using dns-4783/dns-test-944ac3cd-749a-45e6-aa22-06efd4721083 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:14:18.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4783" for this suite.
Jul 25 09:14:24.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:14:24.761: INFO: namespace dns-4783 deletion completed in 6.076332766s

• [SLOW TEST:8.163 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:14:24.761: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 25 09:14:24.791: INFO: (0) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.914221ms)
Jul 25 09:14:24.794: INFO: (1) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.274529ms)
Jul 25 09:14:24.798: INFO: (2) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.948917ms)
Jul 25 09:14:24.801: INFO: (3) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.062716ms)
Jul 25 09:14:24.807: INFO: (4) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.420602ms)
Jul 25 09:14:24.810: INFO: (5) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.705387ms)
Jul 25 09:14:24.814: INFO: (6) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.368823ms)
Jul 25 09:14:24.823: INFO: (7) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 9.024182ms)
Jul 25 09:14:24.827: INFO: (8) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.474389ms)
Jul 25 09:14:24.832: INFO: (9) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.211654ms)
Jul 25 09:14:24.836: INFO: (10) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.965933ms)
Jul 25 09:14:24.839: INFO: (11) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.744123ms)
Jul 25 09:14:24.846: INFO: (12) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 6.109261ms)
Jul 25 09:14:24.849: INFO: (13) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.465887ms)
Jul 25 09:14:24.853: INFO: (14) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.312774ms)
Jul 25 09:14:24.857: INFO: (15) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.037248ms)
Jul 25 09:14:24.860: INFO: (16) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.638641ms)
Jul 25 09:14:24.863: INFO: (17) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.90565ms)
Jul 25 09:14:24.866: INFO: (18) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.230224ms)
Jul 25 09:14:24.870: INFO: (19) /api/v1/nodes/essentialpks-conformance-2:10250/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.476299ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:14:24.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-7334" for this suite.
Jul 25 09:14:30.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:14:30.959: INFO: namespace proxy-7334 deletion completed in 6.08565335s

• [SLOW TEST:6.198 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:14:30.961: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jul 25 09:14:30.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 create -f - --namespace=kubectl-5298'
Jul 25 09:14:31.151: INFO: stderr: ""
Jul 25 09:14:31.151: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 25 09:14:32.154: INFO: Selector matched 1 pods for map[app:redis]
Jul 25 09:14:32.154: INFO: Found 0 / 1
Jul 25 09:14:33.155: INFO: Selector matched 1 pods for map[app:redis]
Jul 25 09:14:33.155: INFO: Found 1 / 1
Jul 25 09:14:33.155: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Jul 25 09:14:33.162: INFO: Selector matched 1 pods for map[app:redis]
Jul 25 09:14:33.162: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 25 09:14:33.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 patch pod redis-master-vznhj --namespace=kubectl-5298 -p {"metadata":{"annotations":{"x":"y"}}}'
Jul 25 09:14:33.247: INFO: stderr: ""
Jul 25 09:14:33.247: INFO: stdout: "pod/redis-master-vznhj patched\n"
STEP: checking annotations
Jul 25 09:14:33.251: INFO: Selector matched 1 pods for map[app:redis]
Jul 25 09:14:33.251: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:14:33.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5298" for this suite.
Jul 25 09:14:55.267: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:14:55.357: INFO: namespace kubectl-5298 deletion completed in 22.101121734s

• [SLOW TEST:24.398 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl patch
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:14:55.364: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a replication controller
Jul 25 09:14:55.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 create -f - --namespace=kubectl-7465'
Jul 25 09:14:55.635: INFO: stderr: ""
Jul 25 09:14:55.635: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 25 09:14:55.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7465'
Jul 25 09:14:55.742: INFO: stderr: ""
Jul 25 09:14:55.742: INFO: stdout: "update-demo-nautilus-fjr7q update-demo-nautilus-wgxs6 "
Jul 25 09:14:55.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-nautilus-fjr7q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7465'
Jul 25 09:14:55.827: INFO: stderr: ""
Jul 25 09:14:55.827: INFO: stdout: ""
Jul 25 09:14:55.827: INFO: update-demo-nautilus-fjr7q is created but not running
Jul 25 09:15:00.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7465'
Jul 25 09:15:00.903: INFO: stderr: ""
Jul 25 09:15:00.903: INFO: stdout: "update-demo-nautilus-fjr7q update-demo-nautilus-wgxs6 "
Jul 25 09:15:00.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-nautilus-fjr7q -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7465'
Jul 25 09:15:00.986: INFO: stderr: ""
Jul 25 09:15:00.986: INFO: stdout: "true"
Jul 25 09:15:00.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-nautilus-fjr7q -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7465'
Jul 25 09:15:01.058: INFO: stderr: ""
Jul 25 09:15:01.058: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 25 09:15:01.058: INFO: validating pod update-demo-nautilus-fjr7q
Jul 25 09:15:01.063: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 25 09:15:01.063: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 25 09:15:01.063: INFO: update-demo-nautilus-fjr7q is verified up and running
Jul 25 09:15:01.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-nautilus-wgxs6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7465'
Jul 25 09:15:01.143: INFO: stderr: ""
Jul 25 09:15:01.143: INFO: stdout: "true"
Jul 25 09:15:01.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-nautilus-wgxs6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7465'
Jul 25 09:15:01.230: INFO: stderr: ""
Jul 25 09:15:01.230: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 25 09:15:01.230: INFO: validating pod update-demo-nautilus-wgxs6
Jul 25 09:15:01.235: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 25 09:15:01.235: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 25 09:15:01.235: INFO: update-demo-nautilus-wgxs6 is verified up and running
STEP: scaling down the replication controller
Jul 25 09:15:01.236: INFO: scanned /root for discovery docs: <nil>
Jul 25 09:15:01.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-7465'
Jul 25 09:15:02.342: INFO: stderr: ""
Jul 25 09:15:02.342: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 25 09:15:02.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7465'
Jul 25 09:15:02.428: INFO: stderr: ""
Jul 25 09:15:02.428: INFO: stdout: "update-demo-nautilus-fjr7q update-demo-nautilus-wgxs6 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 25 09:15:07.428: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7465'
Jul 25 09:15:07.513: INFO: stderr: ""
Jul 25 09:15:07.513: INFO: stdout: "update-demo-nautilus-fjr7q update-demo-nautilus-wgxs6 "
STEP: Replicas for name=update-demo: expected=1 actual=2
Jul 25 09:15:12.514: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7465'
Jul 25 09:15:12.595: INFO: stderr: ""
Jul 25 09:15:12.595: INFO: stdout: "update-demo-nautilus-wgxs6 "
Jul 25 09:15:12.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-nautilus-wgxs6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7465'
Jul 25 09:15:12.665: INFO: stderr: ""
Jul 25 09:15:12.665: INFO: stdout: "true"
Jul 25 09:15:12.665: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-nautilus-wgxs6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7465'
Jul 25 09:15:12.743: INFO: stderr: ""
Jul 25 09:15:12.743: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 25 09:15:12.743: INFO: validating pod update-demo-nautilus-wgxs6
Jul 25 09:15:12.749: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 25 09:15:12.749: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 25 09:15:12.749: INFO: update-demo-nautilus-wgxs6 is verified up and running
STEP: scaling up the replication controller
Jul 25 09:15:12.755: INFO: scanned /root for discovery docs: <nil>
Jul 25 09:15:12.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-7465'
Jul 25 09:15:13.862: INFO: stderr: ""
Jul 25 09:15:13.862: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 25 09:15:13.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-7465'
Jul 25 09:15:13.970: INFO: stderr: ""
Jul 25 09:15:13.970: INFO: stdout: "update-demo-nautilus-7xj9x update-demo-nautilus-wgxs6 "
Jul 25 09:15:13.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-nautilus-7xj9x -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7465'
Jul 25 09:15:14.053: INFO: stderr: ""
Jul 25 09:15:14.053: INFO: stdout: "true"
Jul 25 09:15:14.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-nautilus-7xj9x -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7465'
Jul 25 09:15:14.132: INFO: stderr: ""
Jul 25 09:15:14.132: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 25 09:15:14.132: INFO: validating pod update-demo-nautilus-7xj9x
Jul 25 09:15:14.139: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 25 09:15:14.139: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 25 09:15:14.139: INFO: update-demo-nautilus-7xj9x is verified up and running
Jul 25 09:15:14.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-nautilus-wgxs6 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7465'
Jul 25 09:15:14.229: INFO: stderr: ""
Jul 25 09:15:14.229: INFO: stdout: "true"
Jul 25 09:15:14.229: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-nautilus-wgxs6 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7465'
Jul 25 09:15:14.321: INFO: stderr: ""
Jul 25 09:15:14.321: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 25 09:15:14.321: INFO: validating pod update-demo-nautilus-wgxs6
Jul 25 09:15:14.324: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 25 09:15:14.324: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 25 09:15:14.324: INFO: update-demo-nautilus-wgxs6 is verified up and running
STEP: using delete to clean up resources
Jul 25 09:15:14.324: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 delete --grace-period=0 --force -f - --namespace=kubectl-7465'
Jul 25 09:15:14.442: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 25 09:15:14.442: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jul 25 09:15:14.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7465'
Jul 25 09:15:14.525: INFO: stderr: "No resources found.\n"
Jul 25 09:15:14.525: INFO: stdout: ""
Jul 25 09:15:14.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods -l name=update-demo --namespace=kubectl-7465 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 25 09:15:14.610: INFO: stderr: ""
Jul 25 09:15:14.610: INFO: stdout: "update-demo-nautilus-7xj9x\nupdate-demo-nautilus-wgxs6\n"
Jul 25 09:15:15.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-7465'
Jul 25 09:15:15.205: INFO: stderr: "No resources found.\n"
Jul 25 09:15:15.205: INFO: stdout: ""
Jul 25 09:15:15.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods -l name=update-demo --namespace=kubectl-7465 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 25 09:15:15.290: INFO: stderr: ""
Jul 25 09:15:15.290: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:15:15.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7465" for this suite.
Jul 25 09:15:21.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:15:21.376: INFO: namespace kubectl-7465 deletion completed in 6.081526804s

• [SLOW TEST:26.013 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:15:21.376: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1457
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 25 09:15:21.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-6869'
Jul 25 09:15:21.501: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 25 09:15:21.501: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: verifying the pod controlled by rc e2e-test-nginx-rc was created
STEP: confirm that you can get logs from an rc
Jul 25 09:15:21.519: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-nginx-rc-7qjzk]
Jul 25 09:15:21.519: INFO: Waiting up to 5m0s for pod "e2e-test-nginx-rc-7qjzk" in namespace "kubectl-6869" to be "running and ready"
Jul 25 09:15:21.524: INFO: Pod "e2e-test-nginx-rc-7qjzk": Phase="Pending", Reason="", readiness=false. Elapsed: 5.086068ms
Jul 25 09:15:23.530: INFO: Pod "e2e-test-nginx-rc-7qjzk": Phase="Running", Reason="", readiness=true. Elapsed: 2.010909125s
Jul 25 09:15:23.530: INFO: Pod "e2e-test-nginx-rc-7qjzk" satisfied condition "running and ready"
Jul 25 09:15:23.530: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-nginx-rc-7qjzk]
Jul 25 09:15:23.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 logs rc/e2e-test-nginx-rc --namespace=kubectl-6869'
Jul 25 09:15:23.630: INFO: stderr: ""
Jul 25 09:15:23.630: INFO: stdout: ""
[AfterEach] [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1462
Jul 25 09:15:23.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 delete rc e2e-test-nginx-rc --namespace=kubectl-6869'
Jul 25 09:15:23.716: INFO: stderr: ""
Jul 25 09:15:23.716: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:15:23.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6869" for this suite.
Jul 25 09:15:45.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:15:45.823: INFO: namespace kubectl-6869 deletion completed in 22.10282894s

• [SLOW TEST:24.446 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run rc
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:15:45.823: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap configmap-2994/configmap-test-1afb79d5-543d-4842-9852-e0e9bdf49717
STEP: Creating a pod to test consume configMaps
Jul 25 09:15:45.905: INFO: Waiting up to 5m0s for pod "pod-configmaps-df4309d0-0699-4fd0-a808-bd9da03f561c" in namespace "configmap-2994" to be "success or failure"
Jul 25 09:15:45.913: INFO: Pod "pod-configmaps-df4309d0-0699-4fd0-a808-bd9da03f561c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.954654ms
Jul 25 09:15:47.918: INFO: Pod "pod-configmaps-df4309d0-0699-4fd0-a808-bd9da03f561c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012925759s
STEP: Saw pod success
Jul 25 09:15:47.918: INFO: Pod "pod-configmaps-df4309d0-0699-4fd0-a808-bd9da03f561c" satisfied condition "success or failure"
Jul 25 09:15:47.920: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-configmaps-df4309d0-0699-4fd0-a808-bd9da03f561c container env-test: <nil>
STEP: delete the pod
Jul 25 09:15:47.937: INFO: Waiting for pod pod-configmaps-df4309d0-0699-4fd0-a808-bd9da03f561c to disappear
Jul 25 09:15:47.940: INFO: Pod pod-configmaps-df4309d0-0699-4fd0-a808-bd9da03f561c no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:15:47.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2994" for this suite.
Jul 25 09:15:53.956: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:15:54.039: INFO: namespace configmap-2994 deletion completed in 6.095861063s

• [SLOW TEST:8.216 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:15:54.039: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-3829
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a new StatefulSet
Jul 25 09:15:54.084: INFO: Found 0 stateful pods, waiting for 3
Jul 25 09:16:04.089: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 25 09:16:04.089: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 25 09:16:04.089: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/nginx:1.14-alpine to docker.io/library/nginx:1.15-alpine
Jul 25 09:16:04.123: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Jul 25 09:16:14.157: INFO: Updating stateful set ss2
Jul 25 09:16:14.164: INFO: Waiting for Pod statefulset-3829/ss2-2 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
STEP: Restoring Pods to the correct revision when they are deleted
Jul 25 09:16:24.261: INFO: Found 2 stateful pods, waiting for 3
Jul 25 09:16:34.268: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 25 09:16:34.268: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 25 09:16:34.268: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Jul 25 09:16:34.301: INFO: Updating stateful set ss2
Jul 25 09:16:34.313: INFO: Waiting for Pod statefulset-3829/ss2-1 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 25 09:16:44.344: INFO: Updating stateful set ss2
Jul 25 09:16:44.364: INFO: Waiting for StatefulSet statefulset-3829/ss2 to complete update
Jul 25 09:16:44.364: INFO: Waiting for Pod statefulset-3829/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
Jul 25 09:16:54.371: INFO: Waiting for StatefulSet statefulset-3829/ss2 to complete update
Jul 25 09:16:54.372: INFO: Waiting for Pod statefulset-3829/ss2-0 to have revision ss2-6c5cd755cd update revision ss2-7c9b54fd4c
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 25 09:17:04.374: INFO: Deleting all statefulset in ns statefulset-3829
Jul 25 09:17:04.377: INFO: Scaling statefulset ss2 to 0
Jul 25 09:17:24.391: INFO: Waiting for statefulset status.replicas updated to 0
Jul 25 09:17:24.399: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:17:24.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-3829" for this suite.
Jul 25 09:17:30.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:17:30.514: INFO: namespace statefulset-3829 deletion completed in 6.08840354s

• [SLOW TEST:96.475 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:17:30.515: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 25 09:17:30.545: INFO: Waiting up to 5m0s for pod "pod-20a029cd-4f10-45a2-9d6d-69c889b8b56d" in namespace "emptydir-9349" to be "success or failure"
Jul 25 09:17:30.548: INFO: Pod "pod-20a029cd-4f10-45a2-9d6d-69c889b8b56d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.328856ms
Jul 25 09:17:32.551: INFO: Pod "pod-20a029cd-4f10-45a2-9d6d-69c889b8b56d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006315454s
STEP: Saw pod success
Jul 25 09:17:32.551: INFO: Pod "pod-20a029cd-4f10-45a2-9d6d-69c889b8b56d" satisfied condition "success or failure"
Jul 25 09:17:32.554: INFO: Trying to get logs from node essentialpks-conformance-3 pod pod-20a029cd-4f10-45a2-9d6d-69c889b8b56d container test-container: <nil>
STEP: delete the pod
Jul 25 09:17:32.573: INFO: Waiting for pod pod-20a029cd-4f10-45a2-9d6d-69c889b8b56d to disappear
Jul 25 09:17:32.575: INFO: Pod pod-20a029cd-4f10-45a2-9d6d-69c889b8b56d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:17:32.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9349" for this suite.
Jul 25 09:17:38.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:17:38.680: INFO: namespace emptydir-9349 deletion completed in 6.101085998s

• [SLOW TEST:8.165 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:17:38.688: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 25 09:17:38.715: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:17:41.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1331" for this suite.
Jul 25 09:17:47.667: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:17:47.754: INFO: namespace init-container-1331 deletion completed in 6.097023579s

• [SLOW TEST:9.067 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:17:47.755: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 25 09:17:47.785: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ac0d5dd1-0d6f-41f9-a55c-27f96d3cec59" in namespace "downward-api-3025" to be "success or failure"
Jul 25 09:17:47.789: INFO: Pod "downwardapi-volume-ac0d5dd1-0d6f-41f9-a55c-27f96d3cec59": Phase="Pending", Reason="", readiness=false. Elapsed: 3.403216ms
Jul 25 09:17:49.791: INFO: Pod "downwardapi-volume-ac0d5dd1-0d6f-41f9-a55c-27f96d3cec59": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006024295s
STEP: Saw pod success
Jul 25 09:17:49.791: INFO: Pod "downwardapi-volume-ac0d5dd1-0d6f-41f9-a55c-27f96d3cec59" satisfied condition "success or failure"
Jul 25 09:17:49.794: INFO: Trying to get logs from node essentialpks-conformance-3 pod downwardapi-volume-ac0d5dd1-0d6f-41f9-a55c-27f96d3cec59 container client-container: <nil>
STEP: delete the pod
Jul 25 09:17:49.811: INFO: Waiting for pod downwardapi-volume-ac0d5dd1-0d6f-41f9-a55c-27f96d3cec59 to disappear
Jul 25 09:17:49.814: INFO: Pod downwardapi-volume-ac0d5dd1-0d6f-41f9-a55c-27f96d3cec59 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:17:49.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3025" for this suite.
Jul 25 09:17:55.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:17:55.908: INFO: namespace downward-api-3025 deletion completed in 6.089857569s

• [SLOW TEST:8.153 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:17:55.908: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating all guestbook components
Jul 25 09:17:55.938: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Jul 25 09:17:55.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 create -f - --namespace=kubectl-9373'
Jul 25 09:17:56.280: INFO: stderr: ""
Jul 25 09:17:56.280: INFO: stdout: "service/redis-slave created\n"
Jul 25 09:17:56.280: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Jul 25 09:17:56.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 create -f - --namespace=kubectl-9373'
Jul 25 09:17:56.479: INFO: stderr: ""
Jul 25 09:17:56.479: INFO: stdout: "service/redis-master created\n"
Jul 25 09:17:56.479: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jul 25 09:17:56.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 create -f - --namespace=kubectl-9373'
Jul 25 09:17:56.643: INFO: stderr: ""
Jul 25 09:17:56.643: INFO: stdout: "service/frontend created\n"
Jul 25 09:17:56.643: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Jul 25 09:17:56.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 create -f - --namespace=kubectl-9373'
Jul 25 09:17:56.807: INFO: stderr: ""
Jul 25 09:17:56.807: INFO: stdout: "deployment.apps/frontend created\n"
Jul 25 09:17:56.807: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: gcr.io/kubernetes-e2e-test-images/redis:1.0
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jul 25 09:17:56.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 create -f - --namespace=kubectl-9373'
Jul 25 09:17:56.965: INFO: stderr: ""
Jul 25 09:17:56.965: INFO: stdout: "deployment.apps/redis-master created\n"
Jul 25 09:17:56.966: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: gcr.io/google-samples/gb-redisslave:v3
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Jul 25 09:17:56.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 create -f - --namespace=kubectl-9373'
Jul 25 09:17:57.145: INFO: stderr: ""
Jul 25 09:17:57.145: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Jul 25 09:17:57.146: INFO: Waiting for all frontend pods to be Running.
Jul 25 09:18:02.199: INFO: Waiting for frontend to serve content.
Jul 25 09:18:02.246: INFO: Trying to add a new entry to the guestbook.
Jul 25 09:18:02.294: INFO: Verifying that added entry can be retrieved.
Jul 25 09:18:02.305: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 25 09:18:07.322: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 25 09:18:12.340: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 25 09:18:17.357: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 25 09:18:22.372: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 25 09:18:27.390: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 25 09:18:32.407: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 25 09:18:37.432: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 25 09:18:42.449: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 25 09:18:47.467: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 25 09:18:52.484: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
Jul 25 09:18:57.502: INFO: Failed to get response from guestbook. err: <nil>, response: {"data": ""}
STEP: using delete to clean up resources
Jul 25 09:19:02.519: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 delete --grace-period=0 --force -f - --namespace=kubectl-9373'
Jul 25 09:19:02.635: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 25 09:19:02.635: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Jul 25 09:19:02.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 delete --grace-period=0 --force -f - --namespace=kubectl-9373'
Jul 25 09:19:02.750: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 25 09:19:02.750: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 25 09:19:02.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 delete --grace-period=0 --force -f - --namespace=kubectl-9373'
Jul 25 09:19:02.861: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 25 09:19:02.861: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 25 09:19:02.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 delete --grace-period=0 --force -f - --namespace=kubectl-9373'
Jul 25 09:19:02.951: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 25 09:19:02.951: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Jul 25 09:19:02.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 delete --grace-period=0 --force -f - --namespace=kubectl-9373'
Jul 25 09:19:03.042: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 25 09:19:03.042: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Jul 25 09:19:03.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 delete --grace-period=0 --force -f - --namespace=kubectl-9373'
Jul 25 09:19:03.129: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 25 09:19:03.129: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:19:03.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9373" for this suite.
Jul 25 09:19:47.145: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:19:47.218: INFO: namespace kubectl-9373 deletion completed in 44.083693122s

• [SLOW TEST:111.310 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Guestbook application
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:19:47.224: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 25 09:19:47.262: INFO: Waiting up to 5m0s for pod "downward-api-ddbf405d-c878-4d07-86b7-ce016f0ee0bb" in namespace "downward-api-8901" to be "success or failure"
Jul 25 09:19:47.267: INFO: Pod "downward-api-ddbf405d-c878-4d07-86b7-ce016f0ee0bb": Phase="Pending", Reason="", readiness=false. Elapsed: 4.379066ms
Jul 25 09:19:49.270: INFO: Pod "downward-api-ddbf405d-c878-4d07-86b7-ce016f0ee0bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007616745s
STEP: Saw pod success
Jul 25 09:19:49.270: INFO: Pod "downward-api-ddbf405d-c878-4d07-86b7-ce016f0ee0bb" satisfied condition "success or failure"
Jul 25 09:19:49.272: INFO: Trying to get logs from node essentialpks-conformance-2 pod downward-api-ddbf405d-c878-4d07-86b7-ce016f0ee0bb container dapi-container: <nil>
STEP: delete the pod
Jul 25 09:19:49.290: INFO: Waiting for pod downward-api-ddbf405d-c878-4d07-86b7-ce016f0ee0bb to disappear
Jul 25 09:19:49.293: INFO: Pod downward-api-ddbf405d-c878-4d07-86b7-ce016f0ee0bb no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:19:49.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8901" for this suite.
Jul 25 09:19:55.306: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:19:55.387: INFO: namespace downward-api-8901 deletion completed in 6.090760056s

• [SLOW TEST:8.163 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:19:55.389: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 25 09:19:55.422: INFO: Waiting up to 5m0s for pod "downwardapi-volume-8ccfb102-cf32-463a-bbf0-70ca9a46666e" in namespace "projected-3198" to be "success or failure"
Jul 25 09:19:55.425: INFO: Pod "downwardapi-volume-8ccfb102-cf32-463a-bbf0-70ca9a46666e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.314768ms
Jul 25 09:19:57.429: INFO: Pod "downwardapi-volume-8ccfb102-cf32-463a-bbf0-70ca9a46666e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007313722s
STEP: Saw pod success
Jul 25 09:19:57.429: INFO: Pod "downwardapi-volume-8ccfb102-cf32-463a-bbf0-70ca9a46666e" satisfied condition "success or failure"
Jul 25 09:19:57.434: INFO: Trying to get logs from node essentialpks-conformance-3 pod downwardapi-volume-8ccfb102-cf32-463a-bbf0-70ca9a46666e container client-container: <nil>
STEP: delete the pod
Jul 25 09:19:57.451: INFO: Waiting for pod downwardapi-volume-8ccfb102-cf32-463a-bbf0-70ca9a46666e to disappear
Jul 25 09:19:57.453: INFO: Pod downwardapi-volume-8ccfb102-cf32-463a-bbf0-70ca9a46666e no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:19:57.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3198" for this suite.
Jul 25 09:20:03.465: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:20:03.546: INFO: namespace projected-3198 deletion completed in 6.090844635s

• [SLOW TEST:8.157 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:20:03.547: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 25 09:20:03.584: INFO: Waiting up to 5m0s for pod "downward-api-eec7eb63-1689-4a76-8d0a-bf45fc05cb2c" in namespace "downward-api-5185" to be "success or failure"
Jul 25 09:20:03.589: INFO: Pod "downward-api-eec7eb63-1689-4a76-8d0a-bf45fc05cb2c": Phase="Pending", Reason="", readiness=false. Elapsed: 5.065829ms
Jul 25 09:20:05.593: INFO: Pod "downward-api-eec7eb63-1689-4a76-8d0a-bf45fc05cb2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008515504s
STEP: Saw pod success
Jul 25 09:20:05.593: INFO: Pod "downward-api-eec7eb63-1689-4a76-8d0a-bf45fc05cb2c" satisfied condition "success or failure"
Jul 25 09:20:05.595: INFO: Trying to get logs from node essentialpks-conformance-2 pod downward-api-eec7eb63-1689-4a76-8d0a-bf45fc05cb2c container dapi-container: <nil>
STEP: delete the pod
Jul 25 09:20:05.613: INFO: Waiting for pod downward-api-eec7eb63-1689-4a76-8d0a-bf45fc05cb2c to disappear
Jul 25 09:20:05.615: INFO: Pod downward-api-eec7eb63-1689-4a76-8d0a-bf45fc05cb2c no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:20:05.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5185" for this suite.
Jul 25 09:20:11.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:20:11.722: INFO: namespace downward-api-5185 deletion completed in 6.103514305s

• [SLOW TEST:8.174 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:20:11.722: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-c536e34f-bc64-40c3-8fe3-b8da866d2940
STEP: Creating a pod to test consume configMaps
Jul 25 09:20:11.757: INFO: Waiting up to 5m0s for pod "pod-configmaps-21d9cc6c-26d5-4131-ae43-a7f31f000505" in namespace "configmap-1462" to be "success or failure"
Jul 25 09:20:11.762: INFO: Pod "pod-configmaps-21d9cc6c-26d5-4131-ae43-a7f31f000505": Phase="Pending", Reason="", readiness=false. Elapsed: 4.702668ms
Jul 25 09:20:13.767: INFO: Pod "pod-configmaps-21d9cc6c-26d5-4131-ae43-a7f31f000505": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009684878s
STEP: Saw pod success
Jul 25 09:20:13.767: INFO: Pod "pod-configmaps-21d9cc6c-26d5-4131-ae43-a7f31f000505" satisfied condition "success or failure"
Jul 25 09:20:13.771: INFO: Trying to get logs from node essentialpks-conformance-3 pod pod-configmaps-21d9cc6c-26d5-4131-ae43-a7f31f000505 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 25 09:20:13.788: INFO: Waiting for pod pod-configmaps-21d9cc6c-26d5-4131-ae43-a7f31f000505 to disappear
Jul 25 09:20:13.791: INFO: Pod pod-configmaps-21d9cc6c-26d5-4131-ae43-a7f31f000505 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:20:13.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1462" for this suite.
Jul 25 09:20:19.809: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:20:19.895: INFO: namespace configmap-1462 deletion completed in 6.099621568s

• [SLOW TEST:8.174 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:20:19.902: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 25 09:20:19.934: INFO: Waiting up to 5m0s for pod "pod-1b1ab90e-94c0-4326-ab8e-83a2ff9b345b" in namespace "emptydir-5156" to be "success or failure"
Jul 25 09:20:19.941: INFO: Pod "pod-1b1ab90e-94c0-4326-ab8e-83a2ff9b345b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.670423ms
Jul 25 09:20:21.945: INFO: Pod "pod-1b1ab90e-94c0-4326-ab8e-83a2ff9b345b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011393828s
STEP: Saw pod success
Jul 25 09:20:21.945: INFO: Pod "pod-1b1ab90e-94c0-4326-ab8e-83a2ff9b345b" satisfied condition "success or failure"
Jul 25 09:20:21.950: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-1b1ab90e-94c0-4326-ab8e-83a2ff9b345b container test-container: <nil>
STEP: delete the pod
Jul 25 09:20:21.967: INFO: Waiting for pod pod-1b1ab90e-94c0-4326-ab8e-83a2ff9b345b to disappear
Jul 25 09:20:21.970: INFO: Pod pod-1b1ab90e-94c0-4326-ab8e-83a2ff9b345b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:20:21.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5156" for this suite.
Jul 25 09:20:27.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:20:28.144: INFO: namespace emptydir-5156 deletion completed in 6.170709581s

• [SLOW TEST:8.246 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:20:28.152: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 25 09:20:30.728: INFO: Successfully updated pod "annotationupdate4a9833b8-d69c-4ca8-80ae-0f78201bbcfd"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:20:34.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2734" for this suite.
Jul 25 09:20:56.771: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:20:56.856: INFO: namespace projected-2734 deletion completed in 22.096545263s

• [SLOW TEST:28.705 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:20:56.857: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 25 09:20:56.902: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aaa7fc9e-0bf1-4074-a699-0dbac7f0551b" in namespace "projected-7455" to be "success or failure"
Jul 25 09:20:56.905: INFO: Pod "downwardapi-volume-aaa7fc9e-0bf1-4074-a699-0dbac7f0551b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.864718ms
Jul 25 09:20:58.911: INFO: Pod "downwardapi-volume-aaa7fc9e-0bf1-4074-a699-0dbac7f0551b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008539564s
STEP: Saw pod success
Jul 25 09:20:58.911: INFO: Pod "downwardapi-volume-aaa7fc9e-0bf1-4074-a699-0dbac7f0551b" satisfied condition "success or failure"
Jul 25 09:20:58.922: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-aaa7fc9e-0bf1-4074-a699-0dbac7f0551b container client-container: <nil>
STEP: delete the pod
Jul 25 09:20:58.941: INFO: Waiting for pod downwardapi-volume-aaa7fc9e-0bf1-4074-a699-0dbac7f0551b to disappear
Jul 25 09:20:58.945: INFO: Pod downwardapi-volume-aaa7fc9e-0bf1-4074-a699-0dbac7f0551b no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:20:58.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7455" for this suite.
Jul 25 09:21:04.963: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:21:05.067: INFO: namespace projected-7455 deletion completed in 6.118394952s

• [SLOW TEST:8.211 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:21:05.074: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6179.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6179.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6179.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6179.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6179.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6179.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6179.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6179.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6179.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6179.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6179.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 4.86.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.86.4_udp@PTR;check="$$(dig +tcp +noall +answer +search 4.86.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.86.4_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6179.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6179.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6179.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6179.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6179.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6179.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6179.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6179.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6179.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6179.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-6179.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 4.86.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.86.4_udp@PTR;check="$$(dig +tcp +noall +answer +search 4.86.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.86.4_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 25 09:21:09.160: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:09.163: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:09.189: INFO: Unable to read jessie_udp@dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:09.193: INFO: Unable to read jessie_tcp@dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:09.197: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:09.201: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:09.222: INFO: Lookups using dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local jessie_udp@dns-test-service.dns-6179.svc.cluster.local jessie_tcp@dns-test-service.dns-6179.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local]

Jul 25 09:21:14.240: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:14.247: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:14.272: INFO: Unable to read jessie_udp@dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:14.276: INFO: Unable to read jessie_tcp@dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:14.278: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:14.281: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:14.301: INFO: Lookups using dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local jessie_udp@dns-test-service.dns-6179.svc.cluster.local jessie_tcp@dns-test-service.dns-6179.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local]

Jul 25 09:21:19.233: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:19.237: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:19.259: INFO: Unable to read jessie_udp@dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:19.268: INFO: Unable to read jessie_tcp@dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:19.271: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:19.273: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:19.292: INFO: Lookups using dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local jessie_udp@dns-test-service.dns-6179.svc.cluster.local jessie_tcp@dns-test-service.dns-6179.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local]

Jul 25 09:21:24.236: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:24.239: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:24.259: INFO: Unable to read jessie_udp@dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:24.262: INFO: Unable to read jessie_tcp@dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:24.265: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:24.269: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:24.288: INFO: Lookups using dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local jessie_udp@dns-test-service.dns-6179.svc.cluster.local jessie_tcp@dns-test-service.dns-6179.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local]

Jul 25 09:21:29.232: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:29.234: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:29.256: INFO: Unable to read jessie_udp@dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:29.260: INFO: Unable to read jessie_tcp@dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:29.263: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:29.266: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:29.283: INFO: Lookups using dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local jessie_udp@dns-test-service.dns-6179.svc.cluster.local jessie_tcp@dns-test-service.dns-6179.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local]

Jul 25 09:21:34.234: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:34.238: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:34.262: INFO: Unable to read jessie_udp@dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:34.265: INFO: Unable to read jessie_tcp@dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:34.269: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:34.272: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local from pod dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06: the server could not find the requested resource (get pods dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06)
Jul 25 09:21:34.297: INFO: Lookups using dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local jessie_udp@dns-test-service.dns-6179.svc.cluster.local jessie_tcp@dns-test-service.dns-6179.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6179.svc.cluster.local]

Jul 25 09:21:39.292: INFO: DNS probes using dns-6179/dns-test-d0e40708-5c7b-4b1f-ad9f-69918eeb1d06 succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:21:39.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6179" for this suite.
Jul 25 09:21:45.390: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:21:45.471: INFO: namespace dns-6179 deletion completed in 6.092864126s

• [SLOW TEST:40.398 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:21:45.472: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul 25 09:21:45.508: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 25 09:21:45.514: INFO: Waiting for terminating namespaces to be deleted...
Jul 25 09:21:45.516: INFO: 
Logging pods the kubelet thinks is on node essentialpks-conformance-2 before test
Jul 25 09:21:45.522: INFO: sonobuoy-systemd-logs-daemon-set-8c8b567fc9ec4bcc-w8s5w from heptio-sonobuoy started at 2019-07-25 08:48:15 +0000 UTC (2 container statuses recorded)
Jul 25 09:21:45.522: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 25 09:21:45.522: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 25 09:21:45.522: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-25 08:47:51 +0000 UTC (1 container statuses recorded)
Jul 25 09:21:45.522: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 25 09:21:45.522: INFO: kube-proxy-x8ptf from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Jul 25 09:21:45.522: INFO: 	Container kube-proxy ready: true, restart count 4
Jul 25 09:21:45.522: INFO: kube-flannel-ds-amd64-c4rpf from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Jul 25 09:21:45.522: INFO: 	Container kube-flannel ready: true, restart count 4
Jul 25 09:21:45.522: INFO: 
Logging pods the kubelet thinks is on node essentialpks-conformance-3 before test
Jul 25 09:21:45.529: INFO: kube-proxy-h9j6t from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Jul 25 09:21:45.529: INFO: 	Container kube-proxy ready: true, restart count 4
Jul 25 09:21:45.529: INFO: sonobuoy-e2e-job-de16814bb8f0492a from heptio-sonobuoy started at 2019-07-25 08:48:14 +0000 UTC (2 container statuses recorded)
Jul 25 09:21:45.529: INFO: 	Container e2e ready: true, restart count 0
Jul 25 09:21:45.529: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 25 09:21:45.529: INFO: sonobuoy-systemd-logs-daemon-set-8c8b567fc9ec4bcc-lld4v from heptio-sonobuoy started at 2019-07-25 08:48:15 +0000 UTC (2 container statuses recorded)
Jul 25 09:21:45.529: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 25 09:21:45.529: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 25 09:21:45.529: INFO: kube-flannel-ds-amd64-5d5mf from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Jul 25 09:21:45.529: INFO: 	Container kube-flannel ready: true, restart count 4
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15b49c02975b8f8b], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:21:46.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1703" for this suite.
Jul 25 09:21:52.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:21:52.653: INFO: namespace sched-pred-1703 deletion completed in 6.101363941s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:7.181 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:21:52.656: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 25 09:21:55.275: INFO: Successfully updated pod "pod-update-019b1408-4102-4ba8-9dd7-6a3993cac493"
STEP: verifying the updated pod is in kubernetes
Jul 25 09:21:55.282: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:21:55.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5880" for this suite.
Jul 25 09:22:17.297: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:22:17.387: INFO: namespace pods-5880 deletion completed in 22.100850996s

• [SLOW TEST:24.733 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:22:17.393: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Performing setup for networking test in namespace pod-network-test-7496
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Jul 25 09:22:17.423: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Jul 25 09:22:35.495: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.36:8080/dial?request=hostName&protocol=http&host=10.244.1.73&port=8080&tries=1'] Namespace:pod-network-test-7496 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 25 09:22:35.495: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
Jul 25 09:22:35.624: INFO: Waiting for endpoints: map[]
Jul 25 09:22:35.628: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.2.36:8080/dial?request=hostName&protocol=http&host=10.244.2.35&port=8080&tries=1'] Namespace:pod-network-test-7496 PodName:host-test-container-pod ContainerName:hostexec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Jul 25 09:22:35.628: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
Jul 25 09:22:35.746: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:22:35.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7496" for this suite.
Jul 25 09:22:57.763: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:22:57.860: INFO: namespace pod-network-test-7496 deletion completed in 22.108634089s

• [SLOW TEST:40.468 seconds]
[sig-network] Networking
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:22:57.863: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:23:03.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5265" for this suite.
Jul 25 09:23:09.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:23:09.565: INFO: namespace watch-5265 deletion completed in 6.193329795s

• [SLOW TEST:11.702 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:23:09.565: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-80bf4dee-fe1a-465a-88b4-cee3827759ea
STEP: Creating a pod to test consume configMaps
Jul 25 09:23:09.600: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-cad85673-ae1b-44a3-853e-e6932d9dc8eb" in namespace "projected-4512" to be "success or failure"
Jul 25 09:23:09.603: INFO: Pod "pod-projected-configmaps-cad85673-ae1b-44a3-853e-e6932d9dc8eb": Phase="Pending", Reason="", readiness=false. Elapsed: 3.35865ms
Jul 25 09:23:11.606: INFO: Pod "pod-projected-configmaps-cad85673-ae1b-44a3-853e-e6932d9dc8eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.0066167s
STEP: Saw pod success
Jul 25 09:23:11.606: INFO: Pod "pod-projected-configmaps-cad85673-ae1b-44a3-853e-e6932d9dc8eb" satisfied condition "success or failure"
Jul 25 09:23:11.609: INFO: Trying to get logs from node essentialpks-conformance-3 pod pod-projected-configmaps-cad85673-ae1b-44a3-853e-e6932d9dc8eb container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 25 09:23:11.628: INFO: Waiting for pod pod-projected-configmaps-cad85673-ae1b-44a3-853e-e6932d9dc8eb to disappear
Jul 25 09:23:11.633: INFO: Pod pod-projected-configmaps-cad85673-ae1b-44a3-853e-e6932d9dc8eb no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:23:11.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4512" for this suite.
Jul 25 09:23:17.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:23:17.750: INFO: namespace projected-4512 deletion completed in 6.104218022s

• [SLOW TEST:8.184 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:23:17.751: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-480
[It] Burst scaling should run to completion even with unhealthy pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating stateful set ss in namespace statefulset-480
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-480
Jul 25 09:23:17.802: INFO: Found 0 stateful pods, waiting for 1
Jul 25 09:23:27.807: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Jul 25 09:23:27.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 25 09:23:28.017: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 25 09:23:28.017: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 25 09:23:28.017: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 25 09:23:28.023: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 25 09:23:38.026: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 25 09:23:38.027: INFO: Waiting for statefulset status.replicas updated to 0
Jul 25 09:23:38.038: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Jul 25 09:23:38.038: INFO: ss-0  essentialpks-conformance-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:28 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:17 +0000 UTC  }]
Jul 25 09:23:38.038: INFO: 
Jul 25 09:23:38.038: INFO: StatefulSet ss has not reached scale 3, at 1
Jul 25 09:23:39.042: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997230077s
Jul 25 09:23:40.048: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.993257451s
Jul 25 09:23:41.052: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.987097491s
Jul 25 09:23:42.056: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982700934s
Jul 25 09:23:43.060: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978821268s
Jul 25 09:23:44.065: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975234972s
Jul 25 09:23:45.069: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.970249186s
Jul 25 09:23:46.073: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.966155747s
Jul 25 09:23:47.078: INFO: Verifying statefulset ss doesn't scale past 3 for another 962.368097ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-480
Jul 25 09:23:48.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:23:48.276: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 25 09:23:48.276: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 25 09:23:48.276: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 25 09:23:48.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:23:48.483: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 25 09:23:48.483: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 25 09:23:48.483: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 25 09:23:48.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:23:48.695: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jul 25 09:23:48.695: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 25 09:23:48.695: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 25 09:23:48.699: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 25 09:23:48.699: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 25 09:23:48.699: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Jul 25 09:23:48.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 25 09:23:48.919: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 25 09:23:48.919: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 25 09:23:48.919: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 25 09:23:48.920: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 25 09:23:49.151: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 25 09:23:49.151: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 25 09:23:49.151: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 25 09:23:49.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 25 09:23:49.366: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 25 09:23:49.366: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 25 09:23:49.366: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 25 09:23:49.366: INFO: Waiting for statefulset status.replicas updated to 0
Jul 25 09:23:49.369: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jul 25 09:23:59.376: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 25 09:23:59.376: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 25 09:23:59.376: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 25 09:23:59.390: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Jul 25 09:23:59.390: INFO: ss-0  essentialpks-conformance-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:17 +0000 UTC  }]
Jul 25 09:23:59.391: INFO: ss-1  essentialpks-conformance-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  }]
Jul 25 09:23:59.391: INFO: ss-2  essentialpks-conformance-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  }]
Jul 25 09:23:59.391: INFO: 
Jul 25 09:23:59.391: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 25 09:24:00.395: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Jul 25 09:24:00.395: INFO: ss-0  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:17 +0000 UTC  }]
Jul 25 09:24:00.395: INFO: ss-1  essentialpks-conformance-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  }]
Jul 25 09:24:00.395: INFO: ss-2  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  }]
Jul 25 09:24:00.395: INFO: 
Jul 25 09:24:00.395: INFO: StatefulSet ss has not reached scale 0, at 3
Jul 25 09:24:01.399: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Jul 25 09:24:01.399: INFO: ss-0  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:17 +0000 UTC  }]
Jul 25 09:24:01.399: INFO: ss-2  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  }]
Jul 25 09:24:01.399: INFO: 
Jul 25 09:24:01.399: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 25 09:24:02.404: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Jul 25 09:24:02.404: INFO: ss-0  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:17 +0000 UTC  }]
Jul 25 09:24:02.404: INFO: ss-2  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  }]
Jul 25 09:24:02.404: INFO: 
Jul 25 09:24:02.404: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 25 09:24:03.408: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Jul 25 09:24:03.408: INFO: ss-0  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:17 +0000 UTC  }]
Jul 25 09:24:03.408: INFO: ss-2  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  }]
Jul 25 09:24:03.408: INFO: 
Jul 25 09:24:03.408: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 25 09:24:04.412: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Jul 25 09:24:04.412: INFO: ss-0  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:17 +0000 UTC  }]
Jul 25 09:24:04.413: INFO: ss-2  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  }]
Jul 25 09:24:04.413: INFO: 
Jul 25 09:24:04.413: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 25 09:24:05.417: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Jul 25 09:24:05.417: INFO: ss-0  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:17 +0000 UTC  }]
Jul 25 09:24:05.417: INFO: ss-2  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  }]
Jul 25 09:24:05.417: INFO: 
Jul 25 09:24:05.417: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 25 09:24:06.421: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Jul 25 09:24:06.421: INFO: ss-0  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:17 +0000 UTC  }]
Jul 25 09:24:06.421: INFO: ss-2  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  }]
Jul 25 09:24:06.421: INFO: 
Jul 25 09:24:06.422: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 25 09:24:07.426: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Jul 25 09:24:07.426: INFO: ss-0  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:17 +0000 UTC  }]
Jul 25 09:24:07.426: INFO: ss-2  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  }]
Jul 25 09:24:07.426: INFO: 
Jul 25 09:24:07.426: INFO: StatefulSet ss has not reached scale 0, at 2
Jul 25 09:24:08.431: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
Jul 25 09:24:08.431: INFO: ss-0  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:17 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:49 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:17 +0000 UTC  }]
Jul 25 09:24:08.431: INFO: ss-2  essentialpks-conformance-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:50 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:23:38 +0000 UTC  }]
Jul 25 09:24:08.431: INFO: 
Jul 25 09:24:08.431: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-480
Jul 25 09:24:09.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:24:09.558: INFO: rc: 1
Jul 25 09:24:09.565: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  error: unable to upgrade connection: container not found ("nginx")
 [] <nil> 0xc002f36360 exit status 1 <nil> <nil> true [0xc0022c7188 0xc0022c71c0 0xc0022c7200] [0xc0022c7188 0xc0022c71c0 0xc0022c7200] [0xc0022c71b0 0xc0022c71e8] [0x9d17b0 0x9d17b0] 0xc002f34300 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("nginx")

error:
exit status 1
Jul 25 09:24:19.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:24:19.649: INFO: rc: 1
Jul 25 09:24:19.649: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00301f0b0 exit status 1 <nil> <nil> true [0xc0024cacc8 0xc0024cace0 0xc0024cacf8] [0xc0024cacc8 0xc0024cace0 0xc0024cacf8] [0xc0024cacd8 0xc0024cacf0] [0x9d17b0 0x9d17b0] 0xc0031f2540 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:24:29.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:24:29.733: INFO: rc: 1
Jul 25 09:24:29.733: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d44300 exit status 1 <nil> <nil> true [0xc000b40350 0xc000b40618 0xc000b407f0] [0xc000b40350 0xc000b40618 0xc000b407f0] [0xc000b404c8 0xc000b40750] [0x9d17b0 0x9d17b0] 0xc003090360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:24:39.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:24:39.815: INFO: rc: 1
Jul 25 09:24:39.821: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d44660 exit status 1 <nil> <nil> true [0xc000b40898 0xc000b40a20 0xc000b40ae8] [0xc000b40898 0xc000b40a20 0xc000b40ae8] [0xc000b40998 0xc000b40a88] [0x9d17b0 0x9d17b0] 0xc0030906c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:24:49.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:24:49.924: INFO: rc: 1
Jul 25 09:24:49.924: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002046300 exit status 1 <nil> <nil> true [0xc0004b7318 0xc0004b73b0 0xc0004b75f8] [0xc0004b7318 0xc0004b73b0 0xc0004b75f8] [0xc0004b7398 0xc0004b7508] [0x9d17b0 0x9d17b0] 0xc001716300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:24:59.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:25:00.009: INFO: rc: 1
Jul 25 09:25:00.009: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002046660 exit status 1 <nil> <nil> true [0xc0004b7618 0xc0004b7648 0xc0004b76e8] [0xc0004b7618 0xc0004b7648 0xc0004b76e8] [0xc0004b7628 0xc0004b76c0] [0x9d17b0 0x9d17b0] 0xc001716660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:25:10.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:25:10.085: INFO: rc: 1
Jul 25 09:25:10.085: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0020469c0 exit status 1 <nil> <nil> true [0xc0004b7788 0xc0004b7820 0xc0004b7b00] [0xc0004b7788 0xc0004b7820 0xc0004b7b00] [0xc0004b7810 0xc0004b7a28] [0x9d17b0 0x9d17b0] 0xc0017169c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:25:20.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:25:20.181: INFO: rc: 1
Jul 25 09:25:20.181: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002046d50 exit status 1 <nil> <nil> true [0xc0004b7b48 0xc0004b7c20 0xc0004b7e30] [0xc0004b7b48 0xc0004b7c20 0xc0004b7e30] [0xc0004b7bd8 0xc0004b7dc8] [0x9d17b0 0x9d17b0] 0xc001716d80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:25:30.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:25:30.268: INFO: rc: 1
Jul 25 09:25:30.268: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0020470b0 exit status 1 <nil> <nil> true [0xc0004b7e48 0xc0004b7f48 0xc000010908] [0xc0004b7e48 0xc0004b7f48 0xc000010908] [0xc0004b7f20 0xc0000c4598] [0x9d17b0 0x9d17b0] 0xc001717140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:25:40.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:25:40.364: INFO: rc: 1
Jul 25 09:25:40.364: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d449f0 exit status 1 <nil> <nil> true [0xc000b40bd0 0xc000b40e98 0xc000b411a8] [0xc000b40bd0 0xc000b40e98 0xc000b411a8] [0xc000b40cf0 0xc000b41118] [0x9d17b0 0x9d17b0] 0xc003090a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:25:50.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:25:50.464: INFO: rc: 1
Jul 25 09:25:50.464: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d44d50 exit status 1 <nil> <nil> true [0xc000b41200 0xc000b41240 0xc000b41310] [0xc000b41200 0xc000b41240 0xc000b41310] [0xc000b41230 0xc000b412e0] [0x9d17b0 0x9d17b0] 0xc003091140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:26:00.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:26:00.554: INFO: rc: 1
Jul 25 09:26:00.554: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002047410 exit status 1 <nil> <nil> true [0xc000010a60 0xc000010c80 0xc000010e20] [0xc000010a60 0xc000010c80 0xc000010e20] [0xc000010c70 0xc000010d58] [0x9d17b0 0x9d17b0] 0xc001717620 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:26:10.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:26:10.640: INFO: rc: 1
Jul 25 09:26:10.640: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002047770 exit status 1 <nil> <nil> true [0xc000010eb0 0xc000011050 0xc000011238] [0xc000010eb0 0xc000011050 0xc000011238] [0xc000010f70 0xc0000111c0] [0x9d17b0 0x9d17b0] 0xc001717980 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:26:20.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:26:20.730: INFO: rc: 1
Jul 25 09:26:20.730: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d450b0 exit status 1 <nil> <nil> true [0xc000b41360 0xc000b41490 0xc000b41508] [0xc000b41360 0xc000b41490 0xc000b41508] [0xc000b41480 0xc000b414c8] [0x9d17b0 0x9d17b0] 0xc0030914a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:26:30.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:26:30.828: INFO: rc: 1
Jul 25 09:26:30.828: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d44330 exit status 1 <nil> <nil> true [0xc0000c4598 0xc0004b7398 0xc0004b7508] [0xc0000c4598 0xc0004b7398 0xc0004b7508] [0xc0004b7378 0xc0004b73c8] [0x9d17b0 0x9d17b0] 0xc003090360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:26:40.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:26:40.925: INFO: rc: 1
Jul 25 09:26:40.925: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d446c0 exit status 1 <nil> <nil> true [0xc0004b75f8 0xc0004b7628 0xc0004b76c0] [0xc0004b75f8 0xc0004b7628 0xc0004b76c0] [0xc0004b7620 0xc0004b7678] [0x9d17b0 0x9d17b0] 0xc0030906c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:26:50.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:26:51.008: INFO: rc: 1
Jul 25 09:26:51.009: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d44a80 exit status 1 <nil> <nil> true [0xc0004b76e8 0xc0004b7810 0xc0004b7a28] [0xc0004b76e8 0xc0004b7810 0xc0004b7a28] [0xc0004b7800 0xc0004b7938] [0x9d17b0 0x9d17b0] 0xc003090a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:27:01.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:27:01.082: INFO: rc: 1
Jul 25 09:27:01.082: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d44e10 exit status 1 <nil> <nil> true [0xc0004b7b00 0xc0004b7bd8 0xc0004b7dc8] [0xc0004b7b00 0xc0004b7bd8 0xc0004b7dc8] [0xc0004b7bb8 0xc0004b7cf8] [0x9d17b0 0x9d17b0] 0xc003091140 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:27:11.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:27:11.159: INFO: rc: 1
Jul 25 09:27:11.159: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002046330 exit status 1 <nil> <nil> true [0xc000b40238 0xc000b404c8 0xc000b40750] [0xc000b40238 0xc000b404c8 0xc000b40750] [0xc000b403c8 0xc000b40688] [0x9d17b0 0x9d17b0] 0xc001716300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:27:21.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:27:21.260: INFO: rc: 1
Jul 25 09:27:21.260: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d451a0 exit status 1 <nil> <nil> true [0xc0004b7e30 0xc0004b7f20 0xc000010a60] [0xc0004b7e30 0xc0004b7f20 0xc000010a60] [0xc0004b7ee8 0xc000010908] [0x9d17b0 0x9d17b0] 0xc0030914a0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:27:31.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:27:31.351: INFO: rc: 1
Jul 25 09:27:31.351: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d45500 exit status 1 <nil> <nil> true [0xc000010af8 0xc000010cf0 0xc000010eb0] [0xc000010af8 0xc000010cf0 0xc000010eb0] [0xc000010c80 0xc000010e20] [0x9d17b0 0x9d17b0] 0xc003091800 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:27:41.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:27:41.448: INFO: rc: 1
Jul 25 09:27:41.450: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d45860 exit status 1 <nil> <nil> true [0xc000010f00 0xc000011078 0xc000011268] [0xc000010f00 0xc000011078 0xc000011268] [0xc000011050 0xc000011238] [0x9d17b0 0x9d17b0] 0xc003091b60 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:27:51.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:27:51.537: INFO: rc: 1
Jul 25 09:27:51.540: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0020466c0 exit status 1 <nil> <nil> true [0xc000b407f0 0xc000b40998 0xc000b40a88] [0xc000b407f0 0xc000b40998 0xc000b40a88] [0xc000b40920 0xc000b40a40] [0x9d17b0 0x9d17b0] 0xc001716660 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:28:01.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:28:01.888: INFO: rc: 1
Jul 25 09:28:01.891: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d45bc0 exit status 1 <nil> <nil> true [0xc000011280 0xc0000114f0 0xc000011770] [0xc000011280 0xc0000114f0 0xc000011770] [0xc0000112d8 0xc0000115b8] [0x9d17b0 0x9d17b0] 0xc003091ec0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:28:11.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:28:11.977: INFO: rc: 1
Jul 25 09:28:11.978: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d45f20 exit status 1 <nil> <nil> true [0xc0000117d8 0xc000011858 0xc0000118e8] [0xc0000117d8 0xc000011858 0xc0000118e8] [0xc000011840 0xc0000118c8] [0x9d17b0 0x9d17b0] 0xc0027fa960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:28:21.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:28:22.069: INFO: rc: 1
Jul 25 09:28:22.069: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002046a50 exit status 1 <nil> <nil> true [0xc000b40ae8 0xc000b40cf0 0xc000b41118] [0xc000b40ae8 0xc000b40cf0 0xc000b41118] [0xc000b40c50 0xc000b40fd8] [0x9d17b0 0x9d17b0] 0xc0017169c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:28:32.069: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:28:32.154: INFO: rc: 1
Jul 25 09:28:32.155: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002046300 exit status 1 <nil> <nil> true [0xc0004b7378 0xc0004b73c8 0xc0004b7618] [0xc0004b7378 0xc0004b73c8 0xc0004b7618] [0xc0004b73b0 0xc0004b75f8] [0x9d17b0 0x9d17b0] 0xc003090360 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:28:42.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:28:42.232: INFO: rc: 1
Jul 25 09:28:42.232: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002046690 exit status 1 <nil> <nil> true [0xc0004b7620 0xc0004b7678 0xc0004b7788] [0xc0004b7620 0xc0004b7678 0xc0004b7788] [0xc0004b7648 0xc0004b76e8] [0x9d17b0 0x9d17b0] 0xc0030906c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:28:52.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:28:52.325: INFO: rc: 1
Jul 25 09:28:52.325: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d44300 exit status 1 <nil> <nil> true [0xc0000c40c0 0xc000b40350 0xc000b40618] [0xc0000c40c0 0xc000b40350 0xc000b40618] [0xc000b40238 0xc000b404c8] [0x9d17b0 0x9d17b0] 0xc001716300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:29:02.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:29:02.403: INFO: rc: 1
Jul 25 09:29:02.403: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002046a20 exit status 1 <nil> <nil> true [0xc0004b7800 0xc0004b7938 0xc0004b7b48] [0xc0004b7800 0xc0004b7938 0xc0004b7b48] [0xc0004b7820 0xc0004b7b00] [0x9d17b0 0x9d17b0] 0xc003090a80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Jul 25 09:29:12.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-480 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:29:12.485: INFO: rc: 1
Jul 25 09:29:12.485: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: 
Jul 25 09:29:12.485: INFO: Scaling statefulset ss to 0
Jul 25 09:29:12.492: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 25 09:29:12.494: INFO: Deleting all statefulset in ns statefulset-480
Jul 25 09:29:12.498: INFO: Scaling statefulset ss to 0
Jul 25 09:29:12.505: INFO: Waiting for statefulset status.replicas updated to 0
Jul 25 09:29:12.507: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:29:12.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-480" for this suite.
Jul 25 09:29:18.545: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:29:18.622: INFO: namespace statefulset-480 deletion completed in 6.09087244s

• [SLOW TEST:360.872 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Burst scaling should run to completion even with unhealthy pods [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:29:18.623: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 25 09:29:19.666: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:29:19.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2130" for this suite.
Jul 25 09:29:25.688: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:29:25.765: INFO: namespace container-runtime-2130 deletion completed in 6.085123485s

• [SLOW TEST:7.141 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:29:25.766: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Jul 25 09:29:25.800: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6688,SelfLink:/api/v1/namespaces/watch-6688/configmaps/e2e-watch-test-configmap-a,UID:e94594aa-a4df-4a92-aaf5-585075537d3b,ResourceVersion:63866,Generation:0,CreationTimestamp:2019-07-25 09:29:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 25 09:29:25.800: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6688,SelfLink:/api/v1/namespaces/watch-6688/configmaps/e2e-watch-test-configmap-a,UID:e94594aa-a4df-4a92-aaf5-585075537d3b,ResourceVersion:63866,Generation:0,CreationTimestamp:2019-07-25 09:29:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Jul 25 09:29:35.809: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6688,SelfLink:/api/v1/namespaces/watch-6688/configmaps/e2e-watch-test-configmap-a,UID:e94594aa-a4df-4a92-aaf5-585075537d3b,ResourceVersion:63883,Generation:0,CreationTimestamp:2019-07-25 09:29:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Jul 25 09:29:35.809: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6688,SelfLink:/api/v1/namespaces/watch-6688/configmaps/e2e-watch-test-configmap-a,UID:e94594aa-a4df-4a92-aaf5-585075537d3b,ResourceVersion:63883,Generation:0,CreationTimestamp:2019-07-25 09:29:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Jul 25 09:29:45.815: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6688,SelfLink:/api/v1/namespaces/watch-6688/configmaps/e2e-watch-test-configmap-a,UID:e94594aa-a4df-4a92-aaf5-585075537d3b,ResourceVersion:63899,Generation:0,CreationTimestamp:2019-07-25 09:29:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 25 09:29:45.816: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6688,SelfLink:/api/v1/namespaces/watch-6688/configmaps/e2e-watch-test-configmap-a,UID:e94594aa-a4df-4a92-aaf5-585075537d3b,ResourceVersion:63899,Generation:0,CreationTimestamp:2019-07-25 09:29:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Jul 25 09:29:55.822: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6688,SelfLink:/api/v1/namespaces/watch-6688/configmaps/e2e-watch-test-configmap-a,UID:e94594aa-a4df-4a92-aaf5-585075537d3b,ResourceVersion:63914,Generation:0,CreationTimestamp:2019-07-25 09:29:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 25 09:29:55.822: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-a,GenerateName:,Namespace:watch-6688,SelfLink:/api/v1/namespaces/watch-6688/configmaps/e2e-watch-test-configmap-a,UID:e94594aa-a4df-4a92-aaf5-585075537d3b,ResourceVersion:63914,Generation:0,CreationTimestamp:2019-07-25 09:29:25 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-A,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Jul 25 09:30:05.829: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6688,SelfLink:/api/v1/namespaces/watch-6688/configmaps/e2e-watch-test-configmap-b,UID:cb538536-20c5-431f-a65d-51bff8607a92,ResourceVersion:63929,Generation:0,CreationTimestamp:2019-07-25 09:30:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 25 09:30:05.829: INFO: Got : ADDED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6688,SelfLink:/api/v1/namespaces/watch-6688/configmaps/e2e-watch-test-configmap-b,UID:cb538536-20c5-431f-a65d-51bff8607a92,ResourceVersion:63929,Generation:0,CreationTimestamp:2019-07-25 09:30:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Jul 25 09:30:15.836: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6688,SelfLink:/api/v1/namespaces/watch-6688/configmaps/e2e-watch-test-configmap-b,UID:cb538536-20c5-431f-a65d-51bff8607a92,ResourceVersion:63944,Generation:0,CreationTimestamp:2019-07-25 09:30:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
Jul 25 09:30:15.836: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-configmap-b,GenerateName:,Namespace:watch-6688,SelfLink:/api/v1/namespaces/watch-6688/configmaps/e2e-watch-test-configmap-b,UID:cb538536-20c5-431f-a65d-51bff8607a92,ResourceVersion:63944,Generation:0,CreationTimestamp:2019-07-25 09:30:05 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: multiple-watchers-B,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:30:25.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6688" for this suite.
Jul 25 09:30:31.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:30:31.948: INFO: namespace watch-6688 deletion completed in 6.105218878s

• [SLOW TEST:66.183 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:30:31.948: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 25 09:30:31.989: INFO: Waiting up to 5m0s for pod "downward-api-ec00ed5c-4634-4ecd-b512-800a8a1faf1e" in namespace "downward-api-5192" to be "success or failure"
Jul 25 09:30:31.993: INFO: Pod "downward-api-ec00ed5c-4634-4ecd-b512-800a8a1faf1e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.374063ms
Jul 25 09:30:33.996: INFO: Pod "downward-api-ec00ed5c-4634-4ecd-b512-800a8a1faf1e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007765646s
STEP: Saw pod success
Jul 25 09:30:33.996: INFO: Pod "downward-api-ec00ed5c-4634-4ecd-b512-800a8a1faf1e" satisfied condition "success or failure"
Jul 25 09:30:34.000: INFO: Trying to get logs from node essentialpks-conformance-2 pod downward-api-ec00ed5c-4634-4ecd-b512-800a8a1faf1e container dapi-container: <nil>
STEP: delete the pod
Jul 25 09:30:34.019: INFO: Waiting for pod downward-api-ec00ed5c-4634-4ecd-b512-800a8a1faf1e to disappear
Jul 25 09:30:34.022: INFO: Pod downward-api-ec00ed5c-4634-4ecd-b512-800a8a1faf1e no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:30:34.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5192" for this suite.
Jul 25 09:30:40.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:30:40.110: INFO: namespace downward-api-5192 deletion completed in 6.083805526s

• [SLOW TEST:8.162 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:30:40.110: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-57eb69af-0c2a-4817-8bc4-b1965bf1b5e7
STEP: Creating a pod to test consume secrets
Jul 25 09:30:40.145: INFO: Waiting up to 5m0s for pod "pod-secrets-7b58d82c-c820-4680-9c0a-a88a77e7fcd6" in namespace "secrets-15" to be "success or failure"
Jul 25 09:30:40.149: INFO: Pod "pod-secrets-7b58d82c-c820-4680-9c0a-a88a77e7fcd6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.256014ms
Jul 25 09:30:42.153: INFO: Pod "pod-secrets-7b58d82c-c820-4680-9c0a-a88a77e7fcd6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007544565s
STEP: Saw pod success
Jul 25 09:30:42.153: INFO: Pod "pod-secrets-7b58d82c-c820-4680-9c0a-a88a77e7fcd6" satisfied condition "success or failure"
Jul 25 09:30:42.156: INFO: Trying to get logs from node essentialpks-conformance-3 pod pod-secrets-7b58d82c-c820-4680-9c0a-a88a77e7fcd6 container secret-volume-test: <nil>
STEP: delete the pod
Jul 25 09:30:42.175: INFO: Waiting for pod pod-secrets-7b58d82c-c820-4680-9c0a-a88a77e7fcd6 to disappear
Jul 25 09:30:42.178: INFO: Pod pod-secrets-7b58d82c-c820-4680-9c0a-a88a77e7fcd6 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:30:42.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-15" for this suite.
Jul 25 09:30:48.199: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:30:48.285: INFO: namespace secrets-15 deletion completed in 6.103365303s

• [SLOW TEST:8.175 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:30:48.290: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1293
STEP: creating an rc
Jul 25 09:30:48.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 create -f - --namespace=kubectl-1184'
Jul 25 09:30:48.479: INFO: stderr: ""
Jul 25 09:30:48.479: INFO: stdout: "replicationcontroller/redis-master created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Waiting for Redis master to start.
Jul 25 09:30:49.482: INFO: Selector matched 1 pods for map[app:redis]
Jul 25 09:30:49.482: INFO: Found 0 / 1
Jul 25 09:30:50.483: INFO: Selector matched 1 pods for map[app:redis]
Jul 25 09:30:50.483: INFO: Found 1 / 1
Jul 25 09:30:50.483: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 25 09:30:50.485: INFO: Selector matched 1 pods for map[app:redis]
Jul 25 09:30:50.486: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
STEP: checking for a matching strings
Jul 25 09:30:50.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 logs redis-master-zf5dr redis-master --namespace=kubectl-1184'
Jul 25 09:30:50.583: INFO: stderr: ""
Jul 25 09:30:50.583: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 25 Jul 09:30:49.295 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 25 Jul 09:30:49.295 # Server started, Redis version 3.2.12\n1:M 25 Jul 09:30:49.295 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 25 Jul 09:30:49.295 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log lines
Jul 25 09:30:50.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 log redis-master-zf5dr redis-master --namespace=kubectl-1184 --tail=1'
Jul 25 09:30:50.676: INFO: stderr: ""
Jul 25 09:30:50.676: INFO: stdout: "1:M 25 Jul 09:30:49.295 * The server is now ready to accept connections on port 6379\n"
STEP: limiting log bytes
Jul 25 09:30:50.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 log redis-master-zf5dr redis-master --namespace=kubectl-1184 --limit-bytes=1'
Jul 25 09:30:50.760: INFO: stderr: ""
Jul 25 09:30:50.760: INFO: stdout: " "
STEP: exposing timestamps
Jul 25 09:30:50.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 log redis-master-zf5dr redis-master --namespace=kubectl-1184 --tail=1 --timestamps'
Jul 25 09:30:50.857: INFO: stderr: ""
Jul 25 09:30:50.857: INFO: stdout: "2019-07-25T09:30:49.29763715Z 1:M 25 Jul 09:30:49.295 * The server is now ready to accept connections on port 6379\n"
STEP: restricting to a time range
Jul 25 09:30:53.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 log redis-master-zf5dr redis-master --namespace=kubectl-1184 --since=1s'
Jul 25 09:30:53.467: INFO: stderr: ""
Jul 25 09:30:53.467: INFO: stdout: ""
Jul 25 09:30:53.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 log redis-master-zf5dr redis-master --namespace=kubectl-1184 --since=24h'
Jul 25 09:30:53.563: INFO: stderr: ""
Jul 25 09:30:53.563: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 25 Jul 09:30:49.295 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 25 Jul 09:30:49.295 # Server started, Redis version 3.2.12\n1:M 25 Jul 09:30:49.295 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 25 Jul 09:30:49.295 * The server is now ready to accept connections on port 6379\n"
[AfterEach] [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1299
STEP: using delete to clean up resources
Jul 25 09:30:53.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 delete --grace-period=0 --force -f - --namespace=kubectl-1184'
Jul 25 09:30:53.652: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jul 25 09:30:53.652: INFO: stdout: "replicationcontroller \"redis-master\" force deleted\n"
Jul 25 09:30:53.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get rc,svc -l name=nginx --no-headers --namespace=kubectl-1184'
Jul 25 09:30:53.758: INFO: stderr: "No resources found.\n"
Jul 25 09:30:53.758: INFO: stdout: ""
Jul 25 09:30:53.758: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods -l name=nginx --namespace=kubectl-1184 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jul 25 09:30:53.852: INFO: stderr: ""
Jul 25 09:30:53.852: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:30:53.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1184" for this suite.
Jul 25 09:30:59.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:30:59.952: INFO: namespace kubectl-1184 deletion completed in 6.092731016s

• [SLOW TEST:11.662 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl logs
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:30:59.955: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 25 09:30:59.989: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a3080a39-b9bd-4d1d-af22-f1292d63f5f0" in namespace "projected-4378" to be "success or failure"
Jul 25 09:30:59.996: INFO: Pod "downwardapi-volume-a3080a39-b9bd-4d1d-af22-f1292d63f5f0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.966229ms
Jul 25 09:31:02.000: INFO: Pod "downwardapi-volume-a3080a39-b9bd-4d1d-af22-f1292d63f5f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010461108s
STEP: Saw pod success
Jul 25 09:31:02.000: INFO: Pod "downwardapi-volume-a3080a39-b9bd-4d1d-af22-f1292d63f5f0" satisfied condition "success or failure"
Jul 25 09:31:02.003: INFO: Trying to get logs from node essentialpks-conformance-3 pod downwardapi-volume-a3080a39-b9bd-4d1d-af22-f1292d63f5f0 container client-container: <nil>
STEP: delete the pod
Jul 25 09:31:02.019: INFO: Waiting for pod downwardapi-volume-a3080a39-b9bd-4d1d-af22-f1292d63f5f0 to disappear
Jul 25 09:31:02.021: INFO: Pod downwardapi-volume-a3080a39-b9bd-4d1d-af22-f1292d63f5f0 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:31:02.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4378" for this suite.
Jul 25 09:31:08.035: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:31:08.161: INFO: namespace projected-4378 deletion completed in 6.136193375s

• [SLOW TEST:8.208 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:31:08.168: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 25 09:31:08.200: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2a3c0f79-6307-4325-a87c-eef82b276676" in namespace "downward-api-346" to be "success or failure"
Jul 25 09:31:08.202: INFO: Pod "downwardapi-volume-2a3c0f79-6307-4325-a87c-eef82b276676": Phase="Pending", Reason="", readiness=false. Elapsed: 2.484524ms
Jul 25 09:31:10.207: INFO: Pod "downwardapi-volume-2a3c0f79-6307-4325-a87c-eef82b276676": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006748269s
STEP: Saw pod success
Jul 25 09:31:10.207: INFO: Pod "downwardapi-volume-2a3c0f79-6307-4325-a87c-eef82b276676" satisfied condition "success or failure"
Jul 25 09:31:10.210: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-2a3c0f79-6307-4325-a87c-eef82b276676 container client-container: <nil>
STEP: delete the pod
Jul 25 09:31:10.227: INFO: Waiting for pod downwardapi-volume-2a3c0f79-6307-4325-a87c-eef82b276676 to disappear
Jul 25 09:31:10.230: INFO: Pod downwardapi-volume-2a3c0f79-6307-4325-a87c-eef82b276676 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:31:10.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-346" for this suite.
Jul 25 09:31:16.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:31:16.317: INFO: namespace downward-api-346 deletion completed in 6.083908867s

• [SLOW TEST:8.149 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:31:16.317: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:31:16.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-943" for this suite.
Jul 25 09:31:22.410: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:31:22.489: INFO: namespace services-943 deletion completed in 6.088793625s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:6.172 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:31:22.491: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:31:24.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3647" for this suite.
Jul 25 09:32:02.555: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:32:02.652: INFO: namespace kubelet-test-3647 deletion completed in 38.106440217s

• [SLOW TEST:40.161 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:32:02.658: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:273
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the initial replication controller
Jul 25 09:32:02.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 create -f - --namespace=kubectl-2909'
Jul 25 09:32:02.861: INFO: stderr: ""
Jul 25 09:32:02.861: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 25 09:32:02.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2909'
Jul 25 09:32:02.948: INFO: stderr: ""
Jul 25 09:32:02.948: INFO: stdout: "update-demo-nautilus-qx5tv update-demo-nautilus-tqlfx "
Jul 25 09:32:02.948: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-nautilus-qx5tv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2909'
Jul 25 09:32:03.043: INFO: stderr: ""
Jul 25 09:32:03.043: INFO: stdout: ""
Jul 25 09:32:03.043: INFO: update-demo-nautilus-qx5tv is created but not running
Jul 25 09:32:08.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2909'
Jul 25 09:32:08.128: INFO: stderr: ""
Jul 25 09:32:08.128: INFO: stdout: "update-demo-nautilus-qx5tv update-demo-nautilus-tqlfx "
Jul 25 09:32:08.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-nautilus-qx5tv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2909'
Jul 25 09:32:08.218: INFO: stderr: ""
Jul 25 09:32:08.218: INFO: stdout: "true"
Jul 25 09:32:08.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-nautilus-qx5tv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2909'
Jul 25 09:32:08.301: INFO: stderr: ""
Jul 25 09:32:08.301: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 25 09:32:08.301: INFO: validating pod update-demo-nautilus-qx5tv
Jul 25 09:32:08.309: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 25 09:32:08.309: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 25 09:32:08.309: INFO: update-demo-nautilus-qx5tv is verified up and running
Jul 25 09:32:08.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-nautilus-tqlfx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2909'
Jul 25 09:32:08.405: INFO: stderr: ""
Jul 25 09:32:08.405: INFO: stdout: "true"
Jul 25 09:32:08.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-nautilus-tqlfx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2909'
Jul 25 09:32:08.488: INFO: stderr: ""
Jul 25 09:32:08.488: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Jul 25 09:32:08.488: INFO: validating pod update-demo-nautilus-tqlfx
Jul 25 09:32:08.493: INFO: got data: {
  "image": "nautilus.jpg"
}

Jul 25 09:32:08.493: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jul 25 09:32:08.493: INFO: update-demo-nautilus-tqlfx is verified up and running
STEP: rolling-update to new replication controller
Jul 25 09:32:08.496: INFO: scanned /root for discovery docs: <nil>
Jul 25 09:32:08.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-2909'
Jul 25 09:32:30.918: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 25 09:32:30.918: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Jul 25 09:32:30.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-2909'
Jul 25 09:32:31.017: INFO: stderr: ""
Jul 25 09:32:31.017: INFO: stdout: "update-demo-kitten-6jwvs update-demo-kitten-vnwmr "
Jul 25 09:32:31.017: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-kitten-6jwvs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2909'
Jul 25 09:32:31.096: INFO: stderr: ""
Jul 25 09:32:31.096: INFO: stdout: "true"
Jul 25 09:32:31.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-kitten-6jwvs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2909'
Jul 25 09:32:31.190: INFO: stderr: ""
Jul 25 09:32:31.190: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 25 09:32:31.190: INFO: validating pod update-demo-kitten-6jwvs
Jul 25 09:32:31.198: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 25 09:32:31.198: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 25 09:32:31.198: INFO: update-demo-kitten-6jwvs is verified up and running
Jul 25 09:32:31.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-kitten-vnwmr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-2909'
Jul 25 09:32:31.279: INFO: stderr: ""
Jul 25 09:32:31.279: INFO: stdout: "true"
Jul 25 09:32:31.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods update-demo-kitten-vnwmr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-2909'
Jul 25 09:32:31.368: INFO: stderr: ""
Jul 25 09:32:31.368: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Jul 25 09:32:31.369: INFO: validating pod update-demo-kitten-vnwmr
Jul 25 09:32:31.376: INFO: got data: {
  "image": "kitten.jpg"
}

Jul 25 09:32:31.376: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Jul 25 09:32:31.376: INFO: update-demo-kitten-vnwmr is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:32:31.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2909" for this suite.
Jul 25 09:32:53.395: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:32:53.465: INFO: namespace kubectl-2909 deletion completed in 22.084016386s

• [SLOW TEST:50.807 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Update Demo
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:32:53.465: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-741cb3ae-5d41-47c1-af2d-f1ae4207193b
STEP: Creating a pod to test consume secrets
Jul 25 09:32:53.502: INFO: Waiting up to 5m0s for pod "pod-secrets-985471e8-9945-4444-9526-d80346ace899" in namespace "secrets-3630" to be "success or failure"
Jul 25 09:32:53.505: INFO: Pod "pod-secrets-985471e8-9945-4444-9526-d80346ace899": Phase="Pending", Reason="", readiness=false. Elapsed: 3.582466ms
Jul 25 09:32:55.510: INFO: Pod "pod-secrets-985471e8-9945-4444-9526-d80346ace899": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008675956s
STEP: Saw pod success
Jul 25 09:32:55.510: INFO: Pod "pod-secrets-985471e8-9945-4444-9526-d80346ace899" satisfied condition "success or failure"
Jul 25 09:32:55.516: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-secrets-985471e8-9945-4444-9526-d80346ace899 container secret-volume-test: <nil>
STEP: delete the pod
Jul 25 09:32:55.533: INFO: Waiting for pod pod-secrets-985471e8-9945-4444-9526-d80346ace899 to disappear
Jul 25 09:32:55.535: INFO: Pod pod-secrets-985471e8-9945-4444-9526-d80346ace899 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:32:55.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3630" for this suite.
Jul 25 09:33:01.548: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:33:01.732: INFO: namespace secrets-3630 deletion completed in 6.192849993s

• [SLOW TEST:8.267 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:33:01.738: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: executing a command with run --rm and attach with stdin
Jul 25 09:33:01.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 --namespace=kubectl-1669 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Jul 25 09:33:03.135: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Jul 25 09:33:03.135: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:33:05.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1669" for this suite.
Jul 25 09:33:11.154: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:33:11.233: INFO: namespace kubectl-1669 deletion completed in 6.088278744s

• [SLOW TEST:9.495 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run --rm job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:33:11.234: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-030e1a0a-2353-4033-bd8d-8e346024f997
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-030e1a0a-2353-4033-bd8d-8e346024f997
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:33:15.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6957" for this suite.
Jul 25 09:33:37.333: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:33:37.436: INFO: namespace configmap-6957 deletion completed in 22.115092977s

• [SLOW TEST:26.202 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:33:37.436: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 25 09:33:37.467: INFO: Waiting up to 5m0s for pod "downwardapi-volume-37f12ad4-6eda-4913-b99c-962224816c02" in namespace "projected-9901" to be "success or failure"
Jul 25 09:33:37.469: INFO: Pod "downwardapi-volume-37f12ad4-6eda-4913-b99c-962224816c02": Phase="Pending", Reason="", readiness=false. Elapsed: 2.016297ms
Jul 25 09:33:39.474: INFO: Pod "downwardapi-volume-37f12ad4-6eda-4913-b99c-962224816c02": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006508087s
STEP: Saw pod success
Jul 25 09:33:39.474: INFO: Pod "downwardapi-volume-37f12ad4-6eda-4913-b99c-962224816c02" satisfied condition "success or failure"
Jul 25 09:33:39.478: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-37f12ad4-6eda-4913-b99c-962224816c02 container client-container: <nil>
STEP: delete the pod
Jul 25 09:33:39.498: INFO: Waiting for pod downwardapi-volume-37f12ad4-6eda-4913-b99c-962224816c02 to disappear
Jul 25 09:33:39.501: INFO: Pod downwardapi-volume-37f12ad4-6eda-4913-b99c-962224816c02 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:33:39.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9901" for this suite.
Jul 25 09:33:45.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:33:45.607: INFO: namespace projected-9901 deletion completed in 6.102917304s

• [SLOW TEST:8.171 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:33:45.614: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-886v
STEP: Creating a pod to test atomic-volume-subpath
Jul 25 09:33:45.658: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-886v" in namespace "subpath-6681" to be "success or failure"
Jul 25 09:33:45.663: INFO: Pod "pod-subpath-test-configmap-886v": Phase="Pending", Reason="", readiness=false. Elapsed: 4.916032ms
Jul 25 09:33:47.665: INFO: Pod "pod-subpath-test-configmap-886v": Phase="Running", Reason="", readiness=true. Elapsed: 2.007757577s
Jul 25 09:33:49.669: INFO: Pod "pod-subpath-test-configmap-886v": Phase="Running", Reason="", readiness=true. Elapsed: 4.011398591s
Jul 25 09:33:51.673: INFO: Pod "pod-subpath-test-configmap-886v": Phase="Running", Reason="", readiness=true. Elapsed: 6.015423904s
Jul 25 09:33:53.678: INFO: Pod "pod-subpath-test-configmap-886v": Phase="Running", Reason="", readiness=true. Elapsed: 8.020355172s
Jul 25 09:33:55.683: INFO: Pod "pod-subpath-test-configmap-886v": Phase="Running", Reason="", readiness=true. Elapsed: 10.025102017s
Jul 25 09:33:57.687: INFO: Pod "pod-subpath-test-configmap-886v": Phase="Running", Reason="", readiness=true. Elapsed: 12.029473021s
Jul 25 09:33:59.691: INFO: Pod "pod-subpath-test-configmap-886v": Phase="Running", Reason="", readiness=true. Elapsed: 14.032952263s
Jul 25 09:34:01.695: INFO: Pod "pod-subpath-test-configmap-886v": Phase="Running", Reason="", readiness=true. Elapsed: 16.037134004s
Jul 25 09:34:03.699: INFO: Pod "pod-subpath-test-configmap-886v": Phase="Running", Reason="", readiness=true. Elapsed: 18.0408393s
Jul 25 09:34:05.702: INFO: Pod "pod-subpath-test-configmap-886v": Phase="Running", Reason="", readiness=true. Elapsed: 20.044508141s
Jul 25 09:34:07.707: INFO: Pod "pod-subpath-test-configmap-886v": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.049268363s
STEP: Saw pod success
Jul 25 09:34:07.707: INFO: Pod "pod-subpath-test-configmap-886v" satisfied condition "success or failure"
Jul 25 09:34:07.711: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-subpath-test-configmap-886v container test-container-subpath-configmap-886v: <nil>
STEP: delete the pod
Jul 25 09:34:07.733: INFO: Waiting for pod pod-subpath-test-configmap-886v to disappear
Jul 25 09:34:07.738: INFO: Pod pod-subpath-test-configmap-886v no longer exists
STEP: Deleting pod pod-subpath-test-configmap-886v
Jul 25 09:34:07.738: INFO: Deleting pod "pod-subpath-test-configmap-886v" in namespace "subpath-6681"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:34:07.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6681" for this suite.
Jul 25 09:34:13.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:34:13.840: INFO: namespace subpath-6681 deletion completed in 6.095526692s

• [SLOW TEST:28.227 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:34:13.846: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Jul 25 09:34:13.893: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-5564,SelfLink:/api/v1/namespaces/watch-5564/configmaps/e2e-watch-test-resource-version,UID:01196ede-904d-405c-8bb8-a30b967e1012,ResourceVersion:64717,Generation:0,CreationTimestamp:2019-07-25 09:34:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Jul 25 09:34:13.893: INFO: Got : DELETED &ConfigMap{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:e2e-watch-test-resource-version,GenerateName:,Namespace:watch-5564,SelfLink:/api/v1/namespaces/watch-5564/configmaps/e2e-watch-test-resource-version,UID:01196ede-904d-405c-8bb8-a30b967e1012,ResourceVersion:64718,Generation:0,CreationTimestamp:2019-07-25 09:34:13 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{watch-this-configmap: from-resource-version,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:34:13.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5564" for this suite.
Jul 25 09:34:19.908: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:34:19.984: INFO: namespace watch-5564 deletion completed in 6.087229413s

• [SLOW TEST:6.139 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:34:19.993: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 25 09:34:24.102: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 25 09:34:24.106: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 25 09:34:26.107: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 25 09:34:26.112: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 25 09:34:28.107: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 25 09:34:28.114: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 25 09:34:30.107: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 25 09:34:30.111: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 25 09:34:32.107: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 25 09:34:32.111: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 25 09:34:34.107: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 25 09:34:34.113: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 25 09:34:36.107: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 25 09:34:36.111: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 25 09:34:38.107: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 25 09:34:38.111: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 25 09:34:40.107: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 25 09:34:40.112: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 25 09:34:42.107: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 25 09:34:42.112: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 25 09:34:44.107: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 25 09:34:44.110: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 25 09:34:46.107: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 25 09:34:46.110: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 25 09:34:48.107: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 25 09:34:48.111: INFO: Pod pod-with-prestop-exec-hook still exists
Jul 25 09:34:50.107: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jul 25 09:34:50.110: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:34:50.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4437" for this suite.
Jul 25 09:35:12.132: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:35:12.212: INFO: namespace container-lifecycle-hook-4437 deletion completed in 22.090680609s

• [SLOW TEST:52.219 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:35:12.212: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test hostPath mode
Jul 25 09:35:12.247: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-7983" to be "success or failure"
Jul 25 09:35:12.255: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 7.335483ms
Jul 25 09:35:14.258: INFO: Pod "pod-host-path-test": Phase="Running", Reason="", readiness=false. Elapsed: 2.011069374s
Jul 25 09:35:16.262: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01460093s
STEP: Saw pod success
Jul 25 09:35:16.262: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Jul 25 09:35:16.265: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Jul 25 09:35:16.282: INFO: Waiting for pod pod-host-path-test to disappear
Jul 25 09:35:16.284: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:35:16.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-7983" for this suite.
Jul 25 09:35:22.298: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:35:22.381: INFO: namespace hostpath-7983 deletion completed in 6.092507093s

• [SLOW TEST:10.169 seconds]
[sig-storage] HostPath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:35:22.387: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Jul 25 09:35:22.470: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:35:29.877: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6164" for this suite.
Jul 25 09:35:35.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:35:35.985: INFO: namespace pods-6164 deletion completed in 6.103800847s

• [SLOW TEST:13.598 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:35:35.985: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 25 09:35:36.014: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:35:39.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-4485" for this suite.
Jul 25 09:36:01.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:36:01.950: INFO: namespace init-container-4485 deletion completed in 22.10057s

• [SLOW TEST:25.965 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:36:01.951: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul 25 09:36:02.030: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 25 09:36:02.040: INFO: Waiting for terminating namespaces to be deleted...
Jul 25 09:36:02.043: INFO: 
Logging pods the kubelet thinks is on node essentialpks-conformance-2 before test
Jul 25 09:36:02.051: INFO: kube-proxy-x8ptf from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Jul 25 09:36:02.051: INFO: 	Container kube-proxy ready: true, restart count 4
Jul 25 09:36:02.051: INFO: kube-flannel-ds-amd64-c4rpf from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Jul 25 09:36:02.051: INFO: 	Container kube-flannel ready: true, restart count 4
Jul 25 09:36:02.051: INFO: sonobuoy-systemd-logs-daemon-set-8c8b567fc9ec4bcc-w8s5w from heptio-sonobuoy started at 2019-07-25 08:48:15 +0000 UTC (2 container statuses recorded)
Jul 25 09:36:02.051: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 25 09:36:02.051: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 25 09:36:02.051: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-25 08:47:51 +0000 UTC (1 container statuses recorded)
Jul 25 09:36:02.051: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 25 09:36:02.051: INFO: 
Logging pods the kubelet thinks is on node essentialpks-conformance-3 before test
Jul 25 09:36:02.059: INFO: kube-flannel-ds-amd64-5d5mf from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Jul 25 09:36:02.059: INFO: 	Container kube-flannel ready: true, restart count 4
Jul 25 09:36:02.059: INFO: kube-proxy-h9j6t from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Jul 25 09:36:02.059: INFO: 	Container kube-proxy ready: true, restart count 4
Jul 25 09:36:02.059: INFO: sonobuoy-e2e-job-de16814bb8f0492a from heptio-sonobuoy started at 2019-07-25 08:48:14 +0000 UTC (2 container statuses recorded)
Jul 25 09:36:02.059: INFO: 	Container e2e ready: true, restart count 0
Jul 25 09:36:02.059: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 25 09:36:02.059: INFO: sonobuoy-systemd-logs-daemon-set-8c8b567fc9ec4bcc-lld4v from heptio-sonobuoy started at 2019-07-25 08:48:15 +0000 UTC (2 container statuses recorded)
Jul 25 09:36:02.059: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 25 09:36:02.059: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-b2c3d5e3-b6a6-4be7-894e-75ed59c9ac01 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-b2c3d5e3-b6a6-4be7-894e-75ed59c9ac01 off the node essentialpks-conformance-2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-b2c3d5e3-b6a6-4be7-894e-75ed59c9ac01
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:36:06.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4668" for this suite.
Jul 25 09:36:24.149: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:36:24.222: INFO: namespace sched-pred-4668 deletion completed in 18.083560094s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:22.272 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:36:24.223: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name secret-emptykey-test-99071703-482a-481f-a1b1-5756a4152ebb
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:36:24.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5785" for this suite.
Jul 25 09:36:30.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:36:30.353: INFO: namespace secrets-5785 deletion completed in 6.099770366s

• [SLOW TEST:6.130 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:36:30.357: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 25 09:36:30.394: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jul 25 09:36:35.398: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 25 09:36:35.399: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jul 25 09:36:37.403: INFO: Creating deployment "test-rollover-deployment"
Jul 25 09:36:37.412: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jul 25 09:36:39.423: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jul 25 09:36:39.429: INFO: Ensure that both replica sets have 1 created replica
Jul 25 09:36:39.436: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jul 25 09:36:39.447: INFO: Updating deployment test-rollover-deployment
Jul 25 09:36:39.447: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jul 25 09:36:41.461: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jul 25 09:36:41.468: INFO: Make sure deployment "test-rollover-deployment" is complete
Jul 25 09:36:41.473: INFO: all replica sets need to contain the pod-template-hash label
Jul 25 09:36:41.474: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699644197, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699644197, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699644200, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699644197, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 25 09:36:43.481: INFO: all replica sets need to contain the pod-template-hash label
Jul 25 09:36:43.481: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699644197, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699644197, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699644200, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699644197, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 25 09:36:45.482: INFO: all replica sets need to contain the pod-template-hash label
Jul 25 09:36:45.482: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699644197, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699644197, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699644200, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699644197, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 25 09:36:47.482: INFO: all replica sets need to contain the pod-template-hash label
Jul 25 09:36:47.482: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699644197, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699644197, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699644200, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699644197, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 25 09:36:49.484: INFO: all replica sets need to contain the pod-template-hash label
Jul 25 09:36:49.484: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699644197, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699644197, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699644200, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699644197, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-854595fc44\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 25 09:36:51.482: INFO: 
Jul 25 09:36:51.482: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 25 09:36:51.490: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment,GenerateName:,Namespace:deployment-1321,SelfLink:/apis/apps/v1/namespaces/deployment-1321/deployments/test-rollover-deployment,UID:ad84164b-2e5e-4d54-8e7d-cc7d77b7f799,ResourceVersion:65220,Generation:2,CreationTimestamp:2019-07-25 09:36:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-25 09:36:37 +0000 UTC 2019-07-25 09:36:37 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-25 09:36:50 +0000 UTC 2019-07-25 09:36:37 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rollover-deployment-854595fc44" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul 25 09:36:51.492: INFO: New ReplicaSet "test-rollover-deployment-854595fc44" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44,GenerateName:,Namespace:deployment-1321,SelfLink:/apis/apps/v1/namespaces/deployment-1321/replicasets/test-rollover-deployment-854595fc44,UID:e0eefcce-1c53-44f8-a939-2fa75db15e40,ResourceVersion:65209,Generation:2,CreationTimestamp:2019-07-25 09:36:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment ad84164b-2e5e-4d54-8e7d-cc7d77b7f799 0xc0025a2a97 0xc0025a2a98}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 25 09:36:51.492: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jul 25 09:36:51.492: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-controller,GenerateName:,Namespace:deployment-1321,SelfLink:/apis/apps/v1/namespaces/deployment-1321/replicasets/test-rollover-controller,UID:f34013a4-4bbf-40a0-9302-4eb9dd115705,ResourceVersion:65219,Generation:2,CreationTimestamp:2019-07-25 09:36:30 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment ad84164b-2e5e-4d54-8e7d-cc7d77b7f799 0xc0025a29c7 0xc0025a29c8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 25 09:36:51.493: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-9b8b997cf,GenerateName:,Namespace:deployment-1321,SelfLink:/apis/apps/v1/namespaces/deployment-1321/replicasets/test-rollover-deployment-9b8b997cf,UID:3d77743f-716d-4246-ae39-163233f10810,ResourceVersion:65185,Generation:2,CreationTimestamp:2019-07-25 09:36:37 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-rollover-deployment ad84164b-2e5e-4d54-8e7d-cc7d77b7f799 0xc0025a2b60 0xc0025a2b61}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 9b8b997cf,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 25 09:36:51.497: INFO: Pod "test-rollover-deployment-854595fc44-kbr6h" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rollover-deployment-854595fc44-kbr6h,GenerateName:test-rollover-deployment-854595fc44-,Namespace:deployment-1321,SelfLink:/api/v1/namespaces/deployment-1321/pods/test-rollover-deployment-854595fc44-kbr6h,UID:65109b6c-a6fd-40cd-9fe7-dbeb1ee4b9dc,ResourceVersion:65190,Generation:0,CreationTimestamp:2019-07-25 09:36:39 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: rollover-pod,pod-template-hash: 854595fc44,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rollover-deployment-854595fc44 e0eefcce-1c53-44f8-a939-2fa75db15e40 0xc0025a3787 0xc0025a3788}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-c6km7 {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-c6km7,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-c6km7 true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc0025a37f0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc0025a3810}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:36:39 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:36:40 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:36:40 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:36:39 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:10.244.2.55,StartTime:2019-07-25 09:36:39 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-25 09:36:40 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://4937675caabf201fa67127d8d57c67595d7cbb1d6d7718e6110f78adf0589f87}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:36:51.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1321" for this suite.
Jul 25 09:36:57.511: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:36:57.601: INFO: namespace deployment-1321 deletion completed in 6.099460369s

• [SLOW TEST:27.244 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:36:57.601: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override all
Jul 25 09:36:57.639: INFO: Waiting up to 5m0s for pod "client-containers-3605d086-556e-4699-93d3-efb157340749" in namespace "containers-4725" to be "success or failure"
Jul 25 09:36:57.645: INFO: Pod "client-containers-3605d086-556e-4699-93d3-efb157340749": Phase="Pending", Reason="", readiness=false. Elapsed: 6.294272ms
Jul 25 09:36:59.648: INFO: Pod "client-containers-3605d086-556e-4699-93d3-efb157340749": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009507221s
STEP: Saw pod success
Jul 25 09:36:59.649: INFO: Pod "client-containers-3605d086-556e-4699-93d3-efb157340749" satisfied condition "success or failure"
Jul 25 09:36:59.651: INFO: Trying to get logs from node essentialpks-conformance-2 pod client-containers-3605d086-556e-4699-93d3-efb157340749 container test-container: <nil>
STEP: delete the pod
Jul 25 09:36:59.669: INFO: Waiting for pod client-containers-3605d086-556e-4699-93d3-efb157340749 to disappear
Jul 25 09:36:59.672: INFO: Pod client-containers-3605d086-556e-4699-93d3-efb157340749 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:36:59.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-4725" for this suite.
Jul 25 09:37:05.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:37:05.759: INFO: namespace containers-4725 deletion completed in 6.083557884s

• [SLOW TEST:8.158 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:37:05.759: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 25 09:37:05.788: INFO: Waiting up to 5m0s for pod "downwardapi-volume-67a2a07b-6b7a-4942-894a-4c24eee8a588" in namespace "downward-api-1773" to be "success or failure"
Jul 25 09:37:05.791: INFO: Pod "downwardapi-volume-67a2a07b-6b7a-4942-894a-4c24eee8a588": Phase="Pending", Reason="", readiness=false. Elapsed: 2.198695ms
Jul 25 09:37:07.794: INFO: Pod "downwardapi-volume-67a2a07b-6b7a-4942-894a-4c24eee8a588": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005783369s
STEP: Saw pod success
Jul 25 09:37:07.794: INFO: Pod "downwardapi-volume-67a2a07b-6b7a-4942-894a-4c24eee8a588" satisfied condition "success or failure"
Jul 25 09:37:07.797: INFO: Trying to get logs from node essentialpks-conformance-3 pod downwardapi-volume-67a2a07b-6b7a-4942-894a-4c24eee8a588 container client-container: <nil>
STEP: delete the pod
Jul 25 09:37:07.812: INFO: Waiting for pod downwardapi-volume-67a2a07b-6b7a-4942-894a-4c24eee8a588 to disappear
Jul 25 09:37:07.815: INFO: Pod downwardapi-volume-67a2a07b-6b7a-4942-894a-4c24eee8a588 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:37:07.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1773" for this suite.
Jul 25 09:37:13.828: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:37:13.919: INFO: namespace downward-api-1773 deletion completed in 6.101063391s

• [SLOW TEST:8.160 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:37:13.920: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1686
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 25 09:37:14.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 run e2e-test-nginx-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-466'
Jul 25 09:37:14.102: INFO: stderr: ""
Jul 25 09:37:14.102: INFO: stdout: "pod/e2e-test-nginx-pod created\n"
STEP: verifying the pod e2e-test-nginx-pod was created
[AfterEach] [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1691
Jul 25 09:37:14.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 delete pods e2e-test-nginx-pod --namespace=kubectl-466'
Jul 25 09:37:19.874: INFO: stderr: ""
Jul 25 09:37:19.874: INFO: stdout: "pod \"e2e-test-nginx-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:37:19.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-466" for this suite.
Jul 25 09:37:25.894: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:37:25.988: INFO: namespace kubectl-466 deletion completed in 6.104684479s

• [SLOW TEST:12.068 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:37:25.994: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap that has name configmap-test-emptyKey-3c5bd5c1-8ef8-45c7-8fb1-c4afe5b3ace9
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:37:26.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5618" for this suite.
Jul 25 09:37:32.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:37:32.128: INFO: namespace configmap-5618 deletion completed in 6.104368806s

• [SLOW TEST:6.135 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:31
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:37:32.135: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test env composition
Jul 25 09:37:32.171: INFO: Waiting up to 5m0s for pod "var-expansion-24f7153e-60d0-4754-8a8e-24f5e2f35d81" in namespace "var-expansion-202" to be "success or failure"
Jul 25 09:37:32.177: INFO: Pod "var-expansion-24f7153e-60d0-4754-8a8e-24f5e2f35d81": Phase="Pending", Reason="", readiness=false. Elapsed: 5.88913ms
Jul 25 09:37:34.182: INFO: Pod "var-expansion-24f7153e-60d0-4754-8a8e-24f5e2f35d81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010989922s
STEP: Saw pod success
Jul 25 09:37:34.182: INFO: Pod "var-expansion-24f7153e-60d0-4754-8a8e-24f5e2f35d81" satisfied condition "success or failure"
Jul 25 09:37:34.185: INFO: Trying to get logs from node essentialpks-conformance-3 pod var-expansion-24f7153e-60d0-4754-8a8e-24f5e2f35d81 container dapi-container: <nil>
STEP: delete the pod
Jul 25 09:37:34.219: INFO: Waiting for pod var-expansion-24f7153e-60d0-4754-8a8e-24f5e2f35d81 to disappear
Jul 25 09:37:34.221: INFO: Pod var-expansion-24f7153e-60d0-4754-8a8e-24f5e2f35d81 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:37:34.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-202" for this suite.
Jul 25 09:37:40.236: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:37:40.329: INFO: namespace var-expansion-202 deletion completed in 6.104920955s

• [SLOW TEST:8.194 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:37:40.329: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 25 09:37:40.365: INFO: Waiting up to 5m0s for pod "pod-357427db-dd72-4ad1-aa85-5704a5ff0caf" in namespace "emptydir-4329" to be "success or failure"
Jul 25 09:37:40.371: INFO: Pod "pod-357427db-dd72-4ad1-aa85-5704a5ff0caf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.625848ms
Jul 25 09:37:42.374: INFO: Pod "pod-357427db-dd72-4ad1-aa85-5704a5ff0caf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00898902s
STEP: Saw pod success
Jul 25 09:37:42.374: INFO: Pod "pod-357427db-dd72-4ad1-aa85-5704a5ff0caf" satisfied condition "success or failure"
Jul 25 09:37:42.377: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-357427db-dd72-4ad1-aa85-5704a5ff0caf container test-container: <nil>
STEP: delete the pod
Jul 25 09:37:42.396: INFO: Waiting for pod pod-357427db-dd72-4ad1-aa85-5704a5ff0caf to disappear
Jul 25 09:37:42.398: INFO: Pod pod-357427db-dd72-4ad1-aa85-5704a5ff0caf no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:37:42.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4329" for this suite.
Jul 25 09:37:48.415: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:37:48.487: INFO: namespace emptydir-4329 deletion completed in 6.082965031s

• [SLOW TEST:8.158 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:37:48.487: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-configmap-bk8m
STEP: Creating a pod to test atomic-volume-subpath
Jul 25 09:37:48.521: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-bk8m" in namespace "subpath-793" to be "success or failure"
Jul 25 09:37:48.525: INFO: Pod "pod-subpath-test-configmap-bk8m": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046083ms
Jul 25 09:37:50.528: INFO: Pod "pod-subpath-test-configmap-bk8m": Phase="Running", Reason="", readiness=true. Elapsed: 2.007324817s
Jul 25 09:37:52.532: INFO: Pod "pod-subpath-test-configmap-bk8m": Phase="Running", Reason="", readiness=true. Elapsed: 4.011496938s
Jul 25 09:37:54.536: INFO: Pod "pod-subpath-test-configmap-bk8m": Phase="Running", Reason="", readiness=true. Elapsed: 6.01546827s
Jul 25 09:37:56.541: INFO: Pod "pod-subpath-test-configmap-bk8m": Phase="Running", Reason="", readiness=true. Elapsed: 8.019676458s
Jul 25 09:37:58.544: INFO: Pod "pod-subpath-test-configmap-bk8m": Phase="Running", Reason="", readiness=true. Elapsed: 10.023247796s
Jul 25 09:38:00.550: INFO: Pod "pod-subpath-test-configmap-bk8m": Phase="Running", Reason="", readiness=true. Elapsed: 12.029560958s
Jul 25 09:38:02.554: INFO: Pod "pod-subpath-test-configmap-bk8m": Phase="Running", Reason="", readiness=true. Elapsed: 14.032786446s
Jul 25 09:38:04.557: INFO: Pod "pod-subpath-test-configmap-bk8m": Phase="Running", Reason="", readiness=true. Elapsed: 16.035881673s
Jul 25 09:38:06.561: INFO: Pod "pod-subpath-test-configmap-bk8m": Phase="Running", Reason="", readiness=true. Elapsed: 18.039696132s
Jul 25 09:38:08.566: INFO: Pod "pod-subpath-test-configmap-bk8m": Phase="Running", Reason="", readiness=true. Elapsed: 20.044755072s
Jul 25 09:38:10.570: INFO: Pod "pod-subpath-test-configmap-bk8m": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.049249931s
STEP: Saw pod success
Jul 25 09:38:10.570: INFO: Pod "pod-subpath-test-configmap-bk8m" satisfied condition "success or failure"
Jul 25 09:38:10.573: INFO: Trying to get logs from node essentialpks-conformance-3 pod pod-subpath-test-configmap-bk8m container test-container-subpath-configmap-bk8m: <nil>
STEP: delete the pod
Jul 25 09:38:10.596: INFO: Waiting for pod pod-subpath-test-configmap-bk8m to disappear
Jul 25 09:38:10.598: INFO: Pod pod-subpath-test-configmap-bk8m no longer exists
STEP: Deleting pod pod-subpath-test-configmap-bk8m
Jul 25 09:38:10.598: INFO: Deleting pod "pod-subpath-test-configmap-bk8m" in namespace "subpath-793"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:38:10.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-793" for this suite.
Jul 25 09:38:16.612: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:38:16.698: INFO: namespace subpath-793 deletion completed in 6.094736773s

• [SLOW TEST:28.210 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:38:16.698: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 25 09:38:18.811: INFO: Waiting up to 5m0s for pod "client-envvars-8de34774-965f-4760-a357-8170bcc25c21" in namespace "pods-7248" to be "success or failure"
Jul 25 09:38:18.820: INFO: Pod "client-envvars-8de34774-965f-4760-a357-8170bcc25c21": Phase="Pending", Reason="", readiness=false. Elapsed: 9.503741ms
Jul 25 09:38:20.824: INFO: Pod "client-envvars-8de34774-965f-4760-a357-8170bcc25c21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01342973s
STEP: Saw pod success
Jul 25 09:38:20.824: INFO: Pod "client-envvars-8de34774-965f-4760-a357-8170bcc25c21" satisfied condition "success or failure"
Jul 25 09:38:20.827: INFO: Trying to get logs from node essentialpks-conformance-3 pod client-envvars-8de34774-965f-4760-a357-8170bcc25c21 container env3cont: <nil>
STEP: delete the pod
Jul 25 09:38:20.843: INFO: Waiting for pod client-envvars-8de34774-965f-4760-a357-8170bcc25c21 to disappear
Jul 25 09:38:20.845: INFO: Pod client-envvars-8de34774-965f-4760-a357-8170bcc25c21 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:38:20.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7248" for this suite.
Jul 25 09:39:00.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:39:00.931: INFO: namespace pods-7248 deletion completed in 40.081998604s

• [SLOW TEST:44.234 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:39:00.931: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Jul 25 09:39:00.984: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 09:39:00.986: INFO: Number of nodes with available pods: 0
Jul 25 09:39:00.986: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 09:39:01.991: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 09:39:01.994: INFO: Number of nodes with available pods: 0
Jul 25 09:39:01.994: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 09:39:02.991: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 09:39:02.999: INFO: Number of nodes with available pods: 2
Jul 25 09:39:02.999: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Stop a daemon pod, check that the daemon pod is revived.
Jul 25 09:39:03.014: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 09:39:03.017: INFO: Number of nodes with available pods: 1
Jul 25 09:39:03.017: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 09:39:04.022: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 09:39:04.025: INFO: Number of nodes with available pods: 1
Jul 25 09:39:04.025: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 09:39:05.022: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 09:39:05.025: INFO: Number of nodes with available pods: 1
Jul 25 09:39:05.025: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 09:39:06.022: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 09:39:06.025: INFO: Number of nodes with available pods: 1
Jul 25 09:39:06.025: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 09:39:07.024: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 09:39:07.027: INFO: Number of nodes with available pods: 1
Jul 25 09:39:07.028: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 09:39:08.022: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 09:39:08.026: INFO: Number of nodes with available pods: 1
Jul 25 09:39:08.026: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 09:39:09.022: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 09:39:09.025: INFO: Number of nodes with available pods: 1
Jul 25 09:39:09.025: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 09:39:10.026: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 09:39:10.031: INFO: Number of nodes with available pods: 1
Jul 25 09:39:10.031: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 09:39:11.022: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 09:39:11.025: INFO: Number of nodes with available pods: 2
Jul 25 09:39:11.025: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9740, will wait for the garbage collector to delete the pods
Jul 25 09:39:11.085: INFO: Deleting DaemonSet.extensions daemon-set took: 4.900708ms
Jul 25 09:39:11.385: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.185073ms
Jul 25 09:39:19.888: INFO: Number of nodes with available pods: 0
Jul 25 09:39:19.888: INFO: Number of running nodes: 0, number of available pods: 0
Jul 25 09:39:19.892: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-9740/daemonsets","resourceVersion":"65725"},"items":null}

Jul 25 09:39:19.895: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-9740/pods","resourceVersion":"65725"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:39:19.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9740" for this suite.
Jul 25 09:39:25.917: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:39:25.993: INFO: namespace daemonsets-9740 deletion completed in 6.085132245s

• [SLOW TEST:25.062 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:39:25.994: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir volume type on node default medium
Jul 25 09:39:26.071: INFO: Waiting up to 5m0s for pod "pod-bd70f6a8-390f-4be0-8b83-79f47f462a10" in namespace "emptydir-28" to be "success or failure"
Jul 25 09:39:26.075: INFO: Pod "pod-bd70f6a8-390f-4be0-8b83-79f47f462a10": Phase="Pending", Reason="", readiness=false. Elapsed: 3.712288ms
Jul 25 09:39:28.080: INFO: Pod "pod-bd70f6a8-390f-4be0-8b83-79f47f462a10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009010013s
STEP: Saw pod success
Jul 25 09:39:28.081: INFO: Pod "pod-bd70f6a8-390f-4be0-8b83-79f47f462a10" satisfied condition "success or failure"
Jul 25 09:39:28.083: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-bd70f6a8-390f-4be0-8b83-79f47f462a10 container test-container: <nil>
STEP: delete the pod
Jul 25 09:39:28.106: INFO: Waiting for pod pod-bd70f6a8-390f-4be0-8b83-79f47f462a10 to disappear
Jul 25 09:39:28.109: INFO: Pod pod-bd70f6a8-390f-4be0-8b83-79f47f462a10 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:39:28.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-28" for this suite.
Jul 25 09:39:34.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:39:34.202: INFO: namespace emptydir-28 deletion completed in 6.088259534s

• [SLOW TEST:8.208 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:39:34.202: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on node default medium
Jul 25 09:39:34.233: INFO: Waiting up to 5m0s for pod "pod-8ed4ad88-9123-4788-95cd-4006e0e4eff3" in namespace "emptydir-9819" to be "success or failure"
Jul 25 09:39:34.241: INFO: Pod "pod-8ed4ad88-9123-4788-95cd-4006e0e4eff3": Phase="Pending", Reason="", readiness=false. Elapsed: 7.338024ms
Jul 25 09:39:36.244: INFO: Pod "pod-8ed4ad88-9123-4788-95cd-4006e0e4eff3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.01048619s
STEP: Saw pod success
Jul 25 09:39:36.244: INFO: Pod "pod-8ed4ad88-9123-4788-95cd-4006e0e4eff3" satisfied condition "success or failure"
Jul 25 09:39:36.247: INFO: Trying to get logs from node essentialpks-conformance-3 pod pod-8ed4ad88-9123-4788-95cd-4006e0e4eff3 container test-container: <nil>
STEP: delete the pod
Jul 25 09:39:36.263: INFO: Waiting for pod pod-8ed4ad88-9123-4788-95cd-4006e0e4eff3 to disappear
Jul 25 09:39:36.265: INFO: Pod pod-8ed4ad88-9123-4788-95cd-4006e0e4eff3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:39:36.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9819" for this suite.
Jul 25 09:39:42.275: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:39:42.353: INFO: namespace emptydir-9819 deletion completed in 6.085549856s

• [SLOW TEST:8.151 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:39:42.353: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-c1b7481b-2573-41b2-a539-d6a21e8b42d2
STEP: Creating a pod to test consume secrets
Jul 25 09:39:42.392: INFO: Waiting up to 5m0s for pod "pod-secrets-c35cce85-7b9c-41bb-9f03-ea70b999db86" in namespace "secrets-5012" to be "success or failure"
Jul 25 09:39:42.400: INFO: Pod "pod-secrets-c35cce85-7b9c-41bb-9f03-ea70b999db86": Phase="Pending", Reason="", readiness=false. Elapsed: 8.000005ms
Jul 25 09:39:44.405: INFO: Pod "pod-secrets-c35cce85-7b9c-41bb-9f03-ea70b999db86": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012440467s
STEP: Saw pod success
Jul 25 09:39:44.405: INFO: Pod "pod-secrets-c35cce85-7b9c-41bb-9f03-ea70b999db86" satisfied condition "success or failure"
Jul 25 09:39:44.408: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-secrets-c35cce85-7b9c-41bb-9f03-ea70b999db86 container secret-volume-test: <nil>
STEP: delete the pod
Jul 25 09:39:44.425: INFO: Waiting for pod pod-secrets-c35cce85-7b9c-41bb-9f03-ea70b999db86 to disappear
Jul 25 09:39:44.427: INFO: Pod pod-secrets-c35cce85-7b9c-41bb-9f03-ea70b999db86 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:39:44.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5012" for this suite.
Jul 25 09:39:50.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:39:50.521: INFO: namespace secrets-5012 deletion completed in 6.090062812s

• [SLOW TEST:8.168 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:39:50.521: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Jul 25 09:39:56.585: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:39:56.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0725 09:39:56.585280      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-1743" for this suite.
Jul 25 09:40:02.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:40:02.680: INFO: namespace gc-1743 deletion completed in 6.091312798s

• [SLOW TEST:12.158 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:40:02.682: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Jul 25 09:40:04.727: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec pod-sharedvolume-63d8b094-302b-4b94-a215-43b5e284cd05 -c busybox-main-container --namespace=emptydir-9731 -- cat /usr/share/volumeshare/shareddata.txt'
Jul 25 09:40:05.083: INFO: stderr: ""
Jul 25 09:40:05.083: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:40:05.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9731" for this suite.
Jul 25 09:40:11.097: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:40:11.176: INFO: namespace emptydir-9731 deletion completed in 6.089267375s

• [SLOW TEST:8.494 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:40:11.184: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 25 09:40:13.229: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:40:13.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1959" for this suite.
Jul 25 09:40:19.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:40:19.348: INFO: namespace container-runtime-1959 deletion completed in 6.103540923s

• [SLOW TEST:8.165 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:40:19.349: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Jul 25 09:40:49.426: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:40:49.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W0725 09:40:49.426648      16 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-3101" for this suite.
Jul 25 09:40:55.448: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:40:55.543: INFO: namespace gc-3101 deletion completed in 6.11152708s

• [SLOW TEST:36.195 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:40:55.545: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 25 09:40:55.572: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Jul 25 09:40:56.596: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:40:56.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8887" for this suite.
Jul 25 09:41:02.615: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:41:02.691: INFO: namespace replication-controller-8887 deletion completed in 6.088521092s

• [SLOW TEST:7.146 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:41:02.691: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-60387c94-3005-47e5-a6e1-907c598c58aa
STEP: Creating a pod to test consume configMaps
Jul 25 09:41:02.723: INFO: Waiting up to 5m0s for pod "pod-configmaps-f93b805d-ad0b-4e08-b093-8a4c3e6f7359" in namespace "configmap-2410" to be "success or failure"
Jul 25 09:41:02.729: INFO: Pod "pod-configmaps-f93b805d-ad0b-4e08-b093-8a4c3e6f7359": Phase="Pending", Reason="", readiness=false. Elapsed: 6.428058ms
Jul 25 09:41:04.733: INFO: Pod "pod-configmaps-f93b805d-ad0b-4e08-b093-8a4c3e6f7359": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010211179s
STEP: Saw pod success
Jul 25 09:41:04.733: INFO: Pod "pod-configmaps-f93b805d-ad0b-4e08-b093-8a4c3e6f7359" satisfied condition "success or failure"
Jul 25 09:41:04.736: INFO: Trying to get logs from node essentialpks-conformance-3 pod pod-configmaps-f93b805d-ad0b-4e08-b093-8a4c3e6f7359 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 25 09:41:04.752: INFO: Waiting for pod pod-configmaps-f93b805d-ad0b-4e08-b093-8a4c3e6f7359 to disappear
Jul 25 09:41:04.754: INFO: Pod pod-configmaps-f93b805d-ad0b-4e08-b093-8a4c3e6f7359 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:41:04.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2410" for this suite.
Jul 25 09:41:10.767: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:41:10.854: INFO: namespace configmap-2410 deletion completed in 6.096576136s

• [SLOW TEST:8.163 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:41:10.854: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-1443
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-1443
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1443
Jul 25 09:41:10.898: INFO: Found 0 stateful pods, waiting for 1
Jul 25 09:41:20.902: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Jul 25 09:41:20.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-1443 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 25 09:41:21.162: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 25 09:41:21.162: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 25 09:41:21.162: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 25 09:41:21.168: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jul 25 09:41:31.172: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 25 09:41:31.172: INFO: Waiting for statefulset status.replicas updated to 0
Jul 25 09:41:31.183: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999698s
Jul 25 09:41:32.187: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997533297s
Jul 25 09:41:33.191: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.992686709s
Jul 25 09:41:34.196: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.988510897s
Jul 25 09:41:35.199: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.984177419s
Jul 25 09:41:36.205: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.980439761s
Jul 25 09:41:37.209: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.97525366s
Jul 25 09:41:38.212: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.970728277s
Jul 25 09:41:39.217: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.967510954s
Jul 25 09:41:40.223: INFO: Verifying statefulset ss doesn't scale past 1 for another 963.034329ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1443
Jul 25 09:41:41.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-1443 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:41:41.433: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 25 09:41:41.433: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 25 09:41:41.433: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 25 09:41:41.440: INFO: Found 1 stateful pods, waiting for 3
Jul 25 09:41:51.443: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jul 25 09:41:51.443: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jul 25 09:41:51.443: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Jul 25 09:41:51.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-1443 ss-0 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 25 09:41:51.648: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 25 09:41:51.648: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 25 09:41:51.648: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-0: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 25 09:41:51.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-1443 ss-1 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 25 09:41:51.894: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 25 09:41:51.894: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 25 09:41:51.894: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-1: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 25 09:41:51.894: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-1443 ss-2 -- /bin/sh -x -c mv -v /usr/share/nginx/html/index.html /tmp/ || true'
Jul 25 09:41:52.120: INFO: stderr: "+ mv -v /usr/share/nginx/html/index.html /tmp/\n"
Jul 25 09:41:52.120: INFO: stdout: "'/usr/share/nginx/html/index.html' -> '/tmp/index.html'\n"
Jul 25 09:41:52.120: INFO: stdout of mv -v /usr/share/nginx/html/index.html /tmp/ || true on ss-2: '/usr/share/nginx/html/index.html' -> '/tmp/index.html'

Jul 25 09:41:52.120: INFO: Waiting for statefulset status.replicas updated to 0
Jul 25 09:41:52.130: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Jul 25 09:42:02.139: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jul 25 09:42:02.139: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jul 25 09:42:02.139: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jul 25 09:42:02.151: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999648s
Jul 25 09:42:03.155: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996054975s
Jul 25 09:42:04.159: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.992447428s
Jul 25 09:42:05.164: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988026312s
Jul 25 09:42:06.168: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.98287462s
Jul 25 09:42:07.172: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.978972722s
Jul 25 09:42:08.177: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.975458102s
Jul 25 09:42:09.181: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.969970205s
Jul 25 09:42:10.185: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.966360047s
Jul 25 09:42:11.190: INFO: Verifying statefulset ss doesn't scale past 3 for another 961.702811ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1443
Jul 25 09:42:12.195: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-1443 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:42:12.415: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 25 09:42:12.415: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 25 09:42:12.415: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-0: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 25 09:42:12.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-1443 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:42:12.609: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 25 09:42:12.609: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 25 09:42:12.609: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-1: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 25 09:42:12.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 exec --namespace=statefulset-1443 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/share/nginx/html/ || true'
Jul 25 09:42:12.818: INFO: stderr: "+ mv -v /tmp/index.html /usr/share/nginx/html/\n"
Jul 25 09:42:12.818: INFO: stdout: "'/tmp/index.html' -> '/usr/share/nginx/html/index.html'\n"
Jul 25 09:42:12.818: INFO: stdout of mv -v /tmp/index.html /usr/share/nginx/html/ || true on ss-2: '/tmp/index.html' -> '/usr/share/nginx/html/index.html'

Jul 25 09:42:12.818: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 25 09:42:22.834: INFO: Deleting all statefulset in ns statefulset-1443
Jul 25 09:42:22.836: INFO: Scaling statefulset ss to 0
Jul 25 09:42:22.844: INFO: Waiting for statefulset status.replicas updated to 0
Jul 25 09:42:22.846: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:42:22.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1443" for this suite.
Jul 25 09:42:28.883: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:42:28.957: INFO: namespace statefulset-1443 deletion completed in 6.092445044s

• [SLOW TEST:78.103 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:42:28.958: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-map-7dd15ed9-ef50-483c-8ee2-430c5b3bfb32
STEP: Creating a pod to test consume secrets
Jul 25 09:42:28.991: INFO: Waiting up to 5m0s for pod "pod-secrets-da8e817a-263f-4693-8f07-97cbdf586646" in namespace "secrets-4601" to be "success or failure"
Jul 25 09:42:29.010: INFO: Pod "pod-secrets-da8e817a-263f-4693-8f07-97cbdf586646": Phase="Pending", Reason="", readiness=false. Elapsed: 18.686843ms
Jul 25 09:42:31.014: INFO: Pod "pod-secrets-da8e817a-263f-4693-8f07-97cbdf586646": Phase="Running", Reason="", readiness=true. Elapsed: 2.02290976s
Jul 25 09:42:33.018: INFO: Pod "pod-secrets-da8e817a-263f-4693-8f07-97cbdf586646": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026461859s
STEP: Saw pod success
Jul 25 09:42:33.018: INFO: Pod "pod-secrets-da8e817a-263f-4693-8f07-97cbdf586646" satisfied condition "success or failure"
Jul 25 09:42:33.020: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-secrets-da8e817a-263f-4693-8f07-97cbdf586646 container secret-volume-test: <nil>
STEP: delete the pod
Jul 25 09:42:33.041: INFO: Waiting for pod pod-secrets-da8e817a-263f-4693-8f07-97cbdf586646 to disappear
Jul 25 09:42:33.043: INFO: Pod pod-secrets-da8e817a-263f-4693-8f07-97cbdf586646 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:42:33.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4601" for this suite.
Jul 25 09:42:39.059: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:42:39.140: INFO: namespace secrets-4601 deletion completed in 6.092169472s

• [SLOW TEST:10.182 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:42:39.141: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:43:01.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-4851" for this suite.
Jul 25 09:43:07.370: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:43:07.466: INFO: namespace container-runtime-4851 deletion completed in 6.106387463s

• [SLOW TEST:28.324 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    when starting a container that exits
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:43:07.466: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating Redis RC
Jul 25 09:43:07.550: INFO: namespace kubectl-8008
Jul 25 09:43:07.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 create -f - --namespace=kubectl-8008'
Jul 25 09:43:07.737: INFO: stderr: ""
Jul 25 09:43:07.737: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Jul 25 09:43:08.740: INFO: Selector matched 1 pods for map[app:redis]
Jul 25 09:43:08.740: INFO: Found 0 / 1
Jul 25 09:43:09.740: INFO: Selector matched 1 pods for map[app:redis]
Jul 25 09:43:09.740: INFO: Found 1 / 1
Jul 25 09:43:09.740: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jul 25 09:43:09.743: INFO: Selector matched 1 pods for map[app:redis]
Jul 25 09:43:09.743: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jul 25 09:43:09.743: INFO: wait on redis-master startup in kubectl-8008 
Jul 25 09:43:09.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 logs redis-master-7g2lt redis-master --namespace=kubectl-8008'
Jul 25 09:43:09.853: INFO: stderr: ""
Jul 25 09:43:09.853: INFO: stdout: "                _._                                                  \n           _.-``__ ''-._                                             \n      _.-``    `.  `_.  ''-._           Redis 3.2.12 (35a5711f/0) 64 bit\n  .-`` .-```.  ```\\/    _.,_ ''-._                                   \n (    '      ,       .-`  | `,    )     Running in standalone mode\n |`-._`-...-` __...-.``-._|'` _.-'|     Port: 6379\n |    `-._   `._    /     _.-'    |     PID: 1\n  `-._    `-._  `-./  _.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |           http://redis.io        \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n |`-._`-._    `-.__.-'    _.-'_.-'|                                  \n |    `-._`-._        _.-'_.-'    |                                  \n  `-._    `-._`-.__.-'_.-'    _.-'                                   \n      `-._    `-.__.-'    _.-'                                       \n          `-._        _.-'                                           \n              `-.__.-'                                               \n\n1:M 25 Jul 09:43:08.562 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 25 Jul 09:43:08.562 # Server started, Redis version 3.2.12\n1:M 25 Jul 09:43:08.562 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 25 Jul 09:43:08.562 * The server is now ready to accept connections on port 6379\n"
STEP: exposing RC
Jul 25 09:43:09.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-8008'
Jul 25 09:43:09.971: INFO: stderr: ""
Jul 25 09:43:09.971: INFO: stdout: "service/rm2 exposed\n"
Jul 25 09:43:09.979: INFO: Service rm2 in namespace kubectl-8008 found.
STEP: exposing service
Jul 25 09:43:11.983: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-8008'
Jul 25 09:43:12.094: INFO: stderr: ""
Jul 25 09:43:12.094: INFO: stdout: "service/rm3 exposed\n"
Jul 25 09:43:12.098: INFO: Service rm3 in namespace kubectl-8008 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:43:14.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8008" for this suite.
Jul 25 09:43:36.119: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:43:36.199: INFO: namespace kubectl-8008 deletion completed in 22.090964281s

• [SLOW TEST:28.733 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl expose
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create services for rc  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:43:36.199: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating replication controller my-hostname-basic-5348f506-c9ba-4589-bc4f-a00bd619c748
Jul 25 09:43:36.239: INFO: Pod name my-hostname-basic-5348f506-c9ba-4589-bc4f-a00bd619c748: Found 1 pods out of 1
Jul 25 09:43:36.239: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-5348f506-c9ba-4589-bc4f-a00bd619c748" are running
Jul 25 09:43:38.247: INFO: Pod "my-hostname-basic-5348f506-c9ba-4589-bc4f-a00bd619c748-mv7wx" is running (conditions: [{Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-07-25 09:43:36 +0000 UTC Reason: Message:}])
Jul 25 09:43:38.248: INFO: Trying to dial the pod
Jul 25 09:43:43.256: INFO: Controller my-hostname-basic-5348f506-c9ba-4589-bc4f-a00bd619c748: Got expected result from replica 1 [my-hostname-basic-5348f506-c9ba-4589-bc4f-a00bd619c748-mv7wx]: "my-hostname-basic-5348f506-c9ba-4589-bc4f-a00bd619c748-mv7wx", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:43:43.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-398" for this suite.
Jul 25 09:43:49.274: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:43:49.361: INFO: namespace replication-controller-398 deletion completed in 6.100955429s

• [SLOW TEST:13.162 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:43:49.362: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1613
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 25 09:43:49.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 run e2e-test-nginx-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-7314'
Jul 25 09:43:49.495: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 25 09:43:49.495: INFO: stdout: "job.batch/e2e-test-nginx-job created\n"
STEP: verifying the job e2e-test-nginx-job was created
[AfterEach] [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1618
Jul 25 09:43:49.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 delete jobs e2e-test-nginx-job --namespace=kubectl-7314'
Jul 25 09:43:49.594: INFO: stderr: ""
Jul 25 09:43:49.594: INFO: stdout: "job.batch \"e2e-test-nginx-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:43:49.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7314" for this suite.
Jul 25 09:44:11.616: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:44:11.702: INFO: namespace kubectl-7314 deletion completed in 22.099842474s

• [SLOW TEST:22.340 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run job
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:44:11.711: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 25 09:44:11.754: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Jul 25 09:44:11.762: INFO: Number of nodes with available pods: 0
Jul 25 09:44:11.762: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Jul 25 09:44:11.781: INFO: Number of nodes with available pods: 0
Jul 25 09:44:11.781: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 09:44:12.784: INFO: Number of nodes with available pods: 0
Jul 25 09:44:12.784: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 09:44:13.785: INFO: Number of nodes with available pods: 0
Jul 25 09:44:13.785: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 09:44:14.784: INFO: Number of nodes with available pods: 1
Jul 25 09:44:14.784: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Jul 25 09:44:14.801: INFO: Number of nodes with available pods: 0
Jul 25 09:44:14.802: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Jul 25 09:44:14.821: INFO: Number of nodes with available pods: 0
Jul 25 09:44:14.821: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 09:44:15.827: INFO: Number of nodes with available pods: 0
Jul 25 09:44:15.827: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 09:44:16.825: INFO: Number of nodes with available pods: 0
Jul 25 09:44:16.825: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 09:44:17.825: INFO: Number of nodes with available pods: 0
Jul 25 09:44:17.825: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 09:44:18.824: INFO: Number of nodes with available pods: 0
Jul 25 09:44:18.824: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 09:44:19.825: INFO: Number of nodes with available pods: 1
Jul 25 09:44:19.825: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3065, will wait for the garbage collector to delete the pods
Jul 25 09:44:19.895: INFO: Deleting DaemonSet.extensions daemon-set took: 7.011796ms
Jul 25 09:44:20.195: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.381467ms
Jul 25 09:44:29.898: INFO: Number of nodes with available pods: 0
Jul 25 09:44:29.898: INFO: Number of running nodes: 0, number of available pods: 0
Jul 25 09:44:29.900: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-3065/daemonsets","resourceVersion":"67137"},"items":null}

Jul 25 09:44:29.901: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-3065/pods","resourceVersion":"67137"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:44:29.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3065" for this suite.
Jul 25 09:44:35.930: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:44:36.008: INFO: namespace daemonsets-3065 deletion completed in 6.087168497s

• [SLOW TEST:24.297 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:44:36.009: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 25 09:44:36.040: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:44:37.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-8932" for this suite.
Jul 25 09:44:43.106: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:44:43.187: INFO: namespace custom-resource-definition-8932 deletion completed in 6.095728653s

• [SLOW TEST:7.178 seconds]
[sig-api-machinery] CustomResourceDefinition resources
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:35
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:44:43.188: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on tmpfs
Jul 25 09:44:43.220: INFO: Waiting up to 5m0s for pod "pod-cd851662-034d-4d78-9200-987575f03a4d" in namespace "emptydir-4116" to be "success or failure"
Jul 25 09:44:43.230: INFO: Pod "pod-cd851662-034d-4d78-9200-987575f03a4d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.306561ms
Jul 25 09:44:45.233: INFO: Pod "pod-cd851662-034d-4d78-9200-987575f03a4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.013056751s
STEP: Saw pod success
Jul 25 09:44:45.233: INFO: Pod "pod-cd851662-034d-4d78-9200-987575f03a4d" satisfied condition "success or failure"
Jul 25 09:44:45.237: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-cd851662-034d-4d78-9200-987575f03a4d container test-container: <nil>
STEP: delete the pod
Jul 25 09:44:45.260: INFO: Waiting for pod pod-cd851662-034d-4d78-9200-987575f03a4d to disappear
Jul 25 09:44:45.263: INFO: Pod pod-cd851662-034d-4d78-9200-987575f03a4d no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:44:45.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4116" for this suite.
Jul 25 09:44:51.278: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:44:51.360: INFO: namespace emptydir-4116 deletion completed in 6.092824828s

• [SLOW TEST:8.173 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:44:51.360: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name cm-test-opt-del-f34ef73e-6800-460b-a86a-2e79a7ddc370
STEP: Creating configMap with name cm-test-opt-upd-bf2af805-51c1-4486-a555-bc9bccc0bd60
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-f34ef73e-6800-460b-a86a-2e79a7ddc370
STEP: Updating configmap cm-test-opt-upd-bf2af805-51c1-4486-a555-bc9bccc0bd60
STEP: Creating configMap with name cm-test-opt-create-ce7ddf51-2325-4f3e-9722-b935f4a70787
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:44:57.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5813" for this suite.
Jul 25 09:45:19.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:45:19.610: INFO: namespace configmap-5813 deletion completed in 22.116788839s

• [SLOW TEST:28.249 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:45:19.610: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 25 09:45:19.667: INFO: Create a RollingUpdate DaemonSet
Jul 25 09:45:19.675: INFO: Check that daemon pods launch on every node of the cluster
Jul 25 09:45:19.679: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 09:45:19.686: INFO: Number of nodes with available pods: 0
Jul 25 09:45:19.686: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 09:45:20.690: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 09:45:20.692: INFO: Number of nodes with available pods: 0
Jul 25 09:45:20.692: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 09:45:21.690: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 09:45:21.693: INFO: Number of nodes with available pods: 2
Jul 25 09:45:21.693: INFO: Number of running nodes: 2, number of available pods: 2
Jul 25 09:45:21.693: INFO: Update the DaemonSet to trigger a rollout
Jul 25 09:45:21.701: INFO: Updating DaemonSet daemon-set
Jul 25 09:45:25.718: INFO: Roll back the DaemonSet before rollout is complete
Jul 25 09:45:25.728: INFO: Updating DaemonSet daemon-set
Jul 25 09:45:25.728: INFO: Make sure DaemonSet rollback is complete
Jul 25 09:45:25.733: INFO: Wrong image for pod: daemon-set-qs6rg. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 25 09:45:25.734: INFO: Pod daemon-set-qs6rg is not available
Jul 25 09:45:25.738: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 09:45:26.742: INFO: Wrong image for pod: daemon-set-qs6rg. Expected: docker.io/library/nginx:1.14-alpine, got: foo:non-existent.
Jul 25 09:45:26.742: INFO: Pod daemon-set-qs6rg is not available
Jul 25 09:45:26.746: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 09:45:27.741: INFO: Pod daemon-set-d5ppg is not available
Jul 25 09:45:27.744: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6446, will wait for the garbage collector to delete the pods
Jul 25 09:45:27.806: INFO: Deleting DaemonSet.extensions daemon-set took: 5.484285ms
Jul 25 09:45:28.107: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.307013ms
Jul 25 09:45:39.911: INFO: Number of nodes with available pods: 0
Jul 25 09:45:39.911: INFO: Number of running nodes: 0, number of available pods: 0
Jul 25 09:45:39.915: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-6446/daemonsets","resourceVersion":"67420"},"items":null}

Jul 25 09:45:39.919: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-6446/pods","resourceVersion":"67420"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:45:39.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6446" for this suite.
Jul 25 09:45:45.944: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:45:46.030: INFO: namespace daemonsets-6446 deletion completed in 6.095505402s

• [SLOW TEST:26.420 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:45:46.038: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:81
Jul 25 09:45:46.064: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jul 25 09:45:46.070: INFO: Waiting for terminating namespaces to be deleted...
Jul 25 09:45:46.072: INFO: 
Logging pods the kubelet thinks is on node essentialpks-conformance-2 before test
Jul 25 09:45:46.078: INFO: kube-flannel-ds-amd64-c4rpf from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Jul 25 09:45:46.078: INFO: 	Container kube-flannel ready: true, restart count 4
Jul 25 09:45:46.078: INFO: sonobuoy-systemd-logs-daemon-set-8c8b567fc9ec4bcc-w8s5w from heptio-sonobuoy started at 2019-07-25 08:48:15 +0000 UTC (2 container statuses recorded)
Jul 25 09:45:46.078: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 25 09:45:46.078: INFO: 	Container systemd-logs ready: true, restart count 0
Jul 25 09:45:46.078: INFO: sonobuoy from heptio-sonobuoy started at 2019-07-25 08:47:51 +0000 UTC (1 container statuses recorded)
Jul 25 09:45:46.078: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jul 25 09:45:46.078: INFO: kube-proxy-x8ptf from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Jul 25 09:45:46.078: INFO: 	Container kube-proxy ready: true, restart count 4
Jul 25 09:45:46.078: INFO: 
Logging pods the kubelet thinks is on node essentialpks-conformance-3 before test
Jul 25 09:45:46.083: INFO: kube-flannel-ds-amd64-5d5mf from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Jul 25 09:45:46.083: INFO: 	Container kube-flannel ready: true, restart count 4
Jul 25 09:45:46.083: INFO: kube-proxy-h9j6t from kube-system started at 2019-03-19 13:24:59 +0000 UTC (1 container statuses recorded)
Jul 25 09:45:46.083: INFO: 	Container kube-proxy ready: true, restart count 4
Jul 25 09:45:46.083: INFO: sonobuoy-e2e-job-de16814bb8f0492a from heptio-sonobuoy started at 2019-07-25 08:48:14 +0000 UTC (2 container statuses recorded)
Jul 25 09:45:46.083: INFO: 	Container e2e ready: true, restart count 0
Jul 25 09:45:46.083: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 25 09:45:46.083: INFO: sonobuoy-systemd-logs-daemon-set-8c8b567fc9ec4bcc-lld4v from heptio-sonobuoy started at 2019-07-25 08:48:15 +0000 UTC (2 container statuses recorded)
Jul 25 09:45:46.083: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jul 25 09:45:46.083: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: verifying the node has the label node essentialpks-conformance-2
STEP: verifying the node has the label node essentialpks-conformance-3
Jul 25 09:45:46.115: INFO: Pod sonobuoy requesting resource cpu=0m on Node essentialpks-conformance-2
Jul 25 09:45:46.115: INFO: Pod sonobuoy-e2e-job-de16814bb8f0492a requesting resource cpu=0m on Node essentialpks-conformance-3
Jul 25 09:45:46.115: INFO: Pod sonobuoy-systemd-logs-daemon-set-8c8b567fc9ec4bcc-lld4v requesting resource cpu=0m on Node essentialpks-conformance-3
Jul 25 09:45:46.115: INFO: Pod sonobuoy-systemd-logs-daemon-set-8c8b567fc9ec4bcc-w8s5w requesting resource cpu=0m on Node essentialpks-conformance-2
Jul 25 09:45:46.115: INFO: Pod kube-flannel-ds-amd64-5d5mf requesting resource cpu=100m on Node essentialpks-conformance-3
Jul 25 09:45:46.115: INFO: Pod kube-flannel-ds-amd64-c4rpf requesting resource cpu=100m on Node essentialpks-conformance-2
Jul 25 09:45:46.115: INFO: Pod kube-proxy-h9j6t requesting resource cpu=0m on Node essentialpks-conformance-3
Jul 25 09:45:46.115: INFO: Pod kube-proxy-x8ptf requesting resource cpu=0m on Node essentialpks-conformance-2
STEP: Starting Pods to consume most of the cluster CPU.
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0e5d9804-9923-442c-9876-38bb61c0ad95.15b49d520104561c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3534/filler-pod-0e5d9804-9923-442c-9876-38bb61c0ad95 to essentialpks-conformance-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0e5d9804-9923-442c-9876-38bb61c0ad95.15b49d5227a97e0d], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0e5d9804-9923-442c-9876-38bb61c0ad95.15b49d522b5d11ca], Reason = [Created], Message = [Created container filler-pod-0e5d9804-9923-442c-9876-38bb61c0ad95]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-0e5d9804-9923-442c-9876-38bb61c0ad95.15b49d523993cda0], Reason = [Started], Message = [Started container filler-pod-0e5d9804-9923-442c-9876-38bb61c0ad95]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b0adf9d0-0429-464f-83fc-5c97a58d9ba7.15b49d520086f5ca], Reason = [Scheduled], Message = [Successfully assigned sched-pred-3534/filler-pod-b0adf9d0-0429-464f-83fc-5c97a58d9ba7 to essentialpks-conformance-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b0adf9d0-0429-464f-83fc-5c97a58d9ba7.15b49d522a5222b6], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b0adf9d0-0429-464f-83fc-5c97a58d9ba7.15b49d522c9c1183], Reason = [Created], Message = [Created container filler-pod-b0adf9d0-0429-464f-83fc-5c97a58d9ba7]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-b0adf9d0-0429-464f-83fc-5c97a58d9ba7.15b49d5234a9a695], Reason = [Started], Message = [Started container filler-pod-b0adf9d0-0429-464f-83fc-5c97a58d9ba7]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15b49d5279a78489], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had taints that the pod didn't tolerate, 2 Insufficient cpu.]
STEP: removing the label node off the node essentialpks-conformance-3
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node essentialpks-conformance-2
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:45:49.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3534" for this suite.
Jul 25 09:45:55.207: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:45:55.302: INFO: namespace sched-pred-3534 deletion completed in 6.106327106s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:72

• [SLOW TEST:9.264 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:45:55.302: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 25 09:45:55.336: INFO: Waiting up to 5m0s for pod "downwardapi-volume-705efc9d-a2e5-41e1-83a5-4f61b7836b8c" in namespace "projected-1840" to be "success or failure"
Jul 25 09:45:55.346: INFO: Pod "downwardapi-volume-705efc9d-a2e5-41e1-83a5-4f61b7836b8c": Phase="Pending", Reason="", readiness=false. Elapsed: 9.376716ms
Jul 25 09:45:57.348: INFO: Pod "downwardapi-volume-705efc9d-a2e5-41e1-83a5-4f61b7836b8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011886578s
STEP: Saw pod success
Jul 25 09:45:57.348: INFO: Pod "downwardapi-volume-705efc9d-a2e5-41e1-83a5-4f61b7836b8c" satisfied condition "success or failure"
Jul 25 09:45:57.351: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-705efc9d-a2e5-41e1-83a5-4f61b7836b8c container client-container: <nil>
STEP: delete the pod
Jul 25 09:45:57.363: INFO: Waiting for pod downwardapi-volume-705efc9d-a2e5-41e1-83a5-4f61b7836b8c to disappear
Jul 25 09:45:57.364: INFO: Pod downwardapi-volume-705efc9d-a2e5-41e1-83a5-4f61b7836b8c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:45:57.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1840" for this suite.
Jul 25 09:46:03.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:46:03.449: INFO: namespace projected-1840 deletion completed in 6.082548987s

• [SLOW TEST:8.147 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:46:03.449: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on tmpfs
Jul 25 09:46:03.481: INFO: Waiting up to 5m0s for pod "pod-48eaaff8-2390-469a-a008-878530c55011" in namespace "emptydir-6906" to be "success or failure"
Jul 25 09:46:03.486: INFO: Pod "pod-48eaaff8-2390-469a-a008-878530c55011": Phase="Pending", Reason="", readiness=false. Elapsed: 5.056083ms
Jul 25 09:46:05.490: INFO: Pod "pod-48eaaff8-2390-469a-a008-878530c55011": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008297166s
STEP: Saw pod success
Jul 25 09:46:05.490: INFO: Pod "pod-48eaaff8-2390-469a-a008-878530c55011" satisfied condition "success or failure"
Jul 25 09:46:05.493: INFO: Trying to get logs from node essentialpks-conformance-3 pod pod-48eaaff8-2390-469a-a008-878530c55011 container test-container: <nil>
STEP: delete the pod
Jul 25 09:46:05.511: INFO: Waiting for pod pod-48eaaff8-2390-469a-a008-878530c55011 to disappear
Jul 25 09:46:05.513: INFO: Pod pod-48eaaff8-2390-469a-a008-878530c55011 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:46:05.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6906" for this suite.
Jul 25 09:46:11.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:46:11.618: INFO: namespace emptydir-6906 deletion completed in 6.101159388s

• [SLOW TEST:8.169 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:46:11.618: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Jul 25 09:46:13.665: INFO: &Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:send-events-00ffbbdb-6c27-443a-888c-336c3b16e4ac,GenerateName:,Namespace:events-559,SelfLink:/api/v1/namespaces/events-559/pods/send-events-00ffbbdb-6c27-443a-888c-336c3b16e4ac,UID:e9b4e285-22b3-4a14-95e8-168677252d82,ResourceVersion:67601,Generation:0,CreationTimestamp:2019-07-25 09:46:11 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: foo,time: 644190551,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-mpsrn {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-mpsrn,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{p gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 [] []  [{ 0 80 TCP }] [] [] {map[] map[]} [{default-token-mpsrn true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002f73490} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002f734b0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:46:11 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:46:13 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:46:13 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:46:11 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:10.244.2.86,StartTime:2019-07-25 09:46:11 +0000 UTC,ContainerStatuses:[{p {nil ContainerStateRunning{StartedAt:2019-07-25 09:46:12 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/serve-hostname:1.1 docker-pullable://gcr.io/kubernetes-e2e-test-images/serve-hostname@sha256:bab70473a6d8ef65a22625dc9a1b0f0452e811530fdbe77e4408523460177ff1 docker://8d2e1e747945a770a75234c8120687f7a546bd10f4c6585eb1cdeacec8f6989d}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}

STEP: checking for scheduler event about the pod
Jul 25 09:46:15.669: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Jul 25 09:46:17.676: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:46:17.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-559" for this suite.
Jul 25 09:47:01.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:47:01.778: INFO: namespace events-559 deletion completed in 44.089383645s

• [SLOW TEST:50.160 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:47:01.785: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 25 09:47:01.809: INFO: Creating deployment "test-recreate-deployment"
Jul 25 09:47:01.814: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jul 25 09:47:01.824: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jul 25 09:47:03.831: INFO: Waiting deployment "test-recreate-deployment" to complete
Jul 25 09:47:03.834: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jul 25 09:47:03.841: INFO: Updating deployment test-recreate-deployment
Jul 25 09:47:03.841: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 25 09:47:03.903: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment,GenerateName:,Namespace:deployment-9784,SelfLink:/apis/apps/v1/namespaces/deployment-9784/deployments/test-recreate-deployment,UID:1e12f69e-f384-4d2f-b792-6f91cba954af,ResourceVersion:67735,Generation:2,CreationTimestamp:2019-07-25 09:47:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{deployment.kubernetes.io/revision: 2,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[{Available False 2019-07-25 09:47:03 +0000 UTC 2019-07-25 09:47:03 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.} {Progressing True 2019-07-25 09:47:03 +0000 UTC 2019-07-25 09:47:01 +0000 UTC ReplicaSetUpdated ReplicaSet "test-recreate-deployment-5c8c9cc69d" is progressing.}],ReadyReplicas:0,CollisionCount:nil,},}

Jul 25 09:47:03.906: INFO: New ReplicaSet "test-recreate-deployment-5c8c9cc69d" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d,GenerateName:,Namespace:deployment-9784,SelfLink:/apis/apps/v1/namespaces/deployment-9784/replicasets/test-recreate-deployment-5c8c9cc69d,UID:13ff383d-6a4b-4877-86db-a617f4bb2490,ResourceVersion:67731,Generation:1,CreationTimestamp:2019-07-25 09:47:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 2,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 1e12f69e-f384-4d2f-b792-6f91cba954af 0xc0020701a7 0xc0020701a8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 25 09:47:03.906: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jul 25 09:47:03.907: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-6df85df6b9,GenerateName:,Namespace:deployment-9784,SelfLink:/apis/apps/v1/namespaces/deployment-9784/replicasets/test-recreate-deployment-6df85df6b9,UID:722775f0-efae-4cbe-8814-42fc18c8d762,ResourceVersion:67723,Generation:2,CreationTimestamp:2019-07-25 09:47:01 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 1,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-recreate-deployment 1e12f69e-f384-4d2f-b792-6f91cba954af 0xc002070277 0xc002070278}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 6df85df6b9,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 25 09:47:03.909: INFO: Pod "test-recreate-deployment-5c8c9cc69d-cr4nv" is not available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-recreate-deployment-5c8c9cc69d-cr4nv,GenerateName:test-recreate-deployment-5c8c9cc69d-,Namespace:deployment-9784,SelfLink:/api/v1/namespaces/deployment-9784/pods/test-recreate-deployment-5c8c9cc69d-cr4nv,UID:07bdc5a6-1960-453f-af5c-56a97bad7c9f,ResourceVersion:67734,Generation:0,CreationTimestamp:2019-07-25 09:47:03 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod-3,pod-template-hash: 5c8c9cc69d,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-recreate-deployment-5c8c9cc69d 13ff383d-6a4b-4877-86db-a617f4bb2490 0xc002070b87 0xc002070b88}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-td2rm {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-td2rm,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [{default-token-td2rm true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc002070bf0} {node.kubernetes.io/unreachable Exists  NoExecute 0xc002070c10}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Pending,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:47:03 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:47:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:47:03 +0000 UTC ContainersNotReady containers with unready status: [nginx]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:47:03 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:,StartTime:2019-07-25 09:47:03 +0000 UTC,ContainerStatuses:[{nginx {ContainerStateWaiting{Reason:ContainerCreating,Message:,} nil nil} {nil nil nil} false 0 docker.io/library/nginx:1.14-alpine  }],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:47:03.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9784" for this suite.
Jul 25 09:47:09.923: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:47:10.001: INFO: namespace deployment-9784 deletion completed in 6.087978541s

• [SLOW TEST:8.216 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:47:10.001: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Jul 25 09:47:14.059: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 25 09:47:14.063: INFO: Pod pod-with-prestop-http-hook still exists
Jul 25 09:47:16.063: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 25 09:47:16.068: INFO: Pod pod-with-prestop-http-hook still exists
Jul 25 09:47:18.063: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jul 25 09:47:18.067: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:47:18.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-9694" for this suite.
Jul 25 09:47:40.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:47:40.214: INFO: namespace container-lifecycle-hook-9694 deletion completed in 22.111532526s

• [SLOW TEST:30.212 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:47:40.214: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 25 09:47:40.254: INFO: Waiting up to 5m0s for pod "downwardapi-volume-afdc1620-f1dc-4b4d-a459-c7d696f5d6f5" in namespace "downward-api-5198" to be "success or failure"
Jul 25 09:47:40.258: INFO: Pod "downwardapi-volume-afdc1620-f1dc-4b4d-a459-c7d696f5d6f5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.998342ms
Jul 25 09:47:42.262: INFO: Pod "downwardapi-volume-afdc1620-f1dc-4b4d-a459-c7d696f5d6f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007784577s
STEP: Saw pod success
Jul 25 09:47:42.262: INFO: Pod "downwardapi-volume-afdc1620-f1dc-4b4d-a459-c7d696f5d6f5" satisfied condition "success or failure"
Jul 25 09:47:42.266: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-afdc1620-f1dc-4b4d-a459-c7d696f5d6f5 container client-container: <nil>
STEP: delete the pod
Jul 25 09:47:42.288: INFO: Waiting for pod downwardapi-volume-afdc1620-f1dc-4b4d-a459-c7d696f5d6f5 to disappear
Jul 25 09:47:42.291: INFO: Pod downwardapi-volume-afdc1620-f1dc-4b4d-a459-c7d696f5d6f5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:47:42.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5198" for this suite.
Jul 25 09:47:48.307: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:47:48.403: INFO: namespace downward-api-5198 deletion completed in 6.10844073s

• [SLOW TEST:8.189 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:47:48.404: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: starting the proxy server
Jul 25 09:47:48.431: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-739010025 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:47:48.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6648" for this suite.
Jul 25 09:47:54.537: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:47:54.636: INFO: namespace kubectl-6648 deletion completed in 6.11078121s

• [SLOW TEST:6.232 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:47:54.636: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:47:54.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6669" for this suite.
Jul 25 09:48:16.746: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:48:16.816: INFO: namespace kubelet-test-6669 deletion completed in 22.133626341s

• [SLOW TEST:22.180 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:48:16.816: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's command
Jul 25 09:48:16.852: INFO: Waiting up to 5m0s for pod "var-expansion-48e95800-1e87-4a52-91d6-a2cd8b2b479a" in namespace "var-expansion-4490" to be "success or failure"
Jul 25 09:48:16.856: INFO: Pod "var-expansion-48e95800-1e87-4a52-91d6-a2cd8b2b479a": Phase="Pending", Reason="", readiness=false. Elapsed: 3.941244ms
Jul 25 09:48:18.859: INFO: Pod "var-expansion-48e95800-1e87-4a52-91d6-a2cd8b2b479a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006781554s
STEP: Saw pod success
Jul 25 09:48:18.859: INFO: Pod "var-expansion-48e95800-1e87-4a52-91d6-a2cd8b2b479a" satisfied condition "success or failure"
Jul 25 09:48:18.862: INFO: Trying to get logs from node essentialpks-conformance-3 pod var-expansion-48e95800-1e87-4a52-91d6-a2cd8b2b479a container dapi-container: <nil>
STEP: delete the pod
Jul 25 09:48:18.882: INFO: Waiting for pod var-expansion-48e95800-1e87-4a52-91d6-a2cd8b2b479a to disappear
Jul 25 09:48:18.884: INFO: Pod var-expansion-48e95800-1e87-4a52-91d6-a2cd8b2b479a no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:48:18.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4490" for this suite.
Jul 25 09:48:24.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:48:25.000: INFO: namespace var-expansion-4490 deletion completed in 6.112882479s

• [SLOW TEST:8.184 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:48:25.000: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 25 09:48:27.573: INFO: Successfully updated pod "annotationupdate3c1853ee-1fb6-47a6-a473-ec77a16521ae"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:48:29.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6841" for this suite.
Jul 25 09:48:51.603: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:48:51.680: INFO: namespace downward-api-6841 deletion completed in 22.086652318s

• [SLOW TEST:26.680 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:48:51.681: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-secret-g6pv
STEP: Creating a pod to test atomic-volume-subpath
Jul 25 09:48:51.716: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-g6pv" in namespace "subpath-9581" to be "success or failure"
Jul 25 09:48:51.718: INFO: Pod "pod-subpath-test-secret-g6pv": Phase="Pending", Reason="", readiness=false. Elapsed: 2.262642ms
Jul 25 09:48:53.722: INFO: Pod "pod-subpath-test-secret-g6pv": Phase="Running", Reason="", readiness=true. Elapsed: 2.00556379s
Jul 25 09:48:55.725: INFO: Pod "pod-subpath-test-secret-g6pv": Phase="Running", Reason="", readiness=true. Elapsed: 4.008831278s
Jul 25 09:48:57.728: INFO: Pod "pod-subpath-test-secret-g6pv": Phase="Running", Reason="", readiness=true. Elapsed: 6.012204062s
Jul 25 09:48:59.732: INFO: Pod "pod-subpath-test-secret-g6pv": Phase="Running", Reason="", readiness=true. Elapsed: 8.016003593s
Jul 25 09:49:01.737: INFO: Pod "pod-subpath-test-secret-g6pv": Phase="Running", Reason="", readiness=true. Elapsed: 10.021186767s
Jul 25 09:49:03.741: INFO: Pod "pod-subpath-test-secret-g6pv": Phase="Running", Reason="", readiness=true. Elapsed: 12.024615405s
Jul 25 09:49:05.744: INFO: Pod "pod-subpath-test-secret-g6pv": Phase="Running", Reason="", readiness=true. Elapsed: 14.02813599s
Jul 25 09:49:07.748: INFO: Pod "pod-subpath-test-secret-g6pv": Phase="Running", Reason="", readiness=true. Elapsed: 16.032083801s
Jul 25 09:49:09.751: INFO: Pod "pod-subpath-test-secret-g6pv": Phase="Running", Reason="", readiness=true. Elapsed: 18.035120076s
Jul 25 09:49:11.756: INFO: Pod "pod-subpath-test-secret-g6pv": Phase="Running", Reason="", readiness=true. Elapsed: 20.039914503s
Jul 25 09:49:13.762: INFO: Pod "pod-subpath-test-secret-g6pv": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.045704171s
STEP: Saw pod success
Jul 25 09:49:13.762: INFO: Pod "pod-subpath-test-secret-g6pv" satisfied condition "success or failure"
Jul 25 09:49:13.765: INFO: Trying to get logs from node essentialpks-conformance-3 pod pod-subpath-test-secret-g6pv container test-container-subpath-secret-g6pv: <nil>
STEP: delete the pod
Jul 25 09:49:13.785: INFO: Waiting for pod pod-subpath-test-secret-g6pv to disappear
Jul 25 09:49:13.788: INFO: Pod pod-subpath-test-secret-g6pv no longer exists
STEP: Deleting pod pod-subpath-test-secret-g6pv
Jul 25 09:49:13.788: INFO: Deleting pod "pod-subpath-test-secret-g6pv" in namespace "subpath-9581"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:49:13.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9581" for this suite.
Jul 25 09:49:19.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:49:19.892: INFO: namespace subpath-9581 deletion completed in 6.098081965s

• [SLOW TEST:28.211 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:49:19.893: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-map-6c4ba07a-aa47-4a83-ab67-37b2bc6ea388
STEP: Creating a pod to test consume secrets
Jul 25 09:49:19.929: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2b214eac-6a87-4ec7-9b4c-3113294f2e70" in namespace "projected-7270" to be "success or failure"
Jul 25 09:49:19.933: INFO: Pod "pod-projected-secrets-2b214eac-6a87-4ec7-9b4c-3113294f2e70": Phase="Pending", Reason="", readiness=false. Elapsed: 3.044415ms
Jul 25 09:49:21.937: INFO: Pod "pod-projected-secrets-2b214eac-6a87-4ec7-9b4c-3113294f2e70": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007723551s
STEP: Saw pod success
Jul 25 09:49:21.937: INFO: Pod "pod-projected-secrets-2b214eac-6a87-4ec7-9b4c-3113294f2e70" satisfied condition "success or failure"
Jul 25 09:49:21.940: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-projected-secrets-2b214eac-6a87-4ec7-9b4c-3113294f2e70 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 25 09:49:21.957: INFO: Waiting for pod pod-projected-secrets-2b214eac-6a87-4ec7-9b4c-3113294f2e70 to disappear
Jul 25 09:49:21.959: INFO: Pod pod-projected-secrets-2b214eac-6a87-4ec7-9b4c-3113294f2e70 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:49:21.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7270" for this suite.
Jul 25 09:49:27.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:49:28.082: INFO: namespace projected-7270 deletion completed in 6.118988466s

• [SLOW TEST:8.190 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:49:28.083: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-35b1ceaa-8174-416c-bca3-40a8ec4c2afc
STEP: Creating a pod to test consume secrets
Jul 25 09:49:28.206: INFO: Waiting up to 5m0s for pod "pod-secrets-d580f6e2-e926-406e-850f-0866323d1800" in namespace "secrets-6039" to be "success or failure"
Jul 25 09:49:28.210: INFO: Pod "pod-secrets-d580f6e2-e926-406e-850f-0866323d1800": Phase="Pending", Reason="", readiness=false. Elapsed: 4.143499ms
Jul 25 09:49:30.214: INFO: Pod "pod-secrets-d580f6e2-e926-406e-850f-0866323d1800": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007952034s
STEP: Saw pod success
Jul 25 09:49:30.214: INFO: Pod "pod-secrets-d580f6e2-e926-406e-850f-0866323d1800" satisfied condition "success or failure"
Jul 25 09:49:30.219: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-secrets-d580f6e2-e926-406e-850f-0866323d1800 container secret-volume-test: <nil>
STEP: delete the pod
Jul 25 09:49:30.240: INFO: Waiting for pod pod-secrets-d580f6e2-e926-406e-850f-0866323d1800 to disappear
Jul 25 09:49:30.243: INFO: Pod pod-secrets-d580f6e2-e926-406e-850f-0866323d1800 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:49:30.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6039" for this suite.
Jul 25 09:49:36.257: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:49:36.348: INFO: namespace secrets-6039 deletion completed in 6.101392691s
STEP: Destroying namespace "secret-namespace-4638" for this suite.
Jul 25 09:49:42.358: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:49:42.447: INFO: namespace secret-namespace-4638 deletion completed in 6.098543723s

• [SLOW TEST:14.364 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:49:42.448: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 25 09:49:42.483: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jul 25 09:49:47.488: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 25 09:49:47.488: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 25 09:49:49.534: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment,GenerateName:,Namespace:deployment-9808,SelfLink:/apis/apps/v1/namespaces/deployment-9808/deployments/test-cleanup-deployment,UID:3ee75f58-9aae-48a3-86f2-3ee06d4383aa,ResourceVersion:68314,Generation:1,CreationTimestamp:2019-07-25 09:49:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 1,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-25 09:49:47 +0000 UTC 2019-07-25 09:49:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-25 09:49:48 +0000 UTC 2019-07-25 09:49:47 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-cleanup-deployment-55bbcbc84c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul 25 09:49:49.540: INFO: New ReplicaSet "test-cleanup-deployment-55bbcbc84c" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c,GenerateName:,Namespace:deployment-9808,SelfLink:/apis/apps/v1/namespaces/deployment-9808/replicasets/test-cleanup-deployment-55bbcbc84c,UID:de2f801b-95ad-483c-a68c-d1519d7f256e,ResourceVersion:68303,Generation:1,CreationTimestamp:2019-07-25 09:49:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 1,},OwnerReferences:[{apps/v1 Deployment test-cleanup-deployment 3ee75f58-9aae-48a3-86f2-3ee06d4383aa 0xc000dd6907 0xc000dd6908}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 25 09:49:49.543: INFO: Pod "test-cleanup-deployment-55bbcbc84c-ftcqk" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-cleanup-deployment-55bbcbc84c-ftcqk,GenerateName:test-cleanup-deployment-55bbcbc84c-,Namespace:deployment-9808,SelfLink:/api/v1/namespaces/deployment-9808/pods/test-cleanup-deployment-55bbcbc84c-ftcqk,UID:8925a084-1321-4b6e-8c2a-882e4f1300aa,ResourceVersion:68302,Generation:0,CreationTimestamp:2019-07-25 09:49:47 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: cleanup-pod,pod-template-hash: 55bbcbc84c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-cleanup-deployment-55bbcbc84c de2f801b-95ad-483c-a68c-d1519d7f256e 0xc000dd6f17 0xc000dd6f18}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-j5mws {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-j5mws,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-j5mws true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc000dd6f80} {node.kubernetes.io/unreachable Exists  NoExecute 0xc000dd6fa0}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:49:47 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:49:48 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:49:48 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 09:49:47 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.4,PodIP:10.244.1.113,StartTime:2019-07-25 09:49:47 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-25 09:49:48 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://5f8f250a25669dbe14a9f01cb2dd98685ba0b1a42ace0b4b50693240b7c3c27a}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:49:49.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9808" for this suite.
Jul 25 09:49:55.559: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:49:55.641: INFO: namespace deployment-9808 deletion completed in 6.094467108s

• [SLOW TEST:13.193 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:49:55.641: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 25 09:49:55.667: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:49:57.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8145" for this suite.
Jul 25 09:50:35.721: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:50:35.810: INFO: namespace pods-8145 deletion completed in 38.097975452s

• [SLOW TEST:40.172 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:50:35.815: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating api versions
Jul 25 09:50:35.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 api-versions'
Jul 25 09:50:35.990: INFO: stderr: ""
Jul 25 09:50:35.990: INFO: stdout: "admissionregistration.k8s.io/v1beta1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napps/v1\napps/v1beta1\napps/v1beta2\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:50:35.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-603" for this suite.
Jul 25 09:50:42.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:50:42.101: INFO: namespace kubectl-603 deletion completed in 6.096841574s

• [SLOW TEST:6.287 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl api-versions
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:50:42.103: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:50:44.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7104" for this suite.
Jul 25 09:51:34.175: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:51:34.260: INFO: namespace kubelet-test-7104 deletion completed in 50.095603286s

• [SLOW TEST:52.158 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a busybox command in a pod
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:51:34.262: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-d7c5fbe5-845e-4137-93f3-10256f780a54 in namespace container-probe-7166
Jul 25 09:51:36.301: INFO: Started pod busybox-d7c5fbe5-845e-4137-93f3-10256f780a54 in namespace container-probe-7166
STEP: checking the pod's current state and verifying that restartCount is present
Jul 25 09:51:36.304: INFO: Initial restart count of pod busybox-d7c5fbe5-845e-4137-93f3-10256f780a54 is 0
Jul 25 09:52:24.409: INFO: Restart count of pod container-probe-7166/busybox-d7c5fbe5-845e-4137-93f3-10256f780a54 is now 1 (48.105222899s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:52:24.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7166" for this suite.
Jul 25 09:52:30.435: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:52:30.523: INFO: namespace container-probe-7166 deletion completed in 6.09924677s

• [SLOW TEST:56.261 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:52:30.523: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:60
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:75
STEP: Creating service test in namespace statefulset-1124
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-1124
STEP: Creating statefulset with conflicting port in namespace statefulset-1124
STEP: Waiting until pod test-pod will start running in namespace statefulset-1124
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-1124
Jul 25 09:52:32.601: INFO: Observed stateful pod in namespace: statefulset-1124, name: ss-0, uid: 4ef87874-102c-4438-a10d-5973d603467d, status phase: Pending. Waiting for statefulset controller to delete.
Jul 25 09:52:32.971: INFO: Observed stateful pod in namespace: statefulset-1124, name: ss-0, uid: 4ef87874-102c-4438-a10d-5973d603467d, status phase: Failed. Waiting for statefulset controller to delete.
Jul 25 09:52:32.982: INFO: Observed stateful pod in namespace: statefulset-1124, name: ss-0, uid: 4ef87874-102c-4438-a10d-5973d603467d, status phase: Failed. Waiting for statefulset controller to delete.
Jul 25 09:52:32.984: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-1124
STEP: Removing pod with conflicting port in namespace statefulset-1124
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-1124 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:86
Jul 25 09:52:35.021: INFO: Deleting all statefulset in ns statefulset-1124
Jul 25 09:52:35.023: INFO: Scaling statefulset ss to 0
Jul 25 09:52:55.035: INFO: Waiting for statefulset status.replicas updated to 0
Jul 25 09:52:55.038: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:52:55.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1124" for this suite.
Jul 25 09:53:01.078: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:53:01.174: INFO: namespace statefulset-1124 deletion completed in 6.107739866s

• [SLOW TEST:30.651 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:53:01.176: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-projected-all-test-volume-111b0236-3e78-4595-ab43-7c1efd76deae
STEP: Creating secret with name secret-projected-all-test-volume-f27a2bc8-41c7-49f3-8c23-f58e1fe267ee
STEP: Creating a pod to test Check all projections for projected volume plugin
Jul 25 09:53:01.211: INFO: Waiting up to 5m0s for pod "projected-volume-6e432f4a-98c4-4dec-be69-8aa2db578d01" in namespace "projected-699" to be "success or failure"
Jul 25 09:53:01.214: INFO: Pod "projected-volume-6e432f4a-98c4-4dec-be69-8aa2db578d01": Phase="Pending", Reason="", readiness=false. Elapsed: 3.219266ms
Jul 25 09:53:03.218: INFO: Pod "projected-volume-6e432f4a-98c4-4dec-be69-8aa2db578d01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007220607s
STEP: Saw pod success
Jul 25 09:53:03.218: INFO: Pod "projected-volume-6e432f4a-98c4-4dec-be69-8aa2db578d01" satisfied condition "success or failure"
Jul 25 09:53:03.221: INFO: Trying to get logs from node essentialpks-conformance-3 pod projected-volume-6e432f4a-98c4-4dec-be69-8aa2db578d01 container projected-all-volume-test: <nil>
STEP: delete the pod
Jul 25 09:53:03.242: INFO: Waiting for pod projected-volume-6e432f4a-98c4-4dec-be69-8aa2db578d01 to disappear
Jul 25 09:53:03.246: INFO: Pod projected-volume-6e432f4a-98c4-4dec-be69-8aa2db578d01 no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:53:03.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-699" for this suite.
Jul 25 09:53:09.263: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:53:09.350: INFO: namespace projected-699 deletion completed in 6.098140379s

• [SLOW TEST:8.180 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:31
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:53:09.356: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating the pod
Jul 25 09:53:11.930: INFO: Successfully updated pod "labelsupdate80127e5b-e179-4d28-a344-76e9c0e5eb4b"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:53:15.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6990" for this suite.
Jul 25 09:53:37.980: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:53:38.134: INFO: namespace downward-api-6990 deletion completed in 22.164367475s

• [SLOW TEST:28.778 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:53:38.138: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward api env vars
Jul 25 09:53:38.175: INFO: Waiting up to 5m0s for pod "downward-api-04f4c848-1fea-4a72-8e5e-14d4912a2984" in namespace "downward-api-3830" to be "success or failure"
Jul 25 09:53:38.179: INFO: Pod "downward-api-04f4c848-1fea-4a72-8e5e-14d4912a2984": Phase="Pending", Reason="", readiness=false. Elapsed: 4.622811ms
Jul 25 09:53:40.184: INFO: Pod "downward-api-04f4c848-1fea-4a72-8e5e-14d4912a2984": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00956153s
STEP: Saw pod success
Jul 25 09:53:40.184: INFO: Pod "downward-api-04f4c848-1fea-4a72-8e5e-14d4912a2984" satisfied condition "success or failure"
Jul 25 09:53:40.188: INFO: Trying to get logs from node essentialpks-conformance-3 pod downward-api-04f4c848-1fea-4a72-8e5e-14d4912a2984 container dapi-container: <nil>
STEP: delete the pod
Jul 25 09:53:40.205: INFO: Waiting for pod downward-api-04f4c848-1fea-4a72-8e5e-14d4912a2984 to disappear
Jul 25 09:53:40.208: INFO: Pod downward-api-04f4c848-1fea-4a72-8e5e-14d4912a2984 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:53:40.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3830" for this suite.
Jul 25 09:53:46.221: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:53:46.308: INFO: namespace downward-api-3830 deletion completed in 6.096075383s

• [SLOW TEST:8.170 seconds]
[sig-node] Downward API
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:53:46.310: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1473
I0725 09:53:46.338352      16 runners.go:180] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1473, replica count: 1
I0725 09:53:47.389023      16 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0725 09:53:48.389447      16 runners.go:180] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jul 25 09:53:48.503: INFO: Created: latency-svc-7x474
Jul 25 09:53:48.506: INFO: Got endpoints: latency-svc-7x474 [16.041603ms]
Jul 25 09:53:48.523: INFO: Created: latency-svc-9k2q4
Jul 25 09:53:48.528: INFO: Got endpoints: latency-svc-9k2q4 [21.923696ms]
Jul 25 09:53:48.551: INFO: Created: latency-svc-gwprp
Jul 25 09:53:48.559: INFO: Got endpoints: latency-svc-gwprp [53.480424ms]
Jul 25 09:53:48.569: INFO: Created: latency-svc-dh68n
Jul 25 09:53:48.577: INFO: Got endpoints: latency-svc-dh68n [70.645741ms]
Jul 25 09:53:48.584: INFO: Created: latency-svc-gtwtg
Jul 25 09:53:48.589: INFO: Got endpoints: latency-svc-gtwtg [82.431258ms]
Jul 25 09:53:48.595: INFO: Created: latency-svc-77g2p
Jul 25 09:53:48.597: INFO: Got endpoints: latency-svc-77g2p [90.669839ms]
Jul 25 09:53:48.614: INFO: Created: latency-svc-w222r
Jul 25 09:53:48.620: INFO: Got endpoints: latency-svc-w222r [113.878375ms]
Jul 25 09:53:48.621: INFO: Created: latency-svc-442v2
Jul 25 09:53:48.627: INFO: Got endpoints: latency-svc-442v2 [120.214611ms]
Jul 25 09:53:48.635: INFO: Created: latency-svc-rgvj2
Jul 25 09:53:48.638: INFO: Got endpoints: latency-svc-rgvj2 [131.069175ms]
Jul 25 09:53:48.649: INFO: Created: latency-svc-cl6f7
Jul 25 09:53:48.652: INFO: Got endpoints: latency-svc-cl6f7 [145.310907ms]
Jul 25 09:53:48.664: INFO: Created: latency-svc-prdr6
Jul 25 09:53:48.668: INFO: Got endpoints: latency-svc-prdr6 [161.586866ms]
Jul 25 09:53:48.679: INFO: Created: latency-svc-ztmpl
Jul 25 09:53:48.692: INFO: Created: latency-svc-fxj52
Jul 25 09:53:48.692: INFO: Got endpoints: latency-svc-ztmpl [185.41432ms]
Jul 25 09:53:48.696: INFO: Got endpoints: latency-svc-fxj52 [189.569149ms]
Jul 25 09:53:48.717: INFO: Created: latency-svc-k4xxf
Jul 25 09:53:48.722: INFO: Got endpoints: latency-svc-k4xxf [215.048795ms]
Jul 25 09:53:48.742: INFO: Created: latency-svc-ctfh9
Jul 25 09:53:48.746: INFO: Got endpoints: latency-svc-ctfh9 [238.556696ms]
Jul 25 09:53:48.751: INFO: Created: latency-svc-d44fm
Jul 25 09:53:48.765: INFO: Got endpoints: latency-svc-d44fm [257.644731ms]
Jul 25 09:53:48.770: INFO: Created: latency-svc-8tvh8
Jul 25 09:53:48.774: INFO: Got endpoints: latency-svc-8tvh8 [245.729633ms]
Jul 25 09:53:48.784: INFO: Created: latency-svc-mw9kd
Jul 25 09:53:48.787: INFO: Got endpoints: latency-svc-mw9kd [228.070906ms]
Jul 25 09:53:48.798: INFO: Created: latency-svc-pk7zh
Jul 25 09:53:48.801: INFO: Got endpoints: latency-svc-pk7zh [223.763751ms]
Jul 25 09:53:48.811: INFO: Created: latency-svc-cdct6
Jul 25 09:53:48.813: INFO: Got endpoints: latency-svc-cdct6 [223.845205ms]
Jul 25 09:53:48.828: INFO: Created: latency-svc-ckn68
Jul 25 09:53:48.829: INFO: Got endpoints: latency-svc-ckn68 [231.512759ms]
Jul 25 09:53:48.841: INFO: Created: latency-svc-4j6nr
Jul 25 09:53:48.843: INFO: Got endpoints: latency-svc-4j6nr [223.185376ms]
Jul 25 09:53:48.869: INFO: Created: latency-svc-h92g5
Jul 25 09:53:48.873: INFO: Got endpoints: latency-svc-h92g5 [246.488822ms]
Jul 25 09:53:48.927: INFO: Created: latency-svc-mnzt4
Jul 25 09:53:48.965: INFO: Got endpoints: latency-svc-mnzt4 [326.792372ms]
Jul 25 09:53:49.016: INFO: Created: latency-svc-2fqnd
Jul 25 09:53:49.019: INFO: Got endpoints: latency-svc-2fqnd [366.620449ms]
Jul 25 09:53:49.025: INFO: Created: latency-svc-t6wgh
Jul 25 09:53:49.034: INFO: Got endpoints: latency-svc-t6wgh [365.386194ms]
Jul 25 09:53:49.037: INFO: Created: latency-svc-8ss75
Jul 25 09:53:49.041: INFO: Got endpoints: latency-svc-8ss75 [348.920059ms]
Jul 25 09:53:49.052: INFO: Created: latency-svc-9tjl4
Jul 25 09:53:49.052: INFO: Got endpoints: latency-svc-9tjl4 [355.664004ms]
Jul 25 09:53:49.061: INFO: Created: latency-svc-7zlvv
Jul 25 09:53:49.062: INFO: Got endpoints: latency-svc-7zlvv [340.208458ms]
Jul 25 09:53:49.072: INFO: Created: latency-svc-j4qzs
Jul 25 09:53:49.077: INFO: Got endpoints: latency-svc-j4qzs [331.745976ms]
Jul 25 09:53:49.098: INFO: Created: latency-svc-bfqk4
Jul 25 09:53:49.100: INFO: Got endpoints: latency-svc-bfqk4 [334.915233ms]
Jul 25 09:53:49.111: INFO: Created: latency-svc-lr4f5
Jul 25 09:53:49.111: INFO: Created: latency-svc-j68t5
Jul 25 09:53:49.114: INFO: Got endpoints: latency-svc-lr4f5 [339.632237ms]
Jul 25 09:53:49.120: INFO: Got endpoints: latency-svc-j68t5 [332.539212ms]
Jul 25 09:53:49.130: INFO: Created: latency-svc-5nccm
Jul 25 09:53:49.147: INFO: Got endpoints: latency-svc-5nccm [346.383825ms]
Jul 25 09:53:49.149: INFO: Created: latency-svc-gzvhs
Jul 25 09:53:49.151: INFO: Got endpoints: latency-svc-gzvhs [337.70028ms]
Jul 25 09:53:49.163: INFO: Created: latency-svc-p7ggg
Jul 25 09:53:49.168: INFO: Got endpoints: latency-svc-p7ggg [338.676597ms]
Jul 25 09:53:49.175: INFO: Created: latency-svc-hdcjj
Jul 25 09:53:49.179: INFO: Got endpoints: latency-svc-hdcjj [335.954711ms]
Jul 25 09:53:49.189: INFO: Created: latency-svc-hvv9q
Jul 25 09:53:49.190: INFO: Got endpoints: latency-svc-hvv9q [316.646891ms]
Jul 25 09:53:49.201: INFO: Created: latency-svc-kbksh
Jul 25 09:53:49.203: INFO: Got endpoints: latency-svc-kbksh [238.097609ms]
Jul 25 09:53:49.214: INFO: Created: latency-svc-pb2hb
Jul 25 09:53:49.217: INFO: Got endpoints: latency-svc-pb2hb [197.97492ms]
Jul 25 09:53:49.230: INFO: Created: latency-svc-85gmg
Jul 25 09:53:49.230: INFO: Created: latency-svc-6ndc2
Jul 25 09:53:49.233: INFO: Got endpoints: latency-svc-6ndc2 [196.322233ms]
Jul 25 09:53:49.235: INFO: Got endpoints: latency-svc-85gmg [194.077628ms]
Jul 25 09:53:49.255: INFO: Created: latency-svc-xmlmb
Jul 25 09:53:49.258: INFO: Got endpoints: latency-svc-xmlmb [205.295578ms]
Jul 25 09:53:49.280: INFO: Created: latency-svc-4h6dj
Jul 25 09:53:49.280: INFO: Got endpoints: latency-svc-4h6dj [217.651287ms]
Jul 25 09:53:49.296: INFO: Created: latency-svc-gm62p
Jul 25 09:53:49.302: INFO: Got endpoints: latency-svc-gm62p [224.597395ms]
Jul 25 09:53:49.308: INFO: Created: latency-svc-q4sgw
Jul 25 09:53:49.316: INFO: Got endpoints: latency-svc-q4sgw [216.606706ms]
Jul 25 09:53:49.323: INFO: Created: latency-svc-8vb2s
Jul 25 09:53:49.332: INFO: Created: latency-svc-22s76
Jul 25 09:53:49.343: INFO: Created: latency-svc-9wxzh
Jul 25 09:53:49.358: INFO: Created: latency-svc-xm44s
Jul 25 09:53:49.363: INFO: Got endpoints: latency-svc-8vb2s [248.882063ms]
Jul 25 09:53:49.369: INFO: Created: latency-svc-h4k2k
Jul 25 09:53:49.386: INFO: Created: latency-svc-zz8j6
Jul 25 09:53:49.395: INFO: Created: latency-svc-2r5bl
Jul 25 09:53:49.409: INFO: Got endpoints: latency-svc-22s76 [288.750763ms]
Jul 25 09:53:49.413: INFO: Created: latency-svc-5bwlh
Jul 25 09:53:49.421: INFO: Created: latency-svc-lfgj7
Jul 25 09:53:49.432: INFO: Created: latency-svc-fr8wn
Jul 25 09:53:49.445: INFO: Created: latency-svc-g887n
Jul 25 09:53:49.451: INFO: Created: latency-svc-9pkzz
Jul 25 09:53:49.455: INFO: Got endpoints: latency-svc-9wxzh [307.466805ms]
Jul 25 09:53:49.462: INFO: Created: latency-svc-pmx9p
Jul 25 09:53:49.493: INFO: Created: latency-svc-sc2j4
Jul 25 09:53:49.508: INFO: Got endpoints: latency-svc-xm44s [357.280395ms]
Jul 25 09:53:49.508: INFO: Created: latency-svc-mf7s9
Jul 25 09:53:49.527: INFO: Created: latency-svc-ztvd7
Jul 25 09:53:49.538: INFO: Created: latency-svc-86z5t
Jul 25 09:53:49.553: INFO: Created: latency-svc-ntlfn
Jul 25 09:53:49.560: INFO: Got endpoints: latency-svc-h4k2k [392.794911ms]
Jul 25 09:53:49.572: INFO: Created: latency-svc-h29sw
Jul 25 09:53:49.578: INFO: Created: latency-svc-czbkn
Jul 25 09:53:49.605: INFO: Got endpoints: latency-svc-zz8j6 [425.766866ms]
Jul 25 09:53:49.619: INFO: Created: latency-svc-bsc8z
Jul 25 09:53:49.656: INFO: Got endpoints: latency-svc-2r5bl [465.735221ms]
Jul 25 09:53:49.667: INFO: Created: latency-svc-q285m
Jul 25 09:53:49.708: INFO: Got endpoints: latency-svc-5bwlh [504.853936ms]
Jul 25 09:53:49.719: INFO: Created: latency-svc-5tttb
Jul 25 09:53:49.755: INFO: Got endpoints: latency-svc-lfgj7 [537.581318ms]
Jul 25 09:53:49.767: INFO: Created: latency-svc-qttcv
Jul 25 09:53:49.806: INFO: Got endpoints: latency-svc-fr8wn [572.650622ms]
Jul 25 09:53:49.818: INFO: Created: latency-svc-nh4c6
Jul 25 09:53:49.854: INFO: Got endpoints: latency-svc-g887n [618.687079ms]
Jul 25 09:53:49.867: INFO: Created: latency-svc-4njsp
Jul 25 09:53:49.904: INFO: Got endpoints: latency-svc-9pkzz [646.771811ms]
Jul 25 09:53:49.918: INFO: Created: latency-svc-pkj4k
Jul 25 09:53:49.955: INFO: Got endpoints: latency-svc-pmx9p [674.425639ms]
Jul 25 09:53:49.963: INFO: Created: latency-svc-s4hgl
Jul 25 09:53:50.006: INFO: Got endpoints: latency-svc-sc2j4 [703.425024ms]
Jul 25 09:53:50.015: INFO: Created: latency-svc-8mvcg
Jul 25 09:53:50.056: INFO: Got endpoints: latency-svc-mf7s9 [739.330245ms]
Jul 25 09:53:50.068: INFO: Created: latency-svc-psdrj
Jul 25 09:53:50.104: INFO: Got endpoints: latency-svc-ztvd7 [741.005833ms]
Jul 25 09:53:50.128: INFO: Created: latency-svc-fgxql
Jul 25 09:53:50.156: INFO: Got endpoints: latency-svc-86z5t [747.072234ms]
Jul 25 09:53:50.165: INFO: Created: latency-svc-lgl2v
Jul 25 09:53:50.205: INFO: Got endpoints: latency-svc-ntlfn [749.909622ms]
Jul 25 09:53:50.219: INFO: Created: latency-svc-hnjnd
Jul 25 09:53:50.254: INFO: Got endpoints: latency-svc-h29sw [745.888954ms]
Jul 25 09:53:50.266: INFO: Created: latency-svc-ncgzp
Jul 25 09:53:50.304: INFO: Got endpoints: latency-svc-czbkn [743.307949ms]
Jul 25 09:53:50.316: INFO: Created: latency-svc-pzsd6
Jul 25 09:53:50.354: INFO: Got endpoints: latency-svc-bsc8z [749.271513ms]
Jul 25 09:53:50.371: INFO: Created: latency-svc-klh89
Jul 25 09:53:50.404: INFO: Got endpoints: latency-svc-q285m [747.674518ms]
Jul 25 09:53:50.415: INFO: Created: latency-svc-ltdqx
Jul 25 09:53:50.454: INFO: Got endpoints: latency-svc-5tttb [745.556602ms]
Jul 25 09:53:50.466: INFO: Created: latency-svc-g4txz
Jul 25 09:53:50.503: INFO: Got endpoints: latency-svc-qttcv [748.860215ms]
Jul 25 09:53:50.514: INFO: Created: latency-svc-knrlh
Jul 25 09:53:50.555: INFO: Got endpoints: latency-svc-nh4c6 [748.70666ms]
Jul 25 09:53:50.567: INFO: Created: latency-svc-zbsfc
Jul 25 09:53:50.604: INFO: Got endpoints: latency-svc-4njsp [749.872723ms]
Jul 25 09:53:50.617: INFO: Created: latency-svc-7g4b8
Jul 25 09:53:50.655: INFO: Got endpoints: latency-svc-pkj4k [747.386558ms]
Jul 25 09:53:50.665: INFO: Created: latency-svc-z2wpg
Jul 25 09:53:50.704: INFO: Got endpoints: latency-svc-s4hgl [749.365293ms]
Jul 25 09:53:50.718: INFO: Created: latency-svc-dv8b8
Jul 25 09:53:50.754: INFO: Got endpoints: latency-svc-8mvcg [748.80347ms]
Jul 25 09:53:50.768: INFO: Created: latency-svc-vz9mg
Jul 25 09:53:50.805: INFO: Got endpoints: latency-svc-psdrj [749.035944ms]
Jul 25 09:53:50.816: INFO: Created: latency-svc-hnnl7
Jul 25 09:53:50.854: INFO: Got endpoints: latency-svc-fgxql [747.612274ms]
Jul 25 09:53:50.869: INFO: Created: latency-svc-2dm56
Jul 25 09:53:50.904: INFO: Got endpoints: latency-svc-lgl2v [748.142828ms]
Jul 25 09:53:50.919: INFO: Created: latency-svc-s44bg
Jul 25 09:53:50.955: INFO: Got endpoints: latency-svc-hnjnd [749.955304ms]
Jul 25 09:53:50.982: INFO: Created: latency-svc-thjjh
Jul 25 09:53:51.015: INFO: Got endpoints: latency-svc-ncgzp [760.649897ms]
Jul 25 09:53:51.038: INFO: Created: latency-svc-dxj6d
Jul 25 09:53:51.054: INFO: Got endpoints: latency-svc-pzsd6 [750.378583ms]
Jul 25 09:53:51.069: INFO: Created: latency-svc-92lxl
Jul 25 09:53:51.104: INFO: Got endpoints: latency-svc-klh89 [749.534943ms]
Jul 25 09:53:51.116: INFO: Created: latency-svc-s4vgd
Jul 25 09:53:51.156: INFO: Got endpoints: latency-svc-ltdqx [752.251816ms]
Jul 25 09:53:51.167: INFO: Created: latency-svc-84257
Jul 25 09:53:51.206: INFO: Got endpoints: latency-svc-g4txz [752.682472ms]
Jul 25 09:53:51.323: INFO: Got endpoints: latency-svc-knrlh [819.866555ms]
Jul 25 09:53:51.326: INFO: Got endpoints: latency-svc-zbsfc [770.920839ms]
Jul 25 09:53:51.332: INFO: Created: latency-svc-9g95f
Jul 25 09:53:51.346: INFO: Created: latency-svc-5rwwz
Jul 25 09:53:51.352: INFO: Created: latency-svc-ptjwf
Jul 25 09:53:51.354: INFO: Got endpoints: latency-svc-7g4b8 [749.72299ms]
Jul 25 09:53:51.364: INFO: Created: latency-svc-nwmqw
Jul 25 09:53:51.404: INFO: Got endpoints: latency-svc-z2wpg [749.358277ms]
Jul 25 09:53:51.417: INFO: Created: latency-svc-zvjzt
Jul 25 09:53:51.460: INFO: Got endpoints: latency-svc-dv8b8 [755.953652ms]
Jul 25 09:53:51.472: INFO: Created: latency-svc-ssq4r
Jul 25 09:53:51.504: INFO: Got endpoints: latency-svc-vz9mg [746.817011ms]
Jul 25 09:53:51.514: INFO: Created: latency-svc-9pfqs
Jul 25 09:53:51.555: INFO: Got endpoints: latency-svc-hnnl7 [750.111388ms]
Jul 25 09:53:51.573: INFO: Created: latency-svc-59kz9
Jul 25 09:53:51.605: INFO: Got endpoints: latency-svc-2dm56 [750.728816ms]
Jul 25 09:53:51.623: INFO: Created: latency-svc-s79fl
Jul 25 09:53:51.656: INFO: Got endpoints: latency-svc-s44bg [751.64753ms]
Jul 25 09:53:51.668: INFO: Created: latency-svc-x2v9p
Jul 25 09:53:51.706: INFO: Got endpoints: latency-svc-thjjh [750.74785ms]
Jul 25 09:53:51.724: INFO: Created: latency-svc-mtrpj
Jul 25 09:53:51.756: INFO: Got endpoints: latency-svc-dxj6d [741.441567ms]
Jul 25 09:53:51.774: INFO: Created: latency-svc-4npxd
Jul 25 09:53:51.805: INFO: Got endpoints: latency-svc-92lxl [750.085755ms]
Jul 25 09:53:51.817: INFO: Created: latency-svc-k4lq8
Jul 25 09:53:51.855: INFO: Got endpoints: latency-svc-s4vgd [750.574619ms]
Jul 25 09:53:51.866: INFO: Created: latency-svc-rpkcz
Jul 25 09:53:51.904: INFO: Got endpoints: latency-svc-84257 [747.668959ms]
Jul 25 09:53:51.934: INFO: Created: latency-svc-kxpz8
Jul 25 09:53:51.954: INFO: Got endpoints: latency-svc-9g95f [747.73984ms]
Jul 25 09:53:51.971: INFO: Created: latency-svc-8x7cp
Jul 25 09:53:52.004: INFO: Got endpoints: latency-svc-5rwwz [680.590308ms]
Jul 25 09:53:52.020: INFO: Created: latency-svc-nz7td
Jul 25 09:53:52.057: INFO: Got endpoints: latency-svc-ptjwf [731.196822ms]
Jul 25 09:53:52.072: INFO: Created: latency-svc-xc2fw
Jul 25 09:53:52.105: INFO: Got endpoints: latency-svc-nwmqw [751.208177ms]
Jul 25 09:53:52.125: INFO: Created: latency-svc-tktjb
Jul 25 09:53:52.156: INFO: Got endpoints: latency-svc-zvjzt [751.368988ms]
Jul 25 09:53:52.167: INFO: Created: latency-svc-krj4k
Jul 25 09:53:52.204: INFO: Got endpoints: latency-svc-ssq4r [744.345931ms]
Jul 25 09:53:52.218: INFO: Created: latency-svc-xr48m
Jul 25 09:53:52.256: INFO: Got endpoints: latency-svc-9pfqs [751.400543ms]
Jul 25 09:53:52.276: INFO: Created: latency-svc-lgbv7
Jul 25 09:53:52.305: INFO: Got endpoints: latency-svc-59kz9 [748.351965ms]
Jul 25 09:53:52.316: INFO: Created: latency-svc-mn9g8
Jul 25 09:53:52.355: INFO: Got endpoints: latency-svc-s79fl [750.02523ms]
Jul 25 09:53:52.366: INFO: Created: latency-svc-hmqbx
Jul 25 09:53:52.405: INFO: Got endpoints: latency-svc-x2v9p [749.436152ms]
Jul 25 09:53:52.424: INFO: Created: latency-svc-j2lm6
Jul 25 09:53:52.460: INFO: Got endpoints: latency-svc-mtrpj [754.289716ms]
Jul 25 09:53:52.473: INFO: Created: latency-svc-kfvrm
Jul 25 09:53:52.508: INFO: Got endpoints: latency-svc-4npxd [751.737379ms]
Jul 25 09:53:52.523: INFO: Created: latency-svc-z7tqk
Jul 25 09:53:52.555: INFO: Got endpoints: latency-svc-k4lq8 [748.620617ms]
Jul 25 09:53:52.569: INFO: Created: latency-svc-xhcj2
Jul 25 09:53:52.607: INFO: Got endpoints: latency-svc-rpkcz [751.961698ms]
Jul 25 09:53:52.624: INFO: Created: latency-svc-rfqgs
Jul 25 09:53:52.654: INFO: Got endpoints: latency-svc-kxpz8 [750.680417ms]
Jul 25 09:53:52.669: INFO: Created: latency-svc-26hbt
Jul 25 09:53:52.704: INFO: Got endpoints: latency-svc-8x7cp [749.918112ms]
Jul 25 09:53:52.740: INFO: Created: latency-svc-rzppb
Jul 25 09:53:52.757: INFO: Got endpoints: latency-svc-nz7td [753.368209ms]
Jul 25 09:53:52.773: INFO: Created: latency-svc-ptx56
Jul 25 09:53:52.806: INFO: Got endpoints: latency-svc-xc2fw [748.489511ms]
Jul 25 09:53:52.819: INFO: Created: latency-svc-j6stw
Jul 25 09:53:52.854: INFO: Got endpoints: latency-svc-tktjb [748.777677ms]
Jul 25 09:53:52.872: INFO: Created: latency-svc-h2gs9
Jul 25 09:53:52.904: INFO: Got endpoints: latency-svc-krj4k [748.136438ms]
Jul 25 09:53:52.917: INFO: Created: latency-svc-xtnb4
Jul 25 09:53:52.958: INFO: Got endpoints: latency-svc-xr48m [753.273438ms]
Jul 25 09:53:52.969: INFO: Created: latency-svc-q84c8
Jul 25 09:53:53.009: INFO: Got endpoints: latency-svc-lgbv7 [750.248338ms]
Jul 25 09:53:53.019: INFO: Created: latency-svc-rf5tk
Jul 25 09:53:53.057: INFO: Got endpoints: latency-svc-mn9g8 [752.275563ms]
Jul 25 09:53:53.069: INFO: Created: latency-svc-h2467
Jul 25 09:53:53.108: INFO: Got endpoints: latency-svc-hmqbx [752.71332ms]
Jul 25 09:53:53.126: INFO: Created: latency-svc-zpd69
Jul 25 09:53:53.155: INFO: Got endpoints: latency-svc-j2lm6 [747.521737ms]
Jul 25 09:53:53.167: INFO: Created: latency-svc-qsmlw
Jul 25 09:53:53.204: INFO: Got endpoints: latency-svc-kfvrm [743.416395ms]
Jul 25 09:53:53.219: INFO: Created: latency-svc-ggbjm
Jul 25 09:53:53.254: INFO: Got endpoints: latency-svc-z7tqk [746.144099ms]
Jul 25 09:53:53.274: INFO: Created: latency-svc-2d2zt
Jul 25 09:53:53.304: INFO: Got endpoints: latency-svc-xhcj2 [749.163907ms]
Jul 25 09:53:53.321: INFO: Created: latency-svc-5ct79
Jul 25 09:53:53.355: INFO: Got endpoints: latency-svc-rfqgs [747.845881ms]
Jul 25 09:53:53.369: INFO: Created: latency-svc-6vlwk
Jul 25 09:53:53.404: INFO: Got endpoints: latency-svc-26hbt [749.567601ms]
Jul 25 09:53:53.420: INFO: Created: latency-svc-kk5cm
Jul 25 09:53:53.454: INFO: Got endpoints: latency-svc-rzppb [749.821607ms]
Jul 25 09:53:53.466: INFO: Created: latency-svc-r7tjw
Jul 25 09:53:53.505: INFO: Got endpoints: latency-svc-ptx56 [747.862984ms]
Jul 25 09:53:53.522: INFO: Created: latency-svc-xv5hj
Jul 25 09:53:53.555: INFO: Got endpoints: latency-svc-j6stw [749.165155ms]
Jul 25 09:53:53.571: INFO: Created: latency-svc-kz64v
Jul 25 09:53:53.605: INFO: Got endpoints: latency-svc-h2gs9 [751.4601ms]
Jul 25 09:53:53.617: INFO: Created: latency-svc-5mpfx
Jul 25 09:53:53.655: INFO: Got endpoints: latency-svc-xtnb4 [750.567171ms]
Jul 25 09:53:53.670: INFO: Created: latency-svc-cnr4l
Jul 25 09:53:53.708: INFO: Got endpoints: latency-svc-q84c8 [750.144358ms]
Jul 25 09:53:53.728: INFO: Created: latency-svc-rhswx
Jul 25 09:53:53.755: INFO: Got endpoints: latency-svc-rf5tk [745.841246ms]
Jul 25 09:53:53.776: INFO: Created: latency-svc-fq8hr
Jul 25 09:53:53.805: INFO: Got endpoints: latency-svc-h2467 [746.510318ms]
Jul 25 09:53:53.826: INFO: Created: latency-svc-qx6lm
Jul 25 09:53:53.854: INFO: Got endpoints: latency-svc-zpd69 [746.522826ms]
Jul 25 09:53:53.871: INFO: Created: latency-svc-xqj7s
Jul 25 09:53:53.904: INFO: Got endpoints: latency-svc-qsmlw [749.781419ms]
Jul 25 09:53:53.928: INFO: Created: latency-svc-kvvgj
Jul 25 09:53:53.954: INFO: Got endpoints: latency-svc-ggbjm [749.82852ms]
Jul 25 09:53:53.966: INFO: Created: latency-svc-bxqqt
Jul 25 09:53:54.004: INFO: Got endpoints: latency-svc-2d2zt [749.776137ms]
Jul 25 09:53:54.023: INFO: Created: latency-svc-tjks5
Jul 25 09:53:54.054: INFO: Got endpoints: latency-svc-5ct79 [749.794709ms]
Jul 25 09:53:54.067: INFO: Created: latency-svc-c6lbs
Jul 25 09:53:54.104: INFO: Got endpoints: latency-svc-6vlwk [746.467385ms]
Jul 25 09:53:54.118: INFO: Created: latency-svc-76mx6
Jul 25 09:53:54.157: INFO: Got endpoints: latency-svc-kk5cm [750.765999ms]
Jul 25 09:53:54.176: INFO: Created: latency-svc-sf85r
Jul 25 09:53:54.205: INFO: Got endpoints: latency-svc-r7tjw [750.661786ms]
Jul 25 09:53:54.217: INFO: Created: latency-svc-jj6sf
Jul 25 09:53:54.254: INFO: Got endpoints: latency-svc-xv5hj [748.594286ms]
Jul 25 09:53:54.269: INFO: Created: latency-svc-4kp28
Jul 25 09:53:54.304: INFO: Got endpoints: latency-svc-kz64v [748.558425ms]
Jul 25 09:53:54.317: INFO: Created: latency-svc-nf55t
Jul 25 09:53:54.354: INFO: Got endpoints: latency-svc-5mpfx [748.819821ms]
Jul 25 09:53:54.369: INFO: Created: latency-svc-q6q4l
Jul 25 09:53:54.405: INFO: Got endpoints: latency-svc-cnr4l [750.502986ms]
Jul 25 09:53:54.418: INFO: Created: latency-svc-mg9j9
Jul 25 09:53:54.456: INFO: Got endpoints: latency-svc-rhswx [747.51732ms]
Jul 25 09:53:54.468: INFO: Created: latency-svc-4tx4g
Jul 25 09:53:54.504: INFO: Got endpoints: latency-svc-fq8hr [749.53864ms]
Jul 25 09:53:54.519: INFO: Created: latency-svc-xpbbk
Jul 25 09:53:54.555: INFO: Got endpoints: latency-svc-qx6lm [750.19224ms]
Jul 25 09:53:54.572: INFO: Created: latency-svc-rrxbg
Jul 25 09:53:54.607: INFO: Got endpoints: latency-svc-xqj7s [750.800775ms]
Jul 25 09:53:54.623: INFO: Created: latency-svc-tf4vt
Jul 25 09:53:54.656: INFO: Got endpoints: latency-svc-kvvgj [751.390192ms]
Jul 25 09:53:54.668: INFO: Created: latency-svc-dsvvn
Jul 25 09:53:54.704: INFO: Got endpoints: latency-svc-bxqqt [750.024493ms]
Jul 25 09:53:54.718: INFO: Created: latency-svc-282vw
Jul 25 09:53:54.754: INFO: Got endpoints: latency-svc-tjks5 [749.923105ms]
Jul 25 09:53:54.766: INFO: Created: latency-svc-pc7zx
Jul 25 09:53:54.805: INFO: Got endpoints: latency-svc-c6lbs [750.44444ms]
Jul 25 09:53:54.892: INFO: Created: latency-svc-7qk88
Jul 25 09:53:54.895: INFO: Got endpoints: latency-svc-76mx6 [790.388313ms]
Jul 25 09:53:54.930: INFO: Got endpoints: latency-svc-sf85r [772.413261ms]
Jul 25 09:53:54.955: INFO: Created: latency-svc-tsjh4
Jul 25 09:53:54.956: INFO: Got endpoints: latency-svc-jj6sf [750.934076ms]
Jul 25 09:53:54.966: INFO: Created: latency-svc-b8cc5
Jul 25 09:53:54.974: INFO: Created: latency-svc-2v8f8
Jul 25 09:53:55.005: INFO: Got endpoints: latency-svc-4kp28 [751.204899ms]
Jul 25 09:53:55.014: INFO: Created: latency-svc-srrd6
Jul 25 09:53:55.054: INFO: Got endpoints: latency-svc-nf55t [750.048491ms]
Jul 25 09:53:55.067: INFO: Created: latency-svc-mshx2
Jul 25 09:53:55.104: INFO: Got endpoints: latency-svc-q6q4l [750.038565ms]
Jul 25 09:53:55.115: INFO: Created: latency-svc-pchv2
Jul 25 09:53:55.154: INFO: Got endpoints: latency-svc-mg9j9 [748.435381ms]
Jul 25 09:53:55.166: INFO: Created: latency-svc-ck2xb
Jul 25 09:53:55.204: INFO: Got endpoints: latency-svc-4tx4g [746.430214ms]
Jul 25 09:53:55.216: INFO: Created: latency-svc-sfmhz
Jul 25 09:53:55.257: INFO: Got endpoints: latency-svc-xpbbk [752.709151ms]
Jul 25 09:53:55.268: INFO: Created: latency-svc-248gx
Jul 25 09:53:55.304: INFO: Got endpoints: latency-svc-rrxbg [746.502655ms]
Jul 25 09:53:55.316: INFO: Created: latency-svc-kzzk7
Jul 25 09:53:55.354: INFO: Got endpoints: latency-svc-tf4vt [744.034245ms]
Jul 25 09:53:55.365: INFO: Created: latency-svc-lxk7h
Jul 25 09:53:55.404: INFO: Got endpoints: latency-svc-dsvvn [747.812456ms]
Jul 25 09:53:55.413: INFO: Created: latency-svc-7g47c
Jul 25 09:53:55.456: INFO: Got endpoints: latency-svc-282vw [751.719683ms]
Jul 25 09:53:55.468: INFO: Created: latency-svc-zzmcr
Jul 25 09:53:55.505: INFO: Got endpoints: latency-svc-pc7zx [751.322263ms]
Jul 25 09:53:55.517: INFO: Created: latency-svc-flhlb
Jul 25 09:53:55.554: INFO: Got endpoints: latency-svc-7qk88 [748.899262ms]
Jul 25 09:53:55.564: INFO: Created: latency-svc-rcxwq
Jul 25 09:53:55.604: INFO: Got endpoints: latency-svc-tsjh4 [707.062085ms]
Jul 25 09:53:55.620: INFO: Created: latency-svc-w2547
Jul 25 09:53:55.654: INFO: Got endpoints: latency-svc-b8cc5 [723.601981ms]
Jul 25 09:53:55.667: INFO: Created: latency-svc-rgl5r
Jul 25 09:53:55.704: INFO: Got endpoints: latency-svc-2v8f8 [748.363773ms]
Jul 25 09:53:55.720: INFO: Created: latency-svc-vz6hb
Jul 25 09:53:55.753: INFO: Got endpoints: latency-svc-srrd6 [748.104683ms]
Jul 25 09:53:55.767: INFO: Created: latency-svc-tfxkl
Jul 25 09:53:55.804: INFO: Got endpoints: latency-svc-mshx2 [749.114068ms]
Jul 25 09:53:55.817: INFO: Created: latency-svc-89zqb
Jul 25 09:53:55.855: INFO: Got endpoints: latency-svc-pchv2 [750.204673ms]
Jul 25 09:53:55.868: INFO: Created: latency-svc-jrx6p
Jul 25 09:53:55.904: INFO: Got endpoints: latency-svc-ck2xb [750.504618ms]
Jul 25 09:53:55.921: INFO: Created: latency-svc-n5xpc
Jul 25 09:53:55.956: INFO: Got endpoints: latency-svc-sfmhz [751.306045ms]
Jul 25 09:53:55.972: INFO: Created: latency-svc-kkgxb
Jul 25 09:53:56.007: INFO: Got endpoints: latency-svc-248gx [749.744216ms]
Jul 25 09:53:56.039: INFO: Created: latency-svc-m8j4g
Jul 25 09:53:56.054: INFO: Got endpoints: latency-svc-kzzk7 [750.152665ms]
Jul 25 09:53:56.068: INFO: Created: latency-svc-jrdvl
Jul 25 09:53:56.104: INFO: Got endpoints: latency-svc-lxk7h [750.800228ms]
Jul 25 09:53:56.118: INFO: Created: latency-svc-nrfxl
Jul 25 09:53:56.154: INFO: Got endpoints: latency-svc-7g47c [750.438602ms]
Jul 25 09:53:56.169: INFO: Created: latency-svc-mghrr
Jul 25 09:53:56.205: INFO: Got endpoints: latency-svc-zzmcr [748.284647ms]
Jul 25 09:53:56.220: INFO: Created: latency-svc-fxg7d
Jul 25 09:53:56.257: INFO: Got endpoints: latency-svc-flhlb [750.750724ms]
Jul 25 09:53:56.271: INFO: Created: latency-svc-j7872
Jul 25 09:53:56.305: INFO: Got endpoints: latency-svc-rcxwq [750.890921ms]
Jul 25 09:53:56.323: INFO: Created: latency-svc-cdfqm
Jul 25 09:53:56.359: INFO: Got endpoints: latency-svc-w2547 [753.89577ms]
Jul 25 09:53:56.404: INFO: Got endpoints: latency-svc-rgl5r [749.379886ms]
Jul 25 09:53:56.455: INFO: Got endpoints: latency-svc-vz6hb [749.659722ms]
Jul 25 09:53:56.504: INFO: Got endpoints: latency-svc-tfxkl [748.045245ms]
Jul 25 09:53:56.562: INFO: Got endpoints: latency-svc-89zqb [757.942429ms]
Jul 25 09:53:56.605: INFO: Got endpoints: latency-svc-jrx6p [749.892571ms]
Jul 25 09:53:56.656: INFO: Got endpoints: latency-svc-n5xpc [751.313694ms]
Jul 25 09:53:56.704: INFO: Got endpoints: latency-svc-kkgxb [748.453588ms]
Jul 25 09:53:56.756: INFO: Got endpoints: latency-svc-m8j4g [749.261323ms]
Jul 25 09:53:56.805: INFO: Got endpoints: latency-svc-jrdvl [750.349732ms]
Jul 25 09:53:56.854: INFO: Got endpoints: latency-svc-nrfxl [749.90647ms]
Jul 25 09:53:56.904: INFO: Got endpoints: latency-svc-mghrr [749.112738ms]
Jul 25 09:53:56.954: INFO: Got endpoints: latency-svc-fxg7d [748.995798ms]
Jul 25 09:53:57.008: INFO: Got endpoints: latency-svc-j7872 [751.048496ms]
Jul 25 09:53:57.055: INFO: Got endpoints: latency-svc-cdfqm [749.855583ms]
Jul 25 09:53:57.055: INFO: Latencies: [21.923696ms 53.480424ms 70.645741ms 82.431258ms 90.669839ms 113.878375ms 120.214611ms 131.069175ms 145.310907ms 161.586866ms 185.41432ms 189.569149ms 194.077628ms 196.322233ms 197.97492ms 205.295578ms 215.048795ms 216.606706ms 217.651287ms 223.185376ms 223.763751ms 223.845205ms 224.597395ms 228.070906ms 231.512759ms 238.097609ms 238.556696ms 245.729633ms 246.488822ms 248.882063ms 257.644731ms 288.750763ms 307.466805ms 316.646891ms 326.792372ms 331.745976ms 332.539212ms 334.915233ms 335.954711ms 337.70028ms 338.676597ms 339.632237ms 340.208458ms 346.383825ms 348.920059ms 355.664004ms 357.280395ms 365.386194ms 366.620449ms 392.794911ms 425.766866ms 465.735221ms 504.853936ms 537.581318ms 572.650622ms 618.687079ms 646.771811ms 674.425639ms 680.590308ms 703.425024ms 707.062085ms 723.601981ms 731.196822ms 739.330245ms 741.005833ms 741.441567ms 743.307949ms 743.416395ms 744.034245ms 744.345931ms 745.556602ms 745.841246ms 745.888954ms 746.144099ms 746.430214ms 746.467385ms 746.502655ms 746.510318ms 746.522826ms 746.817011ms 747.072234ms 747.386558ms 747.51732ms 747.521737ms 747.612274ms 747.668959ms 747.674518ms 747.73984ms 747.812456ms 747.845881ms 747.862984ms 748.045245ms 748.104683ms 748.136438ms 748.142828ms 748.284647ms 748.351965ms 748.363773ms 748.435381ms 748.453588ms 748.489511ms 748.558425ms 748.594286ms 748.620617ms 748.70666ms 748.777677ms 748.80347ms 748.819821ms 748.860215ms 748.899262ms 748.995798ms 749.035944ms 749.112738ms 749.114068ms 749.163907ms 749.165155ms 749.261323ms 749.271513ms 749.358277ms 749.365293ms 749.379886ms 749.436152ms 749.534943ms 749.53864ms 749.567601ms 749.659722ms 749.72299ms 749.744216ms 749.776137ms 749.781419ms 749.794709ms 749.821607ms 749.82852ms 749.855583ms 749.872723ms 749.892571ms 749.90647ms 749.909622ms 749.918112ms 749.923105ms 749.955304ms 750.024493ms 750.02523ms 750.038565ms 750.048491ms 750.085755ms 750.111388ms 750.144358ms 750.152665ms 750.19224ms 750.204673ms 750.248338ms 750.349732ms 750.378583ms 750.438602ms 750.44444ms 750.502986ms 750.504618ms 750.567171ms 750.574619ms 750.661786ms 750.680417ms 750.728816ms 750.74785ms 750.750724ms 750.765999ms 750.800228ms 750.800775ms 750.890921ms 750.934076ms 751.048496ms 751.204899ms 751.208177ms 751.306045ms 751.313694ms 751.322263ms 751.368988ms 751.390192ms 751.400543ms 751.4601ms 751.64753ms 751.719683ms 751.737379ms 751.961698ms 752.251816ms 752.275563ms 752.682472ms 752.709151ms 752.71332ms 753.273438ms 753.368209ms 753.89577ms 754.289716ms 755.953652ms 757.942429ms 760.649897ms 770.920839ms 772.413261ms 790.388313ms 819.866555ms]
Jul 25 09:53:57.055: INFO: 50 %ile: 748.489511ms
Jul 25 09:53:57.055: INFO: 90 %ile: 751.64753ms
Jul 25 09:53:57.055: INFO: 99 %ile: 790.388313ms
Jul 25 09:53:57.055: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:53:57.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1473" for this suite.
Jul 25 09:54:09.076: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:54:09.150: INFO: namespace svc-latency-1473 deletion completed in 12.089034489s

• [SLOW TEST:22.841 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:54:09.152: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name s-test-opt-del-570ef4f8-4e1f-4eba-a3f4-50355c4d49e7
STEP: Creating secret with name s-test-opt-upd-b1d235a9-396f-470f-856d-80aa31ced812
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-570ef4f8-4e1f-4eba-a3f4-50355c4d49e7
STEP: Updating secret s-test-opt-upd-b1d235a9-396f-470f-856d-80aa31ced812
STEP: Creating secret with name s-test-opt-create-31046a4f-2a19-43c3-9078-7338e7b59754
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:54:13.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3019" for this suite.
Jul 25 09:54:31.270: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:54:31.340: INFO: namespace projected-3019 deletion completed in 18.077239924s

• [SLOW TEST:22.188 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:54:31.341: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
Jul 25 09:54:31.435: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:54:35.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1077" for this suite.
Jul 25 09:54:41.190: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:54:41.289: INFO: namespace init-container-1077 deletion completed in 6.111560473s

• [SLOW TEST:9.948 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:54:41.296: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:76
Jul 25 09:54:41.320: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Registering the sample API server.
Jul 25 09:54:41.575: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jul 25 09:54:43.620: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 25 09:54:45.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 25 09:54:47.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 25 09:54:49.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 25 09:54:51.628: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 25 09:54:53.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 25 09:54:55.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 25 09:54:57.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 25 09:54:59.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 25 09:55:01.626: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 25 09:55:03.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 25 09:55:05.625: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 25 09:55:07.626: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 25 09:55:09.624: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63699645281, loc:(*time.Location)(0x80bb5c0)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-7c4bdb86cc\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jul 25 09:55:39.848: INFO: Waited 28.216830623s for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:67
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:55:40.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-2786" for this suite.
Jul 25 09:55:46.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:55:46.485: INFO: namespace aggregator-2786 deletion completed in 6.197927117s

• [SLOW TEST:65.189 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:55:46.485: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test override command
Jul 25 09:55:46.517: INFO: Waiting up to 5m0s for pod "client-containers-18c58fcd-0f0b-43f8-817e-e1fe20548204" in namespace "containers-2854" to be "success or failure"
Jul 25 09:55:46.519: INFO: Pod "client-containers-18c58fcd-0f0b-43f8-817e-e1fe20548204": Phase="Pending", Reason="", readiness=false. Elapsed: 2.343121ms
Jul 25 09:55:48.523: INFO: Pod "client-containers-18c58fcd-0f0b-43f8-817e-e1fe20548204": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006263831s
STEP: Saw pod success
Jul 25 09:55:48.523: INFO: Pod "client-containers-18c58fcd-0f0b-43f8-817e-e1fe20548204" satisfied condition "success or failure"
Jul 25 09:55:48.526: INFO: Trying to get logs from node essentialpks-conformance-3 pod client-containers-18c58fcd-0f0b-43f8-817e-e1fe20548204 container test-container: <nil>
STEP: delete the pod
Jul 25 09:55:48.544: INFO: Waiting for pod client-containers-18c58fcd-0f0b-43f8-817e-e1fe20548204 to disappear
Jul 25 09:55:48.547: INFO: Pod client-containers-18c58fcd-0f0b-43f8-817e-e1fe20548204 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:55:48.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-2854" for this suite.
Jul 25 09:55:54.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:55:54.655: INFO: namespace containers-2854 deletion completed in 6.100960253s

• [SLOW TEST:8.170 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:55:54.661: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-3488570c-6a86-4f1b-a484-8de9875e7efd
STEP: Creating a pod to test consume secrets
Jul 25 09:55:54.704: INFO: Waiting up to 5m0s for pod "pod-secrets-ac717a8d-0c16-4993-a798-3e46874bbb0c" in namespace "secrets-9704" to be "success or failure"
Jul 25 09:55:54.709: INFO: Pod "pod-secrets-ac717a8d-0c16-4993-a798-3e46874bbb0c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.447448ms
Jul 25 09:55:56.712: INFO: Pod "pod-secrets-ac717a8d-0c16-4993-a798-3e46874bbb0c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007378168s
STEP: Saw pod success
Jul 25 09:55:56.712: INFO: Pod "pod-secrets-ac717a8d-0c16-4993-a798-3e46874bbb0c" satisfied condition "success or failure"
Jul 25 09:55:56.714: INFO: Trying to get logs from node essentialpks-conformance-3 pod pod-secrets-ac717a8d-0c16-4993-a798-3e46874bbb0c container secret-volume-test: <nil>
STEP: delete the pod
Jul 25 09:55:56.727: INFO: Waiting for pod pod-secrets-ac717a8d-0c16-4993-a798-3e46874bbb0c to disappear
Jul 25 09:55:56.730: INFO: Pod pod-secrets-ac717a8d-0c16-4993-a798-3e46874bbb0c no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:55:56.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9704" for this suite.
Jul 25 09:56:02.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:56:02.820: INFO: namespace secrets-9704 deletion completed in 6.085964316s

• [SLOW TEST:8.159 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:56:02.820: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Jul 25 09:56:05.883: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:56:05.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2188" for this suite.
Jul 25 09:56:27.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:56:28.030: INFO: namespace replicaset-2188 deletion completed in 22.118065493s

• [SLOW TEST:25.210 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:56:28.030: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 25 09:56:28.079: INFO: Waiting up to 5m0s for pod "downwardapi-volume-555970ab-8d60-41a6-bbd1-50613edf4926" in namespace "projected-6738" to be "success or failure"
Jul 25 09:56:28.088: INFO: Pod "downwardapi-volume-555970ab-8d60-41a6-bbd1-50613edf4926": Phase="Pending", Reason="", readiness=false. Elapsed: 9.136758ms
Jul 25 09:56:30.093: INFO: Pod "downwardapi-volume-555970ab-8d60-41a6-bbd1-50613edf4926": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014601491s
STEP: Saw pod success
Jul 25 09:56:30.094: INFO: Pod "downwardapi-volume-555970ab-8d60-41a6-bbd1-50613edf4926" satisfied condition "success or failure"
Jul 25 09:56:30.099: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-555970ab-8d60-41a6-bbd1-50613edf4926 container client-container: <nil>
STEP: delete the pod
Jul 25 09:56:30.119: INFO: Waiting for pod downwardapi-volume-555970ab-8d60-41a6-bbd1-50613edf4926 to disappear
Jul 25 09:56:30.122: INFO: Pod downwardapi-volume-555970ab-8d60-41a6-bbd1-50613edf4926 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:56:30.122: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6738" for this suite.
Jul 25 09:56:36.138: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:56:36.248: INFO: namespace projected-6738 deletion completed in 6.121423559s

• [SLOW TEST:8.218 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:56:36.251: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-e00d2f7e-eaa9-4870-81bb-2a897d6adeeb
STEP: Creating a pod to test consume secrets
Jul 25 09:56:36.294: INFO: Waiting up to 5m0s for pod "pod-secrets-a89b7823-c0dd-48d1-a078-761436cda68b" in namespace "secrets-5776" to be "success or failure"
Jul 25 09:56:36.301: INFO: Pod "pod-secrets-a89b7823-c0dd-48d1-a078-761436cda68b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.796515ms
Jul 25 09:56:38.304: INFO: Pod "pod-secrets-a89b7823-c0dd-48d1-a078-761436cda68b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010127538s
STEP: Saw pod success
Jul 25 09:56:38.304: INFO: Pod "pod-secrets-a89b7823-c0dd-48d1-a078-761436cda68b" satisfied condition "success or failure"
Jul 25 09:56:38.306: INFO: Trying to get logs from node essentialpks-conformance-3 pod pod-secrets-a89b7823-c0dd-48d1-a078-761436cda68b container secret-volume-test: <nil>
STEP: delete the pod
Jul 25 09:56:38.320: INFO: Waiting for pod pod-secrets-a89b7823-c0dd-48d1-a078-761436cda68b to disappear
Jul 25 09:56:38.322: INFO: Pod pod-secrets-a89b7823-c0dd-48d1-a078-761436cda68b no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 09:56:38.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5776" for this suite.
Jul 25 09:56:44.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 09:56:44.423: INFO: namespace secrets-5776 deletion completed in 6.097786786s

• [SLOW TEST:8.172 seconds]
[sig-storage] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 09:56:44.423: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod busybox-e9ee4fc2-9945-4d45-9a5b-25c9c22b18b3 in namespace container-probe-2537
Jul 25 09:56:46.463: INFO: Started pod busybox-e9ee4fc2-9945-4d45-9a5b-25c9c22b18b3 in namespace container-probe-2537
STEP: checking the pod's current state and verifying that restartCount is present
Jul 25 09:56:46.468: INFO: Initial restart count of pod busybox-e9ee4fc2-9945-4d45-9a5b-25c9c22b18b3 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:00:47.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2537" for this suite.
Jul 25 10:00:53.023: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:00:53.099: INFO: namespace container-probe-2537 deletion completed in 6.087189664s

• [SLOW TEST:248.676 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:00:53.100: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Jul 25 10:00:53.364: INFO: Pod name wrapped-volume-race-546d4b6c-1aa6-4acc-99b0-413b61a259de: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-546d4b6c-1aa6-4acc-99b0-413b61a259de in namespace emptydir-wrapper-6681, will wait for the garbage collector to delete the pods
Jul 25 10:01:09.486: INFO: Deleting ReplicationController wrapped-volume-race-546d4b6c-1aa6-4acc-99b0-413b61a259de took: 8.792965ms
Jul 25 10:01:09.786: INFO: Terminating ReplicationController wrapped-volume-race-546d4b6c-1aa6-4acc-99b0-413b61a259de pods took: 300.449416ms
STEP: Creating RC which spawns configmap-volume pods
Jul 25 10:01:50.004: INFO: Pod name wrapped-volume-race-f89bada2-345d-4d63-9eb8-fca7e7380eb5: Found 0 pods out of 5
Jul 25 10:01:55.012: INFO: Pod name wrapped-volume-race-f89bada2-345d-4d63-9eb8-fca7e7380eb5: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-f89bada2-345d-4d63-9eb8-fca7e7380eb5 in namespace emptydir-wrapper-6681, will wait for the garbage collector to delete the pods
Jul 25 10:02:05.093: INFO: Deleting ReplicationController wrapped-volume-race-f89bada2-345d-4d63-9eb8-fca7e7380eb5 took: 9.330564ms
Jul 25 10:02:05.394: INFO: Terminating ReplicationController wrapped-volume-race-f89bada2-345d-4d63-9eb8-fca7e7380eb5 pods took: 300.2759ms
STEP: Creating RC which spawns configmap-volume pods
Jul 25 10:02:44.816: INFO: Pod name wrapped-volume-race-8d30f3a4-0815-47c4-ae60-28855825ba45: Found 0 pods out of 5
Jul 25 10:02:49.825: INFO: Pod name wrapped-volume-race-8d30f3a4-0815-47c4-ae60-28855825ba45: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-8d30f3a4-0815-47c4-ae60-28855825ba45 in namespace emptydir-wrapper-6681, will wait for the garbage collector to delete the pods
Jul 25 10:02:59.903: INFO: Deleting ReplicationController wrapped-volume-race-8d30f3a4-0815-47c4-ae60-28855825ba45 took: 6.454458ms
Jul 25 10:03:00.203: INFO: Terminating ReplicationController wrapped-volume-race-8d30f3a4-0815-47c4-ae60-28855825ba45 pods took: 300.35624ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:03:41.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-6681" for this suite.
Jul 25 10:03:47.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:03:47.232: INFO: namespace emptydir-wrapper-6681 deletion completed in 6.075037326s

• [SLOW TEST:174.133 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:03:47.233: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1294.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1294.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 25 10:03:49.297: INFO: DNS probes using dns-1294/dns-test-c1fa290c-c1b6-4d28-8afa-949ca4961a70 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:03:49.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1294" for this suite.
Jul 25 10:03:55.318: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:03:55.402: INFO: namespace dns-1294 deletion completed in 6.093872945s

• [SLOW TEST:8.170 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:03:55.404: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:03:57.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2872" for this suite.
Jul 25 10:04:41.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:04:41.565: INFO: namespace kubelet-test-2872 deletion completed in 44.102596795s

• [SLOW TEST:46.165 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when scheduling a read only busybox container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:04:41.571: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: validating cluster-info
Jul 25 10:04:41.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 cluster-info'
Jul 25 10:04:41.918: INFO: stderr: ""
Jul 25 10:04:41.918: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:04:41.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1114" for this suite.
Jul 25 10:04:47.931: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:04:48.181: INFO: namespace kubectl-1114 deletion completed in 6.259473207s

• [SLOW TEST:6.610 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl cluster-info
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:04:48.181: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-map-e96d65b5-396c-4372-b35d-19a6ba79b8e2
STEP: Creating a pod to test consume configMaps
Jul 25 10:04:48.216: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-2036262a-50ff-4dab-a727-ac1995a5c3cf" in namespace "projected-9475" to be "success or failure"
Jul 25 10:04:48.220: INFO: Pod "pod-projected-configmaps-2036262a-50ff-4dab-a727-ac1995a5c3cf": Phase="Pending", Reason="", readiness=false. Elapsed: 4.112534ms
Jul 25 10:04:50.223: INFO: Pod "pod-projected-configmaps-2036262a-50ff-4dab-a727-ac1995a5c3cf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007176823s
STEP: Saw pod success
Jul 25 10:04:50.223: INFO: Pod "pod-projected-configmaps-2036262a-50ff-4dab-a727-ac1995a5c3cf" satisfied condition "success or failure"
Jul 25 10:04:50.227: INFO: Trying to get logs from node essentialpks-conformance-3 pod pod-projected-configmaps-2036262a-50ff-4dab-a727-ac1995a5c3cf container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 25 10:04:50.248: INFO: Waiting for pod pod-projected-configmaps-2036262a-50ff-4dab-a727-ac1995a5c3cf to disappear
Jul 25 10:04:50.250: INFO: Pod pod-projected-configmaps-2036262a-50ff-4dab-a727-ac1995a5c3cf no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:04:50.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9475" for this suite.
Jul 25 10:04:56.264: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:04:56.345: INFO: namespace projected-9475 deletion completed in 6.091462451s

• [SLOW TEST:8.164 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:04:56.347: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 25 10:04:56.382: INFO: Waiting up to 5m0s for pod "pod-c2221a3e-1837-4bc5-b306-dbf1e8a120d4" in namespace "emptydir-9491" to be "success or failure"
Jul 25 10:04:56.385: INFO: Pod "pod-c2221a3e-1837-4bc5-b306-dbf1e8a120d4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.348879ms
Jul 25 10:04:58.388: INFO: Pod "pod-c2221a3e-1837-4bc5-b306-dbf1e8a120d4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005307042s
STEP: Saw pod success
Jul 25 10:04:58.388: INFO: Pod "pod-c2221a3e-1837-4bc5-b306-dbf1e8a120d4" satisfied condition "success or failure"
Jul 25 10:04:58.391: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-c2221a3e-1837-4bc5-b306-dbf1e8a120d4 container test-container: <nil>
STEP: delete the pod
Jul 25 10:04:58.409: INFO: Waiting for pod pod-c2221a3e-1837-4bc5-b306-dbf1e8a120d4 to disappear
Jul 25 10:04:58.411: INFO: Pod pod-c2221a3e-1837-4bc5-b306-dbf1e8a120d4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:04:58.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9491" for this suite.
Jul 25 10:05:04.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:05:04.525: INFO: namespace emptydir-9491 deletion completed in 6.109440079s

• [SLOW TEST:8.179 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:05:04.526: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 25 10:05:04.608: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jul 25 10:05:04.614: INFO: Pod name sample-pod: Found 0 pods out of 1
Jul 25 10:05:09.619: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Jul 25 10:05:09.619: INFO: Creating deployment "test-rolling-update-deployment"
Jul 25 10:05:09.625: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jul 25 10:05:09.632: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jul 25 10:05:11.638: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jul 25 10:05:11.641: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Jul 25 10:05:11.648: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment,GenerateName:,Namespace:deployment-1476,SelfLink:/apis/apps/v1/namespaces/deployment-1476/deployments/test-rolling-update-deployment,UID:7816b1ef-3794-4ed5-b8a3-35e1b4a81378,ResourceVersion:72551,Generation:1,CreationTimestamp:2019-07-25 10:05:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:DeploymentSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[{Available True 2019-07-25 10:05:09 +0000 UTC 2019-07-25 10:05:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2019-07-25 10:05:11 +0000 UTC 2019-07-25 10:05:09 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-rolling-update-deployment-79f6b9d75c" has successfully progressed.}],ReadyReplicas:1,CollisionCount:nil,},}

Jul 25 10:05:11.651: INFO: New ReplicaSet "test-rolling-update-deployment-79f6b9d75c" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c,GenerateName:,Namespace:deployment-1476,SelfLink:/apis/apps/v1/namespaces/deployment-1476/replicasets/test-rolling-update-deployment-79f6b9d75c,UID:54c9d92a-5db5-479a-89ff-c7f9e8e57c3e,ResourceVersion:72541,Generation:1,CreationTimestamp:2019-07-25 10:05:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305833,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 7816b1ef-3794-4ed5-b8a3-35e1b4a81378 0xc0032310d7 0xc0032310d8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*1,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[],},}
Jul 25 10:05:11.651: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jul 25 10:05:11.651: INFO: &ReplicaSet{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-controller,GenerateName:,Namespace:deployment-1476,SelfLink:/apis/apps/v1/namespaces/deployment-1476/replicasets/test-rolling-update-controller,UID:648c522d-4127-46fd-aee9-ca4fe723fa62,ResourceVersion:72550,Generation:2,CreationTimestamp:2019-07-25 10:05:04 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{deployment.kubernetes.io/desired-replicas: 1,deployment.kubernetes.io/max-replicas: 2,deployment.kubernetes.io/revision: 3546343826724305832,},OwnerReferences:[{apps/v1 Deployment test-rolling-update-deployment 7816b1ef-3794-4ed5-b8a3-35e1b4a81378 0xc003231007 0xc003231008}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:ReplicaSetSpec{Replicas:*0,Selector:&k8s_io_apimachinery_pkg_apis_meta_v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: nginx,},MatchExpressions:[],},Template:k8s_io_api_core_v1.PodTemplateSpec{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:,GenerateName:,Namespace:,SelfLink:,UID:,ResourceVersion:,Generation:0,CreationTimestamp:0001-01-01 00:00:00 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod: nginx,},Annotations:map[string]string{},OwnerReferences:[],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[],Containers:[{nginx docker.io/library/nginx:1.14-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil /dev/termination-log File IfNotPresent nil false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:,DeprecatedServiceAccount:,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[],HostAliases:[],PriorityClassName:,Priority:nil,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:nil,PreemptionPolicy:nil,},},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[],},}
Jul 25 10:05:11.655: INFO: Pod "test-rolling-update-deployment-79f6b9d75c-zkwk2" is available:
&Pod{ObjectMeta:k8s_io_apimachinery_pkg_apis_meta_v1.ObjectMeta{Name:test-rolling-update-deployment-79f6b9d75c-zkwk2,GenerateName:test-rolling-update-deployment-79f6b9d75c-,Namespace:deployment-1476,SelfLink:/api/v1/namespaces/deployment-1476/pods/test-rolling-update-deployment-79f6b9d75c-zkwk2,UID:9242974d-a3d9-43cb-b784-cf0dccccf260,ResourceVersion:72540,Generation:0,CreationTimestamp:2019-07-25 10:05:09 +0000 UTC,DeletionTimestamp:<nil>,DeletionGracePeriodSeconds:nil,Labels:map[string]string{name: sample-pod,pod-template-hash: 79f6b9d75c,},Annotations:map[string]string{},OwnerReferences:[{apps/v1 ReplicaSet test-rolling-update-deployment-79f6b9d75c 54c9d92a-5db5-479a-89ff-c7f9e8e57c3e 0xc003231bc7 0xc003231bc8}],Finalizers:[],ClusterName:,Initializers:nil,ManagedFields:[],},Spec:PodSpec{Volumes:[{default-token-75lkw {nil nil nil nil nil SecretVolumeSource{SecretName:default-token-75lkw,Items:[],DefaultMode:*420,Optional:nil,} nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil nil}}],Containers:[{redis gcr.io/kubernetes-e2e-test-images/redis:1.0 [] []  [] [] [] {map[] map[]} [{default-token-75lkw true /var/run/secrets/kubernetes.io/serviceaccount  <nil> }] [] nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}],RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:essentialpks-conformance-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[],WindowsOptions:nil,},ImagePullSecrets:[],Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[],AutomountServiceAccountToken:nil,Tolerations:[{node.kubernetes.io/not-ready Exists  NoExecute 0xc003231c30} {node.kubernetes.io/unreachable Exists  NoExecute 0xc003231c50}],HostAliases:[],PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[],RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,},Status:PodStatus{Phase:Running,Conditions:[{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 10:05:09 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 10:05:11 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 10:05:11 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-07-25 10:05:09 +0000 UTC  }],Message:,Reason:,HostIP:192.168.102.3,PodIP:10.244.2.125,StartTime:2019-07-25 10:05:09 +0000 UTC,ContainerStatuses:[{redis {nil ContainerStateRunning{StartedAt:2019-07-25 10:05:10 +0000 UTC,} nil} {nil nil nil} true 0 gcr.io/kubernetes-e2e-test-images/redis:1.0 docker-pullable://gcr.io/kubernetes-e2e-test-images/redis@sha256:af4748d1655c08dc54d4be5182135395db9ce87aba2d4699b26b14ae197c5830 docker://0b4118c4704fbed97b4cb655e32f80bf94c200710bfe2b7b4a7ee7298623b0b0}],QOSClass:BestEffort,InitContainerStatuses:[],NominatedNodeName:,},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:05:11.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1476" for this suite.
Jul 25 10:05:17.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:05:17.759: INFO: namespace deployment-1476 deletion completed in 6.100482724s

• [SLOW TEST:13.233 seconds]
[sig-apps] Deployment
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:05:17.760: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:88
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating service multi-endpoint-test in namespace services-7462
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7462 to expose endpoints map[]
Jul 25 10:05:17.852: INFO: Get endpoints failed (9.549594ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Jul 25 10:05:18.857: INFO: successfully validated that service multi-endpoint-test in namespace services-7462 exposes endpoints map[] (1.013979s elapsed)
STEP: Creating pod pod1 in namespace services-7462
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7462 to expose endpoints map[pod1:[100]]
Jul 25 10:05:20.884: INFO: successfully validated that service multi-endpoint-test in namespace services-7462 exposes endpoints map[pod1:[100]] (2.022583745s elapsed)
STEP: Creating pod pod2 in namespace services-7462
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7462 to expose endpoints map[pod1:[100] pod2:[101]]
Jul 25 10:05:22.917: INFO: successfully validated that service multi-endpoint-test in namespace services-7462 exposes endpoints map[pod1:[100] pod2:[101]] (2.028346253s elapsed)
STEP: Deleting pod pod1 in namespace services-7462
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7462 to expose endpoints map[pod2:[101]]
Jul 25 10:05:23.937: INFO: successfully validated that service multi-endpoint-test in namespace services-7462 exposes endpoints map[pod2:[101]] (1.016849182s elapsed)
STEP: Deleting pod pod2 in namespace services-7462
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-7462 to expose endpoints map[]
Jul 25 10:05:24.952: INFO: successfully validated that service multi-endpoint-test in namespace services-7462 exposes endpoints map[] (1.011188995s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:05:24.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7462" for this suite.
Jul 25 10:05:46.988: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:05:47.092: INFO: namespace services-7462 deletion completed in 22.113602529s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:92

• [SLOW TEST:29.333 seconds]
[sig-network] Services
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:05:47.092: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-map-f58a390c-6203-4119-8b7a-68c0debe9fc2
STEP: Creating a pod to test consume configMaps
Jul 25 10:05:47.138: INFO: Waiting up to 5m0s for pod "pod-configmaps-b804aae2-0559-4f23-97fa-75fef9c9521d" in namespace "configmap-9023" to be "success or failure"
Jul 25 10:05:47.146: INFO: Pod "pod-configmaps-b804aae2-0559-4f23-97fa-75fef9c9521d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.360375ms
Jul 25 10:05:49.149: INFO: Pod "pod-configmaps-b804aae2-0559-4f23-97fa-75fef9c9521d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010946761s
STEP: Saw pod success
Jul 25 10:05:49.150: INFO: Pod "pod-configmaps-b804aae2-0559-4f23-97fa-75fef9c9521d" satisfied condition "success or failure"
Jul 25 10:05:49.155: INFO: Trying to get logs from node essentialpks-conformance-3 pod pod-configmaps-b804aae2-0559-4f23-97fa-75fef9c9521d container configmap-volume-test: <nil>
STEP: delete the pod
Jul 25 10:05:49.172: INFO: Waiting for pod pod-configmaps-b804aae2-0559-4f23-97fa-75fef9c9521d to disappear
Jul 25 10:05:49.174: INFO: Pod pod-configmaps-b804aae2-0559-4f23-97fa-75fef9c9521d no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:05:49.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9023" for this suite.
Jul 25 10:05:55.186: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:05:55.289: INFO: namespace configmap-9023 deletion completed in 6.111883328s

• [SLOW TEST:8.196 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:05:55.291: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Starting the proxy
Jul 25 10:05:55.318: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-739010025 proxy --unix-socket=/tmp/kubectl-proxy-unix150793384/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:05:55.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5173" for this suite.
Jul 25 10:06:01.399: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:06:01.483: INFO: namespace kubectl-5173 deletion completed in 6.093724632s

• [SLOW TEST:6.195 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Proxy server
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:06:01.487: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-b0443088-b752-4b4f-a287-0e10f24eeb3a
STEP: Creating a pod to test consume configMaps
Jul 25 10:06:01.521: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-edc82740-18cc-4084-95af-e22b48d40051" in namespace "projected-4148" to be "success or failure"
Jul 25 10:06:01.523: INFO: Pod "pod-projected-configmaps-edc82740-18cc-4084-95af-e22b48d40051": Phase="Pending", Reason="", readiness=false. Elapsed: 2.900472ms
Jul 25 10:06:03.530: INFO: Pod "pod-projected-configmaps-edc82740-18cc-4084-95af-e22b48d40051": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009553594s
STEP: Saw pod success
Jul 25 10:06:03.530: INFO: Pod "pod-projected-configmaps-edc82740-18cc-4084-95af-e22b48d40051" satisfied condition "success or failure"
Jul 25 10:06:03.535: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-projected-configmaps-edc82740-18cc-4084-95af-e22b48d40051 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 25 10:06:03.556: INFO: Waiting for pod pod-projected-configmaps-edc82740-18cc-4084-95af-e22b48d40051 to disappear
Jul 25 10:06:03.559: INFO: Pod pod-projected-configmaps-edc82740-18cc-4084-95af-e22b48d40051 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:06:03.559: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4148" for this suite.
Jul 25 10:06:09.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:06:09.658: INFO: namespace projected-4148 deletion completed in 6.094920424s

• [SLOW TEST:8.171 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:06:09.659: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Jul 25 10:06:11.716: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-739010025 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Jul 25 10:06:16.817: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:06:16.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4122" for this suite.
Jul 25 10:06:22.842: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:06:22.921: INFO: namespace pods-4122 deletion completed in 6.092569888s

• [SLOW TEST:13.262 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should be submitted and removed [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:06:22.922: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0666 on node default medium
Jul 25 10:06:22.953: INFO: Waiting up to 5m0s for pod "pod-1359d6b0-9475-41fe-8e76-66b9e4c2a347" in namespace "emptydir-6393" to be "success or failure"
Jul 25 10:06:22.960: INFO: Pod "pod-1359d6b0-9475-41fe-8e76-66b9e4c2a347": Phase="Pending", Reason="", readiness=false. Elapsed: 6.718367ms
Jul 25 10:06:24.964: INFO: Pod "pod-1359d6b0-9475-41fe-8e76-66b9e4c2a347": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011242688s
STEP: Saw pod success
Jul 25 10:06:24.964: INFO: Pod "pod-1359d6b0-9475-41fe-8e76-66b9e4c2a347" satisfied condition "success or failure"
Jul 25 10:06:24.969: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-1359d6b0-9475-41fe-8e76-66b9e4c2a347 container test-container: <nil>
STEP: delete the pod
Jul 25 10:06:24.988: INFO: Waiting for pod pod-1359d6b0-9475-41fe-8e76-66b9e4c2a347 to disappear
Jul 25 10:06:24.991: INFO: Pod pod-1359d6b0-9475-41fe-8e76-66b9e4c2a347 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:06:24.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6393" for this suite.
Jul 25 10:06:31.016: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:06:31.094: INFO: namespace emptydir-6393 deletion completed in 6.096928708s

• [SLOW TEST:8.173 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:06:31.101: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1517
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 25 10:06:31.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 run e2e-test-nginx-rc --image=docker.io/library/nginx:1.14-alpine --generator=run/v1 --namespace=kubectl-3517'
Jul 25 10:06:31.207: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 25 10:06:31.207: INFO: stdout: "replicationcontroller/e2e-test-nginx-rc created\n"
STEP: verifying the rc e2e-test-nginx-rc was created
STEP: rolling-update to same image controller
Jul 25 10:06:31.218: INFO: scanned /root for discovery docs: <nil>
Jul 25 10:06:31.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 rolling-update e2e-test-nginx-rc --update-period=1s --image=docker.io/library/nginx:1.14-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-3517'
Jul 25 10:06:46.999: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Jul 25 10:06:46.999: INFO: stdout: "Created e2e-test-nginx-rc-dcfb3daafadbd88cb495ff9b4f9c9cb5\nScaling up e2e-test-nginx-rc-dcfb3daafadbd88cb495ff9b4f9c9cb5 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-dcfb3daafadbd88cb495ff9b4f9c9cb5 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-dcfb3daafadbd88cb495ff9b4f9c9cb5 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
Jul 25 10:06:46.999: INFO: stdout: "Created e2e-test-nginx-rc-dcfb3daafadbd88cb495ff9b4f9c9cb5\nScaling up e2e-test-nginx-rc-dcfb3daafadbd88cb495ff9b4f9c9cb5 from 0 to 1, scaling down e2e-test-nginx-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-nginx-rc-dcfb3daafadbd88cb495ff9b4f9c9cb5 up to 1\nScaling e2e-test-nginx-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-nginx-rc\nRenaming e2e-test-nginx-rc-dcfb3daafadbd88cb495ff9b4f9c9cb5 to e2e-test-nginx-rc\nreplicationcontroller/e2e-test-nginx-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-nginx-rc pods to come up.
Jul 25 10:06:46.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-nginx-rc --namespace=kubectl-3517'
Jul 25 10:06:47.097: INFO: stderr: ""
Jul 25 10:06:47.097: INFO: stdout: "e2e-test-nginx-rc-dcfb3daafadbd88cb495ff9b4f9c9cb5-24fvd "
Jul 25 10:06:47.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods e2e-test-nginx-rc-dcfb3daafadbd88cb495ff9b4f9c9cb5-24fvd -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-nginx-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-3517'
Jul 25 10:06:47.197: INFO: stderr: ""
Jul 25 10:06:47.197: INFO: stdout: "true"
Jul 25 10:06:47.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 get pods e2e-test-nginx-rc-dcfb3daafadbd88cb495ff9b4f9c9cb5-24fvd -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-nginx-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-3517'
Jul 25 10:06:47.300: INFO: stderr: ""
Jul 25 10:06:47.300: INFO: stdout: "docker.io/library/nginx:1.14-alpine"
Jul 25 10:06:47.300: INFO: e2e-test-nginx-rc-dcfb3daafadbd88cb495ff9b4f9c9cb5-24fvd is verified up and running
[AfterEach] [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1523
Jul 25 10:06:47.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 delete rc e2e-test-nginx-rc --namespace=kubectl-3517'
Jul 25 10:06:47.409: INFO: stderr: ""
Jul 25 10:06:47.409: INFO: stdout: "replicationcontroller \"e2e-test-nginx-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:06:47.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3517" for this suite.
Jul 25 10:07:09.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:07:09.507: INFO: namespace kubectl-3517 deletion completed in 22.091819088s

• [SLOW TEST:38.407 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl rolling-update
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:07:09.508: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating pod
Jul 25 10:07:11.554: INFO: Pod pod-hostip-12ac6ce4-ce2a-4931-be94-cbf52efb46ad has hostIP: 192.168.102.4
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:07:11.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9509" for this suite.
Jul 25 10:07:33.571: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:07:33.658: INFO: namespace pods-9509 deletion completed in 22.098846291s

• [SLOW TEST:24.150 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:07:33.660: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 25 10:07:33.703: INFO: (0) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.711653ms)
Jul 25 10:07:33.708: INFO: (1) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.151632ms)
Jul 25 10:07:33.711: INFO: (2) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.662856ms)
Jul 25 10:07:33.716: INFO: (3) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.285128ms)
Jul 25 10:07:33.719: INFO: (4) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.548825ms)
Jul 25 10:07:33.722: INFO: (5) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.936244ms)
Jul 25 10:07:33.726: INFO: (6) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.778395ms)
Jul 25 10:07:33.729: INFO: (7) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.980251ms)
Jul 25 10:07:33.733: INFO: (8) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.419498ms)
Jul 25 10:07:33.736: INFO: (9) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.101235ms)
Jul 25 10:07:33.739: INFO: (10) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.532485ms)
Jul 25 10:07:33.743: INFO: (11) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.269591ms)
Jul 25 10:07:33.747: INFO: (12) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 4.010712ms)
Jul 25 10:07:33.751: INFO: (13) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.843986ms)
Jul 25 10:07:33.755: INFO: (14) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.925098ms)
Jul 25 10:07:33.757: INFO: (15) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.685912ms)
Jul 25 10:07:33.760: INFO: (16) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.863496ms)
Jul 25 10:07:33.763: INFO: (17) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 2.625847ms)
Jul 25 10:07:33.766: INFO: (18) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 3.195085ms)
Jul 25 10:07:33.771: INFO: (19) /api/v1/nodes/essentialpks-conformance-2/proxy/logs/: <pre>
<a href="alternatives.log">alternatives.log</a>
<a href="apt/">apt/</a>
<a href="auth.log">... (200; 5.35038ms)
[AfterEach] version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:07:33.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-2691" for this suite.
Jul 25 10:07:39.786: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:07:39.877: INFO: namespace proxy-2691 deletion completed in 6.101757626s

• [SLOW TEST:6.217 seconds]
[sig-network] Proxy
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:58
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:07:39.878: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name projected-secret-test-607dd8a8-3d29-477e-b1c6-22424b799bc9
STEP: Creating a pod to test consume secrets
Jul 25 10:07:39.912: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-b4e8c346-919a-4996-aa84-77250b67afcf" in namespace "projected-608" to be "success or failure"
Jul 25 10:07:39.918: INFO: Pod "pod-projected-secrets-b4e8c346-919a-4996-aa84-77250b67afcf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.310119ms
Jul 25 10:07:41.921: INFO: Pod "pod-projected-secrets-b4e8c346-919a-4996-aa84-77250b67afcf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009136048s
STEP: Saw pod success
Jul 25 10:07:41.922: INFO: Pod "pod-projected-secrets-b4e8c346-919a-4996-aa84-77250b67afcf" satisfied condition "success or failure"
Jul 25 10:07:41.925: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-projected-secrets-b4e8c346-919a-4996-aa84-77250b67afcf container secret-volume-test: <nil>
STEP: delete the pod
Jul 25 10:07:41.941: INFO: Waiting for pod pod-projected-secrets-b4e8c346-919a-4996-aa84-77250b67afcf to disappear
Jul 25 10:07:41.945: INFO: Pod pod-projected-secrets-b4e8c346-919a-4996-aa84-77250b67afcf no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:07:41.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-608" for this suite.
Jul 25 10:07:47.960: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:07:48.074: INFO: namespace projected-608 deletion completed in 6.124256463s

• [SLOW TEST:8.196 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:07:48.078: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 25 10:07:48.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 version'
Jul 25 10:07:48.211: INFO: stderr: ""
Jul 25 10:07:48.211: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.0\", GitCommit:\"e8462b5b5dc2584fdcd18e6bcfe9f1e4d970a529\", GitTreeState:\"clean\", BuildDate:\"2019-06-19T16:40:16Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"15\", GitVersion:\"v1.15.1+vmware.1\", GitCommit:\"a8b45a650288c3666840a23177a7bccc1a9ce1ea\", GitTreeState:\"clean\", BuildDate:\"2019-07-22T23:58:14Z\", GoVersion:\"go1.12.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:07:48.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6986" for this suite.
Jul 25 10:07:54.225: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:07:54.329: INFO: namespace kubectl-6986 deletion completed in 6.113919162s

• [SLOW TEST:6.253 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl version
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:07:54.339: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0777 on tmpfs
Jul 25 10:07:54.375: INFO: Waiting up to 5m0s for pod "pod-c5de7c53-3928-4ecb-a73f-766df469ac08" in namespace "emptydir-8385" to be "success or failure"
Jul 25 10:07:54.384: INFO: Pod "pod-c5de7c53-3928-4ecb-a73f-766df469ac08": Phase="Pending", Reason="", readiness=false. Elapsed: 8.052974ms
Jul 25 10:07:56.387: INFO: Pod "pod-c5de7c53-3928-4ecb-a73f-766df469ac08": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.011341224s
STEP: Saw pod success
Jul 25 10:07:56.387: INFO: Pod "pod-c5de7c53-3928-4ecb-a73f-766df469ac08" satisfied condition "success or failure"
Jul 25 10:07:56.393: INFO: Trying to get logs from node essentialpks-conformance-3 pod pod-c5de7c53-3928-4ecb-a73f-766df469ac08 container test-container: <nil>
STEP: delete the pod
Jul 25 10:07:56.407: INFO: Waiting for pod pod-c5de7c53-3928-4ecb-a73f-766df469ac08 to disappear
Jul 25 10:07:56.409: INFO: Pod pod-c5de7c53-3928-4ecb-a73f-766df469ac08 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:07:56.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8385" for this suite.
Jul 25 10:08:02.422: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:08:02.517: INFO: namespace emptydir-8385 deletion completed in 6.104861534s

• [SLOW TEST:8.178 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:08:02.527: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-downwardapi-nsms
STEP: Creating a pod to test atomic-volume-subpath
Jul 25 10:08:02.567: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-nsms" in namespace "subpath-8846" to be "success or failure"
Jul 25 10:08:02.572: INFO: Pod "pod-subpath-test-downwardapi-nsms": Phase="Pending", Reason="", readiness=false. Elapsed: 5.093877ms
Jul 25 10:08:04.576: INFO: Pod "pod-subpath-test-downwardapi-nsms": Phase="Running", Reason="", readiness=true. Elapsed: 2.008738617s
Jul 25 10:08:06.579: INFO: Pod "pod-subpath-test-downwardapi-nsms": Phase="Running", Reason="", readiness=true. Elapsed: 4.011853757s
Jul 25 10:08:08.583: INFO: Pod "pod-subpath-test-downwardapi-nsms": Phase="Running", Reason="", readiness=true. Elapsed: 6.016451855s
Jul 25 10:08:10.587: INFO: Pod "pod-subpath-test-downwardapi-nsms": Phase="Running", Reason="", readiness=true. Elapsed: 8.020522746s
Jul 25 10:08:12.593: INFO: Pod "pod-subpath-test-downwardapi-nsms": Phase="Running", Reason="", readiness=true. Elapsed: 10.025885862s
Jul 25 10:08:14.597: INFO: Pod "pod-subpath-test-downwardapi-nsms": Phase="Running", Reason="", readiness=true. Elapsed: 12.029673961s
Jul 25 10:08:16.600: INFO: Pod "pod-subpath-test-downwardapi-nsms": Phase="Running", Reason="", readiness=true. Elapsed: 14.033455478s
Jul 25 10:08:18.604: INFO: Pod "pod-subpath-test-downwardapi-nsms": Phase="Running", Reason="", readiness=true. Elapsed: 16.037291602s
Jul 25 10:08:20.610: INFO: Pod "pod-subpath-test-downwardapi-nsms": Phase="Running", Reason="", readiness=true. Elapsed: 18.043228952s
Jul 25 10:08:22.614: INFO: Pod "pod-subpath-test-downwardapi-nsms": Phase="Running", Reason="", readiness=true. Elapsed: 20.047324915s
Jul 25 10:08:24.618: INFO: Pod "pod-subpath-test-downwardapi-nsms": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.051026216s
STEP: Saw pod success
Jul 25 10:08:24.618: INFO: Pod "pod-subpath-test-downwardapi-nsms" satisfied condition "success or failure"
Jul 25 10:08:24.622: INFO: Trying to get logs from node essentialpks-conformance-3 pod pod-subpath-test-downwardapi-nsms container test-container-subpath-downwardapi-nsms: <nil>
STEP: delete the pod
Jul 25 10:08:24.637: INFO: Waiting for pod pod-subpath-test-downwardapi-nsms to disappear
Jul 25 10:08:24.642: INFO: Pod pod-subpath-test-downwardapi-nsms no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-nsms
Jul 25 10:08:24.642: INFO: Deleting pod "pod-subpath-test-downwardapi-nsms" in namespace "subpath-8846"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:08:24.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8846" for this suite.
Jul 25 10:08:30.660: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:08:30.758: INFO: namespace subpath-8846 deletion completed in 6.109744577s

• [SLOW TEST:28.232 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:08:30.759: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-volume-3c16df89-1003-4a5f-92db-fac670d368d2
STEP: Creating a pod to test consume configMaps
Jul 25 10:08:30.800: INFO: Waiting up to 5m0s for pod "pod-configmaps-bd94bfb5-4ead-40c3-82f6-d3410c4402c0" in namespace "configmap-7454" to be "success or failure"
Jul 25 10:08:30.809: INFO: Pod "pod-configmaps-bd94bfb5-4ead-40c3-82f6-d3410c4402c0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.741371ms
Jul 25 10:08:32.814: INFO: Pod "pod-configmaps-bd94bfb5-4ead-40c3-82f6-d3410c4402c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.014013438s
STEP: Saw pod success
Jul 25 10:08:32.814: INFO: Pod "pod-configmaps-bd94bfb5-4ead-40c3-82f6-d3410c4402c0" satisfied condition "success or failure"
Jul 25 10:08:32.819: INFO: Trying to get logs from node essentialpks-conformance-3 pod pod-configmaps-bd94bfb5-4ead-40c3-82f6-d3410c4402c0 container configmap-volume-test: <nil>
STEP: delete the pod
Jul 25 10:08:32.841: INFO: Waiting for pod pod-configmaps-bd94bfb5-4ead-40c3-82f6-d3410c4402c0 to disappear
Jul 25 10:08:32.844: INFO: Pod pod-configmaps-bd94bfb5-4ead-40c3-82f6-d3410c4402c0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:08:32.844: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7454" for this suite.
Jul 25 10:08:38.856: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:08:38.942: INFO: namespace configmap-7454 deletion completed in 6.095020631s

• [SLOW TEST:8.183 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:08:38.942: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:09:38.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3473" for this suite.
Jul 25 10:10:01.003: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:10:01.083: INFO: namespace container-probe-3473 deletion completed in 22.096556898s

• [SLOW TEST:82.141 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:10:01.083: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-a9823942-2082-4968-ad0f-153bd4d3ad73
STEP: Creating a pod to test consume secrets
Jul 25 10:10:01.121: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-bc61c7b8-5d42-4d1e-a5ca-89a95062139c" in namespace "projected-9410" to be "success or failure"
Jul 25 10:10:01.126: INFO: Pod "pod-projected-secrets-bc61c7b8-5d42-4d1e-a5ca-89a95062139c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.129294ms
Jul 25 10:10:03.128: INFO: Pod "pod-projected-secrets-bc61c7b8-5d42-4d1e-a5ca-89a95062139c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.007069475s
STEP: Saw pod success
Jul 25 10:10:03.128: INFO: Pod "pod-projected-secrets-bc61c7b8-5d42-4d1e-a5ca-89a95062139c" satisfied condition "success or failure"
Jul 25 10:10:03.132: INFO: Trying to get logs from node essentialpks-conformance-3 pod pod-projected-secrets-bc61c7b8-5d42-4d1e-a5ca-89a95062139c container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 25 10:10:03.151: INFO: Waiting for pod pod-projected-secrets-bc61c7b8-5d42-4d1e-a5ca-89a95062139c to disappear
Jul 25 10:10:03.153: INFO: Pod pod-projected-secrets-bc61c7b8-5d42-4d1e-a5ca-89a95062139c no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:10:03.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9410" for this suite.
Jul 25 10:10:09.165: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:10:09.246: INFO: namespace projected-9410 deletion completed in 6.090125722s

• [SLOW TEST:8.163 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:10:09.248: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating secret secrets-73/secret-test-35dd54e8-13cb-4662-a87b-82e24d9e610f
STEP: Creating a pod to test consume secrets
Jul 25 10:10:09.284: INFO: Waiting up to 5m0s for pod "pod-configmaps-104588e8-b7b2-4cf9-b16a-47eb3cc1aade" in namespace "secrets-73" to be "success or failure"
Jul 25 10:10:09.288: INFO: Pod "pod-configmaps-104588e8-b7b2-4cf9-b16a-47eb3cc1aade": Phase="Pending", Reason="", readiness=false. Elapsed: 3.469751ms
Jul 25 10:10:11.291: INFO: Pod "pod-configmaps-104588e8-b7b2-4cf9-b16a-47eb3cc1aade": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006857869s
STEP: Saw pod success
Jul 25 10:10:11.291: INFO: Pod "pod-configmaps-104588e8-b7b2-4cf9-b16a-47eb3cc1aade" satisfied condition "success or failure"
Jul 25 10:10:11.294: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-configmaps-104588e8-b7b2-4cf9-b16a-47eb3cc1aade container env-test: <nil>
STEP: delete the pod
Jul 25 10:10:11.312: INFO: Waiting for pod pod-configmaps-104588e8-b7b2-4cf9-b16a-47eb3cc1aade to disappear
Jul 25 10:10:11.315: INFO: Pod pod-configmaps-104588e8-b7b2-4cf9-b16a-47eb3cc1aade no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:10:11.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-73" for this suite.
Jul 25 10:10:17.327: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:10:17.408: INFO: namespace secrets-73 deletion completed in 6.089840359s

• [SLOW TEST:8.160 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:10:17.408: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name configmap-test-upd-9eca1d34-e4a6-42f4-a3a7-c4a46ae97a9d
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:10:21.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2855" for this suite.
Jul 25 10:10:43.497: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:10:43.578: INFO: namespace configmap-2855 deletion completed in 22.090335696s

• [SLOW TEST:26.169 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:32
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:10:43.580: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Jul 25 10:10:45.641: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:10:45.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8302" for this suite.
Jul 25 10:10:51.665: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:10:51.767: INFO: namespace container-runtime-8302 deletion completed in 6.111987239s

• [SLOW TEST:8.188 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  blackbox test
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:38
    on terminated container
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:129
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:10:51.772: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 25 10:10:51.832: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"fe1748b4-90c8-4ee3-8b3e-45a9d817e8a2", Controller:(*bool)(0xc00255bb36), BlockOwnerDeletion:(*bool)(0xc00255bb37)}}
Jul 25 10:10:51.836: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"0bfb874a-67bc-4b0b-99ef-b1f9dc848d49", Controller:(*bool)(0xc0025516be), BlockOwnerDeletion:(*bool)(0xc0025516bf)}}
Jul 25 10:10:51.840: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"dacdc394-d77d-4343-a003-c7a815ca5c2a", Controller:(*bool)(0xc00255bd66), BlockOwnerDeletion:(*bool)(0xc00255bd67)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:10:56.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-3086" for this suite.
Jul 25 10:11:02.878: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:11:02.975: INFO: namespace gc-3086 deletion completed in 6.108462719s

• [SLOW TEST:11.203 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:11:02.976: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:39
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 25 10:11:03.074: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c863544a-7733-4329-bdeb-39544218339f" in namespace "downward-api-2143" to be "success or failure"
Jul 25 10:11:03.081: INFO: Pod "downwardapi-volume-c863544a-7733-4329-bdeb-39544218339f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.756649ms
Jul 25 10:11:05.084: INFO: Pod "downwardapi-volume-c863544a-7733-4329-bdeb-39544218339f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010043707s
STEP: Saw pod success
Jul 25 10:11:05.084: INFO: Pod "downwardapi-volume-c863544a-7733-4329-bdeb-39544218339f" satisfied condition "success or failure"
Jul 25 10:11:05.087: INFO: Trying to get logs from node essentialpks-conformance-2 pod downwardapi-volume-c863544a-7733-4329-bdeb-39544218339f container client-container: <nil>
STEP: delete the pod
Jul 25 10:11:05.103: INFO: Waiting for pod downwardapi-volume-c863544a-7733-4329-bdeb-39544218339f to disappear
Jul 25 10:11:05.106: INFO: Pod downwardapi-volume-c863544a-7733-4329-bdeb-39544218339f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:11:05.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2143" for this suite.
Jul 25 10:11:11.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:11:11.207: INFO: namespace downward-api-2143 deletion completed in 6.096344575s

• [SLOW TEST:8.231 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:11:11.208: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1558
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 25 10:11:11.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --generator=deployment/v1beta1 --namespace=kubectl-8317'
Jul 25 10:11:11.340: INFO: stderr: "kubectl run --generator=deployment/v1beta1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 25 10:11:11.340: INFO: stdout: "deployment.extensions/e2e-test-nginx-deployment created\n"
STEP: verifying the deployment e2e-test-nginx-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-nginx-deployment was created
[AfterEach] [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1563
Jul 25 10:11:13.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 delete deployment e2e-test-nginx-deployment --namespace=kubectl-8317'
Jul 25 10:11:13.478: INFO: stderr: ""
Jul 25 10:11:13.478: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:11:13.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8317" for this suite.
Jul 25 10:11:35.493: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:11:35.593: INFO: namespace kubectl-8317 deletion completed in 22.108667964s

• [SLOW TEST:24.385 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run deployment
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:11:35.593: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:11:41.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-5789" for this suite.
Jul 25 10:11:47.698: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:11:47.784: INFO: namespace namespaces-5789 deletion completed in 6.095301292s
STEP: Destroying namespace "nsdeletetest-8920" for this suite.
Jul 25 10:11:47.787: INFO: Namespace nsdeletetest-8920 was already deleted
STEP: Destroying namespace "nsdeletetest-5767" for this suite.
Jul 25 10:11:53.798: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:11:53.890: INFO: namespace nsdeletetest-5767 deletion completed in 6.103162965s

• [SLOW TEST:18.297 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:11:53.891: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: getting the auto-created API token
Jul 25 10:11:54.473: INFO: created pod pod-service-account-defaultsa
Jul 25 10:11:54.473: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jul 25 10:11:54.478: INFO: created pod pod-service-account-mountsa
Jul 25 10:11:54.478: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jul 25 10:11:54.486: INFO: created pod pod-service-account-nomountsa
Jul 25 10:11:54.486: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jul 25 10:11:54.493: INFO: created pod pod-service-account-defaultsa-mountspec
Jul 25 10:11:54.493: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jul 25 10:11:54.499: INFO: created pod pod-service-account-mountsa-mountspec
Jul 25 10:11:54.499: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jul 25 10:11:54.512: INFO: created pod pod-service-account-nomountsa-mountspec
Jul 25 10:11:54.512: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jul 25 10:11:54.520: INFO: created pod pod-service-account-defaultsa-nomountspec
Jul 25 10:11:54.520: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jul 25 10:11:54.530: INFO: created pod pod-service-account-mountsa-nomountspec
Jul 25 10:11:54.530: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jul 25 10:11:54.540: INFO: created pod pod-service-account-nomountsa-nomountspec
Jul 25 10:11:54.540: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:11:54.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-1453" for this suite.
Jul 25 10:12:00.565: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:12:00.675: INFO: namespace svcaccounts-1453 deletion completed in 6.127102439s

• [SLOW TEST:6.784 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:12:00.678: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating secret with name secret-test-c2710d45-63aa-4f62-b030-68a698eabef8
STEP: Creating a pod to test consume secrets
Jul 25 10:12:00.712: INFO: Waiting up to 5m0s for pod "pod-secrets-07336d05-dc5c-4a63-be45-24e8b34af892" in namespace "secrets-8066" to be "success or failure"
Jul 25 10:12:00.716: INFO: Pod "pod-secrets-07336d05-dc5c-4a63-be45-24e8b34af892": Phase="Pending", Reason="", readiness=false. Elapsed: 3.580176ms
Jul 25 10:12:02.719: INFO: Pod "pod-secrets-07336d05-dc5c-4a63-be45-24e8b34af892": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006727372s
STEP: Saw pod success
Jul 25 10:12:02.719: INFO: Pod "pod-secrets-07336d05-dc5c-4a63-be45-24e8b34af892" satisfied condition "success or failure"
Jul 25 10:12:02.722: INFO: Trying to get logs from node essentialpks-conformance-3 pod pod-secrets-07336d05-dc5c-4a63-be45-24e8b34af892 container secret-env-test: <nil>
STEP: delete the pod
Jul 25 10:12:02.739: INFO: Waiting for pod pod-secrets-07336d05-dc5c-4a63-be45-24e8b34af892 to disappear
Jul 25 10:12:02.741: INFO: Pod pod-secrets-07336d05-dc5c-4a63-be45-24e8b34af892 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:12:02.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8066" for this suite.
Jul 25 10:12:08.753: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:12:08.826: INFO: namespace secrets-8066 deletion completed in 6.082565103s

• [SLOW TEST:8.148 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:31
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:12:08.827: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod liveness-2c9d7bdb-64d8-4df2-b3fa-d62ee0bc07f8 in namespace container-probe-497
Jul 25 10:12:10.865: INFO: Started pod liveness-2c9d7bdb-64d8-4df2-b3fa-d62ee0bc07f8 in namespace container-probe-497
STEP: checking the pod's current state and verifying that restartCount is present
Jul 25 10:12:10.869: INFO: Initial restart count of pod liveness-2c9d7bdb-64d8-4df2-b3fa-d62ee0bc07f8 is 0
Jul 25 10:12:34.929: INFO: Restart count of pod container-probe-497/liveness-2c9d7bdb-64d8-4df2-b3fa-d62ee0bc07f8 is now 1 (24.060713967s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:12:34.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-497" for this suite.
Jul 25 10:12:40.958: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:12:41.043: INFO: namespace container-probe-497 deletion completed in 6.094032839s

• [SLOW TEST:32.216 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:12:41.043: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test emptydir 0644 on node default medium
Jul 25 10:12:41.076: INFO: Waiting up to 5m0s for pod "pod-3689f149-ce54-4dad-beb0-b104bfdfcadb" in namespace "emptydir-3950" to be "success or failure"
Jul 25 10:12:41.084: INFO: Pod "pod-3689f149-ce54-4dad-beb0-b104bfdfcadb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.487207ms
Jul 25 10:12:43.088: INFO: Pod "pod-3689f149-ce54-4dad-beb0-b104bfdfcadb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.012573188s
STEP: Saw pod success
Jul 25 10:12:43.088: INFO: Pod "pod-3689f149-ce54-4dad-beb0-b104bfdfcadb" satisfied condition "success or failure"
Jul 25 10:12:43.092: INFO: Trying to get logs from node essentialpks-conformance-3 pod pod-3689f149-ce54-4dad-beb0-b104bfdfcadb container test-container: <nil>
STEP: delete the pod
Jul 25 10:12:43.108: INFO: Waiting for pod pod-3689f149-ce54-4dad-beb0-b104bfdfcadb to disappear
Jul 25 10:12:43.111: INFO: Pod pod-3689f149-ce54-4dad-beb0-b104bfdfcadb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:12:43.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3950" for this suite.
Jul 25 10:12:49.123: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:12:49.210: INFO: namespace emptydir-3950 deletion completed in 6.096830396s

• [SLOW TEST:8.168 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:12:49.212: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:51
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod test-webserver-8acc4543-a660-46c6-b36e-db088bbd62b3 in namespace container-probe-3944
Jul 25 10:12:51.254: INFO: Started pod test-webserver-8acc4543-a660-46c6-b36e-db088bbd62b3 in namespace container-probe-3944
STEP: checking the pod's current state and verifying that restartCount is present
Jul 25 10:12:51.259: INFO: Initial restart count of pod test-webserver-8acc4543-a660-46c6-b36e-db088bbd62b3 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:16:51.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3944" for this suite.
Jul 25 10:16:57.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:16:57.889: INFO: namespace container-probe-3944 deletion completed in 6.090500318s

• [SLOW TEST:248.676 seconds]
[k8s.io] Probing container
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:16:57.889: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4079.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4079.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4079.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4079.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 25 10:16:59.943: INFO: DNS probes using dns-test-db75b4aa-6088-4ae3-a349-a07942ef4729 succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4079.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4079.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4079.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4079.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 25 10:17:01.977: INFO: File wheezy_udp@dns-test-service-3.dns-4079.svc.cluster.local from pod  dns-4079/dns-test-e404a529-cd5d-496a-a620-fd585a5333c1 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 25 10:17:01.980: INFO: File jessie_udp@dns-test-service-3.dns-4079.svc.cluster.local from pod  dns-4079/dns-test-e404a529-cd5d-496a-a620-fd585a5333c1 contains 'foo.example.com.
' instead of 'bar.example.com.'
Jul 25 10:17:01.980: INFO: Lookups using dns-4079/dns-test-e404a529-cd5d-496a-a620-fd585a5333c1 failed for: [wheezy_udp@dns-test-service-3.dns-4079.svc.cluster.local jessie_udp@dns-test-service-3.dns-4079.svc.cluster.local]

Jul 25 10:17:06.988: INFO: DNS probes using dns-test-e404a529-cd5d-496a-a620-fd585a5333c1 succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4079.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4079.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4079.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4079.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Jul 25 10:17:09.062: INFO: DNS probes using dns-test-608bb4bb-20cd-4473-82b8-a131653d1bd4 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:17:09.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4079" for this suite.
Jul 25 10:17:15.108: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:17:15.203: INFO: namespace dns-4079 deletion completed in 6.110381849s

• [SLOW TEST:17.315 seconds]
[sig-network] DNS
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:17:15.204: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating pod pod-subpath-test-projected-96zg
STEP: Creating a pod to test atomic-volume-subpath
Jul 25 10:17:15.251: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-96zg" in namespace "subpath-9994" to be "success or failure"
Jul 25 10:17:15.261: INFO: Pod "pod-subpath-test-projected-96zg": Phase="Pending", Reason="", readiness=false. Elapsed: 9.375329ms
Jul 25 10:17:17.264: INFO: Pod "pod-subpath-test-projected-96zg": Phase="Running", Reason="", readiness=true. Elapsed: 2.012421189s
Jul 25 10:17:19.268: INFO: Pod "pod-subpath-test-projected-96zg": Phase="Running", Reason="", readiness=true. Elapsed: 4.01611906s
Jul 25 10:17:21.271: INFO: Pod "pod-subpath-test-projected-96zg": Phase="Running", Reason="", readiness=true. Elapsed: 6.020061553s
Jul 25 10:17:23.276: INFO: Pod "pod-subpath-test-projected-96zg": Phase="Running", Reason="", readiness=true. Elapsed: 8.025019563s
Jul 25 10:17:25.282: INFO: Pod "pod-subpath-test-projected-96zg": Phase="Running", Reason="", readiness=true. Elapsed: 10.03046107s
Jul 25 10:17:27.285: INFO: Pod "pod-subpath-test-projected-96zg": Phase="Running", Reason="", readiness=true. Elapsed: 12.033636721s
Jul 25 10:17:29.288: INFO: Pod "pod-subpath-test-projected-96zg": Phase="Running", Reason="", readiness=true. Elapsed: 14.036989079s
Jul 25 10:17:31.294: INFO: Pod "pod-subpath-test-projected-96zg": Phase="Running", Reason="", readiness=true. Elapsed: 16.042626874s
Jul 25 10:17:33.299: INFO: Pod "pod-subpath-test-projected-96zg": Phase="Running", Reason="", readiness=true. Elapsed: 18.047572897s
Jul 25 10:17:35.303: INFO: Pod "pod-subpath-test-projected-96zg": Phase="Running", Reason="", readiness=true. Elapsed: 20.051839583s
Jul 25 10:17:37.307: INFO: Pod "pod-subpath-test-projected-96zg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.055905814s
STEP: Saw pod success
Jul 25 10:17:37.307: INFO: Pod "pod-subpath-test-projected-96zg" satisfied condition "success or failure"
Jul 25 10:17:37.312: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-subpath-test-projected-96zg container test-container-subpath-projected-96zg: <nil>
STEP: delete the pod
Jul 25 10:17:37.345: INFO: Waiting for pod pod-subpath-test-projected-96zg to disappear
Jul 25 10:17:37.350: INFO: Pod pod-subpath-test-projected-96zg no longer exists
STEP: Deleting pod pod-subpath-test-projected-96zg
Jul 25 10:17:37.350: INFO: Deleting pod "pod-subpath-test-projected-96zg" in namespace "subpath-9994"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:17:37.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9994" for this suite.
Jul 25 10:17:43.368: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:17:43.451: INFO: namespace subpath-9994 deletion completed in 6.094041997s

• [SLOW TEST:28.247 seconds]
[sig-storage] Subpath
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:22
  Atomic writer volumes
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:17:43.456: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test substitution in container's args
Jul 25 10:17:43.492: INFO: Waiting up to 5m0s for pod "var-expansion-3b8c885f-430a-47a5-869b-1290a02d677c" in namespace "var-expansion-3339" to be "success or failure"
Jul 25 10:17:43.495: INFO: Pod "var-expansion-3b8c885f-430a-47a5-869b-1290a02d677c": Phase="Pending", Reason="", readiness=false. Elapsed: 3.169422ms
Jul 25 10:17:45.500: INFO: Pod "var-expansion-3b8c885f-430a-47a5-869b-1290a02d677c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.008218068s
STEP: Saw pod success
Jul 25 10:17:45.501: INFO: Pod "var-expansion-3b8c885f-430a-47a5-869b-1290a02d677c" satisfied condition "success or failure"
Jul 25 10:17:45.507: INFO: Trying to get logs from node essentialpks-conformance-3 pod var-expansion-3b8c885f-430a-47a5-869b-1290a02d677c container dapi-container: <nil>
STEP: delete the pod
Jul 25 10:17:45.527: INFO: Waiting for pod var-expansion-3b8c885f-430a-47a5-869b-1290a02d677c to disappear
Jul 25 10:17:45.529: INFO: Pod var-expansion-3b8c885f-430a-47a5-869b-1290a02d677c no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:17:45.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3339" for this suite.
Jul 25 10:17:51.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:17:51.634: INFO: namespace var-expansion-3339 deletion completed in 6.102385881s

• [SLOW TEST:8.179 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:17:51.640: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-751c7e7a-7dcd-4fed-8fce-1a02202a2cc9
STEP: Creating a pod to test consume secrets
Jul 25 10:17:51.676: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-49ff6001-d31f-4062-8670-4f1747d2f168" in namespace "projected-1692" to be "success or failure"
Jul 25 10:17:51.683: INFO: Pod "pod-projected-secrets-49ff6001-d31f-4062-8670-4f1747d2f168": Phase="Pending", Reason="", readiness=false. Elapsed: 6.388495ms
Jul 25 10:17:53.686: INFO: Pod "pod-projected-secrets-49ff6001-d31f-4062-8670-4f1747d2f168": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.009776413s
STEP: Saw pod success
Jul 25 10:17:53.686: INFO: Pod "pod-projected-secrets-49ff6001-d31f-4062-8670-4f1747d2f168" satisfied condition "success or failure"
Jul 25 10:17:53.689: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-projected-secrets-49ff6001-d31f-4062-8670-4f1747d2f168 container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 25 10:17:53.705: INFO: Waiting for pod pod-projected-secrets-49ff6001-d31f-4062-8670-4f1747d2f168 to disappear
Jul 25 10:17:53.708: INFO: Pod pod-projected-secrets-49ff6001-d31f-4062-8670-4f1747d2f168 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:17:53.708: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1692" for this suite.
Jul 25 10:17:59.722: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:17:59.804: INFO: namespace projected-1692 deletion completed in 6.092656748s

• [SLOW TEST:8.164 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:17:59.805: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:164
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Jul 25 10:18:02.358: INFO: Successfully updated pod "pod-update-activedeadlineseconds-9c7856dc-add2-47d4-94e5-378e1557cd22"
Jul 25 10:18:02.358: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-9c7856dc-add2-47d4-94e5-378e1557cd22" in namespace "pods-7632" to be "terminated due to deadline exceeded"
Jul 25 10:18:02.361: INFO: Pod "pod-update-activedeadlineseconds-9c7856dc-add2-47d4-94e5-378e1557cd22": Phase="Running", Reason="", readiness=true. Elapsed: 2.902206ms
Jul 25 10:18:04.364: INFO: Pod "pod-update-activedeadlineseconds-9c7856dc-add2-47d4-94e5-378e1557cd22": Phase="Running", Reason="", readiness=true. Elapsed: 2.006430045s
Jul 25 10:18:06.368: INFO: Pod "pod-update-activedeadlineseconds-9c7856dc-add2-47d4-94e5-378e1557cd22": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.01085462s
Jul 25 10:18:06.369: INFO: Pod "pod-update-activedeadlineseconds-9c7856dc-add2-47d4-94e5-378e1557cd22" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:18:06.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7632" for this suite.
Jul 25 10:18:12.385: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:18:12.472: INFO: namespace pods-7632 deletion completed in 6.097781013s

• [SLOW TEST:12.668 seconds]
[k8s.io] Pods
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:18:12.480: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:63
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Jul 25 10:18:16.544: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 25 10:18:16.546: INFO: Pod pod-with-poststart-http-hook still exists
Jul 25 10:18:18.547: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 25 10:18:18.550: INFO: Pod pod-with-poststart-http-hook still exists
Jul 25 10:18:20.547: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 25 10:18:20.550: INFO: Pod pod-with-poststart-http-hook still exists
Jul 25 10:18:22.547: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 25 10:18:22.551: INFO: Pod pod-with-poststart-http-hook still exists
Jul 25 10:18:24.547: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 25 10:18:24.550: INFO: Pod pod-with-poststart-http-hook still exists
Jul 25 10:18:26.547: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jul 25 10:18:26.553: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:18:26.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6006" for this suite.
Jul 25 10:18:48.574: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:18:48.655: INFO: namespace container-lifecycle-hook-6006 deletion completed in 22.094622308s

• [SLOW TEST:36.176 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
  when create a pod with lifecycle hook
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:18:48.661: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:103
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
Jul 25 10:18:48.700: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Jul 25 10:18:48.709: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 10:18:48.713: INFO: Number of nodes with available pods: 0
Jul 25 10:18:48.713: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 10:18:49.717: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 10:18:49.720: INFO: Number of nodes with available pods: 0
Jul 25 10:18:49.720: INFO: Node essentialpks-conformance-2 is running more than one daemon pod
Jul 25 10:18:50.718: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 10:18:50.721: INFO: Number of nodes with available pods: 2
Jul 25 10:18:50.721: INFO: Number of running nodes: 2, number of available pods: 2
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Jul 25 10:18:50.751: INFO: Wrong image for pod: daemon-set-k5q46. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 25 10:18:50.751: INFO: Wrong image for pod: daemon-set-l8fgb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 25 10:18:50.755: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 10:18:51.759: INFO: Wrong image for pod: daemon-set-k5q46. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 25 10:18:51.759: INFO: Wrong image for pod: daemon-set-l8fgb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 25 10:18:51.765: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 10:18:52.759: INFO: Wrong image for pod: daemon-set-k5q46. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 25 10:18:52.759: INFO: Wrong image for pod: daemon-set-l8fgb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 25 10:18:52.763: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 10:18:53.759: INFO: Wrong image for pod: daemon-set-k5q46. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 25 10:18:53.759: INFO: Pod daemon-set-k5q46 is not available
Jul 25 10:18:53.759: INFO: Wrong image for pod: daemon-set-l8fgb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 25 10:18:53.763: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 10:18:54.760: INFO: Wrong image for pod: daemon-set-l8fgb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 25 10:18:54.760: INFO: Pod daemon-set-pj9z7 is not available
Jul 25 10:18:54.765: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 10:18:55.760: INFO: Wrong image for pod: daemon-set-l8fgb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 25 10:18:55.760: INFO: Pod daemon-set-l8fgb is not available
Jul 25 10:18:55.767: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 10:18:56.760: INFO: Wrong image for pod: daemon-set-l8fgb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 25 10:18:56.760: INFO: Pod daemon-set-l8fgb is not available
Jul 25 10:18:56.764: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 10:18:57.761: INFO: Wrong image for pod: daemon-set-l8fgb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 25 10:18:57.761: INFO: Pod daemon-set-l8fgb is not available
Jul 25 10:18:57.766: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 10:18:58.760: INFO: Wrong image for pod: daemon-set-l8fgb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 25 10:18:58.760: INFO: Pod daemon-set-l8fgb is not available
Jul 25 10:18:58.765: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 10:18:59.760: INFO: Wrong image for pod: daemon-set-l8fgb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 25 10:18:59.760: INFO: Pod daemon-set-l8fgb is not available
Jul 25 10:18:59.766: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 10:19:00.760: INFO: Wrong image for pod: daemon-set-l8fgb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 25 10:19:00.760: INFO: Pod daemon-set-l8fgb is not available
Jul 25 10:19:00.767: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 10:19:01.759: INFO: Wrong image for pod: daemon-set-l8fgb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 25 10:19:01.759: INFO: Pod daemon-set-l8fgb is not available
Jul 25 10:19:01.764: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 10:19:02.759: INFO: Wrong image for pod: daemon-set-l8fgb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 25 10:19:02.759: INFO: Pod daemon-set-l8fgb is not available
Jul 25 10:19:02.762: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 10:19:03.761: INFO: Wrong image for pod: daemon-set-l8fgb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 25 10:19:03.761: INFO: Pod daemon-set-l8fgb is not available
Jul 25 10:19:03.771: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 10:19:04.759: INFO: Wrong image for pod: daemon-set-l8fgb. Expected: gcr.io/kubernetes-e2e-test-images/redis:1.0, got: docker.io/library/nginx:1.14-alpine.
Jul 25 10:19:04.759: INFO: Pod daemon-set-l8fgb is not available
Jul 25 10:19:04.764: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 10:19:05.760: INFO: Pod daemon-set-5x2n9 is not available
Jul 25 10:19:05.765: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster.
Jul 25 10:19:05.770: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 10:19:05.773: INFO: Number of nodes with available pods: 1
Jul 25 10:19:05.773: INFO: Node essentialpks-conformance-3 is running more than one daemon pod
Jul 25 10:19:06.777: INFO: DaemonSet pods can't tolerate node essentialpks-conformance-1 with taints [{Key:node-role.kubernetes.io/master Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Jul 25 10:19:06.780: INFO: Number of nodes with available pods: 2
Jul 25 10:19:06.780: INFO: Number of running nodes: 2, number of available pods: 2
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:69
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-873, will wait for the garbage collector to delete the pods
Jul 25 10:19:06.851: INFO: Deleting DaemonSet.extensions daemon-set took: 4.553687ms
Jul 25 10:19:07.152: INFO: Terminating DaemonSet.extensions daemon-set pods took: 300.333362ms
Jul 25 10:19:19.956: INFO: Number of nodes with available pods: 0
Jul 25 10:19:19.956: INFO: Number of running nodes: 0, number of available pods: 0
Jul 25 10:19:19.960: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-873/daemonsets","resourceVersion":"75007"},"items":null}

Jul 25 10:19:19.962: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-873/pods","resourceVersion":"75007"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:19:19.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-873" for this suite.
Jul 25 10:19:25.982: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:19:26.062: INFO: namespace daemonsets-873 deletion completed in 6.088524942s

• [SLOW TEST:37.401 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:19:26.064: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating projection with secret that has name projected-secret-test-41543a9e-f63c-41aa-890e-d2055b8d5a06
STEP: Creating a pod to test consume secrets
Jul 25 10:19:26.098: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2e046d4a-6324-45c5-9df8-dccc92d8fb2e" in namespace "projected-7549" to be "success or failure"
Jul 25 10:19:26.107: INFO: Pod "pod-projected-secrets-2e046d4a-6324-45c5-9df8-dccc92d8fb2e": Phase="Pending", Reason="", readiness=false. Elapsed: 8.894986ms
Jul 25 10:19:28.116: INFO: Pod "pod-projected-secrets-2e046d4a-6324-45c5-9df8-dccc92d8fb2e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.018313516s
STEP: Saw pod success
Jul 25 10:19:28.116: INFO: Pod "pod-projected-secrets-2e046d4a-6324-45c5-9df8-dccc92d8fb2e" satisfied condition "success or failure"
Jul 25 10:19:28.127: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-projected-secrets-2e046d4a-6324-45c5-9df8-dccc92d8fb2e container projected-secret-volume-test: <nil>
STEP: delete the pod
Jul 25 10:19:28.147: INFO: Waiting for pod pod-projected-secrets-2e046d4a-6324-45c5-9df8-dccc92d8fb2e to disappear
Jul 25 10:19:28.149: INFO: Pod pod-projected-secrets-2e046d4a-6324-45c5-9df8-dccc92d8fb2e no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:19:28.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7549" for this suite.
Jul 25 10:19:34.163: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:19:34.254: INFO: namespace projected-7549 deletion completed in 6.101331198s

• [SLOW TEST:8.191 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:19:34.260: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:39
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating a pod to test downward API volume plugin
Jul 25 10:19:34.289: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f975f68b-9c8e-4d0e-8f07-e7daa2768bef" in namespace "projected-1783" to be "success or failure"
Jul 25 10:19:34.294: INFO: Pod "downwardapi-volume-f975f68b-9c8e-4d0e-8f07-e7daa2768bef": Phase="Pending", Reason="", readiness=false. Elapsed: 5.078642ms
Jul 25 10:19:36.299: INFO: Pod "downwardapi-volume-f975f68b-9c8e-4d0e-8f07-e7daa2768bef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.010333978s
STEP: Saw pod success
Jul 25 10:19:36.299: INFO: Pod "downwardapi-volume-f975f68b-9c8e-4d0e-8f07-e7daa2768bef" satisfied condition "success or failure"
Jul 25 10:19:36.305: INFO: Trying to get logs from node essentialpks-conformance-3 pod downwardapi-volume-f975f68b-9c8e-4d0e-8f07-e7daa2768bef container client-container: <nil>
STEP: delete the pod
Jul 25 10:19:36.322: INFO: Waiting for pod downwardapi-volume-f975f68b-9c8e-4d0e-8f07-e7daa2768bef to disappear
Jul 25 10:19:36.324: INFO: Pod downwardapi-volume-f975f68b-9c8e-4d0e-8f07-e7daa2768bef no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:19:36.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1783" for this suite.
Jul 25 10:19:42.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:19:42.416: INFO: namespace projected-1783 deletion completed in 6.088037027s

• [SLOW TEST:8.156 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:33
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client [k8s.io] Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:19:42.419: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:221
[BeforeEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1421
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: running the image docker.io/library/nginx:1.14-alpine
Jul 25 10:19:42.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 run e2e-test-nginx-deployment --image=docker.io/library/nginx:1.14-alpine --namespace=kubectl-4078'
Jul 25 10:19:42.660: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Jul 25 10:19:42.660: INFO: stdout: "deployment.apps/e2e-test-nginx-deployment created\n"
STEP: verifying the pod controlled by e2e-test-nginx-deployment gets created
[AfterEach] [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1427
Jul 25 10:19:44.673: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-739010025 delete deployment e2e-test-nginx-deployment --namespace=kubectl-4078'
Jul 25 10:19:44.796: INFO: stderr: ""
Jul 25 10:19:44.796: INFO: stdout: "deployment.extensions \"e2e-test-nginx-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:19:44.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4078" for this suite.
Jul 25 10:19:50.812: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:19:50.899: INFO: namespace kubectl-4078 deletion completed in 6.099799076s

• [SLOW TEST:8.481 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  [k8s.io] Kubectl run default
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:692
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:150
STEP: Creating a kubernetes client
Jul 25 10:19:50.900: INFO: >>> kubeConfig: /tmp/kubeconfig-739010025
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
STEP: Creating configMap with name projected-configmap-test-volume-9e3af968-f796-4c65-a9e9-bda31b5d05e4
STEP: Creating a pod to test consume configMaps
Jul 25 10:19:50.936: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-bc8a22ad-39ae-41e9-a96e-7550e544b3b9" in namespace "projected-212" to be "success or failure"
Jul 25 10:19:50.940: INFO: Pod "pod-projected-configmaps-bc8a22ad-39ae-41e9-a96e-7550e544b3b9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.561474ms
Jul 25 10:19:52.944: INFO: Pod "pod-projected-configmaps-bc8a22ad-39ae-41e9-a96e-7550e544b3b9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00803889s
STEP: Saw pod success
Jul 25 10:19:52.944: INFO: Pod "pod-projected-configmaps-bc8a22ad-39ae-41e9-a96e-7550e544b3b9" satisfied condition "success or failure"
Jul 25 10:19:52.946: INFO: Trying to get logs from node essentialpks-conformance-2 pod pod-projected-configmaps-bc8a22ad-39ae-41e9-a96e-7550e544b3b9 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Jul 25 10:19:52.960: INFO: Waiting for pod pod-projected-configmaps-bc8a22ad-39ae-41e9-a96e-7550e544b3b9 to disappear
Jul 25 10:19:52.963: INFO: Pod pod-projected-configmaps-bc8a22ad-39ae-41e9-a96e-7550e544b3b9 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
Jul 25 10:19:52.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-212" for this suite.
Jul 25 10:19:58.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Jul 25 10:19:59.077: INFO: namespace projected-212 deletion completed in 6.1116336s

• [SLOW TEST:8.178 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:33
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.15.0-rc.1.19+e8462b5b5dc258/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:697
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSJul 25 10:19:59.080: INFO: Running AfterSuite actions on all nodes
Jul 25 10:19:59.081: INFO: Running AfterSuite actions on node 1
Jul 25 10:19:59.081: INFO: Skipping dumping logs from cluster

Ran 215 of 4411 Specs in 5447.980 seconds
SUCCESS! -- 215 Passed | 0 Failed | 0 Pending | 4196 Skipped
PASS

Ginkgo ran 1 suite in 1h30m49.53201965s
Test Suite Passed
