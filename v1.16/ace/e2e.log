I1228 07:53:26.625120      22 test_context.go:414] Using a temporary kubeconfig file from in-cluster config : /tmp/kubeconfig-076158834
I1228 07:53:26.625245      22 e2e.go:92] Starting e2e run "f813ffda-7eda-45bb-a767-6bc472fd87d3" on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1577519605 - Will randomize all specs
Will run 276 of 4732 specs

Dec 28 07:53:26.643: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 07:53:26.646: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Dec 28 07:53:26.660: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Dec 28 07:53:26.682: INFO: 13 / 13 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Dec 28 07:53:26.682: INFO: expected 3 pod replicas in namespace 'kube-system', 3 are Running and Ready.
Dec 28 07:53:26.682: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Dec 28 07:53:26.688: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-flannel' (0 seconds elapsed)
Dec 28 07:53:26.688: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Dec 28 07:53:26.688: INFO: e2e test version: v1.16.3
Dec 28 07:53:26.689: INFO: kube-apiserver version: v1.16.3
Dec 28 07:53:26.689: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 07:53:26.692: INFO: Cluster IP family: ipv4
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run deployment 
  should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 07:53:26.693: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubectl
Dec 28 07:53:26.710: INFO: No PodSecurityPolicies found; assuming PodSecurityPolicy is disabled.
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1540
[It] should create a deployment from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 28 07:53:26.711: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --generator=deployment/apps.v1 --namespace=kubectl-5122'
Dec 28 07:53:26.869: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 28 07:53:26.869: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the deployment e2e-test-httpd-deployment was created
STEP: verifying the pod controlled by deployment e2e-test-httpd-deployment was created
[AfterEach] Kubectl run deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1545
Dec 28 07:53:28.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 delete deployment e2e-test-httpd-deployment --namespace=kubectl-5122'
Dec 28 07:53:28.963: INFO: stderr: ""
Dec 28 07:53:28.963: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 07:53:28.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5122" for this suite.
Dec 28 07:53:56.972: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 07:53:57.035: INFO: namespace kubectl-5122 deletion completed in 28.069787138s

• [SLOW TEST:30.342 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1536
    should create a deployment from an image  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a read only busybox container 
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 07:53:57.035: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 07:53:59.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3671" for this suite.
Dec 28 07:54:43.081: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 07:54:43.154: INFO: namespace kubelet-test-3671 deletion completed in 44.079271117s

• [SLOW TEST:46.119 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a read only busybox container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:187
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 07:54:43.154: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 28 07:54:43.173: INFO: Waiting up to 5m0s for pod "pod-09ba9361-20a7-40b7-83c9-6bceacb67c43" in namespace "emptydir-4356" to be "success or failure"
Dec 28 07:54:43.175: INFO: Pod "pod-09ba9361-20a7-40b7-83c9-6bceacb67c43": Phase="Pending", Reason="", readiness=false. Elapsed: 1.493247ms
Dec 28 07:54:45.177: INFO: Pod "pod-09ba9361-20a7-40b7-83c9-6bceacb67c43": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003661251s
STEP: Saw pod success
Dec 28 07:54:45.177: INFO: Pod "pod-09ba9361-20a7-40b7-83c9-6bceacb67c43" satisfied condition "success or failure"
Dec 28 07:54:45.178: INFO: Trying to get logs from node hxx-m-2 pod pod-09ba9361-20a7-40b7-83c9-6bceacb67c43 container test-container: <nil>
STEP: delete the pod
Dec 28 07:54:45.189: INFO: Waiting for pod pod-09ba9361-20a7-40b7-83c9-6bceacb67c43 to disappear
Dec 28 07:54:45.195: INFO: Pod pod-09ba9361-20a7-40b7-83c9-6bceacb67c43 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 07:54:45.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4356" for this suite.
Dec 28 07:54:51.205: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 07:54:51.274: INFO: namespace emptydir-4356 deletion completed in 6.076605157s

• [SLOW TEST:8.121 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 07:54:51.275: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support proxy with --port 0  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting the proxy server
Dec 28 07:54:51.293: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-076158834 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 07:54:51.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5550" for this suite.
Dec 28 07:54:57.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 07:54:57.433: INFO: namespace kubectl-5550 deletion completed in 6.077501909s

• [SLOW TEST:6.158 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support proxy with --port 0  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 07:54:57.433: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-85c074d6-1a8a-48ba-9ad9-df11a472d20f
STEP: Creating secret with name s-test-opt-upd-14fad3fa-af7c-456e-84bf-5321a2cc2a2b
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-85c074d6-1a8a-48ba-9ad9-df11a472d20f
STEP: Updating secret s-test-opt-upd-14fad3fa-af7c-456e-84bf-5321a2cc2a2b
STEP: Creating secret with name s-test-opt-create-e7404883-2ef1-4109-9782-b01bf0234379
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 07:55:01.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6540" for this suite.
Dec 28 07:55:13.513: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 07:55:13.581: INFO: namespace projected-6540 deletion completed in 12.075643809s

• [SLOW TEST:16.148 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 07:55:13.581: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 28 07:55:13.604: INFO: Waiting up to 5m0s for pod "downwardapi-volume-193b3b5e-9bec-4351-9017-f346ea4b141a" in namespace "projected-2571" to be "success or failure"
Dec 28 07:55:13.606: INFO: Pod "downwardapi-volume-193b3b5e-9bec-4351-9017-f346ea4b141a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.395523ms
Dec 28 07:55:15.608: INFO: Pod "downwardapi-volume-193b3b5e-9bec-4351-9017-f346ea4b141a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004588284s
STEP: Saw pod success
Dec 28 07:55:15.608: INFO: Pod "downwardapi-volume-193b3b5e-9bec-4351-9017-f346ea4b141a" satisfied condition "success or failure"
Dec 28 07:55:15.610: INFO: Trying to get logs from node hxx-m-2 pod downwardapi-volume-193b3b5e-9bec-4351-9017-f346ea4b141a container client-container: <nil>
STEP: delete the pod
Dec 28 07:55:15.621: INFO: Waiting for pod downwardapi-volume-193b3b5e-9bec-4351-9017-f346ea4b141a to disappear
Dec 28 07:55:15.622: INFO: Pod downwardapi-volume-193b3b5e-9bec-4351-9017-f346ea4b141a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 07:55:15.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2571" for this suite.
Dec 28 07:55:21.633: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 07:55:21.700: INFO: namespace projected-2571 deletion completed in 6.074833777s

• [SLOW TEST:8.119 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Pods 
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 07:55:21.700: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 07:55:21.716: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 07:55:23.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3877" for this suite.
Dec 28 07:56:13.744: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 07:56:13.805: INFO: namespace pods-3877 deletion completed in 50.068707944s

• [SLOW TEST:52.105 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 07:56:13.805: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Dec 28 07:56:13.823: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: rename a version
STEP: check the new version name is served
STEP: check the old version name is removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 07:56:34.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-671" for this suite.
Dec 28 07:56:40.432: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 07:56:40.497: INFO: namespace crd-publish-openapi-671 deletion completed in 6.07214063s

• [SLOW TEST:26.692 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 07:56:40.497: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 28 07:56:40.531: INFO: Number of nodes with available pods: 0
Dec 28 07:56:40.531: INFO: Node hxx-m-1 is running more than one daemon pod
Dec 28 07:56:41.536: INFO: Number of nodes with available pods: 0
Dec 28 07:56:41.536: INFO: Node hxx-m-1 is running more than one daemon pod
Dec 28 07:56:42.536: INFO: Number of nodes with available pods: 3
Dec 28 07:56:42.536: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Stop a daemon pod, check that the daemon pod is revived.
Dec 28 07:56:42.546: INFO: Number of nodes with available pods: 2
Dec 28 07:56:42.546: INFO: Node hxx-m-2 is running more than one daemon pod
Dec 28 07:56:43.551: INFO: Number of nodes with available pods: 2
Dec 28 07:56:43.551: INFO: Node hxx-m-2 is running more than one daemon pod
Dec 28 07:56:44.550: INFO: Number of nodes with available pods: 2
Dec 28 07:56:44.550: INFO: Node hxx-m-2 is running more than one daemon pod
Dec 28 07:56:45.551: INFO: Number of nodes with available pods: 2
Dec 28 07:56:45.551: INFO: Node hxx-m-2 is running more than one daemon pod
Dec 28 07:56:46.551: INFO: Number of nodes with available pods: 2
Dec 28 07:56:46.551: INFO: Node hxx-m-2 is running more than one daemon pod
Dec 28 07:56:47.550: INFO: Number of nodes with available pods: 2
Dec 28 07:56:47.550: INFO: Node hxx-m-2 is running more than one daemon pod
Dec 28 07:56:48.551: INFO: Number of nodes with available pods: 2
Dec 28 07:56:48.551: INFO: Node hxx-m-2 is running more than one daemon pod
Dec 28 07:56:49.551: INFO: Number of nodes with available pods: 2
Dec 28 07:56:49.551: INFO: Node hxx-m-2 is running more than one daemon pod
Dec 28 07:56:50.551: INFO: Number of nodes with available pods: 2
Dec 28 07:56:50.551: INFO: Node hxx-m-2 is running more than one daemon pod
Dec 28 07:56:51.551: INFO: Number of nodes with available pods: 2
Dec 28 07:56:51.551: INFO: Node hxx-m-2 is running more than one daemon pod
Dec 28 07:56:52.551: INFO: Number of nodes with available pods: 3
Dec 28 07:56:52.551: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2931, will wait for the garbage collector to delete the pods
Dec 28 07:56:52.608: INFO: Deleting DaemonSet.extensions daemon-set took: 3.810861ms
Dec 28 07:56:53.508: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.221376ms
Dec 28 07:57:01.010: INFO: Number of nodes with available pods: 0
Dec 28 07:57:01.010: INFO: Number of running nodes: 0, number of available pods: 0
Dec 28 07:57:01.013: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-2931/daemonsets","resourceVersion":"269920"},"items":null}

Dec 28 07:57:01.015: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-2931/pods","resourceVersion":"269920"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 07:57:01.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2931" for this suite.
Dec 28 07:57:07.033: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 07:57:07.097: INFO: namespace daemonsets-2931 deletion completed in 6.072790371s

• [SLOW TEST:26.600 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 07:57:07.098: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 28 07:57:07.117: INFO: Waiting up to 5m0s for pod "downwardapi-volume-ee51ee12-890c-48d8-a94c-6548c16bb2c1" in namespace "downward-api-5730" to be "success or failure"
Dec 28 07:57:07.119: INFO: Pod "downwardapi-volume-ee51ee12-890c-48d8-a94c-6548c16bb2c1": Phase="Pending", Reason="", readiness=false. Elapsed: 1.530784ms
Dec 28 07:57:09.121: INFO: Pod "downwardapi-volume-ee51ee12-890c-48d8-a94c-6548c16bb2c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004039065s
STEP: Saw pod success
Dec 28 07:57:09.121: INFO: Pod "downwardapi-volume-ee51ee12-890c-48d8-a94c-6548c16bb2c1" satisfied condition "success or failure"
Dec 28 07:57:09.123: INFO: Trying to get logs from node hxx-m-2 pod downwardapi-volume-ee51ee12-890c-48d8-a94c-6548c16bb2c1 container client-container: <nil>
STEP: delete the pod
Dec 28 07:57:09.139: INFO: Waiting for pod downwardapi-volume-ee51ee12-890c-48d8-a94c-6548c16bb2c1 to disappear
Dec 28 07:57:09.141: INFO: Pod downwardapi-volume-ee51ee12-890c-48d8-a94c-6548c16bb2c1 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 07:57:09.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5730" for this suite.
Dec 28 07:57:15.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 07:57:15.218: INFO: namespace downward-api-5730 deletion completed in 6.074697724s

• [SLOW TEST:8.120 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected combined 
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 07:57:15.218: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-projected-all-test-volume-6f24b8a8-06af-4e82-b8d6-770d952d2494
STEP: Creating secret with name secret-projected-all-test-volume-64eacb62-8be3-4827-a971-866cdc398368
STEP: Creating a pod to test Check all projections for projected volume plugin
Dec 28 07:57:15.243: INFO: Waiting up to 5m0s for pod "projected-volume-5bce0a66-8462-416f-8ddd-3eb897224f0f" in namespace "projected-8638" to be "success or failure"
Dec 28 07:57:15.245: INFO: Pod "projected-volume-5bce0a66-8462-416f-8ddd-3eb897224f0f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042109ms
Dec 28 07:57:17.247: INFO: Pod "projected-volume-5bce0a66-8462-416f-8ddd-3eb897224f0f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004290944s
STEP: Saw pod success
Dec 28 07:57:17.247: INFO: Pod "projected-volume-5bce0a66-8462-416f-8ddd-3eb897224f0f" satisfied condition "success or failure"
Dec 28 07:57:17.249: INFO: Trying to get logs from node hxx-m-2 pod projected-volume-5bce0a66-8462-416f-8ddd-3eb897224f0f container projected-all-volume-test: <nil>
STEP: delete the pod
Dec 28 07:57:17.258: INFO: Waiting for pod projected-volume-5bce0a66-8462-416f-8ddd-3eb897224f0f to disappear
Dec 28 07:57:17.259: INFO: Pod projected-volume-5bce0a66-8462-416f-8ddd-3eb897224f0f no longer exists
[AfterEach] [sig-storage] Projected combined
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 07:57:17.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8638" for this suite.
Dec 28 07:57:23.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 07:57:23.340: INFO: namespace projected-8638 deletion completed in 6.078344795s

• [SLOW TEST:8.122 seconds]
[sig-storage] Projected combined
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_combined.go:32
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] HostPath 
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 07:57:23.340: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename hostpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] HostPath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:37
[It] should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test hostPath mode
Dec 28 07:57:23.360: INFO: Waiting up to 5m0s for pod "pod-host-path-test" in namespace "hostpath-9069" to be "success or failure"
Dec 28 07:57:23.362: INFO: Pod "pod-host-path-test": Phase="Pending", Reason="", readiness=false. Elapsed: 1.456028ms
Dec 28 07:57:25.364: INFO: Pod "pod-host-path-test": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00407788s
STEP: Saw pod success
Dec 28 07:57:25.364: INFO: Pod "pod-host-path-test" satisfied condition "success or failure"
Dec 28 07:57:25.366: INFO: Trying to get logs from node hxx-m-2 pod pod-host-path-test container test-container-1: <nil>
STEP: delete the pod
Dec 28 07:57:25.376: INFO: Waiting for pod pod-host-path-test to disappear
Dec 28 07:57:25.377: INFO: Pod pod-host-path-test no longer exists
[AfterEach] [sig-storage] HostPath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 07:57:25.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostpath-9069" for this suite.
Dec 28 07:57:31.387: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 07:57:31.450: INFO: namespace hostpath-9069 deletion completed in 6.070026011s

• [SLOW TEST:8.110 seconds]
[sig-storage] HostPath
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/host_path.go:34
  should give a volume the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions 
  should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 07:57:31.450: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if v1 is in available api versions  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating api versions
Dec 28 07:57:31.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 api-versions'
Dec 28 07:57:31.550: INFO: stderr: ""
Dec 28 07:57:31.550: INFO: stdout: "ace3.alauda.io/v1\nadmission.certmanager.k8s.io/v1beta1\nadmissionregistration.k8s.io/v1\nadmissionregistration.k8s.io/v1beta1\naiops.alauda.io/v1beta1\nalauda.io/v1\napiextensions.k8s.io/v1\napiextensions.k8s.io/v1beta1\napiregistration.k8s.io/v1\napiregistration.k8s.io/v1beta1\napp.alauda.io/v1alpha1\napps/v1\nauth.alauda.io/v1\nauth.alauda.io/v1beta1\nauthentication.k8s.io/v1\nauthentication.k8s.io/v1beta1\nauthorization.k8s.io/v1\nauthorization.k8s.io/v1beta1\nautoscaling/v1\nautoscaling/v2beta1\nautoscaling/v2beta2\nbatch/v1\nbatch/v1beta1\ncertificates.k8s.io/v1beta1\ncertmanager.k8s.io/v1alpha1\nclusterregistry.k8s.io/v1alpha1\ncoordination.k8s.io/v1\ncoordination.k8s.io/v1beta1\ncrd.alauda.io/v2\ndex.coreos.com/v1\nevents.k8s.io/v1beta1\nextensions/v1beta1\nnetworking.k8s.io/v1\nnetworking.k8s.io/v1beta1\nnode.k8s.io/v1beta1\npolicy/v1beta1\nportal.alauda.io/v1alpha1\nrbac.authorization.k8s.io/v1\nrbac.authorization.k8s.io/v1beta1\nscheduling.k8s.io/v1\nscheduling.k8s.io/v1beta1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 07:57:31.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6585" for this suite.
Dec 28 07:57:37.560: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 07:57:37.624: INFO: namespace kubectl-6585 deletion completed in 6.071569716s

• [SLOW TEST:6.174 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:738
    should check if v1 is in available api versions  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 07:57:37.624: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-261e0442-ac2a-4cd3-a8bd-a087e0cdee04
STEP: Creating a pod to test consume secrets
Dec 28 07:57:37.650: INFO: Waiting up to 5m0s for pod "pod-secrets-f93430fe-de4e-4401-9193-d9dfb22283e3" in namespace "secrets-6556" to be "success or failure"
Dec 28 07:57:37.651: INFO: Pod "pod-secrets-f93430fe-de4e-4401-9193-d9dfb22283e3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.407013ms
Dec 28 07:57:39.653: INFO: Pod "pod-secrets-f93430fe-de4e-4401-9193-d9dfb22283e3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003474512s
STEP: Saw pod success
Dec 28 07:57:39.653: INFO: Pod "pod-secrets-f93430fe-de4e-4401-9193-d9dfb22283e3" satisfied condition "success or failure"
Dec 28 07:57:39.655: INFO: Trying to get logs from node hxx-m-2 pod pod-secrets-f93430fe-de4e-4401-9193-d9dfb22283e3 container secret-env-test: <nil>
STEP: delete the pod
Dec 28 07:57:39.665: INFO: Waiting for pod pod-secrets-f93430fe-de4e-4401-9193-d9dfb22283e3 to disappear
Dec 28 07:57:39.666: INFO: Pod pod-secrets-f93430fe-de4e-4401-9193-d9dfb22283e3 no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 07:57:39.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6556" for this suite.
Dec 28 07:57:45.676: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 07:57:45.747: INFO: namespace secrets-6556 deletion completed in 6.078449392s

• [SLOW TEST:8.123 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run default 
  should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 07:57:45.747: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run default
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1403
[It] should create an rc or deployment from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 28 07:57:45.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 run e2e-test-httpd-deployment --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-7545'
Dec 28 07:57:45.842: INFO: stderr: "kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 28 07:57:45.842: INFO: stdout: "deployment.apps/e2e-test-httpd-deployment created\n"
STEP: verifying the pod controlled by e2e-test-httpd-deployment gets created
[AfterEach] Kubectl run default
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1409
Dec 28 07:57:45.845: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 delete deployment e2e-test-httpd-deployment --namespace=kubectl-7545'
Dec 28 07:57:45.923: INFO: stderr: ""
Dec 28 07:57:45.923: INFO: stdout: "deployment.apps \"e2e-test-httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 07:57:45.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7545" for this suite.
Dec 28 07:58:13.934: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 07:58:14.004: INFO: namespace kubectl-7545 deletion completed in 28.077705166s

• [SLOW TEST:28.257 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run default
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1397
    should create an rc or deployment from an image  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 07:58:14.004: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-675669f7-e378-4f1f-aa6d-28257462a08d
STEP: Creating a pod to test consume configMaps
Dec 28 07:58:14.026: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-37dfcc08-f9e3-4d6f-9417-aff992b2cd1d" in namespace "projected-3689" to be "success or failure"
Dec 28 07:58:14.027: INFO: Pod "pod-projected-configmaps-37dfcc08-f9e3-4d6f-9417-aff992b2cd1d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.343514ms
Dec 28 07:58:16.029: INFO: Pod "pod-projected-configmaps-37dfcc08-f9e3-4d6f-9417-aff992b2cd1d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003641685s
STEP: Saw pod success
Dec 28 07:58:16.029: INFO: Pod "pod-projected-configmaps-37dfcc08-f9e3-4d6f-9417-aff992b2cd1d" satisfied condition "success or failure"
Dec 28 07:58:16.031: INFO: Trying to get logs from node hxx-m-2 pod pod-projected-configmaps-37dfcc08-f9e3-4d6f-9417-aff992b2cd1d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 28 07:58:16.040: INFO: Waiting for pod pod-projected-configmaps-37dfcc08-f9e3-4d6f-9417-aff992b2cd1d to disappear
Dec 28 07:58:16.041: INFO: Pod pod-projected-configmaps-37dfcc08-f9e3-4d6f-9417-aff992b2cd1d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 07:58:16.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3689" for this suite.
Dec 28 07:58:22.053: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 07:58:22.125: INFO: namespace projected-3689 deletion completed in 6.079194022s

• [SLOW TEST:8.121 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 07:58:22.125: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-f6cb9e67-c9d1-4978-a8db-c16903885524
STEP: Creating a pod to test consume configMaps
Dec 28 07:58:22.147: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-e5e17b73-ae4d-420e-b34a-1a661fa7e5e6" in namespace "projected-3684" to be "success or failure"
Dec 28 07:58:22.148: INFO: Pod "pod-projected-configmaps-e5e17b73-ae4d-420e-b34a-1a661fa7e5e6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.556078ms
Dec 28 07:58:24.150: INFO: Pod "pod-projected-configmaps-e5e17b73-ae4d-420e-b34a-1a661fa7e5e6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003918541s
STEP: Saw pod success
Dec 28 07:58:24.151: INFO: Pod "pod-projected-configmaps-e5e17b73-ae4d-420e-b34a-1a661fa7e5e6" satisfied condition "success or failure"
Dec 28 07:58:24.152: INFO: Trying to get logs from node hxx-m-2 pod pod-projected-configmaps-e5e17b73-ae4d-420e-b34a-1a661fa7e5e6 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 28 07:58:24.163: INFO: Waiting for pod pod-projected-configmaps-e5e17b73-ae4d-420e-b34a-1a661fa7e5e6 to disappear
Dec 28 07:58:24.164: INFO: Pod pod-projected-configmaps-e5e17b73-ae4d-420e-b34a-1a661fa7e5e6 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 07:58:24.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3684" for this suite.
Dec 28 07:58:30.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 07:58:30.240: INFO: namespace projected-3684 deletion completed in 6.072532316s

• [SLOW TEST:8.115 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 07:58:30.241: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override all
Dec 28 07:58:30.266: INFO: Waiting up to 5m0s for pod "client-containers-8052b706-4250-457e-bfde-ee742fe0612e" in namespace "containers-6959" to be "success or failure"
Dec 28 07:58:30.267: INFO: Pod "client-containers-8052b706-4250-457e-bfde-ee742fe0612e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.596384ms
Dec 28 07:58:32.270: INFO: Pod "client-containers-8052b706-4250-457e-bfde-ee742fe0612e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003659756s
STEP: Saw pod success
Dec 28 07:58:32.270: INFO: Pod "client-containers-8052b706-4250-457e-bfde-ee742fe0612e" satisfied condition "success or failure"
Dec 28 07:58:32.271: INFO: Trying to get logs from node hxx-m-2 pod client-containers-8052b706-4250-457e-bfde-ee742fe0612e container test-container: <nil>
STEP: delete the pod
Dec 28 07:58:32.281: INFO: Waiting for pod client-containers-8052b706-4250-457e-bfde-ee742fe0612e to disappear
Dec 28 07:58:32.283: INFO: Pod client-containers-8052b706-4250-457e-bfde-ee742fe0612e no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 07:58:32.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-6959" for this suite.
Dec 28 07:58:38.292: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 07:58:38.360: INFO: namespace containers-6959 deletion completed in 6.074745828s

• [SLOW TEST:8.119 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 07:58:38.360: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override command
Dec 28 07:58:38.381: INFO: Waiting up to 5m0s for pod "client-containers-9979f3f8-4495-4529-9c0c-8071dd045340" in namespace "containers-7902" to be "success or failure"
Dec 28 07:58:38.382: INFO: Pod "client-containers-9979f3f8-4495-4529-9c0c-8071dd045340": Phase="Pending", Reason="", readiness=false. Elapsed: 1.400903ms
Dec 28 07:58:40.385: INFO: Pod "client-containers-9979f3f8-4495-4529-9c0c-8071dd045340": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003697765s
STEP: Saw pod success
Dec 28 07:58:40.385: INFO: Pod "client-containers-9979f3f8-4495-4529-9c0c-8071dd045340" satisfied condition "success or failure"
Dec 28 07:58:40.386: INFO: Trying to get logs from node hxx-m-2 pod client-containers-9979f3f8-4495-4529-9c0c-8071dd045340 container test-container: <nil>
STEP: delete the pod
Dec 28 07:58:40.396: INFO: Waiting for pod client-containers-9979f3f8-4495-4529-9c0c-8071dd045340 to disappear
Dec 28 07:58:40.397: INFO: Pod client-containers-9979f3f8-4495-4529-9c0c-8071dd045340 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 07:58:40.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7902" for this suite.
Dec 28 07:58:46.407: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 07:58:46.475: INFO: namespace containers-7902 deletion completed in 6.074922187s

• [SLOW TEST:8.115 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default command (docker entrypoint) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 07:58:46.475: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on node default medium
Dec 28 07:58:46.495: INFO: Waiting up to 5m0s for pod "pod-98e8ebf8-c8e5-46f9-8db6-a1f7fe767983" in namespace "emptydir-4750" to be "success or failure"
Dec 28 07:58:46.497: INFO: Pod "pod-98e8ebf8-c8e5-46f9-8db6-a1f7fe767983": Phase="Pending", Reason="", readiness=false. Elapsed: 1.555298ms
Dec 28 07:58:48.499: INFO: Pod "pod-98e8ebf8-c8e5-46f9-8db6-a1f7fe767983": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003957878s
STEP: Saw pod success
Dec 28 07:58:48.500: INFO: Pod "pod-98e8ebf8-c8e5-46f9-8db6-a1f7fe767983" satisfied condition "success or failure"
Dec 28 07:58:48.501: INFO: Trying to get logs from node hxx-m-2 pod pod-98e8ebf8-c8e5-46f9-8db6-a1f7fe767983 container test-container: <nil>
STEP: delete the pod
Dec 28 07:58:48.512: INFO: Waiting for pod pod-98e8ebf8-c8e5-46f9-8db6-a1f7fe767983 to disappear
Dec 28 07:58:48.513: INFO: Pod pod-98e8ebf8-c8e5-46f9-8db6-a1f7fe767983 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 07:58:48.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4750" for this suite.
Dec 28 07:58:54.523: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 07:58:54.592: INFO: namespace emptydir-4750 deletion completed in 6.076004279s

• [SLOW TEST:8.117 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] DNS 
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 07:58:54.592: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7871.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7871.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7871.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7871.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7871.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7871.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7871.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7871.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7871.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7871.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7871.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 98.246.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.246.98_udp@PTR;check="$$(dig +tcp +noall +answer +search 98.246.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.246.98_tcp@PTR;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7871.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7871.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7871.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7871.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7871.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7871.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7871.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7871.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7871.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7871.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-7871.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;check="$$(dig +notcp +noall +answer +search 98.246.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.246.98_udp@PTR;check="$$(dig +tcp +noall +answer +search 98.246.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.246.98_tcp@PTR;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 28 07:58:58.630: INFO: Unable to read wheezy_udp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:58:58.632: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:58:58.633: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:58:58.636: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:58:58.649: INFO: Unable to read jessie_udp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:58:58.651: INFO: Unable to read jessie_tcp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:58:58.653: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:58:58.655: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:58:58.666: INFO: Lookups using dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd failed for: [wheezy_udp@dns-test-service.dns-7871.svc.cluster.local wheezy_tcp@dns-test-service.dns-7871.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local jessie_udp@dns-test-service.dns-7871.svc.cluster.local jessie_tcp@dns-test-service.dns-7871.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local]

Dec 28 07:59:03.668: INFO: Unable to read wheezy_udp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:03.671: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:03.673: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:03.674: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:03.687: INFO: Unable to read jessie_udp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:03.689: INFO: Unable to read jessie_tcp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:03.690: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:03.692: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:03.702: INFO: Lookups using dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd failed for: [wheezy_udp@dns-test-service.dns-7871.svc.cluster.local wheezy_tcp@dns-test-service.dns-7871.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local jessie_udp@dns-test-service.dns-7871.svc.cluster.local jessie_tcp@dns-test-service.dns-7871.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local]

Dec 28 07:59:08.668: INFO: Unable to read wheezy_udp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:08.670: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:08.672: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:08.675: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:08.688: INFO: Unable to read jessie_udp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:08.690: INFO: Unable to read jessie_tcp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:08.692: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:08.694: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:08.705: INFO: Lookups using dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd failed for: [wheezy_udp@dns-test-service.dns-7871.svc.cluster.local wheezy_tcp@dns-test-service.dns-7871.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local jessie_udp@dns-test-service.dns-7871.svc.cluster.local jessie_tcp@dns-test-service.dns-7871.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local]

Dec 28 07:59:13.669: INFO: Unable to read wheezy_udp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:13.671: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:13.673: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:13.674: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:13.687: INFO: Unable to read jessie_udp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:13.689: INFO: Unable to read jessie_tcp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:13.691: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:13.693: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:13.703: INFO: Lookups using dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd failed for: [wheezy_udp@dns-test-service.dns-7871.svc.cluster.local wheezy_tcp@dns-test-service.dns-7871.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local jessie_udp@dns-test-service.dns-7871.svc.cluster.local jessie_tcp@dns-test-service.dns-7871.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local]

Dec 28 07:59:18.669: INFO: Unable to read wheezy_udp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:18.671: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:18.673: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:18.674: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:18.688: INFO: Unable to read jessie_udp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:18.690: INFO: Unable to read jessie_tcp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:18.692: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:18.694: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:18.706: INFO: Lookups using dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd failed for: [wheezy_udp@dns-test-service.dns-7871.svc.cluster.local wheezy_tcp@dns-test-service.dns-7871.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local jessie_udp@dns-test-service.dns-7871.svc.cluster.local jessie_tcp@dns-test-service.dns-7871.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local]

Dec 28 07:59:23.669: INFO: Unable to read wheezy_udp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:23.671: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:23.672: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:23.674: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:23.687: INFO: Unable to read jessie_udp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:23.689: INFO: Unable to read jessie_tcp@dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:23.691: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:23.692: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local from pod dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd: the server could not find the requested resource (get pods dns-test-98e88258-a106-4820-aca1-d435169f47cd)
Dec 28 07:59:23.703: INFO: Lookups using dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd failed for: [wheezy_udp@dns-test-service.dns-7871.svc.cluster.local wheezy_tcp@dns-test-service.dns-7871.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local jessie_udp@dns-test-service.dns-7871.svc.cluster.local jessie_tcp@dns-test-service.dns-7871.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-7871.svc.cluster.local]

Dec 28 07:59:28.703: INFO: DNS probes using dns-7871/dns-test-98e88258-a106-4820-aca1-d435169f47cd succeeded

STEP: deleting the pod
STEP: deleting the test service
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 07:59:28.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-7871" for this suite.
Dec 28 07:59:34.758: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 07:59:34.852: INFO: namespace dns-7871 deletion completed in 6.104409792s

• [SLOW TEST:40.260 seconds]
[sig-network] DNS
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for services  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 07:59:34.852: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-696
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Dec 28 07:59:34.875: INFO: Found 0 stateful pods, waiting for 3
Dec 28 07:59:44.877: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 28 07:59:44.877: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 28 07:59:44.877: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Dec 28 07:59:44.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-696 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 28 07:59:45.110: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 28 07:59:45.110: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 28 07:59:45.110: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec 28 07:59:55.133: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Updating Pods in reverse ordinal order
Dec 28 08:00:05.141: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-696 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 08:00:05.367: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 28 08:00:05.367: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 28 08:00:05.367: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 28 08:00:15.378: INFO: Waiting for StatefulSet statefulset-696/ss2 to complete update
Dec 28 08:00:15.378: INFO: Waiting for Pod statefulset-696/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 28 08:00:15.378: INFO: Waiting for Pod statefulset-696/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 28 08:00:15.378: INFO: Waiting for Pod statefulset-696/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 28 08:00:25.383: INFO: Waiting for StatefulSet statefulset-696/ss2 to complete update
Dec 28 08:00:25.383: INFO: Waiting for Pod statefulset-696/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 28 08:00:25.383: INFO: Waiting for Pod statefulset-696/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 28 08:00:35.390: INFO: Waiting for StatefulSet statefulset-696/ss2 to complete update
Dec 28 08:00:35.390: INFO: Waiting for Pod statefulset-696/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Rolling back to a previous revision
Dec 28 08:00:45.382: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-696 ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 28 08:00:45.614: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 28 08:00:45.614: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 28 08:00:45.614: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 28 08:00:55.636: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order
Dec 28 08:01:05.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-696 ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 08:01:05.874: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 28 08:01:05.874: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 28 08:01:05.874: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 28 08:01:15.887: INFO: Waiting for StatefulSet statefulset-696/ss2 to complete update
Dec 28 08:01:15.887: INFO: Waiting for Pod statefulset-696/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 28 08:01:15.887: INFO: Waiting for Pod statefulset-696/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 28 08:01:15.887: INFO: Waiting for Pod statefulset-696/ss2-2 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 28 08:01:25.891: INFO: Waiting for StatefulSet statefulset-696/ss2 to complete update
Dec 28 08:01:25.891: INFO: Waiting for Pod statefulset-696/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 28 08:01:25.891: INFO: Waiting for Pod statefulset-696/ss2-1 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
Dec 28 08:01:35.891: INFO: Waiting for StatefulSet statefulset-696/ss2 to complete update
Dec 28 08:01:35.891: INFO: Waiting for Pod statefulset-696/ss2-0 to have revision ss2-65c7964b94 update revision ss2-84f9d6bf57
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 28 08:01:45.891: INFO: Deleting all statefulset in ns statefulset-696
Dec 28 08:01:45.893: INFO: Scaling statefulset ss2 to 0
Dec 28 08:02:15.901: INFO: Waiting for statefulset status.replicas updated to 0
Dec 28 08:02:15.903: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:02:15.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-696" for this suite.
Dec 28 08:02:21.925: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:02:21.991: INFO: namespace statefulset-696 deletion completed in 6.078650543s

• [SLOW TEST:167.139 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform rolling updates and roll backs of template modifications [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:02:21.991: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 28 08:02:22.376: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 28 08:02:25.388: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a validating webhook configuration
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Updating a validating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Patching a validating webhook configuration's rules to include the create operation
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:02:25.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-713" for this suite.
Dec 28 08:02:31.428: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:02:31.497: INFO: namespace webhook-713 deletion completed in 6.076557578s
STEP: Destroying namespace "webhook-713-markers" for this suite.
Dec 28 08:02:37.505: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:02:37.579: INFO: namespace webhook-713-markers deletion completed in 6.082204737s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:15.597 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-node] ConfigMap 
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:02:37.589: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-6192/configmap-test-6e547baf-1572-4b57-8549-04a6f7f321fb
STEP: Creating a pod to test consume configMaps
Dec 28 08:02:37.614: INFO: Waiting up to 5m0s for pod "pod-configmaps-6b1bca7e-a54b-4978-94b3-fad85d9b7889" in namespace "configmap-6192" to be "success or failure"
Dec 28 08:02:37.615: INFO: Pod "pod-configmaps-6b1bca7e-a54b-4978-94b3-fad85d9b7889": Phase="Pending", Reason="", readiness=false. Elapsed: 1.57698ms
Dec 28 08:02:39.618: INFO: Pod "pod-configmaps-6b1bca7e-a54b-4978-94b3-fad85d9b7889": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004085125s
STEP: Saw pod success
Dec 28 08:02:39.618: INFO: Pod "pod-configmaps-6b1bca7e-a54b-4978-94b3-fad85d9b7889" satisfied condition "success or failure"
Dec 28 08:02:39.619: INFO: Trying to get logs from node hxx-m-2 pod pod-configmaps-6b1bca7e-a54b-4978-94b3-fad85d9b7889 container env-test: <nil>
STEP: delete the pod
Dec 28 08:02:39.637: INFO: Waiting for pod pod-configmaps-6b1bca7e-a54b-4978-94b3-fad85d9b7889 to disappear
Dec 28 08:02:39.638: INFO: Pod pod-configmaps-6b1bca7e-a54b-4978-94b3-fad85d9b7889 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:02:39.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6192" for this suite.
Dec 28 08:02:45.648: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:02:45.714: INFO: namespace configmap-6192 deletion completed in 6.073430508s

• [SLOW TEST:8.125 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via environment variable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:02:45.715: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec 28 08:02:45.732: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:02:49.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-1118" for this suite.
Dec 28 08:02:55.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:02:55.488: INFO: namespace init-container-1118 deletion completed in 6.077037832s

• [SLOW TEST:9.774 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:02:55.489: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 28 08:02:55.510: INFO: Waiting up to 5m0s for pod "downward-api-5cf14f81-5bec-4fec-8c5c-c9ca891dfb58" in namespace "downward-api-5842" to be "success or failure"
Dec 28 08:02:55.511: INFO: Pod "downward-api-5cf14f81-5bec-4fec-8c5c-c9ca891dfb58": Phase="Pending", Reason="", readiness=false. Elapsed: 1.639088ms
Dec 28 08:02:57.514: INFO: Pod "downward-api-5cf14f81-5bec-4fec-8c5c-c9ca891dfb58": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004172646s
STEP: Saw pod success
Dec 28 08:02:57.514: INFO: Pod "downward-api-5cf14f81-5bec-4fec-8c5c-c9ca891dfb58" satisfied condition "success or failure"
Dec 28 08:02:57.515: INFO: Trying to get logs from node hxx-m-2 pod downward-api-5cf14f81-5bec-4fec-8c5c-c9ca891dfb58 container dapi-container: <nil>
STEP: delete the pod
Dec 28 08:02:57.531: INFO: Waiting for pod downward-api-5cf14f81-5bec-4fec-8c5c-c9ca891dfb58 to disappear
Dec 28 08:02:57.533: INFO: Pod downward-api-5cf14f81-5bec-4fec-8c5c-c9ca891dfb58 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:02:57.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5842" for this suite.
Dec 28 08:03:03.542: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:03:03.609: INFO: namespace downward-api-5842 deletion completed in 6.07330677s

• [SLOW TEST:8.120 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod UID as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:03:03.609: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Discovering how many secrets are in namespace by default
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Secret
STEP: Ensuring resource quota status captures secret creation
STEP: Deleting a secret
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:03:20.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1475" for this suite.
Dec 28 08:03:26.657: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:03:26.729: INFO: namespace resourcequota-1475 deletion completed in 6.078686673s

• [SLOW TEST:23.120 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:03:26.729: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not terminating scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a long running pod
STEP: Ensuring resource quota with not terminating scope captures the pod usage
STEP: Ensuring resource quota with terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a terminating pod
STEP: Ensuring resource quota with terminating scope captures the pod usage
STEP: Ensuring resource quota with not terminating scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:03:42.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7638" for this suite.
Dec 28 08:03:48.802: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:03:48.870: INFO: namespace resourcequota-7638 deletion completed in 6.074009877s

• [SLOW TEST:22.141 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:03:48.870: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods
STEP: Gathering metrics
Dec 28 08:04:28.904: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:04:28.904: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1228 08:04:28.904466      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-3589" for this suite.
Dec 28 08:04:34.913: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:04:34.985: INFO: namespace gc-3589 deletion completed in 6.078658181s

• [SLOW TEST:46.115 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should honor timeout [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:04:34.985: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 28 08:04:35.356: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 28 08:04:38.367: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting timeout (1s) shorter than webhook latency (5s)
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s)
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is longer than webhook latency
STEP: Registering slow webhook via the AdmissionRegistration API
STEP: Having no error when timeout is empty (defaulted to 10s in v1)
STEP: Registering slow webhook via the AdmissionRegistration API
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:04:50.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6852" for this suite.
Dec 28 08:04:56.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:04:56.507: INFO: namespace webhook-6852 deletion completed in 6.073770156s
STEP: Destroying namespace "webhook-6852-markers" for this suite.
Dec 28 08:05:02.514: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:05:02.580: INFO: namespace webhook-6852-markers deletion completed in 6.073076829s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:27.601 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:05:02.587: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota with best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a ResourceQuota with not best effort scope
STEP: Ensuring ResourceQuota status is calculated
STEP: Creating a best-effort pod
STEP: Ensuring resource quota with best effort scope captures the pod usage
STEP: Ensuring resource quota with not best effort ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
STEP: Creating a not best-effort pod
STEP: Ensuring resource quota with not best effort scope captures the pod usage
STEP: Ensuring resource quota with best effort scope ignored the pod usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:05:18.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-317" for this suite.
Dec 28 08:05:24.658: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:05:24.723: INFO: namespace resourcequota-317 deletion completed in 6.072821372s

• [SLOW TEST:22.137 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:05:24.723: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 28 08:05:24.740: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 28 08:05:24.746: INFO: Waiting for terminating namespaces to be deleted...
Dec 28 08:05:24.747: INFO: 
Logging pods the kubelet thinks is on node hxx-m-1 before test
Dec 28 08:05:24.762: INFO: sonobuoy-systemd-logs-daemon-set-690f17abb3794222-tlb58 from sonobuoy started at 2019-12-28 07:53:22 +0000 UTC (2 container statuses recorded)
Dec 28 08:05:24.762: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 28 08:05:24.762: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 28 08:05:24.762: INFO: coredns-66447b44c9-zphmm from kube-system started at 2019-12-27 13:58:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.762: INFO: 	Container coredns ready: true, restart count 0
Dec 28 08:05:24.762: INFO: cert-manager-77f5bf4f5-h86rt from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.762: INFO: 	Container cert-manager ready: true, restart count 0
Dec 28 08:05:24.762: INFO: cert-manager-webhook-578c59dddd-k697c from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.762: INFO: 	Container webhook ready: true, restart count 0
Dec 28 08:05:24.762: INFO: apollo-cfdd64bb4-4lssk from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.762: INFO: 	Container apollo ready: true, restart count 0
Dec 28 08:05:24.762: INFO: underlord-5c45b96c5d-nmjjq from cpaas-system started at 2019-12-27 14:46:59 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.762: INFO: 	Container alauda-console ready: true, restart count 0
Dec 28 08:05:24.762: INFO: dex-8448b48ff8-2bd5h from cpaas-system started at 2019-12-27 14:09:25 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.762: INFO: 	Container dex ready: true, restart count 0
Dec 28 08:05:24.762: INFO: erebus-5597f9565d-zwjnz from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.762: INFO: 	Container erebus ready: true, restart count 0
Dec 28 08:05:24.762: INFO: auth-controller2-79ff55cd75-cbjbn from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.762: INFO: 	Container auth-controller2-manager ready: true, restart count 0
Dec 28 08:05:24.762: INFO: furion-679c948779-jz6lf from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.762: INFO: 	Container furion ready: true, restart count 0
Dec 28 08:05:24.762: INFO: cert-manager-cainjector-67d4dd59ff-8jhs9 from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.762: INFO: 	Container cainjector ready: true, restart count 0
Dec 28 08:05:24.762: INFO: archon-5fdc59d78c-rhtzm from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.762: INFO: 	Container archon-api ready: true, restart count 0
Dec 28 08:05:24.762: INFO: blink-7d4fb788b5-s9q49 from cpaas-system started at 2019-12-28 07:52:16 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.762: INFO: 	Container alauda-console ready: true, restart count 0
Dec 28 08:05:24.762: INFO: kube-proxy-bnhj6 from kube-system started at 2019-12-27 13:58:01 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.762: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 28 08:05:24.762: INFO: agon-75b987dff5-7bht5 from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.762: INFO: 	Container agon ready: true, restart count 2
Dec 28 08:05:24.762: INFO: cluster-registry-controller-manager-76774c98d-7c7mz from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (2 container statuses recorded)
Dec 28 08:05:24.762: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 28 08:05:24.762: INFO: 	Container manager ready: true, restart count 0
Dec 28 08:05:24.762: INFO: sonobuoy-e2e-job-f0b9f709fd414134 from sonobuoy started at 2019-12-28 07:53:22 +0000 UTC (2 container statuses recorded)
Dec 28 08:05:24.762: INFO: 	Container e2e ready: true, restart count 0
Dec 28 08:05:24.762: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 28 08:05:24.762: INFO: kube-flannel-8hgpf from kube-system started at 2019-12-27 13:58:14 +0000 UTC (2 container statuses recorded)
Dec 28 08:05:24.762: INFO: 	Container install-cni ready: true, restart count 0
Dec 28 08:05:24.762: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 28 08:05:24.762: INFO: 
Logging pods the kubelet thinks is on node hxx-m-2 before test
Dec 28 08:05:24.773: INFO: sonobuoy-systemd-logs-daemon-set-690f17abb3794222-jftvp from sonobuoy started at 2019-12-28 07:53:22 +0000 UTC (2 container statuses recorded)
Dec 28 08:05:24.773: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 28 08:05:24.773: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 28 08:05:24.773: INFO: kube-scheduler-hxx-m-2 from kube-system started at 2019-12-27 13:57:11 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.773: INFO: 	Container kube-scheduler ready: true, restart count 0
Dec 28 08:05:24.773: INFO: captain-7cf8c65b49-jt888 from cpaas-system started at 2019-12-28 07:23:32 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.773: INFO: 	Container captain ready: true, restart count 1
Dec 28 08:05:24.773: INFO: kube-apiserver-hxx-m-2 from kube-system started at 2019-12-27 13:57:11 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.773: INFO: 	Container kube-apiserver ready: true, restart count 0
Dec 28 08:05:24.773: INFO: kube-controller-manager-hxx-m-2 from kube-system started at 2019-12-27 13:57:11 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.773: INFO: 	Container kube-controller-manager ready: true, restart count 0
Dec 28 08:05:24.773: INFO: sonobuoy from sonobuoy started at 2019-12-28 07:53:21 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.773: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 28 08:05:24.773: INFO: etcd-hxx-m-2 from kube-system started at 2019-12-27 13:57:11 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.773: INFO: 	Container etcd ready: true, restart count 0
Dec 28 08:05:24.773: INFO: nginx-ingress-controller-f9b5d49fd-x4774 from cpaas-system started at 2019-12-28 05:10:02 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.773: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec 28 08:05:24.773: INFO: blink-7d4fb788b5-bpwqq from cpaas-system started at 2019-12-28 07:52:16 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.773: INFO: 	Container alauda-console ready: true, restart count 0
Dec 28 08:05:24.773: INFO: kube-flannel-fxzw7 from kube-system started at 2019-12-28 05:10:03 +0000 UTC (2 container statuses recorded)
Dec 28 08:05:24.774: INFO: 	Container install-cni ready: true, restart count 0
Dec 28 08:05:24.774: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 28 08:05:24.774: INFO: kube-proxy-44f6q from kube-system started at 2019-12-27 13:57:35 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.774: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 28 08:05:24.774: INFO: 
Logging pods the kubelet thinks is on node hxx-m-3 before test
Dec 28 08:05:24.787: INFO: coredns-66447b44c9-5qbrm from kube-system started at 2019-12-27 13:58:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.787: INFO: 	Container coredns ready: true, restart count 0
Dec 28 08:05:24.787: INFO: cluster-registry-controller-manager-76774c98d-rmnh2 from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (2 container statuses recorded)
Dec 28 08:05:24.787: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 28 08:05:24.787: INFO: 	Container manager ready: true, restart count 0
Dec 28 08:05:24.787: INFO: furion-679c948779-xjs2n from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.787: INFO: 	Container furion ready: true, restart count 0
Dec 28 08:05:24.787: INFO: archon-5fdc59d78c-k5jbm from cpaas-system started at 2019-12-27 14:46:59 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.787: INFO: 	Container archon-api ready: true, restart count 0
Dec 28 08:05:24.787: INFO: tiller-deploy-7c757c6f9c-67l6b from kube-system started at 2019-12-27 13:59:57 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.787: INFO: 	Container tiller ready: true, restart count 0
Dec 28 08:05:24.787: INFO: auth-controller2-79ff55cd75-4clk9 from cpaas-system started at 2019-12-27 14:46:59 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.787: INFO: 	Container auth-controller2-manager ready: true, restart count 0
Dec 28 08:05:24.787: INFO: cert-manager-cainjector-67d4dd59ff-tngcj from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.787: INFO: 	Container cainjector ready: true, restart count 0
Dec 28 08:05:24.787: INFO: erebus-5597f9565d-ptdps from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.787: INFO: 	Container erebus ready: true, restart count 0
Dec 28 08:05:24.787: INFO: sonobuoy-systemd-logs-daemon-set-690f17abb3794222-vskml from sonobuoy started at 2019-12-28 07:53:22 +0000 UTC (2 container statuses recorded)
Dec 28 08:05:24.787: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 28 08:05:24.787: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 28 08:05:24.787: INFO: kube-proxy-5kzvk from kube-system started at 2019-12-27 13:57:57 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.787: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 28 08:05:24.787: INFO: kube-flannel-f87zg from kube-system started at 2019-12-27 13:58:14 +0000 UTC (2 container statuses recorded)
Dec 28 08:05:24.787: INFO: 	Container install-cni ready: true, restart count 0
Dec 28 08:05:24.787: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 28 08:05:24.787: INFO: cert-manager-webhook-578c59dddd-flp52 from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.787: INFO: 	Container webhook ready: true, restart count 0
Dec 28 08:05:24.787: INFO: dex-8448b48ff8-9qn2j from cpaas-system started at 2019-12-27 14:09:25 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.787: INFO: 	Container dex ready: true, restart count 0
Dec 28 08:05:24.787: INFO: cert-manager-77f5bf4f5-2wz6n from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.787: INFO: 	Container cert-manager ready: true, restart count 0
Dec 28 08:05:24.787: INFO: underlord-5c45b96c5d-6cvvc from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.787: INFO: 	Container alauda-console ready: true, restart count 0
Dec 28 08:05:24.787: INFO: agon-75b987dff5-bl7sb from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.787: INFO: 	Container agon ready: true, restart count 2
Dec 28 08:05:24.787: INFO: apollo-cfdd64bb4-hjth5 from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:05:24.787: INFO: 	Container apollo ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-d139db72-58b5-454e-9fa1-27861114b5ea 42
STEP: Trying to relaunch the pod, now with labels.
STEP: removing the label kubernetes.io/e2e-d139db72-58b5-454e-9fa1-27861114b5ea off the node hxx-m-2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-d139db72-58b5-454e-9fa1-27861114b5ea
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:05:28.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-2709" for this suite.
Dec 28 08:05:46.832: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:05:46.900: INFO: namespace sched-pred-2709 deletion completed in 18.074572099s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:22.176 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if matching  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:05:46.900: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service endpoint-test2 in namespace services-1356
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1356 to expose endpoints map[]
Dec 28 08:05:46.923: INFO: Get endpoints failed (1.869676ms elapsed, ignoring for 5s): endpoints "endpoint-test2" not found
Dec 28 08:05:47.925: INFO: successfully validated that service endpoint-test2 in namespace services-1356 exposes endpoints map[] (1.004005833s elapsed)
STEP: Creating pod pod1 in namespace services-1356
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1356 to expose endpoints map[pod1:[80]]
Dec 28 08:05:49.945: INFO: successfully validated that service endpoint-test2 in namespace services-1356 exposes endpoints map[pod1:[80]] (2.01514438s elapsed)
STEP: Creating pod pod2 in namespace services-1356
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1356 to expose endpoints map[pod1:[80] pod2:[80]]
Dec 28 08:05:51.964: INFO: successfully validated that service endpoint-test2 in namespace services-1356 exposes endpoints map[pod1:[80] pod2:[80]] (2.016012339s elapsed)
STEP: Deleting pod pod1 in namespace services-1356
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1356 to expose endpoints map[pod2:[80]]
Dec 28 08:05:51.973: INFO: successfully validated that service endpoint-test2 in namespace services-1356 exposes endpoints map[pod2:[80]] (5.401359ms elapsed)
STEP: Deleting pod pod2 in namespace services-1356
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1356 to expose endpoints map[]
Dec 28 08:05:52.980: INFO: successfully validated that service endpoint-test2 in namespace services-1356 exposes endpoints map[] (1.004429856s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:05:52.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1356" for this suite.
Dec 28 08:05:59.004: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:05:59.070: INFO: namespace services-1356 deletion completed in 6.075402209s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:12.170 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl replace 
  should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:05:59.071: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl replace
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1704
[It] should update a single-container pod's image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 28 08:05:59.088: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 run e2e-test-httpd-pod --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --labels=run=e2e-test-httpd-pod --namespace=kubectl-846'
Dec 28 08:05:59.259: INFO: stderr: ""
Dec 28 08:05:59.259: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running
STEP: verifying the pod e2e-test-httpd-pod was created
Dec 28 08:06:04.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pod e2e-test-httpd-pod --namespace=kubectl-846 -o json'
Dec 28 08:06:04.380: INFO: stderr: ""
Dec 28 08:06:04.380: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2019-12-28T08:05:59Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-846\",\n        \"resourceVersion\": \"273433\",\n        \"selfLink\": \"/api/v1/namespaces/kubectl-846/pods/e2e-test-httpd-pod\",\n        \"uid\": \"b3c69d0a-6af6-4962-b5ef-2e79b5d27dbf\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"docker.io/library/httpd:2.4.38-alpine\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"default-token-946j7\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"hxx-m-2\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"default-token-946j7\",\n                \"secret\": {\n                    \"defaultMode\": 420,\n                    \"secretName\": \"default-token-946j7\"\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-28T08:05:59Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-28T08:06:00Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-28T08:06:00Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2019-12-28T08:05:59Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://25acb6f84e07415d9c18d2cf2a2e713381ddf92cd77d1b5030a5b3c61095b1f1\",\n                \"image\": \"httpd:2.4.38-alpine\",\n                \"imageID\": \"docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2019-12-28T08:06:00Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.0.128.16\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.199.0.249\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.199.0.249\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2019-12-28T08:05:59Z\"\n    }\n}\n"
STEP: replace the image in the pod
Dec 28 08:06:04.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 replace -f - --namespace=kubectl-846'
Dec 28 08:06:04.580: INFO: stderr: ""
Dec 28 08:06:04.580: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image docker.io/library/busybox:1.29
[AfterEach] Kubectl replace
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1709
Dec 28 08:06:04.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 delete pods e2e-test-httpd-pod --namespace=kubectl-846'
Dec 28 08:06:06.989: INFO: stderr: ""
Dec 28 08:06:06.989: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:06:06.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-846" for this suite.
Dec 28 08:06:13.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:06:13.069: INFO: namespace kubectl-846 deletion completed in 6.076531523s

• [SLOW TEST:13.998 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl replace
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1700
    should update a single-container pod's image  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Probing container 
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:06:13.069: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-4ba692ec-0dd0-4bef-8fab-8ce2fbed6dba in namespace container-probe-5517
Dec 28 08:06:15.092: INFO: Started pod liveness-4ba692ec-0dd0-4bef-8fab-8ce2fbed6dba in namespace container-probe-5517
STEP: checking the pod's current state and verifying that restartCount is present
Dec 28 08:06:15.094: INFO: Initial restart count of pod liveness-4ba692ec-0dd0-4bef-8fab-8ce2fbed6dba is 0
Dec 28 08:06:33.117: INFO: Restart count of pod container-probe-5517/liveness-4ba692ec-0dd0-4bef-8fab-8ce2fbed6dba is now 1 (18.022982781s elapsed)
Dec 28 08:06:53.145: INFO: Restart count of pod container-probe-5517/liveness-4ba692ec-0dd0-4bef-8fab-8ce2fbed6dba is now 2 (38.050948622s elapsed)
Dec 28 08:07:13.168: INFO: Restart count of pod container-probe-5517/liveness-4ba692ec-0dd0-4bef-8fab-8ce2fbed6dba is now 3 (58.074436545s elapsed)
Dec 28 08:07:33.191: INFO: Restart count of pod container-probe-5517/liveness-4ba692ec-0dd0-4bef-8fab-8ce2fbed6dba is now 4 (1m18.097057084s elapsed)
Dec 28 08:07:53.213: INFO: Restart count of pod container-probe-5517/liveness-4ba692ec-0dd0-4bef-8fab-8ce2fbed6dba is now 5 (1m38.119316084s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:07:53.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5517" for this suite.
Dec 28 08:07:59.234: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:07:59.298: INFO: namespace container-probe-5517 deletion completed in 6.076332396s

• [SLOW TEST:106.229 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs 
  should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:07:59.298: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl logs
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1274
STEP: creating an pod
Dec 28 08:07:59.315: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 run logs-generator --generator=run-pod/v1 --image=gcr.io/kubernetes-e2e-test-images/agnhost:2.6 --namespace=kubectl-8100 -- logs-generator --log-lines-total 100 --run-duration 20s'
Dec 28 08:07:59.398: INFO: stderr: ""
Dec 28 08:07:59.398: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Waiting for log generator to start.
Dec 28 08:07:59.398: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Dec 28 08:07:59.398: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-8100" to be "running and ready, or succeeded"
Dec 28 08:07:59.400: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 1.655743ms
Dec 28 08:08:01.403: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.004303402s
Dec 28 08:08:01.403: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Dec 28 08:08:01.403: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings
Dec 28 08:08:01.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 logs logs-generator logs-generator --namespace=kubectl-8100'
Dec 28 08:08:01.493: INFO: stderr: ""
Dec 28 08:08:01.493: INFO: stdout: "I1228 08:08:00.206248       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/7dz7 269\nI1228 08:08:00.406360       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/cjqs 421\nI1228 08:08:00.606427       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/c2n 456\nI1228 08:08:00.806381       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/thhv 232\nI1228 08:08:01.006377       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/qt4 378\nI1228 08:08:01.206380       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/tcs 573\nI1228 08:08:01.406389       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/blbt 410\n"
STEP: limiting log lines
Dec 28 08:08:01.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 logs logs-generator logs-generator --namespace=kubectl-8100 --tail=1'
Dec 28 08:08:01.577: INFO: stderr: ""
Dec 28 08:08:01.577: INFO: stdout: "I1228 08:08:01.406389       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/blbt 410\n"
STEP: limiting log bytes
Dec 28 08:08:01.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 logs logs-generator logs-generator --namespace=kubectl-8100 --limit-bytes=1'
Dec 28 08:08:01.663: INFO: stderr: ""
Dec 28 08:08:01.663: INFO: stdout: "I"
STEP: exposing timestamps
Dec 28 08:08:01.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 logs logs-generator logs-generator --namespace=kubectl-8100 --tail=1 --timestamps'
Dec 28 08:08:01.747: INFO: stderr: ""
Dec 28 08:08:01.747: INFO: stdout: "2019-12-28T08:08:01.606647906Z I1228 08:08:01.606466       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/jvmt 384\n"
STEP: restricting to a time range
Dec 28 08:08:04.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 logs logs-generator logs-generator --namespace=kubectl-8100 --since=1s'
Dec 28 08:08:04.328: INFO: stderr: ""
Dec 28 08:08:04.328: INFO: stdout: "I1228 08:08:03.406396       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/mr4c 446\nI1228 08:08:03.606373       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/nbq 302\nI1228 08:08:03.806385       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/829k 519\nI1228 08:08:04.006378       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/ls2w 529\nI1228 08:08:04.206378       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/v6x 317\n"
Dec 28 08:08:04.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 logs logs-generator logs-generator --namespace=kubectl-8100 --since=24h'
Dec 28 08:08:04.409: INFO: stderr: ""
Dec 28 08:08:04.409: INFO: stdout: "I1228 08:08:00.206248       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/7dz7 269\nI1228 08:08:00.406360       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/cjqs 421\nI1228 08:08:00.606427       1 logs_generator.go:76] 2 POST /api/v1/namespaces/default/pods/c2n 456\nI1228 08:08:00.806381       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/thhv 232\nI1228 08:08:01.006377       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/qt4 378\nI1228 08:08:01.206380       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/kube-system/pods/tcs 573\nI1228 08:08:01.406389       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/blbt 410\nI1228 08:08:01.606466       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/default/pods/jvmt 384\nI1228 08:08:01.806413       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/6g6 419\nI1228 08:08:02.006384       1 logs_generator.go:76] 9 POST /api/v1/namespaces/default/pods/8mll 525\nI1228 08:08:02.206385       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/pbp 588\nI1228 08:08:02.406394       1 logs_generator.go:76] 11 GET /api/v1/namespaces/kube-system/pods/g8z 400\nI1228 08:08:02.606385       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/6tp 428\nI1228 08:08:02.806400       1 logs_generator.go:76] 13 GET /api/v1/namespaces/ns/pods/zzb 506\nI1228 08:08:03.006416       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/8kjm 511\nI1228 08:08:03.206390       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/5jtd 205\nI1228 08:08:03.406396       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/default/pods/mr4c 446\nI1228 08:08:03.606373       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/nbq 302\nI1228 08:08:03.806385       1 logs_generator.go:76] 18 GET /api/v1/namespaces/kube-system/pods/829k 519\nI1228 08:08:04.006378       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/ls2w 529\nI1228 08:08:04.206378       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/v6x 317\n"
[AfterEach] Kubectl logs
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1280
Dec 28 08:08:04.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 delete pod logs-generator --namespace=kubectl-8100'
Dec 28 08:08:10.932: INFO: stderr: ""
Dec 28 08:08:10.932: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:08:10.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8100" for this suite.
Dec 28 08:08:16.943: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:08:17.012: INFO: namespace kubectl-8100 deletion completed in 6.07717219s

• [SLOW TEST:17.714 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl logs
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1270
    should be able to retrieve and filter logs  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[k8s.io] Probing container 
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:08:17.012: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 08:08:35.037: INFO: Container started at 2019-12-28 08:08:17 +0000 UTC, pod became ready at 2019-12-28 08:08:33 +0000 UTC
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:08:35.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8319" for this suite.
Dec 28 08:08:47.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:08:47.114: INFO: namespace container-probe-8319 deletion completed in 12.074799693s

• [SLOW TEST:30.102 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:08:47.114: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-8bda8df3-1060-4c04-a358-be88ef1d1d9b
STEP: Creating a pod to test consume secrets
Dec 28 08:08:47.136: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-62613963-4dd5-44fe-b28e-588ff79ebc82" in namespace "projected-8982" to be "success or failure"
Dec 28 08:08:47.138: INFO: Pod "pod-projected-secrets-62613963-4dd5-44fe-b28e-588ff79ebc82": Phase="Pending", Reason="", readiness=false. Elapsed: 1.393987ms
Dec 28 08:08:49.140: INFO: Pod "pod-projected-secrets-62613963-4dd5-44fe-b28e-588ff79ebc82": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003711027s
STEP: Saw pod success
Dec 28 08:08:49.140: INFO: Pod "pod-projected-secrets-62613963-4dd5-44fe-b28e-588ff79ebc82" satisfied condition "success or failure"
Dec 28 08:08:49.142: INFO: Trying to get logs from node hxx-m-2 pod pod-projected-secrets-62613963-4dd5-44fe-b28e-588ff79ebc82 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 28 08:08:49.153: INFO: Waiting for pod pod-projected-secrets-62613963-4dd5-44fe-b28e-588ff79ebc82 to disappear
Dec 28 08:08:49.155: INFO: Pod pod-projected-secrets-62613963-4dd5-44fe-b28e-588ff79ebc82 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:08:49.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8982" for this suite.
Dec 28 08:08:55.164: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:08:55.297: INFO: namespace projected-8982 deletion completed in 6.139631337s

• [SLOW TEST:8.182 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:08:55.297: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-bb69628b-a627-4338-a65a-ca180de4268a
STEP: Creating a pod to test consume secrets
Dec 28 08:08:55.318: INFO: Waiting up to 5m0s for pod "pod-secrets-18bee31a-a9c2-4c12-8fcc-00f1c27a7e30" in namespace "secrets-4310" to be "success or failure"
Dec 28 08:08:55.319: INFO: Pod "pod-secrets-18bee31a-a9c2-4c12-8fcc-00f1c27a7e30": Phase="Pending", Reason="", readiness=false. Elapsed: 1.343378ms
Dec 28 08:08:57.322: INFO: Pod "pod-secrets-18bee31a-a9c2-4c12-8fcc-00f1c27a7e30": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003728315s
STEP: Saw pod success
Dec 28 08:08:57.322: INFO: Pod "pod-secrets-18bee31a-a9c2-4c12-8fcc-00f1c27a7e30" satisfied condition "success or failure"
Dec 28 08:08:57.323: INFO: Trying to get logs from node hxx-m-2 pod pod-secrets-18bee31a-a9c2-4c12-8fcc-00f1c27a7e30 container secret-volume-test: <nil>
STEP: delete the pod
Dec 28 08:08:57.334: INFO: Waiting for pod pod-secrets-18bee31a-a9c2-4c12-8fcc-00f1c27a7e30 to disappear
Dec 28 08:08:57.335: INFO: Pod pod-secrets-18bee31a-a9c2-4c12-8fcc-00f1c27a7e30 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:08:57.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4310" for this suite.
Dec 28 08:09:03.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:09:03.409: INFO: namespace secrets-4310 deletion completed in 6.071022976s

• [SLOW TEST:8.112 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:09:03.409: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service nodeport-service with the type=NodePort in namespace services-2209
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-2209
STEP: creating replication controller externalsvc in namespace services-2209
I1228 08:09:03.441099      22 runners.go:184] Created replication controller with name: externalsvc, namespace: services-2209, replica count: 2
I1228 08:09:06.491480      22 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName
Dec 28 08:09:06.504: INFO: Creating new exec pod
Dec 28 08:09:08.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=services-2209 execpodx2fpz -- /bin/sh -x -c nslookup nodeport-service'
Dec 28 08:09:08.737: INFO: stderr: "+ nslookup nodeport-service\n"
Dec 28 08:09:08.737: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-2209.svc.cluster.local\tcanonical name = externalsvc.services-2209.svc.cluster.local.\nName:\texternalsvc.services-2209.svc.cluster.local\nAddress: 10.102.76.45\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-2209, will wait for the garbage collector to delete the pods
Dec 28 08:09:08.793: INFO: Deleting ReplicationController externalsvc took: 3.90608ms
Dec 28 08:09:09.694: INFO: Terminating ReplicationController externalsvc pods took: 900.251612ms
Dec 28 08:09:13.304: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:09:13.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2209" for this suite.
Dec 28 08:09:19.320: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:09:19.384: INFO: namespace services-2209 deletion completed in 6.071201695s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:15.976 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:09:19.385: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-2d1398c4-06ba-401f-b62e-68153325ca4a
STEP: Creating a pod to test consume secrets
Dec 28 08:09:19.406: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-29229fe2-8f43-4984-b4d8-b7cfc7f3a315" in namespace "projected-3310" to be "success or failure"
Dec 28 08:09:19.407: INFO: Pod "pod-projected-secrets-29229fe2-8f43-4984-b4d8-b7cfc7f3a315": Phase="Pending", Reason="", readiness=false. Elapsed: 1.357942ms
Dec 28 08:09:21.410: INFO: Pod "pod-projected-secrets-29229fe2-8f43-4984-b4d8-b7cfc7f3a315": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00374776s
STEP: Saw pod success
Dec 28 08:09:21.410: INFO: Pod "pod-projected-secrets-29229fe2-8f43-4984-b4d8-b7cfc7f3a315" satisfied condition "success or failure"
Dec 28 08:09:21.411: INFO: Trying to get logs from node hxx-m-2 pod pod-projected-secrets-29229fe2-8f43-4984-b4d8-b7cfc7f3a315 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 28 08:09:21.421: INFO: Waiting for pod pod-projected-secrets-29229fe2-8f43-4984-b4d8-b7cfc7f3a315 to disappear
Dec 28 08:09:21.423: INFO: Pod pod-projected-secrets-29229fe2-8f43-4984-b4d8-b7cfc7f3a315 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:09:21.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3310" for this suite.
Dec 28 08:09:27.433: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:09:27.496: INFO: namespace projected-3310 deletion completed in 6.07088353s

• [SLOW TEST:8.112 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch 
  watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:09:27.497: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename crd-watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] watch on custom resource definition objects [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 08:09:27.513: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Creating first CR 
Dec 28 08:09:28.032: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-28T08:09:28Z generation:1 name:name1 resourceVersion:274552 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:ebe1b15b-215d-4fef-9804-b4098fdf061f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR
Dec 28 08:09:38.035: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-28T08:09:38Z generation:1 name:name2 resourceVersion:274592 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:6eba127f-868b-4cf2-a0f4-e2f1752756f1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR
Dec 28 08:09:48.038: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-28T08:09:28Z generation:2 name:name1 resourceVersion:274631 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:ebe1b15b-215d-4fef-9804-b4098fdf061f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR
Dec 28 08:09:58.042: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-28T08:09:38Z generation:2 name:name2 resourceVersion:274674 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:6eba127f-868b-4cf2-a0f4-e2f1752756f1] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR
Dec 28 08:10:08.046: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-28T08:09:28Z generation:2 name:name1 resourceVersion:274715 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name1 uid:ebe1b15b-215d-4fef-9804-b4098fdf061f] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR
Dec 28 08:10:18.051: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2019-12-28T08:09:38Z generation:2 name:name2 resourceVersion:274752 selfLink:/apis/mygroup.example.com/v1beta1/noxus/name2 uid:6eba127f-868b-4cf2-a0f4-e2f1752756f1] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:10:28.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-5749" for this suite.
Dec 28 08:10:34.568: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:10:34.635: INFO: namespace crd-watch-5749 deletion completed in 6.074881805s

• [SLOW TEST:67.139 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_watch.go:42
    watch on custom resource definition objects [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:10:34.635: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test override arguments
Dec 28 08:10:34.655: INFO: Waiting up to 5m0s for pod "client-containers-7ef417e1-451a-4cb2-9fdd-758b4ade2922" in namespace "containers-1057" to be "success or failure"
Dec 28 08:10:34.657: INFO: Pod "client-containers-7ef417e1-451a-4cb2-9fdd-758b4ade2922": Phase="Pending", Reason="", readiness=false. Elapsed: 1.904444ms
Dec 28 08:10:36.659: INFO: Pod "client-containers-7ef417e1-451a-4cb2-9fdd-758b4ade2922": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004304427s
STEP: Saw pod success
Dec 28 08:10:36.659: INFO: Pod "client-containers-7ef417e1-451a-4cb2-9fdd-758b4ade2922" satisfied condition "success or failure"
Dec 28 08:10:36.661: INFO: Trying to get logs from node hxx-m-2 pod client-containers-7ef417e1-451a-4cb2-9fdd-758b4ade2922 container test-container: <nil>
STEP: delete the pod
Dec 28 08:10:36.672: INFO: Waiting for pod client-containers-7ef417e1-451a-4cb2-9fdd-758b4ade2922 to disappear
Dec 28 08:10:36.674: INFO: Pod client-containers-7ef417e1-451a-4cb2-9fdd-758b4ade2922 no longer exists
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:10:36.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1057" for this suite.
Dec 28 08:10:42.683: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:10:42.752: INFO: namespace containers-1057 deletion completed in 6.075299659s

• [SLOW TEST:8.116 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be able to override the image's default arguments (docker cmd) [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:10:42.752: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 28 08:10:42.771: INFO: Waiting up to 5m0s for pod "pod-3719456d-4c78-40d3-9e5d-b429101091f4" in namespace "emptydir-7911" to be "success or failure"
Dec 28 08:10:42.773: INFO: Pod "pod-3719456d-4c78-40d3-9e5d-b429101091f4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.54713ms
Dec 28 08:10:44.775: INFO: Pod "pod-3719456d-4c78-40d3-9e5d-b429101091f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003972762s
STEP: Saw pod success
Dec 28 08:10:44.775: INFO: Pod "pod-3719456d-4c78-40d3-9e5d-b429101091f4" satisfied condition "success or failure"
Dec 28 08:10:44.777: INFO: Trying to get logs from node hxx-m-2 pod pod-3719456d-4c78-40d3-9e5d-b429101091f4 container test-container: <nil>
STEP: delete the pod
Dec 28 08:10:44.786: INFO: Waiting for pod pod-3719456d-4c78-40d3-9e5d-b429101091f4 to disappear
Dec 28 08:10:44.788: INFO: Pod pod-3719456d-4c78-40d3-9e5d-b429101091f4 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:10:44.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7911" for this suite.
Dec 28 08:10:50.797: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:10:50.863: INFO: namespace emptydir-7911 deletion completed in 6.072871045s

• [SLOW TEST:8.111 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:10:50.863: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-be90d9f0-432a-4668-bfdc-e7a6517be69a
STEP: Creating a pod to test consume configMaps
Dec 28 08:10:50.885: INFO: Waiting up to 5m0s for pod "pod-configmaps-8f66b282-b723-49e0-a5f0-71ef1f4413bc" in namespace "configmap-7286" to be "success or failure"
Dec 28 08:10:50.886: INFO: Pod "pod-configmaps-8f66b282-b723-49e0-a5f0-71ef1f4413bc": Phase="Pending", Reason="", readiness=false. Elapsed: 1.371951ms
Dec 28 08:10:52.889: INFO: Pod "pod-configmaps-8f66b282-b723-49e0-a5f0-71ef1f4413bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003637227s
STEP: Saw pod success
Dec 28 08:10:52.889: INFO: Pod "pod-configmaps-8f66b282-b723-49e0-a5f0-71ef1f4413bc" satisfied condition "success or failure"
Dec 28 08:10:52.890: INFO: Trying to get logs from node hxx-m-2 pod pod-configmaps-8f66b282-b723-49e0-a5f0-71ef1f4413bc container configmap-volume-test: <nil>
STEP: delete the pod
Dec 28 08:10:52.901: INFO: Waiting for pod pod-configmaps-8f66b282-b723-49e0-a5f0-71ef1f4413bc to disappear
Dec 28 08:10:52.902: INFO: Pod pod-configmaps-8f66b282-b723-49e0-a5f0-71ef1f4413bc no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:10:52.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7286" for this suite.
Dec 28 08:10:58.912: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:10:58.985: INFO: namespace configmap-7286 deletion completed in 6.080194725s

• [SLOW TEST:8.122 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:10:58.985: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-9zcm
STEP: Creating a pod to test atomic-volume-subpath
Dec 28 08:10:59.010: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-9zcm" in namespace "subpath-4047" to be "success or failure"
Dec 28 08:10:59.011: INFO: Pod "pod-subpath-test-configmap-9zcm": Phase="Pending", Reason="", readiness=false. Elapsed: 1.389473ms
Dec 28 08:11:01.014: INFO: Pod "pod-subpath-test-configmap-9zcm": Phase="Running", Reason="", readiness=true. Elapsed: 2.003970268s
Dec 28 08:11:03.016: INFO: Pod "pod-subpath-test-configmap-9zcm": Phase="Running", Reason="", readiness=true. Elapsed: 4.006000557s
Dec 28 08:11:05.018: INFO: Pod "pod-subpath-test-configmap-9zcm": Phase="Running", Reason="", readiness=true. Elapsed: 6.008233109s
Dec 28 08:11:07.021: INFO: Pod "pod-subpath-test-configmap-9zcm": Phase="Running", Reason="", readiness=true. Elapsed: 8.01073367s
Dec 28 08:11:09.023: INFO: Pod "pod-subpath-test-configmap-9zcm": Phase="Running", Reason="", readiness=true. Elapsed: 10.013097529s
Dec 28 08:11:11.025: INFO: Pod "pod-subpath-test-configmap-9zcm": Phase="Running", Reason="", readiness=true. Elapsed: 12.015594005s
Dec 28 08:11:13.028: INFO: Pod "pod-subpath-test-configmap-9zcm": Phase="Running", Reason="", readiness=true. Elapsed: 14.017834476s
Dec 28 08:11:15.030: INFO: Pod "pod-subpath-test-configmap-9zcm": Phase="Running", Reason="", readiness=true. Elapsed: 16.020214615s
Dec 28 08:11:17.032: INFO: Pod "pod-subpath-test-configmap-9zcm": Phase="Running", Reason="", readiness=true. Elapsed: 18.022653009s
Dec 28 08:11:19.035: INFO: Pod "pod-subpath-test-configmap-9zcm": Phase="Running", Reason="", readiness=true. Elapsed: 20.024870862s
Dec 28 08:11:21.037: INFO: Pod "pod-subpath-test-configmap-9zcm": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.02742838s
STEP: Saw pod success
Dec 28 08:11:21.037: INFO: Pod "pod-subpath-test-configmap-9zcm" satisfied condition "success or failure"
Dec 28 08:11:21.039: INFO: Trying to get logs from node hxx-m-2 pod pod-subpath-test-configmap-9zcm container test-container-subpath-configmap-9zcm: <nil>
STEP: delete the pod
Dec 28 08:11:21.049: INFO: Waiting for pod pod-subpath-test-configmap-9zcm to disappear
Dec 28 08:11:21.051: INFO: Pod pod-subpath-test-configmap-9zcm no longer exists
STEP: Deleting pod pod-subpath-test-configmap-9zcm
Dec 28 08:11:21.051: INFO: Deleting pod "pod-subpath-test-configmap-9zcm" in namespace "subpath-4047"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:11:21.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-4047" for this suite.
Dec 28 08:11:27.061: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:11:27.134: INFO: namespace subpath-4047 deletion completed in 6.079217289s

• [SLOW TEST:28.148 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:11:27.134: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with downward pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-downwardapi-mvdx
STEP: Creating a pod to test atomic-volume-subpath
Dec 28 08:11:27.160: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-mvdx" in namespace "subpath-8641" to be "success or failure"
Dec 28 08:11:27.162: INFO: Pod "pod-subpath-test-downwardapi-mvdx": Phase="Pending", Reason="", readiness=false. Elapsed: 1.270085ms
Dec 28 08:11:29.164: INFO: Pod "pod-subpath-test-downwardapi-mvdx": Phase="Running", Reason="", readiness=true. Elapsed: 2.003665222s
Dec 28 08:11:31.166: INFO: Pod "pod-subpath-test-downwardapi-mvdx": Phase="Running", Reason="", readiness=true. Elapsed: 4.005902395s
Dec 28 08:11:33.168: INFO: Pod "pod-subpath-test-downwardapi-mvdx": Phase="Running", Reason="", readiness=true. Elapsed: 6.00801862s
Dec 28 08:11:35.171: INFO: Pod "pod-subpath-test-downwardapi-mvdx": Phase="Running", Reason="", readiness=true. Elapsed: 8.010361613s
Dec 28 08:11:37.173: INFO: Pod "pod-subpath-test-downwardapi-mvdx": Phase="Running", Reason="", readiness=true. Elapsed: 10.012584024s
Dec 28 08:11:39.175: INFO: Pod "pod-subpath-test-downwardapi-mvdx": Phase="Running", Reason="", readiness=true. Elapsed: 12.014879889s
Dec 28 08:11:41.177: INFO: Pod "pod-subpath-test-downwardapi-mvdx": Phase="Running", Reason="", readiness=true. Elapsed: 14.017104219s
Dec 28 08:11:43.180: INFO: Pod "pod-subpath-test-downwardapi-mvdx": Phase="Running", Reason="", readiness=true. Elapsed: 16.019382537s
Dec 28 08:11:45.182: INFO: Pod "pod-subpath-test-downwardapi-mvdx": Phase="Running", Reason="", readiness=true. Elapsed: 18.021702442s
Dec 28 08:11:47.184: INFO: Pod "pod-subpath-test-downwardapi-mvdx": Phase="Running", Reason="", readiness=true. Elapsed: 20.02389455s
Dec 28 08:11:49.187: INFO: Pod "pod-subpath-test-downwardapi-mvdx": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.026299694s
STEP: Saw pod success
Dec 28 08:11:49.187: INFO: Pod "pod-subpath-test-downwardapi-mvdx" satisfied condition "success or failure"
Dec 28 08:11:49.188: INFO: Trying to get logs from node hxx-m-2 pod pod-subpath-test-downwardapi-mvdx container test-container-subpath-downwardapi-mvdx: <nil>
STEP: delete the pod
Dec 28 08:11:49.198: INFO: Waiting for pod pod-subpath-test-downwardapi-mvdx to disappear
Dec 28 08:11:49.199: INFO: Pod pod-subpath-test-downwardapi-mvdx no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-mvdx
Dec 28 08:11:49.199: INFO: Deleting pod "pod-subpath-test-downwardapi-mvdx" in namespace "subpath-8641"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:11:49.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8641" for this suite.
Dec 28 08:11:55.209: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:11:55.273: INFO: namespace subpath-8641 deletion completed in 6.070688122s

• [SLOW TEST:28.140 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with downward pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version 
  should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:11:55.273: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check is all data is printed  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 08:11:55.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 version'
Dec 28 08:11:55.355: INFO: stderr: ""
Dec 28 08:11:55.355: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.3\", GitCommit:\"b3cbbae08ec52a7fc73d334838e18d17e8512749\", GitTreeState:\"clean\", BuildDate:\"2019-11-13T11:23:11Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nServer Version: version.Info{Major:\"1\", Minor:\"16\", GitVersion:\"v1.16.3\", GitCommit:\"b3cbbae08ec52a7fc73d334838e18d17e8512749\", GitTreeState:\"clean\", BuildDate:\"2019-11-13T11:13:49Z\", GoVersion:\"go1.12.12\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:11:55.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-978" for this suite.
Dec 28 08:12:01.366: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:12:01.437: INFO: namespace kubectl-978 deletion completed in 6.07874218s

• [SLOW TEST:6.163 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl version
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1380
    should check is all data is printed  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:12:01.437: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 28 08:12:03.970: INFO: Successfully updated pod "pod-update-d92003c4-fe6f-4894-b2e7-47aacbe531e5"
STEP: verifying the updated pod is in kubernetes
Dec 28 08:12:03.973: INFO: Pod update OK
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:12:03.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3347" for this suite.
Dec 28 08:12:31.983: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:12:32.047: INFO: namespace pods-3347 deletion completed in 28.070929003s

• [SLOW TEST:30.610 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:12:32.048: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1304.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-1304.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 28 08:12:34.088: INFO: DNS probes using dns-1304/dns-test-bdc05c38-ed0d-4e1a-9d66-ef7c37a8779e succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:12:34.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1304" for this suite.
Dec 28 08:12:40.104: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:12:40.171: INFO: namespace dns-1304 deletion completed in 6.074988241s

• [SLOW TEST:8.123 seconds]
[sig-network] DNS
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for the cluster  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:12:40.171: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4033.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-4033.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4033.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-4033.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-4033.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-4033.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 28 08:12:42.217: INFO: DNS probes using dns-4033/dns-test-b85b9adc-5538-41db-b1f6-eb43dfab7859 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:12:42.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4033" for this suite.
Dec 28 08:12:48.241: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:12:48.309: INFO: namespace dns-4033 deletion completed in 6.076844366s

• [SLOW TEST:8.138 seconds]
[sig-network] DNS
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Hostname [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:12:48.310: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test headless service
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2908.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2908.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2908.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2908.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2908.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2908.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2908.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2908.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2908.svc.cluster.local;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-2908.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 28 08:12:50.339: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:12:50.341: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:12:50.343: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:12:50.345: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:12:50.350: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:12:50.352: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:12:50.353: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:12:50.355: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:12:50.358: INFO: Lookups using dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2908.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2908.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local jessie_udp@dns-test-service-2.dns-2908.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2908.svc.cluster.local]

Dec 28 08:12:55.361: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:12:55.363: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:12:55.365: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:12:55.367: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:12:55.372: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:12:55.374: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:12:55.376: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:12:55.378: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:12:55.381: INFO: Lookups using dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2908.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2908.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local jessie_udp@dns-test-service-2.dns-2908.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2908.svc.cluster.local]

Dec 28 08:13:00.361: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:00.363: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:00.365: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:00.366: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:00.371: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:00.373: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:00.375: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:00.376: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:00.379: INFO: Lookups using dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2908.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2908.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local jessie_udp@dns-test-service-2.dns-2908.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2908.svc.cluster.local]

Dec 28 08:13:05.361: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:05.363: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:05.365: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:05.366: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:05.372: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:05.374: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:05.376: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:05.378: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:05.382: INFO: Lookups using dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2908.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2908.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local jessie_udp@dns-test-service-2.dns-2908.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2908.svc.cluster.local]

Dec 28 08:13:10.361: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:10.363: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:10.365: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:10.367: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:10.372: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:10.374: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:10.375: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:10.377: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:10.380: INFO: Lookups using dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2908.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2908.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local jessie_udp@dns-test-service-2.dns-2908.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2908.svc.cluster.local]

Dec 28 08:13:15.361: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:15.363: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:15.365: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:15.366: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:15.371: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:15.373: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:15.375: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:15.377: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2908.svc.cluster.local from pod dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647: the server could not find the requested resource (get pods dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647)
Dec 28 08:13:15.380: INFO: Lookups using dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2908.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2908.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2908.svc.cluster.local jessie_udp@dns-test-service-2.dns-2908.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2908.svc.cluster.local]

Dec 28 08:13:20.382: INFO: DNS probes using dns-2908/dns-test-a1a082e3-0d5c-4eb0-801d-21e5b2c71647 succeeded

STEP: deleting the pod
STEP: deleting the test headless service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:13:20.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-2908" for this suite.
Dec 28 08:13:26.412: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:13:26.480: INFO: namespace dns-2908 deletion completed in 6.077324893s

• [SLOW TEST:38.171 seconds]
[sig-network] DNS
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:13:26.480: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 28 08:13:26.501: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5df65e6d-d282-4a2c-b54a-78455191be7d" in namespace "projected-4059" to be "success or failure"
Dec 28 08:13:26.503: INFO: Pod "downwardapi-volume-5df65e6d-d282-4a2c-b54a-78455191be7d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.886353ms
Dec 28 08:13:28.505: INFO: Pod "downwardapi-volume-5df65e6d-d282-4a2c-b54a-78455191be7d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004148943s
STEP: Saw pod success
Dec 28 08:13:28.506: INFO: Pod "downwardapi-volume-5df65e6d-d282-4a2c-b54a-78455191be7d" satisfied condition "success or failure"
Dec 28 08:13:28.507: INFO: Trying to get logs from node hxx-m-2 pod downwardapi-volume-5df65e6d-d282-4a2c-b54a-78455191be7d container client-container: <nil>
STEP: delete the pod
Dec 28 08:13:28.522: INFO: Waiting for pod downwardapi-volume-5df65e6d-d282-4a2c-b54a-78455191be7d to disappear
Dec 28 08:13:28.524: INFO: Pod downwardapi-volume-5df65e6d-d282-4a2c-b54a-78455191be7d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:13:28.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4059" for this suite.
Dec 28 08:13:34.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:13:34.607: INFO: namespace projected-4059 deletion completed in 6.080453019s

• [SLOW TEST:8.127 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:13:34.608: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-7330
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 28 08:13:34.625: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 28 08:13:56.663: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.199.0.18:8080/dial?request=hostName&protocol=udp&host=10.199.1.159&port=8081&tries=1'] Namespace:pod-network-test-7330 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 28 08:13:56.664: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 08:13:56.811: INFO: Waiting for endpoints: map[]
Dec 28 08:13:56.813: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.199.0.18:8080/dial?request=hostName&protocol=udp&host=10.199.2.252&port=8081&tries=1'] Namespace:pod-network-test-7330 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 28 08:13:56.813: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 08:13:56.987: INFO: Waiting for endpoints: map[]
Dec 28 08:13:56.989: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.199.0.18:8080/dial?request=hostName&protocol=udp&host=10.199.0.17&port=8081&tries=1'] Namespace:pod-network-test-7330 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 28 08:13:56.989: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 08:13:57.160: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:13:57.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7330" for this suite.
Dec 28 08:14:09.174: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:14:09.242: INFO: namespace pod-network-test-7330 deletion completed in 12.075998731s

• [SLOW TEST:34.634 seconds]
[sig-network] Networking
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-network] Proxy version v1 
  should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:14:09.242: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy through a service and a pod  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting an echo server on multiple ports
STEP: creating replication controller proxy-service-dxkq7 in namespace proxy-3830
I1228 08:14:09.267108      22 runners.go:184] Created replication controller with name: proxy-service-dxkq7, namespace: proxy-3830, replica count: 1
I1228 08:14:10.317459      22 runners.go:184] proxy-service-dxkq7 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1228 08:14:11.317654      22 runners.go:184] proxy-service-dxkq7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1228 08:14:12.317820      22 runners.go:184] proxy-service-dxkq7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1228 08:14:13.317996      22 runners.go:184] proxy-service-dxkq7 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I1228 08:14:14.318186      22 runners.go:184] proxy-service-dxkq7 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 28 08:14:14.320: INFO: setup took 5.061753281s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts
Dec 28 08:14:14.325: INFO: (0) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 4.918154ms)
Dec 28 08:14:14.325: INFO: (0) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 5.04787ms)
Dec 28 08:14:14.325: INFO: (0) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">... (200; 5.247849ms)
Dec 28 08:14:14.325: INFO: (0) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 5.198221ms)
Dec 28 08:14:14.326: INFO: (0) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">test<... (200; 5.937406ms)
Dec 28 08:14:14.327: INFO: (0) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname1/proxy/: foo (200; 7.496555ms)
Dec 28 08:14:14.327: INFO: (0) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname2/proxy/: bar (200; 7.330071ms)
Dec 28 08:14:14.327: INFO: (0) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname1/proxy/: foo (200; 7.429793ms)
Dec 28 08:14:14.327: INFO: (0) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname2/proxy/: bar (200; 7.534441ms)
Dec 28 08:14:14.330: INFO: (0) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/rewriteme">test</a> (200; 10.043165ms)
Dec 28 08:14:14.330: INFO: (0) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 10.156072ms)
Dec 28 08:14:14.331: INFO: (0) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:462/proxy/: tls qux (200; 10.770203ms)
Dec 28 08:14:14.331: INFO: (0) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/tlsrewritem... (200; 10.835791ms)
Dec 28 08:14:14.331: INFO: (0) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname1/proxy/: tls baz (200; 11.130514ms)
Dec 28 08:14:14.331: INFO: (0) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:460/proxy/: tls baz (200; 11.197819ms)
Dec 28 08:14:14.332: INFO: (0) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname2/proxy/: tls qux (200; 11.862366ms)
Dec 28 08:14:14.335: INFO: (1) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 2.76052ms)
Dec 28 08:14:14.336: INFO: (1) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 3.06453ms)
Dec 28 08:14:14.336: INFO: (1) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname1/proxy/: foo (200; 3.25306ms)
Dec 28 08:14:14.336: INFO: (1) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname2/proxy/: tls qux (200; 3.428187ms)
Dec 28 08:14:14.336: INFO: (1) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname2/proxy/: bar (200; 4.217572ms)
Dec 28 08:14:14.336: INFO: (1) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname1/proxy/: foo (200; 3.632929ms)
Dec 28 08:14:14.337: INFO: (1) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">... (200; 4.470418ms)
Dec 28 08:14:14.337: INFO: (1) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 4.218716ms)
Dec 28 08:14:14.337: INFO: (1) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname1/proxy/: tls baz (200; 4.418099ms)
Dec 28 08:14:14.337: INFO: (1) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/tlsrewritem... (200; 4.118528ms)
Dec 28 08:14:14.337: INFO: (1) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:460/proxy/: tls baz (200; 4.390614ms)
Dec 28 08:14:14.337: INFO: (1) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 4.758605ms)
Dec 28 08:14:14.337: INFO: (1) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname2/proxy/: bar (200; 4.915007ms)
Dec 28 08:14:14.337: INFO: (1) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">test<... (200; 4.369943ms)
Dec 28 08:14:14.337: INFO: (1) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:462/proxy/: tls qux (200; 4.443404ms)
Dec 28 08:14:14.337: INFO: (1) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/rewriteme">test</a> (200; 4.724546ms)
Dec 28 08:14:14.340: INFO: (2) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 2.920847ms)
Dec 28 08:14:14.341: INFO: (2) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:460/proxy/: tls baz (200; 3.634383ms)
Dec 28 08:14:14.342: INFO: (2) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/rewriteme">test</a> (200; 5.021663ms)
Dec 28 08:14:14.342: INFO: (2) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 4.973297ms)
Dec 28 08:14:14.349: INFO: (2) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname1/proxy/: tls baz (200; 11.933868ms)
Dec 28 08:14:14.349: INFO: (2) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/tlsrewritem... (200; 11.436242ms)
Dec 28 08:14:14.350: INFO: (2) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname2/proxy/: bar (200; 11.374539ms)
Dec 28 08:14:14.350: INFO: (2) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname2/proxy/: tls qux (200; 12.053659ms)
Dec 28 08:14:14.350: INFO: (2) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:462/proxy/: tls qux (200; 11.789864ms)
Dec 28 08:14:14.350: INFO: (2) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname1/proxy/: foo (200; 11.725108ms)
Dec 28 08:14:14.350: INFO: (2) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 11.529539ms)
Dec 28 08:14:14.350: INFO: (2) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">test<... (200; 12.096826ms)
Dec 28 08:14:14.350: INFO: (2) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname1/proxy/: foo (200; 12.212538ms)
Dec 28 08:14:14.350: INFO: (2) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname2/proxy/: bar (200; 11.947051ms)
Dec 28 08:14:14.350: INFO: (2) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">... (200; 11.946833ms)
Dec 28 08:14:14.351: INFO: (2) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 13.924375ms)
Dec 28 08:14:14.357: INFO: (3) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:462/proxy/: tls qux (200; 4.716571ms)
Dec 28 08:14:14.357: INFO: (3) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 5.168261ms)
Dec 28 08:14:14.357: INFO: (3) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname2/proxy/: bar (200; 5.287263ms)
Dec 28 08:14:14.357: INFO: (3) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/rewriteme">test</a> (200; 4.92612ms)
Dec 28 08:14:14.357: INFO: (3) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">test<... (200; 4.521064ms)
Dec 28 08:14:14.357: INFO: (3) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:460/proxy/: tls baz (200; 5.122325ms)
Dec 28 08:14:14.357: INFO: (3) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/tlsrewritem... (200; 4.903787ms)
Dec 28 08:14:14.357: INFO: (3) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">... (200; 4.844618ms)
Dec 28 08:14:14.357: INFO: (3) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 4.939189ms)
Dec 28 08:14:14.357: INFO: (3) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 5.127627ms)
Dec 28 08:14:14.357: INFO: (3) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 4.883078ms)
Dec 28 08:14:14.358: INFO: (3) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname2/proxy/: bar (200; 5.287903ms)
Dec 28 08:14:14.358: INFO: (3) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname1/proxy/: foo (200; 5.655883ms)
Dec 28 08:14:14.358: INFO: (3) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname1/proxy/: tls baz (200; 6.425179ms)
Dec 28 08:14:14.358: INFO: (3) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname1/proxy/: foo (200; 5.73638ms)
Dec 28 08:14:14.358: INFO: (3) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname2/proxy/: tls qux (200; 6.172724ms)
Dec 28 08:14:14.361: INFO: (4) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 2.376283ms)
Dec 28 08:14:14.361: INFO: (4) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">test<... (200; 2.9398ms)
Dec 28 08:14:14.363: INFO: (4) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/rewriteme">test</a> (200; 3.605506ms)
Dec 28 08:14:14.363: INFO: (4) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:460/proxy/: tls baz (200; 3.440588ms)
Dec 28 08:14:14.363: INFO: (4) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">... (200; 3.971695ms)
Dec 28 08:14:14.363: INFO: (4) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:462/proxy/: tls qux (200; 3.168563ms)
Dec 28 08:14:14.364: INFO: (4) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 5.480523ms)
Dec 28 08:14:14.365: INFO: (4) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 5.340831ms)
Dec 28 08:14:14.365: INFO: (4) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname1/proxy/: foo (200; 5.942077ms)
Dec 28 08:14:14.365: INFO: (4) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname2/proxy/: bar (200; 5.596744ms)
Dec 28 08:14:14.365: INFO: (4) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname2/proxy/: tls qux (200; 6.413606ms)
Dec 28 08:14:14.365: INFO: (4) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 5.180136ms)
Dec 28 08:14:14.365: INFO: (4) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/tlsrewritem... (200; 5.112485ms)
Dec 28 08:14:14.365: INFO: (4) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname2/proxy/: bar (200; 5.948992ms)
Dec 28 08:14:14.365: INFO: (4) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname1/proxy/: tls baz (200; 5.28476ms)
Dec 28 08:14:14.365: INFO: (4) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname1/proxy/: foo (200; 6.344324ms)
Dec 28 08:14:14.368: INFO: (5) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:460/proxy/: tls baz (200; 3.285312ms)
Dec 28 08:14:14.368: INFO: (5) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">... (200; 3.444278ms)
Dec 28 08:14:14.368: INFO: (5) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/rewriteme">test</a> (200; 3.176068ms)
Dec 28 08:14:14.368: INFO: (5) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:462/proxy/: tls qux (200; 3.063365ms)
Dec 28 08:14:14.368: INFO: (5) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 3.13597ms)
Dec 28 08:14:14.369: INFO: (5) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/tlsrewritem... (200; 3.197647ms)
Dec 28 08:14:14.370: INFO: (5) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 4.520451ms)
Dec 28 08:14:14.370: INFO: (5) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 4.482725ms)
Dec 28 08:14:14.370: INFO: (5) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 4.744553ms)
Dec 28 08:14:14.370: INFO: (5) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">test<... (200; 4.667771ms)
Dec 28 08:14:14.370: INFO: (5) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname1/proxy/: tls baz (200; 5.139826ms)
Dec 28 08:14:14.371: INFO: (5) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname1/proxy/: foo (200; 5.82034ms)
Dec 28 08:14:14.371: INFO: (5) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname2/proxy/: bar (200; 5.723597ms)
Dec 28 08:14:14.371: INFO: (5) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname2/proxy/: tls qux (200; 6.032673ms)
Dec 28 08:14:14.371: INFO: (5) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname1/proxy/: foo (200; 5.76535ms)
Dec 28 08:14:14.371: INFO: (5) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname2/proxy/: bar (200; 5.761061ms)
Dec 28 08:14:14.374: INFO: (6) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 2.540639ms)
Dec 28 08:14:14.374: INFO: (6) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/tlsrewritem... (200; 2.982072ms)
Dec 28 08:14:14.375: INFO: (6) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 3.020986ms)
Dec 28 08:14:14.378: INFO: (6) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 6.446638ms)
Dec 28 08:14:14.378: INFO: (6) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">test<... (200; 6.906513ms)
Dec 28 08:14:14.378: INFO: (6) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 6.777981ms)
Dec 28 08:14:14.378: INFO: (6) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">... (200; 6.796817ms)
Dec 28 08:14:14.378: INFO: (6) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/rewriteme">test</a> (200; 6.75616ms)
Dec 28 08:14:14.379: INFO: (6) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname1/proxy/: foo (200; 7.614017ms)
Dec 28 08:14:14.379: INFO: (6) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:462/proxy/: tls qux (200; 7.67945ms)
Dec 28 08:14:14.379: INFO: (6) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:460/proxy/: tls baz (200; 7.53248ms)
Dec 28 08:14:14.379: INFO: (6) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname1/proxy/: tls baz (200; 7.59593ms)
Dec 28 08:14:14.379: INFO: (6) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname2/proxy/: bar (200; 7.793919ms)
Dec 28 08:14:14.380: INFO: (6) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname2/proxy/: bar (200; 8.015646ms)
Dec 28 08:14:14.380: INFO: (6) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname2/proxy/: tls qux (200; 7.965684ms)
Dec 28 08:14:14.380: INFO: (6) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname1/proxy/: foo (200; 8.099664ms)
Dec 28 08:14:14.382: INFO: (7) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/rewriteme">test</a> (200; 2.507057ms)
Dec 28 08:14:14.382: INFO: (7) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/tlsrewritem... (200; 2.445715ms)
Dec 28 08:14:14.383: INFO: (7) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 2.610539ms)
Dec 28 08:14:14.383: INFO: (7) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">test<... (200; 2.624214ms)
Dec 28 08:14:14.383: INFO: (7) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:462/proxy/: tls qux (200; 2.890361ms)
Dec 28 08:14:14.385: INFO: (7) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname1/proxy/: foo (200; 4.185149ms)
Dec 28 08:14:14.385: INFO: (7) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:460/proxy/: tls baz (200; 3.936341ms)
Dec 28 08:14:14.386: INFO: (7) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname2/proxy/: tls qux (200; 5.556502ms)
Dec 28 08:14:14.386: INFO: (7) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 4.886211ms)
Dec 28 08:14:14.386: INFO: (7) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname2/proxy/: bar (200; 5.035495ms)
Dec 28 08:14:14.386: INFO: (7) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">... (200; 5.185678ms)
Dec 28 08:14:14.386: INFO: (7) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname1/proxy/: foo (200; 5.292112ms)
Dec 28 08:14:14.386: INFO: (7) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 5.441876ms)
Dec 28 08:14:14.386: INFO: (7) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname1/proxy/: tls baz (200; 4.843572ms)
Dec 28 08:14:14.386: INFO: (7) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname2/proxy/: bar (200; 5.106821ms)
Dec 28 08:14:14.386: INFO: (7) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 5.262054ms)
Dec 28 08:14:14.391: INFO: (8) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">test<... (200; 4.728703ms)
Dec 28 08:14:14.391: INFO: (8) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 4.645163ms)
Dec 28 08:14:14.391: INFO: (8) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/tlsrewritem... (200; 5.031811ms)
Dec 28 08:14:14.391: INFO: (8) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:462/proxy/: tls qux (200; 5.642927ms)
Dec 28 08:14:14.392: INFO: (8) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:460/proxy/: tls baz (200; 5.221601ms)
Dec 28 08:14:14.392: INFO: (8) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 5.617858ms)
Dec 28 08:14:14.392: INFO: (8) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">... (200; 5.677476ms)
Dec 28 08:14:14.392: INFO: (8) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 5.929431ms)
Dec 28 08:14:14.392: INFO: (8) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 6.144156ms)
Dec 28 08:14:14.392: INFO: (8) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/rewriteme">test</a> (200; 6.05883ms)
Dec 28 08:14:14.393: INFO: (8) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname1/proxy/: foo (200; 6.714191ms)
Dec 28 08:14:14.393: INFO: (8) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname1/proxy/: tls baz (200; 6.71159ms)
Dec 28 08:14:14.393: INFO: (8) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname2/proxy/: tls qux (200; 6.842444ms)
Dec 28 08:14:14.396: INFO: (8) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname2/proxy/: bar (200; 9.341431ms)
Dec 28 08:14:14.396: INFO: (8) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname1/proxy/: foo (200; 10.300644ms)
Dec 28 08:14:14.396: INFO: (8) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname2/proxy/: bar (200; 9.961511ms)
Dec 28 08:14:14.445: INFO: (9) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 48.754967ms)
Dec 28 08:14:14.447: INFO: (9) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname1/proxy/: tls baz (200; 50.200857ms)
Dec 28 08:14:14.447: INFO: (9) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/tlsrewritem... (200; 50.265824ms)
Dec 28 08:14:14.447: INFO: (9) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname2/proxy/: bar (200; 50.619919ms)
Dec 28 08:14:14.447: INFO: (9) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname2/proxy/: tls qux (200; 50.869809ms)
Dec 28 08:14:14.447: INFO: (9) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname1/proxy/: foo (200; 50.709322ms)
Dec 28 08:14:14.447: INFO: (9) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 50.67203ms)
Dec 28 08:14:14.447: INFO: (9) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">test<... (200; 50.639313ms)
Dec 28 08:14:14.448: INFO: (9) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname2/proxy/: bar (200; 51.715061ms)
Dec 28 08:14:14.448: INFO: (9) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 51.581517ms)
Dec 28 08:14:14.448: INFO: (9) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname1/proxy/: foo (200; 51.814919ms)
Dec 28 08:14:14.449: INFO: (9) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:462/proxy/: tls qux (200; 52.011907ms)
Dec 28 08:14:14.449: INFO: (9) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:460/proxy/: tls baz (200; 52.377015ms)
Dec 28 08:14:14.449: INFO: (9) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 52.213255ms)
Dec 28 08:14:14.449: INFO: (9) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">... (200; 52.066057ms)
Dec 28 08:14:14.449: INFO: (9) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/rewriteme">test</a> (200; 52.137517ms)
Dec 28 08:14:14.464: INFO: (10) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 14.862592ms)
Dec 28 08:14:14.464: INFO: (10) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 15.179212ms)
Dec 28 08:14:14.464: INFO: (10) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">... (200; 15.298196ms)
Dec 28 08:14:14.464: INFO: (10) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">test<... (200; 14.432286ms)
Dec 28 08:14:14.464: INFO: (10) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 15.708595ms)
Dec 28 08:14:14.464: INFO: (10) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/rewriteme">test</a> (200; 15.166623ms)
Dec 28 08:14:14.465: INFO: (10) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 14.955941ms)
Dec 28 08:14:14.465: INFO: (10) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:462/proxy/: tls qux (200; 14.917748ms)
Dec 28 08:14:14.465: INFO: (10) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname1/proxy/: foo (200; 14.9964ms)
Dec 28 08:14:14.465: INFO: (10) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/tlsrewritem... (200; 15.234955ms)
Dec 28 08:14:14.465: INFO: (10) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:460/proxy/: tls baz (200; 15.524952ms)
Dec 28 08:14:14.465: INFO: (10) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname2/proxy/: bar (200; 16.25747ms)
Dec 28 08:14:14.466: INFO: (10) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname1/proxy/: tls baz (200; 16.318473ms)
Dec 28 08:14:14.466: INFO: (10) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname1/proxy/: foo (200; 15.733924ms)
Dec 28 08:14:14.466: INFO: (10) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname2/proxy/: tls qux (200; 16.06078ms)
Dec 28 08:14:14.466: INFO: (10) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname2/proxy/: bar (200; 16.69577ms)
Dec 28 08:14:14.469: INFO: (11) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">test<... (200; 2.1932ms)
Dec 28 08:14:14.470: INFO: (11) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 3.981852ms)
Dec 28 08:14:14.471: INFO: (11) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 4.73984ms)
Dec 28 08:14:14.471: INFO: (11) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">... (200; 4.848052ms)
Dec 28 08:14:14.471: INFO: (11) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname1/proxy/: foo (200; 3.934349ms)
Dec 28 08:14:14.471: INFO: (11) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:460/proxy/: tls baz (200; 4.721837ms)
Dec 28 08:14:14.471: INFO: (11) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 4.55846ms)
Dec 28 08:14:14.471: INFO: (11) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/rewriteme">test</a> (200; 4.925042ms)
Dec 28 08:14:14.471: INFO: (11) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname2/proxy/: bar (200; 5.35153ms)
Dec 28 08:14:14.471: INFO: (11) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:462/proxy/: tls qux (200; 4.434505ms)
Dec 28 08:14:14.471: INFO: (11) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/tlsrewritem... (200; 4.843281ms)
Dec 28 08:14:14.471: INFO: (11) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 5.278112ms)
Dec 28 08:14:14.472: INFO: (11) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname1/proxy/: foo (200; 5.047008ms)
Dec 28 08:14:14.472: INFO: (11) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname2/proxy/: bar (200; 6.009756ms)
Dec 28 08:14:14.472: INFO: (11) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname1/proxy/: tls baz (200; 5.772776ms)
Dec 28 08:14:14.472: INFO: (11) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname2/proxy/: tls qux (200; 5.67243ms)
Dec 28 08:14:14.476: INFO: (12) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 3.758389ms)
Dec 28 08:14:14.476: INFO: (12) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:460/proxy/: tls baz (200; 4.26973ms)
Dec 28 08:14:14.477: INFO: (12) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/tlsrewritem... (200; 5.119479ms)
Dec 28 08:14:14.478: INFO: (12) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 4.723683ms)
Dec 28 08:14:14.478: INFO: (12) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">... (200; 4.958646ms)
Dec 28 08:14:14.478: INFO: (12) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/rewriteme">test</a> (200; 5.183836ms)
Dec 28 08:14:14.478: INFO: (12) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">test<... (200; 5.028051ms)
Dec 28 08:14:14.478: INFO: (12) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:462/proxy/: tls qux (200; 5.515349ms)
Dec 28 08:14:14.478: INFO: (12) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 5.199139ms)
Dec 28 08:14:14.478: INFO: (12) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 5.338118ms)
Dec 28 08:14:14.480: INFO: (12) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname2/proxy/: tls qux (200; 7.639159ms)
Dec 28 08:14:14.480: INFO: (12) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname2/proxy/: bar (200; 6.885028ms)
Dec 28 08:14:14.480: INFO: (12) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname1/proxy/: tls baz (200; 7.307258ms)
Dec 28 08:14:14.480: INFO: (12) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname1/proxy/: foo (200; 7.238532ms)
Dec 28 08:14:14.480: INFO: (12) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname1/proxy/: foo (200; 7.552738ms)
Dec 28 08:14:14.480: INFO: (12) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname2/proxy/: bar (200; 7.707652ms)
Dec 28 08:14:14.483: INFO: (13) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 2.170228ms)
Dec 28 08:14:14.483: INFO: (13) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 2.558881ms)
Dec 28 08:14:14.483: INFO: (13) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/rewriteme">test</a> (200; 2.35668ms)
Dec 28 08:14:14.485: INFO: (13) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">test<... (200; 4.236185ms)
Dec 28 08:14:14.485: INFO: (13) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 4.1988ms)
Dec 28 08:14:14.485: INFO: (13) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:462/proxy/: tls qux (200; 4.50967ms)
Dec 28 08:14:14.485: INFO: (13) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">... (200; 3.993215ms)
Dec 28 08:14:14.485: INFO: (13) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 4.695159ms)
Dec 28 08:14:14.485: INFO: (13) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/tlsrewritem... (200; 4.630579ms)
Dec 28 08:14:14.485: INFO: (13) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname1/proxy/: tls baz (200; 5.145485ms)
Dec 28 08:14:14.486: INFO: (13) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:460/proxy/: tls baz (200; 5.196576ms)
Dec 28 08:14:14.487: INFO: (13) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname1/proxy/: foo (200; 6.173768ms)
Dec 28 08:14:14.488: INFO: (13) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname1/proxy/: foo (200; 6.508982ms)
Dec 28 08:14:14.488: INFO: (13) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname2/proxy/: bar (200; 6.158255ms)
Dec 28 08:14:14.488: INFO: (13) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname2/proxy/: tls qux (200; 6.851292ms)
Dec 28 08:14:14.488: INFO: (13) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname2/proxy/: bar (200; 6.50474ms)
Dec 28 08:14:14.491: INFO: (14) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:462/proxy/: tls qux (200; 3.146578ms)
Dec 28 08:14:14.493: INFO: (14) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/tlsrewritem... (200; 4.661558ms)
Dec 28 08:14:14.493: INFO: (14) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 4.770182ms)
Dec 28 08:14:14.494: INFO: (14) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/rewriteme">test</a> (200; 5.083346ms)
Dec 28 08:14:14.494: INFO: (14) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:460/proxy/: tls baz (200; 5.063165ms)
Dec 28 08:14:14.494: INFO: (14) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 5.299577ms)
Dec 28 08:14:14.494: INFO: (14) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">test<... (200; 5.385112ms)
Dec 28 08:14:14.494: INFO: (14) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 5.001818ms)
Dec 28 08:14:14.494: INFO: (14) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 5.15397ms)
Dec 28 08:14:14.494: INFO: (14) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">... (200; 5.099837ms)
Dec 28 08:14:14.494: INFO: (14) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname2/proxy/: bar (200; 5.762934ms)
Dec 28 08:14:14.495: INFO: (14) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname1/proxy/: foo (200; 6.208217ms)
Dec 28 08:14:14.495: INFO: (14) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname2/proxy/: bar (200; 6.015219ms)
Dec 28 08:14:14.495: INFO: (14) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname2/proxy/: tls qux (200; 6.817111ms)
Dec 28 08:14:14.495: INFO: (14) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname1/proxy/: tls baz (200; 7.151401ms)
Dec 28 08:14:14.496: INFO: (14) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname1/proxy/: foo (200; 8.038511ms)
Dec 28 08:14:14.502: INFO: (15) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 5.316162ms)
Dec 28 08:14:14.502: INFO: (15) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/rewriteme">test</a> (200; 5.681149ms)
Dec 28 08:14:14.502: INFO: (15) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 5.597275ms)
Dec 28 08:14:14.502: INFO: (15) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">test<... (200; 4.955458ms)
Dec 28 08:14:14.502: INFO: (15) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">... (200; 4.649483ms)
Dec 28 08:14:14.502: INFO: (15) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/tlsrewritem... (200; 5.287858ms)
Dec 28 08:14:14.502: INFO: (15) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:462/proxy/: tls qux (200; 5.221252ms)
Dec 28 08:14:14.502: INFO: (15) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 4.905185ms)
Dec 28 08:14:14.502: INFO: (15) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:460/proxy/: tls baz (200; 5.559764ms)
Dec 28 08:14:14.503: INFO: (15) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname2/proxy/: tls qux (200; 6.490642ms)
Dec 28 08:14:14.503: INFO: (15) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname1/proxy/: foo (200; 6.11522ms)
Dec 28 08:14:14.503: INFO: (15) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname1/proxy/: tls baz (200; 6.892617ms)
Dec 28 08:14:14.503: INFO: (15) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname2/proxy/: bar (200; 5.826033ms)
Dec 28 08:14:14.503: INFO: (15) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname1/proxy/: foo (200; 6.254575ms)
Dec 28 08:14:14.503: INFO: (15) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname2/proxy/: bar (200; 5.944498ms)
Dec 28 08:14:14.503: INFO: (15) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 7.130284ms)
Dec 28 08:14:14.505: INFO: (16) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 1.704762ms)
Dec 28 08:14:14.507: INFO: (16) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 3.555725ms)
Dec 28 08:14:14.507: INFO: (16) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:462/proxy/: tls qux (200; 3.195659ms)
Dec 28 08:14:14.507: INFO: (16) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/rewriteme">test</a> (200; 3.757532ms)
Dec 28 08:14:14.507: INFO: (16) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname1/proxy/: tls baz (200; 3.919951ms)
Dec 28 08:14:14.508: INFO: (16) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/tlsrewritem... (200; 3.668438ms)
Dec 28 08:14:14.508: INFO: (16) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:460/proxy/: tls baz (200; 4.574904ms)
Dec 28 08:14:14.508: INFO: (16) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 3.996474ms)
Dec 28 08:14:14.508: INFO: (16) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname1/proxy/: foo (200; 4.180613ms)
Dec 28 08:14:14.508: INFO: (16) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">test<... (200; 4.137416ms)
Dec 28 08:14:14.508: INFO: (16) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">... (200; 3.88545ms)
Dec 28 08:14:14.509: INFO: (16) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 4.565016ms)
Dec 28 08:14:14.509: INFO: (16) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname1/proxy/: foo (200; 4.41396ms)
Dec 28 08:14:14.509: INFO: (16) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname2/proxy/: bar (200; 4.109735ms)
Dec 28 08:14:14.509: INFO: (16) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname2/proxy/: bar (200; 4.246218ms)
Dec 28 08:14:14.509: INFO: (16) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname2/proxy/: tls qux (200; 4.920745ms)
Dec 28 08:14:14.512: INFO: (17) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 2.727669ms)
Dec 28 08:14:14.512: INFO: (17) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 3.176406ms)
Dec 28 08:14:14.513: INFO: (17) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 3.955357ms)
Dec 28 08:14:14.513: INFO: (17) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">... (200; 3.747701ms)
Dec 28 08:14:14.514: INFO: (17) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname2/proxy/: bar (200; 4.9969ms)
Dec 28 08:14:14.514: INFO: (17) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">test<... (200; 5.085775ms)
Dec 28 08:14:14.514: INFO: (17) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 5.230072ms)
Dec 28 08:14:14.514: INFO: (17) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:460/proxy/: tls baz (200; 5.361893ms)
Dec 28 08:14:14.514: INFO: (17) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:462/proxy/: tls qux (200; 5.24908ms)
Dec 28 08:14:14.514: INFO: (17) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/tlsrewritem... (200; 5.549651ms)
Dec 28 08:14:14.514: INFO: (17) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/rewriteme">test</a> (200; 5.524122ms)
Dec 28 08:14:14.514: INFO: (17) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname1/proxy/: foo (200; 5.298439ms)
Dec 28 08:14:14.514: INFO: (17) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname2/proxy/: bar (200; 5.22617ms)
Dec 28 08:14:14.515: INFO: (17) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname2/proxy/: tls qux (200; 5.518993ms)
Dec 28 08:14:14.515: INFO: (17) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname1/proxy/: tls baz (200; 5.774115ms)
Dec 28 08:14:14.515: INFO: (17) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname1/proxy/: foo (200; 5.555961ms)
Dec 28 08:14:14.519: INFO: (18) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname2/proxy/: bar (200; 3.830986ms)
Dec 28 08:14:14.519: INFO: (18) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 3.679141ms)
Dec 28 08:14:14.519: INFO: (18) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 3.467851ms)
Dec 28 08:14:14.519: INFO: (18) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname1/proxy/: tls baz (200; 3.569211ms)
Dec 28 08:14:14.519: INFO: (18) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">test<... (200; 3.088353ms)
Dec 28 08:14:14.519: INFO: (18) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 3.338524ms)
Dec 28 08:14:14.519: INFO: (18) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:462/proxy/: tls qux (200; 3.737983ms)
Dec 28 08:14:14.519: INFO: (18) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/rewriteme">test</a> (200; 4.282871ms)
Dec 28 08:14:14.519: INFO: (18) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:460/proxy/: tls baz (200; 4.130818ms)
Dec 28 08:14:14.519: INFO: (18) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/tlsrewritem... (200; 3.895022ms)
Dec 28 08:14:14.519: INFO: (18) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">... (200; 3.257231ms)
Dec 28 08:14:14.519: INFO: (18) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 3.193586ms)
Dec 28 08:14:14.520: INFO: (18) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname1/proxy/: foo (200; 4.112681ms)
Dec 28 08:14:14.520: INFO: (18) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname2/proxy/: bar (200; 3.945774ms)
Dec 28 08:14:14.520: INFO: (18) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname2/proxy/: tls qux (200; 4.392369ms)
Dec 28 08:14:14.520: INFO: (18) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname1/proxy/: foo (200; 4.518737ms)
Dec 28 08:14:14.523: INFO: (19) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:462/proxy/: tls qux (200; 2.449479ms)
Dec 28 08:14:14.527: INFO: (19) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname1/proxy/: foo (200; 6.432125ms)
Dec 28 08:14:14.527: INFO: (19) /api/v1/namespaces/proxy-3830/services/proxy-service-dxkq7:portname2/proxy/: bar (200; 6.294237ms)
Dec 28 08:14:14.527: INFO: (19) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:443/proxy/tlsrewritem... (200; 6.057434ms)
Dec 28 08:14:14.527: INFO: (19) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 6.46269ms)
Dec 28 08:14:14.527: INFO: (19) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname1/proxy/: foo (200; 6.40599ms)
Dec 28 08:14:14.527: INFO: (19) /api/v1/namespaces/proxy-3830/pods/https:proxy-service-dxkq7-9k4t5:460/proxy/: tls baz (200; 6.252989ms)
Dec 28 08:14:14.527: INFO: (19) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">... (200; 6.444174ms)
Dec 28 08:14:14.527: INFO: (19) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 6.332976ms)
Dec 28 08:14:14.527: INFO: (19) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname1/proxy/: tls baz (200; 6.452185ms)
Dec 28 08:14:14.527: INFO: (19) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5/proxy/rewriteme">test</a> (200; 6.427415ms)
Dec 28 08:14:14.527: INFO: (19) /api/v1/namespaces/proxy-3830/pods/http:proxy-service-dxkq7-9k4t5:162/proxy/: bar (200; 6.623026ms)
Dec 28 08:14:14.527: INFO: (19) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:160/proxy/: foo (200; 6.450781ms)
Dec 28 08:14:14.527: INFO: (19) /api/v1/namespaces/proxy-3830/services/https:proxy-service-dxkq7:tlsportname2/proxy/: tls qux (200; 6.572926ms)
Dec 28 08:14:14.527: INFO: (19) /api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/: <a href="/api/v1/namespaces/proxy-3830/pods/proxy-service-dxkq7-9k4t5:1080/proxy/rewriteme">test<... (200; 6.739302ms)
Dec 28 08:14:14.527: INFO: (19) /api/v1/namespaces/proxy-3830/services/http:proxy-service-dxkq7:portname2/proxy/: bar (200; 6.673572ms)
STEP: deleting ReplicationController proxy-service-dxkq7 in namespace proxy-3830, will wait for the garbage collector to delete the pods
Dec 28 08:14:14.583: INFO: Deleting ReplicationController proxy-service-dxkq7 took: 3.721693ms
Dec 28 08:14:15.483: INFO: Terminating ReplicationController proxy-service-dxkq7 pods took: 900.189402ms
[AfterEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:14:17.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-3830" for this suite.
Dec 28 08:14:23.693: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:14:23.760: INFO: namespace proxy-3830 deletion completed in 6.074240598s

• [SLOW TEST:14.518 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy through a service and a pod  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:14:23.760: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 28 08:14:24.239: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 28 08:14:27.250: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/admissionregistration.k8s.io discovery document
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:14:27.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1874" for this suite.
Dec 28 08:14:33.265: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:14:33.331: INFO: namespace webhook-1874 deletion completed in 6.073301761s
STEP: Destroying namespace "webhook-1874-markers" for this suite.
Dec 28 08:14:39.338: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:14:39.403: INFO: namespace webhook-1874-markers deletion completed in 6.072207455s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:15.650 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-apps] Job 
  should delete a job [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:14:39.410: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete a job [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: delete a job
STEP: deleting Job.batch foo in namespace job-2127, will wait for the garbage collector to delete the pods
Dec 28 08:14:41.492: INFO: Deleting Job.batch foo took: 3.608339ms
Dec 28 08:14:42.393: INFO: Terminating Job.batch foo pods took: 900.20475ms
STEP: Ensuring job was deleted
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:15:15.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2127" for this suite.
Dec 28 08:15:21.504: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:15:21.574: INFO: namespace job-2127 deletion completed in 6.07654721s

• [SLOW TEST:42.164 seconds]
[sig-apps] Job
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:15:21.574: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption-release is created
STEP: When a replicaset with a matching selector is created
STEP: Then the orphan pod is adopted
STEP: When the matched label of one of its pods change
Dec 28 08:15:24.607: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:15:25.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3448" for this suite.
Dec 28 08:15:53.625: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:15:53.694: INFO: namespace replicaset-3448 deletion completed in 28.076064614s

• [SLOW TEST:32.120 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:15:53.695: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-7ae30373-6e04-49b5-8693-5ae5999ef66e
STEP: Creating a pod to test consume configMaps
Dec 28 08:15:53.716: INFO: Waiting up to 5m0s for pod "pod-configmaps-8c9c5cae-8066-4bf9-b2ac-ef81d0d01eaf" in namespace "configmap-9628" to be "success or failure"
Dec 28 08:15:53.718: INFO: Pod "pod-configmaps-8c9c5cae-8066-4bf9-b2ac-ef81d0d01eaf": Phase="Pending", Reason="", readiness=false. Elapsed: 1.441211ms
Dec 28 08:15:55.720: INFO: Pod "pod-configmaps-8c9c5cae-8066-4bf9-b2ac-ef81d0d01eaf": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00375645s
STEP: Saw pod success
Dec 28 08:15:55.720: INFO: Pod "pod-configmaps-8c9c5cae-8066-4bf9-b2ac-ef81d0d01eaf" satisfied condition "success or failure"
Dec 28 08:15:55.721: INFO: Trying to get logs from node hxx-m-2 pod pod-configmaps-8c9c5cae-8066-4bf9-b2ac-ef81d0d01eaf container configmap-volume-test: <nil>
STEP: delete the pod
Dec 28 08:15:55.738: INFO: Waiting for pod pod-configmaps-8c9c5cae-8066-4bf9-b2ac-ef81d0d01eaf to disappear
Dec 28 08:15:55.740: INFO: Pod pod-configmaps-8c9c5cae-8066-4bf9-b2ac-ef81d0d01eaf no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:15:55.740: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9628" for this suite.
Dec 28 08:16:01.749: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:16:01.813: INFO: namespace configmap-9628 deletion completed in 6.071318217s

• [SLOW TEST:8.118 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:16:01.813: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-956decae-7755-451c-b2f5-a7f9ed143eb8
STEP: Creating a pod to test consume configMaps
Dec 28 08:16:01.835: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-9b155af5-9c37-46eb-bc97-2e4d132bf76d" in namespace "projected-7223" to be "success or failure"
Dec 28 08:16:01.837: INFO: Pod "pod-projected-configmaps-9b155af5-9c37-46eb-bc97-2e4d132bf76d": Phase="Pending", Reason="", readiness=false. Elapsed: 1.865223ms
Dec 28 08:16:03.839: INFO: Pod "pod-projected-configmaps-9b155af5-9c37-46eb-bc97-2e4d132bf76d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004226159s
STEP: Saw pod success
Dec 28 08:16:03.839: INFO: Pod "pod-projected-configmaps-9b155af5-9c37-46eb-bc97-2e4d132bf76d" satisfied condition "success or failure"
Dec 28 08:16:03.841: INFO: Trying to get logs from node hxx-m-2 pod pod-projected-configmaps-9b155af5-9c37-46eb-bc97-2e4d132bf76d container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 28 08:16:03.849: INFO: Waiting for pod pod-projected-configmaps-9b155af5-9c37-46eb-bc97-2e4d132bf76d to disappear
Dec 28 08:16:03.851: INFO: Pod pod-projected-configmaps-9b155af5-9c37-46eb-bc97-2e4d132bf76d no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:16:03.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7223" for this suite.
Dec 28 08:16:09.860: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:16:09.926: INFO: namespace projected-7223 deletion completed in 6.072953833s

• [SLOW TEST:8.113 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:16:09.926: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 28 08:16:10.954: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:16:10.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2717" for this suite.
Dec 28 08:16:16.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:16:17.042: INFO: namespace container-runtime-2717 deletion completed in 6.074293904s

• [SLOW TEST:7.116 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:16:17.043: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-15da0516-2780-415b-8157-8f9eb8327c32
STEP: Creating a pod to test consume configMaps
Dec 28 08:16:17.063: INFO: Waiting up to 5m0s for pod "pod-configmaps-14731bc5-d4be-4556-9531-21d89ff3d3f3" in namespace "configmap-9292" to be "success or failure"
Dec 28 08:16:17.064: INFO: Pod "pod-configmaps-14731bc5-d4be-4556-9531-21d89ff3d3f3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.321534ms
Dec 28 08:16:19.067: INFO: Pod "pod-configmaps-14731bc5-d4be-4556-9531-21d89ff3d3f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003704582s
STEP: Saw pod success
Dec 28 08:16:19.067: INFO: Pod "pod-configmaps-14731bc5-d4be-4556-9531-21d89ff3d3f3" satisfied condition "success or failure"
Dec 28 08:16:19.068: INFO: Trying to get logs from node hxx-m-2 pod pod-configmaps-14731bc5-d4be-4556-9531-21d89ff3d3f3 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 28 08:16:19.077: INFO: Waiting for pod pod-configmaps-14731bc5-d4be-4556-9531-21d89ff3d3f3 to disappear
Dec 28 08:16:19.079: INFO: Pod pod-configmaps-14731bc5-d4be-4556-9531-21d89ff3d3f3 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:16:19.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9292" for this suite.
Dec 28 08:16:25.094: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:16:25.159: INFO: namespace configmap-9292 deletion completed in 6.078101282s

• [SLOW TEST:8.117 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:16:25.159: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 08:16:25.176: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 28 08:16:28.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 --namespace=crd-publish-openapi-4922 create -f -'
Dec 28 08:16:29.326: INFO: stderr: ""
Dec 28 08:16:29.326: INFO: stdout: "e2e-test-crd-publish-openapi-1361-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 28 08:16:29.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 --namespace=crd-publish-openapi-4922 delete e2e-test-crd-publish-openapi-1361-crds test-cr'
Dec 28 08:16:29.401: INFO: stderr: ""
Dec 28 08:16:29.401: INFO: stdout: "e2e-test-crd-publish-openapi-1361-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Dec 28 08:16:29.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 --namespace=crd-publish-openapi-4922 apply -f -'
Dec 28 08:16:29.545: INFO: stderr: ""
Dec 28 08:16:29.545: INFO: stdout: "e2e-test-crd-publish-openapi-1361-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Dec 28 08:16:29.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 --namespace=crd-publish-openapi-4922 delete e2e-test-crd-publish-openapi-1361-crds test-cr'
Dec 28 08:16:29.621: INFO: stderr: ""
Dec 28 08:16:29.621: INFO: stdout: "e2e-test-crd-publish-openapi-1361-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema
Dec 28 08:16:29.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 explain e2e-test-crd-publish-openapi-1361-crds'
Dec 28 08:16:29.757: INFO: stderr: ""
Dec 28 08:16:29.757: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-1361-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:16:33.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4922" for this suite.
Dec 28 08:16:39.474: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:16:39.539: INFO: namespace crd-publish-openapi-4922 deletion completed in 6.072089576s

• [SLOW TEST:14.380 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:16:39.539: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-fbcb2b95-4ecf-4ba3-a261-d39644dcc0f5
STEP: Creating a pod to test consume secrets
Dec 28 08:16:39.560: INFO: Waiting up to 5m0s for pod "pod-secrets-5c7491d6-6a2f-4679-a532-2017027f3577" in namespace "secrets-1977" to be "success or failure"
Dec 28 08:16:39.563: INFO: Pod "pod-secrets-5c7491d6-6a2f-4679-a532-2017027f3577": Phase="Pending", Reason="", readiness=false. Elapsed: 2.185699ms
Dec 28 08:16:41.565: INFO: Pod "pod-secrets-5c7491d6-6a2f-4679-a532-2017027f3577": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004814922s
STEP: Saw pod success
Dec 28 08:16:41.565: INFO: Pod "pod-secrets-5c7491d6-6a2f-4679-a532-2017027f3577" satisfied condition "success or failure"
Dec 28 08:16:41.567: INFO: Trying to get logs from node hxx-m-2 pod pod-secrets-5c7491d6-6a2f-4679-a532-2017027f3577 container secret-volume-test: <nil>
STEP: delete the pod
Dec 28 08:16:41.576: INFO: Waiting for pod pod-secrets-5c7491d6-6a2f-4679-a532-2017027f3577 to disappear
Dec 28 08:16:41.578: INFO: Pod pod-secrets-5c7491d6-6a2f-4679-a532-2017027f3577 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:16:41.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1977" for this suite.
Dec 28 08:16:47.588: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:16:47.654: INFO: namespace secrets-1977 deletion completed in 6.073799244s

• [SLOW TEST:8.115 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:16:47.654: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on node default medium
Dec 28 08:16:47.675: INFO: Waiting up to 5m0s for pod "pod-2d043e75-e555-421e-99bd-40458276c2f3" in namespace "emptydir-3806" to be "success or failure"
Dec 28 08:16:47.677: INFO: Pod "pod-2d043e75-e555-421e-99bd-40458276c2f3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.622707ms
Dec 28 08:16:49.679: INFO: Pod "pod-2d043e75-e555-421e-99bd-40458276c2f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003896303s
STEP: Saw pod success
Dec 28 08:16:49.679: INFO: Pod "pod-2d043e75-e555-421e-99bd-40458276c2f3" satisfied condition "success or failure"
Dec 28 08:16:49.681: INFO: Trying to get logs from node hxx-m-2 pod pod-2d043e75-e555-421e-99bd-40458276c2f3 container test-container: <nil>
STEP: delete the pod
Dec 28 08:16:49.694: INFO: Waiting for pod pod-2d043e75-e555-421e-99bd-40458276c2f3 to disappear
Dec 28 08:16:49.695: INFO: Pod pod-2d043e75-e555-421e-99bd-40458276c2f3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:16:49.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3806" for this suite.
Dec 28 08:16:55.707: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:16:55.782: INFO: namespace emptydir-3806 deletion completed in 6.081538648s

• [SLOW TEST:8.128 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:16:55.782: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-16da962a-463c-4fd9-b69d-0ba4a7311cab
STEP: Creating configMap with name cm-test-opt-upd-d85da0d6-9202-4483-832f-ad42c264ee18
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-16da962a-463c-4fd9-b69d-0ba4a7311cab
STEP: Updating configmap cm-test-opt-upd-d85da0d6-9202-4483-832f-ad42c264ee18
STEP: Creating configMap with name cm-test-opt-create-51a4e1a0-2e6e-4d0c-907e-524571ed199a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:16:59.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1242" for this suite.
Dec 28 08:17:11.869: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:17:11.934: INFO: namespace projected-1242 deletion completed in 12.074680492s

• [SLOW TEST:16.153 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:17:11.936: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-504
STEP: changing the ExternalName service to type=NodePort
STEP: creating replication controller externalname-service in namespace services-504
I1228 08:17:12.031866      22 runners.go:184] Created replication controller with name: externalname-service, namespace: services-504, replica count: 2
Dec 28 08:17:15.082: INFO: Creating new exec pod
I1228 08:17:15.082248      22 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 28 08:17:18.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=services-504 execpod448p6 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 28 08:17:18.314: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 28 08:17:18.314: INFO: stdout: ""
Dec 28 08:17:18.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=services-504 execpod448p6 -- /bin/sh -x -c nc -zv -t -w 2 10.99.251.4 80'
Dec 28 08:17:18.535: INFO: stderr: "+ nc -zv -t -w 2 10.99.251.4 80\nConnection to 10.99.251.4 80 port [tcp/http] succeeded!\n"
Dec 28 08:17:18.535: INFO: stdout: ""
Dec 28 08:17:18.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=services-504 execpod448p6 -- /bin/sh -x -c nc -zv -t -w 2 10.0.128.42 31393'
Dec 28 08:17:18.749: INFO: stderr: "+ nc -zv -t -w 2 10.0.128.42 31393\nConnection to 10.0.128.42 31393 port [tcp/31393] succeeded!\n"
Dec 28 08:17:18.749: INFO: stdout: ""
Dec 28 08:17:18.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=services-504 execpod448p6 -- /bin/sh -x -c nc -zv -t -w 2 10.0.128.16 31393'
Dec 28 08:17:18.973: INFO: stderr: "+ nc -zv -t -w 2 10.0.128.16 31393\nConnection to 10.0.128.16 31393 port [tcp/31393] succeeded!\n"
Dec 28 08:17:18.973: INFO: stdout: ""
Dec 28 08:17:18.973: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:17:18.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-504" for this suite.
Dec 28 08:17:24.997: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:17:25.066: INFO: namespace services-504 deletion completed in 6.076873888s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:13.131 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:17:25.067: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 28 08:17:25.087: INFO: Waiting up to 5m0s for pod "downwardapi-volume-af1fb8ec-b75d-47dd-a049-9463f747b020" in namespace "downward-api-9569" to be "success or failure"
Dec 28 08:17:25.088: INFO: Pod "downwardapi-volume-af1fb8ec-b75d-47dd-a049-9463f747b020": Phase="Pending", Reason="", readiness=false. Elapsed: 1.323943ms
Dec 28 08:17:27.090: INFO: Pod "downwardapi-volume-af1fb8ec-b75d-47dd-a049-9463f747b020": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003306492s
STEP: Saw pod success
Dec 28 08:17:27.090: INFO: Pod "downwardapi-volume-af1fb8ec-b75d-47dd-a049-9463f747b020" satisfied condition "success or failure"
Dec 28 08:17:27.092: INFO: Trying to get logs from node hxx-m-2 pod downwardapi-volume-af1fb8ec-b75d-47dd-a049-9463f747b020 container client-container: <nil>
STEP: delete the pod
Dec 28 08:17:27.102: INFO: Waiting for pod downwardapi-volume-af1fb8ec-b75d-47dd-a049-9463f747b020 to disappear
Dec 28 08:17:27.104: INFO: Pod downwardapi-volume-af1fb8ec-b75d-47dd-a049-9463f747b020 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:17:27.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9569" for this suite.
Dec 28 08:17:33.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:17:33.183: INFO: namespace downward-api-9569 deletion completed in 6.076090536s

• [SLOW TEST:8.116 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers 
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:17:33.183: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with a certain label
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: changing the label value of the configmap
STEP: Expecting to observe a delete notification for the watched object
Dec 28 08:17:33.208: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7425 /api/v1/namespaces/watch-7425/configmaps/e2e-watch-test-label-changed e4e30190-5921-4ebc-8728-1077b0166a4c 277426 0 2019-12-28 08:17:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 28 08:17:33.208: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7425 /api/v1/namespaces/watch-7425/configmaps/e2e-watch-test-label-changed e4e30190-5921-4ebc-8728-1077b0166a4c 277427 0 2019-12-28 08:17:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 28 08:17:33.208: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7425 /api/v1/namespaces/watch-7425/configmaps/e2e-watch-test-label-changed e4e30190-5921-4ebc-8728-1077b0166a4c 277428 0 2019-12-28 08:17:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements
STEP: changing the label value of the configmap back
STEP: modifying the configmap a third time
STEP: deleting the configmap
STEP: Expecting to observe an add notification for the watched object when the label value was restored
Dec 28 08:17:43.222: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7425 /api/v1/namespaces/watch-7425/configmaps/e2e-watch-test-label-changed e4e30190-5921-4ebc-8728-1077b0166a4c 277469 0 2019-12-28 08:17:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 28 08:17:43.222: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7425 /api/v1/namespaces/watch-7425/configmaps/e2e-watch-test-label-changed e4e30190-5921-4ebc-8728-1077b0166a4c 277470 0 2019-12-28 08:17:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
Dec 28 08:17:43.222: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7425 /api/v1/namespaces/watch-7425/configmaps/e2e-watch-test-label-changed e4e30190-5921-4ebc-8728-1077b0166a4c 277471 0 2019-12-28 08:17:33 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] []  []},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:17:43.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-7425" for this suite.
Dec 28 08:17:49.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:17:49.301: INFO: namespace watch-7425 deletion completed in 6.076029249s

• [SLOW TEST:16.118 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:17:49.301: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 28 08:17:49.322: INFO: Waiting up to 5m0s for pod "downwardapi-volume-456ca64d-3db2-460c-ab7d-92530e1cf8d8" in namespace "projected-8457" to be "success or failure"
Dec 28 08:17:49.324: INFO: Pod "downwardapi-volume-456ca64d-3db2-460c-ab7d-92530e1cf8d8": Phase="Pending", Reason="", readiness=false. Elapsed: 1.555101ms
Dec 28 08:17:51.326: INFO: Pod "downwardapi-volume-456ca64d-3db2-460c-ab7d-92530e1cf8d8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004093683s
STEP: Saw pod success
Dec 28 08:17:51.326: INFO: Pod "downwardapi-volume-456ca64d-3db2-460c-ab7d-92530e1cf8d8" satisfied condition "success or failure"
Dec 28 08:17:51.328: INFO: Trying to get logs from node hxx-m-2 pod downwardapi-volume-456ca64d-3db2-460c-ab7d-92530e1cf8d8 container client-container: <nil>
STEP: delete the pod
Dec 28 08:17:51.339: INFO: Waiting for pod downwardapi-volume-456ca64d-3db2-460c-ab7d-92530e1cf8d8 to disappear
Dec 28 08:17:51.340: INFO: Pod downwardapi-volume-456ca64d-3db2-460c-ab7d-92530e1cf8d8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:17:51.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8457" for this suite.
Dec 28 08:17:57.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:17:57.417: INFO: namespace projected-8457 deletion completed in 6.074182962s

• [SLOW TEST:8.116 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:17:57.418: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 28 08:17:57.735: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 28 08:17:59.741: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713117877, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713117877, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713117877, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713117877, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 28 08:18:02.749: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 08:18:02.752: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Registering the custom resource webhook via the AdmissionRegistration API
STEP: Creating a custom resource that should be denied by the webhook
STEP: Creating a custom resource whose deletion would be denied by the webhook
STEP: Updating the custom resource with disallowed data should be denied
STEP: Deleting the custom resource should be denied
STEP: Remove the offending key and value from the custom resource data
STEP: Deleting the updated custom resource should be successful
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:18:03.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1157" for this suite.
Dec 28 08:18:09.843: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:18:09.909: INFO: namespace webhook-1157 deletion completed in 6.07328768s
STEP: Destroying namespace "webhook-1157-markers" for this suite.
Dec 28 08:18:15.918: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:18:15.987: INFO: namespace webhook-1157-markers deletion completed in 6.077498539s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.576 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job 
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:18:15.993: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring job reaches completions
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:18:22.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-6172" for this suite.
Dec 28 08:18:28.024: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:18:28.092: INFO: namespace job-6172 deletion completed in 6.075320583s

• [SLOW TEST:12.099 seconds]
[sig-apps] Job
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:18:28.092: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 28 08:18:29.295: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 28 08:18:31.301: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713117909, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713117909, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713117909, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713117909, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 28 08:18:34.309: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a mutating webhook configuration
STEP: Updating a mutating webhook configuration's rules to not include the create operation
STEP: Creating a configMap that should not be mutated
STEP: Patching a mutating webhook configuration's rules to include the create operation
STEP: Creating a configMap that should be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:18:34.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6690" for this suite.
Dec 28 08:18:40.352: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:18:40.419: INFO: namespace webhook-6690 deletion completed in 6.073961216s
STEP: Destroying namespace "webhook-6690-markers" for this suite.
Dec 28 08:18:46.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:18:46.488: INFO: namespace webhook-6690-markers deletion completed in 6.068959717s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.403 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:18:46.495: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for all pods to be garbage collected
STEP: Gathering metrics
Dec 28 08:18:56.525: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:18:56.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
W1228 08:18:56.525703      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
STEP: Destroying namespace "gc-8064" for this suite.
Dec 28 08:19:02.534: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:19:02.603: INFO: namespace gc-8064 deletion completed in 6.075823794s

• [SLOW TEST:16.108 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:19:02.604: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 28 08:19:02.625: INFO: Waiting up to 5m0s for pod "pod-e8a9a288-7393-4c71-ad5d-961bbe2ca2f7" in namespace "emptydir-7124" to be "success or failure"
Dec 28 08:19:02.627: INFO: Pod "pod-e8a9a288-7393-4c71-ad5d-961bbe2ca2f7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.534817ms
Dec 28 08:19:04.630: INFO: Pod "pod-e8a9a288-7393-4c71-ad5d-961bbe2ca2f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00513159s
STEP: Saw pod success
Dec 28 08:19:04.630: INFO: Pod "pod-e8a9a288-7393-4c71-ad5d-961bbe2ca2f7" satisfied condition "success or failure"
Dec 28 08:19:04.632: INFO: Trying to get logs from node hxx-m-2 pod pod-e8a9a288-7393-4c71-ad5d-961bbe2ca2f7 container test-container: <nil>
STEP: delete the pod
Dec 28 08:19:04.642: INFO: Waiting for pod pod-e8a9a288-7393-4c71-ad5d-961bbe2ca2f7 to disappear
Dec 28 08:19:04.645: INFO: Pod pod-e8a9a288-7393-4c71-ad5d-961bbe2ca2f7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:19:04.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7124" for this suite.
Dec 28 08:19:10.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:19:10.777: INFO: namespace emptydir-7124 deletion completed in 6.130190157s

• [SLOW TEST:8.173 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:19:10.777: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 28 08:19:10.796: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1473c214-a75b-4715-9ce4-c3eb3de8bf81" in namespace "projected-3545" to be "success or failure"
Dec 28 08:19:10.798: INFO: Pod "downwardapi-volume-1473c214-a75b-4715-9ce4-c3eb3de8bf81": Phase="Pending", Reason="", readiness=false. Elapsed: 1.590395ms
Dec 28 08:19:12.801: INFO: Pod "downwardapi-volume-1473c214-a75b-4715-9ce4-c3eb3de8bf81": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004035064s
STEP: Saw pod success
Dec 28 08:19:12.801: INFO: Pod "downwardapi-volume-1473c214-a75b-4715-9ce4-c3eb3de8bf81" satisfied condition "success or failure"
Dec 28 08:19:12.802: INFO: Trying to get logs from node hxx-m-2 pod downwardapi-volume-1473c214-a75b-4715-9ce4-c3eb3de8bf81 container client-container: <nil>
STEP: delete the pod
Dec 28 08:19:12.815: INFO: Waiting for pod downwardapi-volume-1473c214-a75b-4715-9ce4-c3eb3de8bf81 to disappear
Dec 28 08:19:12.816: INFO: Pod downwardapi-volume-1473c214-a75b-4715-9ce4-c3eb3de8bf81 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:19:12.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3545" for this suite.
Dec 28 08:19:18.826: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:19:18.891: INFO: namespace projected-3545 deletion completed in 6.072627626s

• [SLOW TEST:8.114 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] Service endpoints latency 
  should not be very high  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:19:18.891: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename svc-latency
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be very high  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating replication controller svc-latency-rc in namespace svc-latency-7450
I1228 08:19:18.909362      22 runners.go:184] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7450, replica count: 1
I1228 08:19:19.959752      22 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I1228 08:19:20.959948      22 runners.go:184] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 28 08:19:21.068: INFO: Created: latency-svc-9dlzv
Dec 28 08:19:21.069: INFO: Got endpoints: latency-svc-9dlzv [8.847714ms]
Dec 28 08:19:21.076: INFO: Created: latency-svc-j89st
Dec 28 08:19:21.079: INFO: Got endpoints: latency-svc-j89st [10.137753ms]
Dec 28 08:19:21.081: INFO: Created: latency-svc-czmlt
Dec 28 08:19:21.085: INFO: Got endpoints: latency-svc-czmlt [16.446465ms]
Dec 28 08:19:21.086: INFO: Created: latency-svc-bpnbr
Dec 28 08:19:21.088: INFO: Got endpoints: latency-svc-bpnbr [19.505879ms]
Dec 28 08:19:21.091: INFO: Created: latency-svc-c4lrd
Dec 28 08:19:21.095: INFO: Got endpoints: latency-svc-c4lrd [26.001447ms]
Dec 28 08:19:21.095: INFO: Created: latency-svc-bzl6h
Dec 28 08:19:21.098: INFO: Got endpoints: latency-svc-bzl6h [29.265334ms]
Dec 28 08:19:21.101: INFO: Created: latency-svc-qhtq4
Dec 28 08:19:21.105: INFO: Got endpoints: latency-svc-qhtq4 [35.790738ms]
Dec 28 08:19:21.107: INFO: Created: latency-svc-c6rsv
Dec 28 08:19:21.111: INFO: Got endpoints: latency-svc-c6rsv [41.810789ms]
Dec 28 08:19:21.112: INFO: Created: latency-svc-dkwsz
Dec 28 08:19:21.114: INFO: Got endpoints: latency-svc-dkwsz [44.730643ms]
Dec 28 08:19:21.116: INFO: Created: latency-svc-rg49n
Dec 28 08:19:21.120: INFO: Got endpoints: latency-svc-rg49n [50.859057ms]
Dec 28 08:19:21.121: INFO: Created: latency-svc-x74pp
Dec 28 08:19:21.123: INFO: Got endpoints: latency-svc-x74pp [54.430038ms]
Dec 28 08:19:21.127: INFO: Created: latency-svc-6grqs
Dec 28 08:19:21.129: INFO: Got endpoints: latency-svc-6grqs [59.689561ms]
Dec 28 08:19:21.134: INFO: Created: latency-svc-gkpp2
Dec 28 08:19:21.137: INFO: Got endpoints: latency-svc-gkpp2 [68.07163ms]
Dec 28 08:19:21.143: INFO: Created: latency-svc-qf7hc
Dec 28 08:19:21.147: INFO: Got endpoints: latency-svc-qf7hc [78.038235ms]
Dec 28 08:19:21.149: INFO: Created: latency-svc-h4rn7
Dec 28 08:19:21.150: INFO: Got endpoints: latency-svc-h4rn7 [80.949901ms]
Dec 28 08:19:21.153: INFO: Created: latency-svc-9s8d7
Dec 28 08:19:21.155: INFO: Got endpoints: latency-svc-9s8d7 [85.900012ms]
Dec 28 08:19:21.157: INFO: Created: latency-svc-65j6c
Dec 28 08:19:21.160: INFO: Got endpoints: latency-svc-65j6c [80.75279ms]
Dec 28 08:19:21.162: INFO: Created: latency-svc-2sw6q
Dec 28 08:19:21.166: INFO: Created: latency-svc-frxdz
Dec 28 08:19:21.167: INFO: Got endpoints: latency-svc-2sw6q [81.432069ms]
Dec 28 08:19:21.169: INFO: Got endpoints: latency-svc-frxdz [80.400356ms]
Dec 28 08:19:21.170: INFO: Created: latency-svc-bpl8h
Dec 28 08:19:21.173: INFO: Got endpoints: latency-svc-bpl8h [78.257237ms]
Dec 28 08:19:21.177: INFO: Created: latency-svc-nqk6f
Dec 28 08:19:21.180: INFO: Got endpoints: latency-svc-nqk6f [81.520377ms]
Dec 28 08:19:21.183: INFO: Created: latency-svc-cqtvh
Dec 28 08:19:21.185: INFO: Got endpoints: latency-svc-cqtvh [80.616757ms]
Dec 28 08:19:21.189: INFO: Created: latency-svc-p8w82
Dec 28 08:19:21.191: INFO: Got endpoints: latency-svc-p8w82 [80.002253ms]
Dec 28 08:19:21.195: INFO: Created: latency-svc-c8mmx
Dec 28 08:19:21.199: INFO: Got endpoints: latency-svc-c8mmx [84.970062ms]
Dec 28 08:19:21.201: INFO: Created: latency-svc-c8wnq
Dec 28 08:19:21.203: INFO: Got endpoints: latency-svc-c8wnq [83.424334ms]
Dec 28 08:19:21.230: INFO: Created: latency-svc-c7n5l
Dec 28 08:19:21.234: INFO: Got endpoints: latency-svc-c7n5l [110.562441ms]
Dec 28 08:19:21.234: INFO: Created: latency-svc-j2zkq
Dec 28 08:19:21.236: INFO: Got endpoints: latency-svc-j2zkq [107.732653ms]
Dec 28 08:19:21.240: INFO: Created: latency-svc-gp29s
Dec 28 08:19:21.245: INFO: Got endpoints: latency-svc-gp29s [108.357251ms]
Dec 28 08:19:21.245: INFO: Created: latency-svc-fmmtm
Dec 28 08:19:21.247: INFO: Got endpoints: latency-svc-fmmtm [100.510055ms]
Dec 28 08:19:21.250: INFO: Created: latency-svc-5mkss
Dec 28 08:19:21.253: INFO: Got endpoints: latency-svc-5mkss [103.310306ms]
Dec 28 08:19:21.254: INFO: Created: latency-svc-vtwmg
Dec 28 08:19:21.257: INFO: Got endpoints: latency-svc-vtwmg [9.107689ms]
Dec 28 08:19:21.262: INFO: Created: latency-svc-v9g9q
Dec 28 08:19:21.267: INFO: Got endpoints: latency-svc-v9g9q [111.925448ms]
Dec 28 08:19:21.268: INFO: Created: latency-svc-7wfrl
Dec 28 08:19:21.269: INFO: Got endpoints: latency-svc-7wfrl [109.5712ms]
Dec 28 08:19:21.283: INFO: Created: latency-svc-tpns5
Dec 28 08:19:21.287: INFO: Got endpoints: latency-svc-tpns5 [120.483728ms]
Dec 28 08:19:21.288: INFO: Created: latency-svc-9vfrk
Dec 28 08:19:21.293: INFO: Created: latency-svc-795fd
Dec 28 08:19:21.298: INFO: Created: latency-svc-bsthm
Dec 28 08:19:21.312: INFO: Created: latency-svc-vqq74
Dec 28 08:19:21.316: INFO: Created: latency-svc-zc8kb
Dec 28 08:19:21.320: INFO: Got endpoints: latency-svc-9vfrk [151.089165ms]
Dec 28 08:19:21.320: INFO: Created: latency-svc-m69zp
Dec 28 08:19:21.326: INFO: Created: latency-svc-c7qdj
Dec 28 08:19:21.334: INFO: Created: latency-svc-rw878
Dec 28 08:19:21.340: INFO: Created: latency-svc-h4zm4
Dec 28 08:19:21.345: INFO: Created: latency-svc-stbnn
Dec 28 08:19:21.350: INFO: Created: latency-svc-tzcbq
Dec 28 08:19:21.354: INFO: Created: latency-svc-m8l2n
Dec 28 08:19:21.358: INFO: Created: latency-svc-2272j
Dec 28 08:19:21.363: INFO: Created: latency-svc-mj9dp
Dec 28 08:19:21.366: INFO: Created: latency-svc-jhhgf
Dec 28 08:19:21.369: INFO: Got endpoints: latency-svc-795fd [195.342748ms]
Dec 28 08:19:21.373: INFO: Created: latency-svc-km7xz
Dec 28 08:19:21.375: INFO: Created: latency-svc-cxb7j
Dec 28 08:19:21.419: INFO: Got endpoints: latency-svc-bsthm [239.184731ms]
Dec 28 08:19:21.426: INFO: Created: latency-svc-fn9zl
Dec 28 08:19:21.469: INFO: Got endpoints: latency-svc-vqq74 [283.772129ms]
Dec 28 08:19:21.476: INFO: Created: latency-svc-mpzl4
Dec 28 08:19:21.519: INFO: Got endpoints: latency-svc-zc8kb [328.215486ms]
Dec 28 08:19:21.525: INFO: Created: latency-svc-jmx79
Dec 28 08:19:21.569: INFO: Got endpoints: latency-svc-m69zp [370.334651ms]
Dec 28 08:19:21.585: INFO: Created: latency-svc-s9kgl
Dec 28 08:19:21.619: INFO: Got endpoints: latency-svc-c7qdj [415.469502ms]
Dec 28 08:19:21.625: INFO: Created: latency-svc-9mm5d
Dec 28 08:19:21.668: INFO: Got endpoints: latency-svc-rw878 [434.542469ms]
Dec 28 08:19:21.674: INFO: Created: latency-svc-v7dr6
Dec 28 08:19:21.719: INFO: Got endpoints: latency-svc-h4zm4 [482.561832ms]
Dec 28 08:19:21.726: INFO: Created: latency-svc-mg825
Dec 28 08:19:21.769: INFO: Got endpoints: latency-svc-stbnn [523.98754ms]
Dec 28 08:19:21.776: INFO: Created: latency-svc-9bvs5
Dec 28 08:19:21.819: INFO: Got endpoints: latency-svc-tzcbq [565.653794ms]
Dec 28 08:19:21.826: INFO: Created: latency-svc-4g78c
Dec 28 08:19:21.869: INFO: Got endpoints: latency-svc-m8l2n [612.523898ms]
Dec 28 08:19:21.876: INFO: Created: latency-svc-m5vj7
Dec 28 08:19:21.919: INFO: Got endpoints: latency-svc-2272j [652.602465ms]
Dec 28 08:19:21.927: INFO: Created: latency-svc-djbs6
Dec 28 08:19:21.969: INFO: Got endpoints: latency-svc-mj9dp [699.851136ms]
Dec 28 08:19:21.975: INFO: Created: latency-svc-czdn9
Dec 28 08:19:22.019: INFO: Got endpoints: latency-svc-jhhgf [731.596082ms]
Dec 28 08:19:22.025: INFO: Created: latency-svc-84xw5
Dec 28 08:19:22.069: INFO: Got endpoints: latency-svc-km7xz [748.787237ms]
Dec 28 08:19:22.075: INFO: Created: latency-svc-w978q
Dec 28 08:19:22.120: INFO: Got endpoints: latency-svc-cxb7j [751.124023ms]
Dec 28 08:19:22.126: INFO: Created: latency-svc-nlqhw
Dec 28 08:19:22.169: INFO: Got endpoints: latency-svc-fn9zl [749.935468ms]
Dec 28 08:19:22.176: INFO: Created: latency-svc-x72h5
Dec 28 08:19:22.218: INFO: Got endpoints: latency-svc-mpzl4 [749.358395ms]
Dec 28 08:19:22.225: INFO: Created: latency-svc-4jm4b
Dec 28 08:19:22.269: INFO: Got endpoints: latency-svc-jmx79 [749.716702ms]
Dec 28 08:19:22.276: INFO: Created: latency-svc-zh78b
Dec 28 08:19:22.319: INFO: Got endpoints: latency-svc-s9kgl [749.72777ms]
Dec 28 08:19:22.326: INFO: Created: latency-svc-8h6v9
Dec 28 08:19:22.369: INFO: Got endpoints: latency-svc-9mm5d [750.018229ms]
Dec 28 08:19:22.375: INFO: Created: latency-svc-nl82p
Dec 28 08:19:22.419: INFO: Got endpoints: latency-svc-v7dr6 [750.187039ms]
Dec 28 08:19:22.425: INFO: Created: latency-svc-t578q
Dec 28 08:19:22.469: INFO: Got endpoints: latency-svc-mg825 [750.375278ms]
Dec 28 08:19:22.476: INFO: Created: latency-svc-c48f7
Dec 28 08:19:22.519: INFO: Got endpoints: latency-svc-9bvs5 [749.209184ms]
Dec 28 08:19:22.527: INFO: Created: latency-svc-mmbhd
Dec 28 08:19:22.569: INFO: Got endpoints: latency-svc-4g78c [750.076573ms]
Dec 28 08:19:22.578: INFO: Created: latency-svc-8dczr
Dec 28 08:19:22.619: INFO: Got endpoints: latency-svc-m5vj7 [749.82802ms]
Dec 28 08:19:22.626: INFO: Created: latency-svc-6x4fs
Dec 28 08:19:22.669: INFO: Got endpoints: latency-svc-djbs6 [749.341752ms]
Dec 28 08:19:22.682: INFO: Created: latency-svc-scsbz
Dec 28 08:19:22.718: INFO: Got endpoints: latency-svc-czdn9 [749.457499ms]
Dec 28 08:19:22.727: INFO: Created: latency-svc-rhlcj
Dec 28 08:19:22.769: INFO: Got endpoints: latency-svc-84xw5 [750.067042ms]
Dec 28 08:19:22.776: INFO: Created: latency-svc-ng2s9
Dec 28 08:19:22.818: INFO: Got endpoints: latency-svc-w978q [749.908306ms]
Dec 28 08:19:22.826: INFO: Created: latency-svc-5ldhz
Dec 28 08:19:22.869: INFO: Got endpoints: latency-svc-nlqhw [749.33047ms]
Dec 28 08:19:22.876: INFO: Created: latency-svc-h2tsc
Dec 28 08:19:22.919: INFO: Got endpoints: latency-svc-x72h5 [749.819107ms]
Dec 28 08:19:22.926: INFO: Created: latency-svc-g7k5v
Dec 28 08:19:22.969: INFO: Got endpoints: latency-svc-4jm4b [750.4791ms]
Dec 28 08:19:22.976: INFO: Created: latency-svc-hn87k
Dec 28 08:19:23.019: INFO: Got endpoints: latency-svc-zh78b [750.120521ms]
Dec 28 08:19:23.026: INFO: Created: latency-svc-tmv5m
Dec 28 08:19:23.069: INFO: Got endpoints: latency-svc-8h6v9 [749.928049ms]
Dec 28 08:19:23.075: INFO: Created: latency-svc-pqgz4
Dec 28 08:19:23.119: INFO: Got endpoints: latency-svc-nl82p [749.933123ms]
Dec 28 08:19:23.125: INFO: Created: latency-svc-b2txn
Dec 28 08:19:23.169: INFO: Got endpoints: latency-svc-t578q [750.600304ms]
Dec 28 08:19:23.176: INFO: Created: latency-svc-8zgdv
Dec 28 08:19:23.219: INFO: Got endpoints: latency-svc-c48f7 [749.96194ms]
Dec 28 08:19:23.227: INFO: Created: latency-svc-g6xdf
Dec 28 08:19:23.270: INFO: Got endpoints: latency-svc-mmbhd [751.090479ms]
Dec 28 08:19:23.277: INFO: Created: latency-svc-7dst6
Dec 28 08:19:23.319: INFO: Got endpoints: latency-svc-8dczr [749.771904ms]
Dec 28 08:19:23.326: INFO: Created: latency-svc-sj94f
Dec 28 08:19:23.368: INFO: Got endpoints: latency-svc-6x4fs [748.949458ms]
Dec 28 08:19:23.376: INFO: Created: latency-svc-w2skd
Dec 28 08:19:23.419: INFO: Got endpoints: latency-svc-scsbz [749.972623ms]
Dec 28 08:19:23.425: INFO: Created: latency-svc-zfhq5
Dec 28 08:19:23.469: INFO: Got endpoints: latency-svc-rhlcj [750.981905ms]
Dec 28 08:19:23.478: INFO: Created: latency-svc-zs7p9
Dec 28 08:19:23.520: INFO: Got endpoints: latency-svc-ng2s9 [750.666235ms]
Dec 28 08:19:23.526: INFO: Created: latency-svc-v76f6
Dec 28 08:19:23.568: INFO: Got endpoints: latency-svc-5ldhz [749.781106ms]
Dec 28 08:19:23.577: INFO: Created: latency-svc-b59fc
Dec 28 08:19:23.619: INFO: Got endpoints: latency-svc-h2tsc [749.66665ms]
Dec 28 08:19:23.625: INFO: Created: latency-svc-fmk7d
Dec 28 08:19:23.670: INFO: Got endpoints: latency-svc-g7k5v [751.349465ms]
Dec 28 08:19:23.679: INFO: Created: latency-svc-s5t6x
Dec 28 08:19:23.720: INFO: Got endpoints: latency-svc-hn87k [750.525803ms]
Dec 28 08:19:23.727: INFO: Created: latency-svc-4w67p
Dec 28 08:19:23.769: INFO: Got endpoints: latency-svc-tmv5m [750.488562ms]
Dec 28 08:19:23.777: INFO: Created: latency-svc-lr6bl
Dec 28 08:19:23.819: INFO: Got endpoints: latency-svc-pqgz4 [749.992652ms]
Dec 28 08:19:23.826: INFO: Created: latency-svc-jw2hg
Dec 28 08:19:23.869: INFO: Got endpoints: latency-svc-b2txn [749.838648ms]
Dec 28 08:19:23.880: INFO: Created: latency-svc-t57l5
Dec 28 08:19:23.919: INFO: Got endpoints: latency-svc-8zgdv [749.460661ms]
Dec 28 08:19:23.926: INFO: Created: latency-svc-hvjd2
Dec 28 08:19:23.969: INFO: Got endpoints: latency-svc-g6xdf [749.191595ms]
Dec 28 08:19:23.976: INFO: Created: latency-svc-nh5rr
Dec 28 08:19:24.019: INFO: Got endpoints: latency-svc-7dst6 [749.572319ms]
Dec 28 08:19:24.027: INFO: Created: latency-svc-tfb8c
Dec 28 08:19:24.069: INFO: Got endpoints: latency-svc-sj94f [750.400075ms]
Dec 28 08:19:24.078: INFO: Created: latency-svc-bx9mn
Dec 28 08:19:24.120: INFO: Got endpoints: latency-svc-w2skd [751.781124ms]
Dec 28 08:19:24.127: INFO: Created: latency-svc-pwsb5
Dec 28 08:19:24.169: INFO: Got endpoints: latency-svc-zfhq5 [749.969775ms]
Dec 28 08:19:24.177: INFO: Created: latency-svc-bhzjp
Dec 28 08:19:24.219: INFO: Got endpoints: latency-svc-zs7p9 [749.047772ms]
Dec 28 08:19:24.226: INFO: Created: latency-svc-9rgqh
Dec 28 08:19:24.269: INFO: Got endpoints: latency-svc-v76f6 [749.176082ms]
Dec 28 08:19:24.276: INFO: Created: latency-svc-mpv59
Dec 28 08:19:24.318: INFO: Got endpoints: latency-svc-b59fc [750.095769ms]
Dec 28 08:19:24.325: INFO: Created: latency-svc-5vqd4
Dec 28 08:19:24.369: INFO: Got endpoints: latency-svc-fmk7d [749.894198ms]
Dec 28 08:19:24.375: INFO: Created: latency-svc-j2kcl
Dec 28 08:19:24.419: INFO: Got endpoints: latency-svc-s5t6x [748.601314ms]
Dec 28 08:19:24.425: INFO: Created: latency-svc-n5n5d
Dec 28 08:19:24.469: INFO: Got endpoints: latency-svc-4w67p [749.035335ms]
Dec 28 08:19:24.476: INFO: Created: latency-svc-xjkh6
Dec 28 08:19:24.520: INFO: Got endpoints: latency-svc-lr6bl [750.415963ms]
Dec 28 08:19:24.527: INFO: Created: latency-svc-7b8x2
Dec 28 08:19:24.568: INFO: Got endpoints: latency-svc-jw2hg [749.743391ms]
Dec 28 08:19:24.575: INFO: Created: latency-svc-hnzcn
Dec 28 08:19:24.618: INFO: Got endpoints: latency-svc-t57l5 [749.801271ms]
Dec 28 08:19:24.625: INFO: Created: latency-svc-xf7ql
Dec 28 08:19:24.669: INFO: Got endpoints: latency-svc-hvjd2 [749.705339ms]
Dec 28 08:19:24.675: INFO: Created: latency-svc-6nnfq
Dec 28 08:19:24.719: INFO: Got endpoints: latency-svc-nh5rr [750.213048ms]
Dec 28 08:19:24.762: INFO: Created: latency-svc-6fgwg
Dec 28 08:19:24.793: INFO: Got endpoints: latency-svc-tfb8c [773.209187ms]
Dec 28 08:19:24.800: INFO: Created: latency-svc-dqnkw
Dec 28 08:19:24.819: INFO: Got endpoints: latency-svc-bx9mn [749.31648ms]
Dec 28 08:19:24.825: INFO: Created: latency-svc-6bmb8
Dec 28 08:19:24.869: INFO: Got endpoints: latency-svc-pwsb5 [748.741571ms]
Dec 28 08:19:24.875: INFO: Created: latency-svc-swgx6
Dec 28 08:19:24.919: INFO: Got endpoints: latency-svc-bhzjp [750.071061ms]
Dec 28 08:19:24.927: INFO: Created: latency-svc-mpkt6
Dec 28 08:19:24.969: INFO: Got endpoints: latency-svc-9rgqh [750.096736ms]
Dec 28 08:19:24.974: INFO: Created: latency-svc-nz4bk
Dec 28 08:19:25.019: INFO: Got endpoints: latency-svc-mpv59 [750.014679ms]
Dec 28 08:19:25.025: INFO: Created: latency-svc-bbbkl
Dec 28 08:19:25.068: INFO: Got endpoints: latency-svc-5vqd4 [749.944109ms]
Dec 28 08:19:25.075: INFO: Created: latency-svc-jbwrf
Dec 28 08:19:25.119: INFO: Got endpoints: latency-svc-j2kcl [749.838827ms]
Dec 28 08:19:25.126: INFO: Created: latency-svc-2sq6d
Dec 28 08:19:25.169: INFO: Got endpoints: latency-svc-n5n5d [749.951892ms]
Dec 28 08:19:25.180: INFO: Created: latency-svc-9x7hh
Dec 28 08:19:25.220: INFO: Got endpoints: latency-svc-xjkh6 [751.325709ms]
Dec 28 08:19:25.227: INFO: Created: latency-svc-9cqtq
Dec 28 08:19:25.269: INFO: Got endpoints: latency-svc-7b8x2 [749.304593ms]
Dec 28 08:19:25.277: INFO: Created: latency-svc-crzgr
Dec 28 08:19:25.319: INFO: Got endpoints: latency-svc-hnzcn [750.119237ms]
Dec 28 08:19:25.326: INFO: Created: latency-svc-ffpsf
Dec 28 08:19:25.370: INFO: Got endpoints: latency-svc-xf7ql [751.460827ms]
Dec 28 08:19:25.376: INFO: Created: latency-svc-lv9m8
Dec 28 08:19:25.419: INFO: Got endpoints: latency-svc-6nnfq [750.801095ms]
Dec 28 08:19:25.426: INFO: Created: latency-svc-lfs4l
Dec 28 08:19:25.470: INFO: Got endpoints: latency-svc-6fgwg [751.526608ms]
Dec 28 08:19:25.479: INFO: Created: latency-svc-wkkdb
Dec 28 08:19:25.519: INFO: Got endpoints: latency-svc-dqnkw [726.696687ms]
Dec 28 08:19:25.526: INFO: Created: latency-svc-2tgnk
Dec 28 08:19:25.570: INFO: Got endpoints: latency-svc-6bmb8 [750.953716ms]
Dec 28 08:19:25.578: INFO: Created: latency-svc-t7crs
Dec 28 08:19:25.619: INFO: Got endpoints: latency-svc-swgx6 [750.301365ms]
Dec 28 08:19:25.626: INFO: Created: latency-svc-9hql5
Dec 28 08:19:25.669: INFO: Got endpoints: latency-svc-mpkt6 [750.308138ms]
Dec 28 08:19:25.676: INFO: Created: latency-svc-rt6z5
Dec 28 08:19:25.719: INFO: Got endpoints: latency-svc-nz4bk [750.030302ms]
Dec 28 08:19:25.725: INFO: Created: latency-svc-lwb6t
Dec 28 08:19:25.769: INFO: Got endpoints: latency-svc-bbbkl [749.858052ms]
Dec 28 08:19:25.775: INFO: Created: latency-svc-nkdvd
Dec 28 08:19:25.819: INFO: Got endpoints: latency-svc-jbwrf [750.818156ms]
Dec 28 08:19:25.828: INFO: Created: latency-svc-rp2vb
Dec 28 08:19:25.869: INFO: Got endpoints: latency-svc-2sq6d [750.30505ms]
Dec 28 08:19:25.877: INFO: Created: latency-svc-fvfh4
Dec 28 08:19:25.919: INFO: Got endpoints: latency-svc-9x7hh [750.409033ms]
Dec 28 08:19:25.926: INFO: Created: latency-svc-9g5w9
Dec 28 08:19:25.969: INFO: Got endpoints: latency-svc-9cqtq [749.398139ms]
Dec 28 08:19:25.976: INFO: Created: latency-svc-dbnvw
Dec 28 08:19:26.019: INFO: Got endpoints: latency-svc-crzgr [750.175586ms]
Dec 28 08:19:26.028: INFO: Created: latency-svc-pvhfr
Dec 28 08:19:26.069: INFO: Got endpoints: latency-svc-ffpsf [750.459097ms]
Dec 28 08:19:26.076: INFO: Created: latency-svc-wvb7f
Dec 28 08:19:26.119: INFO: Got endpoints: latency-svc-lv9m8 [749.285765ms]
Dec 28 08:19:26.126: INFO: Created: latency-svc-dpvrx
Dec 28 08:19:26.169: INFO: Got endpoints: latency-svc-lfs4l [750.057706ms]
Dec 28 08:19:26.177: INFO: Created: latency-svc-ltgjs
Dec 28 08:19:26.219: INFO: Got endpoints: latency-svc-wkkdb [748.467473ms]
Dec 28 08:19:26.226: INFO: Created: latency-svc-llz89
Dec 28 08:19:26.269: INFO: Got endpoints: latency-svc-2tgnk [749.604098ms]
Dec 28 08:19:26.275: INFO: Created: latency-svc-lskqr
Dec 28 08:19:26.320: INFO: Got endpoints: latency-svc-t7crs [750.058215ms]
Dec 28 08:19:26.327: INFO: Created: latency-svc-dfj4l
Dec 28 08:19:26.369: INFO: Got endpoints: latency-svc-9hql5 [749.586402ms]
Dec 28 08:19:26.376: INFO: Created: latency-svc-tt7vl
Dec 28 08:19:26.418: INFO: Got endpoints: latency-svc-rt6z5 [749.038032ms]
Dec 28 08:19:26.427: INFO: Created: latency-svc-8h62z
Dec 28 08:19:26.469: INFO: Got endpoints: latency-svc-lwb6t [750.076422ms]
Dec 28 08:19:26.476: INFO: Created: latency-svc-btvst
Dec 28 08:19:26.519: INFO: Got endpoints: latency-svc-nkdvd [749.997098ms]
Dec 28 08:19:26.534: INFO: Created: latency-svc-lnkk6
Dec 28 08:19:26.569: INFO: Got endpoints: latency-svc-rp2vb [749.875604ms]
Dec 28 08:19:26.575: INFO: Created: latency-svc-v7nkk
Dec 28 08:19:26.621: INFO: Got endpoints: latency-svc-fvfh4 [752.316763ms]
Dec 28 08:19:26.629: INFO: Created: latency-svc-hfb64
Dec 28 08:19:26.669: INFO: Got endpoints: latency-svc-9g5w9 [749.809497ms]
Dec 28 08:19:26.676: INFO: Created: latency-svc-nq57h
Dec 28 08:19:26.719: INFO: Got endpoints: latency-svc-dbnvw [749.82267ms]
Dec 28 08:19:26.727: INFO: Created: latency-svc-gn9nh
Dec 28 08:19:26.769: INFO: Got endpoints: latency-svc-pvhfr [749.569332ms]
Dec 28 08:19:26.775: INFO: Created: latency-svc-cqj97
Dec 28 08:19:26.819: INFO: Got endpoints: latency-svc-wvb7f [750.198122ms]
Dec 28 08:19:26.827: INFO: Created: latency-svc-8fj48
Dec 28 08:19:26.869: INFO: Got endpoints: latency-svc-dpvrx [749.568245ms]
Dec 28 08:19:26.875: INFO: Created: latency-svc-h4zmf
Dec 28 08:19:26.919: INFO: Got endpoints: latency-svc-ltgjs [749.594768ms]
Dec 28 08:19:26.926: INFO: Created: latency-svc-d44p4
Dec 28 08:19:26.969: INFO: Got endpoints: latency-svc-llz89 [749.660485ms]
Dec 28 08:19:26.976: INFO: Created: latency-svc-qtl5n
Dec 28 08:19:27.019: INFO: Got endpoints: latency-svc-lskqr [749.570411ms]
Dec 28 08:19:27.026: INFO: Created: latency-svc-dh7gd
Dec 28 08:19:27.069: INFO: Got endpoints: latency-svc-dfj4l [749.322426ms]
Dec 28 08:19:27.077: INFO: Created: latency-svc-mtdxj
Dec 28 08:19:27.119: INFO: Got endpoints: latency-svc-tt7vl [749.94642ms]
Dec 28 08:19:27.125: INFO: Created: latency-svc-zzldz
Dec 28 08:19:27.169: INFO: Got endpoints: latency-svc-8h62z [750.348265ms]
Dec 28 08:19:27.176: INFO: Created: latency-svc-g88sd
Dec 28 08:19:27.219: INFO: Got endpoints: latency-svc-btvst [749.80579ms]
Dec 28 08:19:27.227: INFO: Created: latency-svc-h8tmj
Dec 28 08:19:27.269: INFO: Got endpoints: latency-svc-lnkk6 [750.018154ms]
Dec 28 08:19:27.275: INFO: Created: latency-svc-zfvz7
Dec 28 08:19:27.318: INFO: Got endpoints: latency-svc-v7nkk [749.206822ms]
Dec 28 08:19:27.325: INFO: Created: latency-svc-fzw7j
Dec 28 08:19:27.368: INFO: Got endpoints: latency-svc-hfb64 [746.948365ms]
Dec 28 08:19:27.375: INFO: Created: latency-svc-jnw27
Dec 28 08:19:27.419: INFO: Got endpoints: latency-svc-nq57h [749.901456ms]
Dec 28 08:19:27.425: INFO: Created: latency-svc-45sqf
Dec 28 08:19:27.469: INFO: Got endpoints: latency-svc-gn9nh [749.195317ms]
Dec 28 08:19:27.475: INFO: Created: latency-svc-57bpf
Dec 28 08:19:27.519: INFO: Got endpoints: latency-svc-cqj97 [750.063873ms]
Dec 28 08:19:27.528: INFO: Created: latency-svc-qcnmm
Dec 28 08:19:27.569: INFO: Got endpoints: latency-svc-8fj48 [749.436531ms]
Dec 28 08:19:27.577: INFO: Created: latency-svc-xkz5h
Dec 28 08:19:27.619: INFO: Got endpoints: latency-svc-h4zmf [749.610493ms]
Dec 28 08:19:27.626: INFO: Created: latency-svc-bd5qd
Dec 28 08:19:27.669: INFO: Got endpoints: latency-svc-d44p4 [750.116182ms]
Dec 28 08:19:27.676: INFO: Created: latency-svc-bgtxj
Dec 28 08:19:27.719: INFO: Got endpoints: latency-svc-qtl5n [750.130907ms]
Dec 28 08:19:27.725: INFO: Created: latency-svc-g86m4
Dec 28 08:19:27.769: INFO: Got endpoints: latency-svc-dh7gd [750.642701ms]
Dec 28 08:19:27.776: INFO: Created: latency-svc-ttcpp
Dec 28 08:19:27.819: INFO: Got endpoints: latency-svc-mtdxj [749.470905ms]
Dec 28 08:19:27.826: INFO: Created: latency-svc-jt854
Dec 28 08:19:27.868: INFO: Got endpoints: latency-svc-zzldz [749.693748ms]
Dec 28 08:19:27.875: INFO: Created: latency-svc-52kw8
Dec 28 08:19:27.919: INFO: Got endpoints: latency-svc-g88sd [749.783533ms]
Dec 28 08:19:27.925: INFO: Created: latency-svc-cdrrk
Dec 28 08:19:27.969: INFO: Got endpoints: latency-svc-h8tmj [750.564227ms]
Dec 28 08:19:27.976: INFO: Created: latency-svc-7gqq5
Dec 28 08:19:28.019: INFO: Got endpoints: latency-svc-zfvz7 [749.777104ms]
Dec 28 08:19:28.028: INFO: Created: latency-svc-ghfh2
Dec 28 08:19:28.070: INFO: Got endpoints: latency-svc-fzw7j [751.727613ms]
Dec 28 08:19:28.078: INFO: Created: latency-svc-w64kb
Dec 28 08:19:28.119: INFO: Got endpoints: latency-svc-jnw27 [750.549974ms]
Dec 28 08:19:28.126: INFO: Created: latency-svc-5ftkd
Dec 28 08:19:28.169: INFO: Got endpoints: latency-svc-45sqf [749.920002ms]
Dec 28 08:19:28.175: INFO: Created: latency-svc-sv9w6
Dec 28 08:19:28.219: INFO: Got endpoints: latency-svc-57bpf [750.094632ms]
Dec 28 08:19:28.226: INFO: Created: latency-svc-xkt2b
Dec 28 08:19:28.269: INFO: Got endpoints: latency-svc-qcnmm [749.568257ms]
Dec 28 08:19:28.275: INFO: Created: latency-svc-wl9xn
Dec 28 08:19:28.320: INFO: Got endpoints: latency-svc-xkz5h [751.183216ms]
Dec 28 08:19:28.328: INFO: Created: latency-svc-qcvcc
Dec 28 08:19:28.369: INFO: Got endpoints: latency-svc-bd5qd [750.666993ms]
Dec 28 08:19:28.378: INFO: Created: latency-svc-vgm8j
Dec 28 08:19:28.419: INFO: Got endpoints: latency-svc-bgtxj [749.970019ms]
Dec 28 08:19:28.426: INFO: Created: latency-svc-psszn
Dec 28 08:19:28.469: INFO: Got endpoints: latency-svc-g86m4 [749.695166ms]
Dec 28 08:19:28.475: INFO: Created: latency-svc-8swwm
Dec 28 08:19:28.519: INFO: Got endpoints: latency-svc-ttcpp [749.671142ms]
Dec 28 08:19:28.525: INFO: Created: latency-svc-5vzpd
Dec 28 08:19:28.569: INFO: Got endpoints: latency-svc-jt854 [750.016039ms]
Dec 28 08:19:28.575: INFO: Created: latency-svc-zjpfw
Dec 28 08:19:28.619: INFO: Got endpoints: latency-svc-52kw8 [750.570906ms]
Dec 28 08:19:28.628: INFO: Created: latency-svc-xk7h9
Dec 28 08:19:28.668: INFO: Got endpoints: latency-svc-cdrrk [749.868169ms]
Dec 28 08:19:28.675: INFO: Created: latency-svc-mw557
Dec 28 08:19:28.720: INFO: Got endpoints: latency-svc-7gqq5 [750.194955ms]
Dec 28 08:19:28.727: INFO: Created: latency-svc-dcnrx
Dec 28 08:19:28.770: INFO: Got endpoints: latency-svc-ghfh2 [750.882088ms]
Dec 28 08:19:28.782: INFO: Created: latency-svc-xc56g
Dec 28 08:19:28.819: INFO: Got endpoints: latency-svc-w64kb [748.289243ms]
Dec 28 08:19:28.827: INFO: Created: latency-svc-nrbvg
Dec 28 08:19:28.869: INFO: Got endpoints: latency-svc-5ftkd [750.477102ms]
Dec 28 08:19:28.877: INFO: Created: latency-svc-vkzh4
Dec 28 08:19:28.918: INFO: Got endpoints: latency-svc-sv9w6 [749.473881ms]
Dec 28 08:19:28.969: INFO: Got endpoints: latency-svc-xkt2b [749.843389ms]
Dec 28 08:19:29.020: INFO: Got endpoints: latency-svc-wl9xn [751.287347ms]
Dec 28 08:19:29.070: INFO: Got endpoints: latency-svc-qcvcc [749.643581ms]
Dec 28 08:19:29.121: INFO: Got endpoints: latency-svc-vgm8j [751.092504ms]
Dec 28 08:19:29.169: INFO: Got endpoints: latency-svc-psszn [749.48891ms]
Dec 28 08:19:29.219: INFO: Got endpoints: latency-svc-8swwm [750.04289ms]
Dec 28 08:19:29.269: INFO: Got endpoints: latency-svc-5vzpd [749.59208ms]
Dec 28 08:19:29.319: INFO: Got endpoints: latency-svc-zjpfw [750.261383ms]
Dec 28 08:19:29.368: INFO: Got endpoints: latency-svc-xk7h9 [749.288979ms]
Dec 28 08:19:29.419: INFO: Got endpoints: latency-svc-mw557 [750.012755ms]
Dec 28 08:19:29.470: INFO: Got endpoints: latency-svc-dcnrx [749.953815ms]
Dec 28 08:19:29.519: INFO: Got endpoints: latency-svc-xc56g [749.703829ms]
Dec 28 08:19:29.569: INFO: Got endpoints: latency-svc-nrbvg [750.631674ms]
Dec 28 08:19:29.619: INFO: Got endpoints: latency-svc-vkzh4 [749.192838ms]
Dec 28 08:19:29.619: INFO: Latencies: [9.107689ms 10.137753ms 16.446465ms 19.505879ms 26.001447ms 29.265334ms 35.790738ms 41.810789ms 44.730643ms 50.859057ms 54.430038ms 59.689561ms 68.07163ms 78.038235ms 78.257237ms 80.002253ms 80.400356ms 80.616757ms 80.75279ms 80.949901ms 81.432069ms 81.520377ms 83.424334ms 84.970062ms 85.900012ms 100.510055ms 103.310306ms 107.732653ms 108.357251ms 109.5712ms 110.562441ms 111.925448ms 120.483728ms 151.089165ms 195.342748ms 239.184731ms 283.772129ms 328.215486ms 370.334651ms 415.469502ms 434.542469ms 482.561832ms 523.98754ms 565.653794ms 612.523898ms 652.602465ms 699.851136ms 726.696687ms 731.596082ms 746.948365ms 748.289243ms 748.467473ms 748.601314ms 748.741571ms 748.787237ms 748.949458ms 749.035335ms 749.038032ms 749.047772ms 749.176082ms 749.191595ms 749.192838ms 749.195317ms 749.206822ms 749.209184ms 749.285765ms 749.288979ms 749.304593ms 749.31648ms 749.322426ms 749.33047ms 749.341752ms 749.358395ms 749.398139ms 749.436531ms 749.457499ms 749.460661ms 749.470905ms 749.473881ms 749.48891ms 749.568245ms 749.568257ms 749.569332ms 749.570411ms 749.572319ms 749.586402ms 749.59208ms 749.594768ms 749.604098ms 749.610493ms 749.643581ms 749.660485ms 749.66665ms 749.671142ms 749.693748ms 749.695166ms 749.703829ms 749.705339ms 749.716702ms 749.72777ms 749.743391ms 749.771904ms 749.777104ms 749.781106ms 749.783533ms 749.801271ms 749.80579ms 749.809497ms 749.819107ms 749.82267ms 749.82802ms 749.838648ms 749.838827ms 749.843389ms 749.858052ms 749.868169ms 749.875604ms 749.894198ms 749.901456ms 749.908306ms 749.920002ms 749.928049ms 749.933123ms 749.935468ms 749.944109ms 749.94642ms 749.951892ms 749.953815ms 749.96194ms 749.969775ms 749.970019ms 749.972623ms 749.992652ms 749.997098ms 750.012755ms 750.014679ms 750.016039ms 750.018154ms 750.018229ms 750.030302ms 750.04289ms 750.057706ms 750.058215ms 750.063873ms 750.067042ms 750.071061ms 750.076422ms 750.076573ms 750.094632ms 750.095769ms 750.096736ms 750.116182ms 750.119237ms 750.120521ms 750.130907ms 750.175586ms 750.187039ms 750.194955ms 750.198122ms 750.213048ms 750.261383ms 750.301365ms 750.30505ms 750.308138ms 750.348265ms 750.375278ms 750.400075ms 750.409033ms 750.415963ms 750.459097ms 750.477102ms 750.4791ms 750.488562ms 750.525803ms 750.549974ms 750.564227ms 750.570906ms 750.600304ms 750.631674ms 750.642701ms 750.666235ms 750.666993ms 750.801095ms 750.818156ms 750.882088ms 750.953716ms 750.981905ms 751.090479ms 751.092504ms 751.124023ms 751.183216ms 751.287347ms 751.325709ms 751.349465ms 751.460827ms 751.526608ms 751.727613ms 751.781124ms 752.316763ms 773.209187ms]
Dec 28 08:19:29.619: INFO: 50 %ile: 749.743391ms
Dec 28 08:19:29.619: INFO: 90 %ile: 750.666235ms
Dec 28 08:19:29.619: INFO: 99 %ile: 752.316763ms
Dec 28 08:19:29.619: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:19:29.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-7450" for this suite.
Dec 28 08:19:43.629: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:19:43.695: INFO: namespace svc-latency-7450 deletion completed in 14.073305244s

• [SLOW TEST:24.804 seconds]
[sig-network] Service endpoints latency
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should not be very high  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-network] Services 
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:19:43.695: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service multi-endpoint-test in namespace services-574
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-574 to expose endpoints map[]
Dec 28 08:19:43.722: INFO: Get endpoints failed (3.538338ms elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Dec 28 08:19:44.725: INFO: Get endpoints failed (1.006437132s elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Dec 28 08:19:45.727: INFO: Get endpoints failed (2.008155524s elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Dec 28 08:19:46.729: INFO: Get endpoints failed (3.010040826s elapsed, ignoring for 5s): endpoints "multi-endpoint-test" not found
Dec 28 08:19:47.731: INFO: successfully validated that service multi-endpoint-test in namespace services-574 exposes endpoints map[] (4.012036044s elapsed)
STEP: Creating pod pod1 in namespace services-574
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-574 to expose endpoints map[pod1:[100]]
Dec 28 08:19:49.745: INFO: successfully validated that service multi-endpoint-test in namespace services-574 exposes endpoints map[pod1:[100]] (2.010981775s elapsed)
STEP: Creating pod pod2 in namespace services-574
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-574 to expose endpoints map[pod1:[100] pod2:[101]]
Dec 28 08:19:51.763: INFO: successfully validated that service multi-endpoint-test in namespace services-574 exposes endpoints map[pod1:[100] pod2:[101]] (2.015396515s elapsed)
STEP: Deleting pod pod1 in namespace services-574
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-574 to expose endpoints map[pod2:[101]]
Dec 28 08:19:51.771: INFO: successfully validated that service multi-endpoint-test in namespace services-574 exposes endpoints map[pod2:[101]] (4.794218ms elapsed)
STEP: Deleting pod pod2 in namespace services-574
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-574 to expose endpoints map[]
Dec 28 08:19:52.780: INFO: successfully validated that service multi-endpoint-test in namespace services-574 exposes endpoints map[] (1.003553605s elapsed)
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:19:52.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-574" for this suite.
Dec 28 08:20:04.804: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:20:04.878: INFO: namespace services-574 deletion completed in 12.083268836s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:21.183 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:20:04.878: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 08:20:04.895: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:20:05.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-750" for this suite.
Dec 28 08:20:11.426: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:20:11.495: INFO: namespace custom-resource-definition-750 deletion completed in 6.076765344s

• [SLOW TEST:6.616 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:20:11.495: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 08:20:11.521: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster.
Dec 28 08:20:11.528: INFO: Number of nodes with available pods: 0
Dec 28 08:20:11.529: INFO: Node hxx-m-1 is running more than one daemon pod
Dec 28 08:20:12.534: INFO: Number of nodes with available pods: 0
Dec 28 08:20:12.534: INFO: Node hxx-m-1 is running more than one daemon pod
Dec 28 08:20:13.534: INFO: Number of nodes with available pods: 3
Dec 28 08:20:13.534: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Update daemon pods image.
STEP: Check that daemon pods images are updated.
Dec 28 08:20:13.547: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:13.547: INFO: Wrong image for pod: daemon-set-lb7g7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:13.547: INFO: Wrong image for pod: daemon-set-x9dcq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:14.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:14.552: INFO: Wrong image for pod: daemon-set-lb7g7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:14.552: INFO: Wrong image for pod: daemon-set-x9dcq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:15.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:15.552: INFO: Wrong image for pod: daemon-set-lb7g7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:15.552: INFO: Wrong image for pod: daemon-set-x9dcq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:16.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:16.552: INFO: Wrong image for pod: daemon-set-lb7g7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:16.552: INFO: Wrong image for pod: daemon-set-x9dcq. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:16.552: INFO: Pod daemon-set-x9dcq is not available
Dec 28 08:20:17.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:17.552: INFO: Wrong image for pod: daemon-set-lb7g7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:17.552: INFO: Pod daemon-set-stdqg is not available
Dec 28 08:20:18.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:18.552: INFO: Wrong image for pod: daemon-set-lb7g7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:19.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:19.552: INFO: Wrong image for pod: daemon-set-lb7g7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:19.552: INFO: Pod daemon-set-lb7g7 is not available
Dec 28 08:20:20.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:20.552: INFO: Wrong image for pod: daemon-set-lb7g7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:20.552: INFO: Pod daemon-set-lb7g7 is not available
Dec 28 08:20:21.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:21.552: INFO: Wrong image for pod: daemon-set-lb7g7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:21.552: INFO: Pod daemon-set-lb7g7 is not available
Dec 28 08:20:22.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:22.552: INFO: Wrong image for pod: daemon-set-lb7g7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:22.552: INFO: Pod daemon-set-lb7g7 is not available
Dec 28 08:20:23.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:23.552: INFO: Wrong image for pod: daemon-set-lb7g7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:23.552: INFO: Pod daemon-set-lb7g7 is not available
Dec 28 08:20:24.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:24.552: INFO: Wrong image for pod: daemon-set-lb7g7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:24.552: INFO: Pod daemon-set-lb7g7 is not available
Dec 28 08:20:25.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:25.552: INFO: Wrong image for pod: daemon-set-lb7g7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:25.552: INFO: Pod daemon-set-lb7g7 is not available
Dec 28 08:20:26.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:26.552: INFO: Wrong image for pod: daemon-set-lb7g7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:26.552: INFO: Pod daemon-set-lb7g7 is not available
Dec 28 08:20:27.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:27.552: INFO: Wrong image for pod: daemon-set-lb7g7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:27.552: INFO: Pod daemon-set-lb7g7 is not available
Dec 28 08:20:28.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:28.552: INFO: Wrong image for pod: daemon-set-lb7g7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:28.552: INFO: Pod daemon-set-lb7g7 is not available
Dec 28 08:20:29.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:29.552: INFO: Wrong image for pod: daemon-set-lb7g7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:29.552: INFO: Pod daemon-set-lb7g7 is not available
Dec 28 08:20:30.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:30.552: INFO: Wrong image for pod: daemon-set-lb7g7. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:30.552: INFO: Pod daemon-set-lb7g7 is not available
Dec 28 08:20:31.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:31.552: INFO: Pod daemon-set-s4l44 is not available
Dec 28 08:20:32.553: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:32.553: INFO: Pod daemon-set-s4l44 is not available
Dec 28 08:20:33.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:33.552: INFO: Pod daemon-set-jqf5z is not available
Dec 28 08:20:34.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:34.552: INFO: Pod daemon-set-jqf5z is not available
Dec 28 08:20:35.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:35.552: INFO: Pod daemon-set-jqf5z is not available
Dec 28 08:20:36.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:36.552: INFO: Pod daemon-set-jqf5z is not available
Dec 28 08:20:37.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:37.552: INFO: Pod daemon-set-jqf5z is not available
Dec 28 08:20:38.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:38.552: INFO: Pod daemon-set-jqf5z is not available
Dec 28 08:20:39.552: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:39.552: INFO: Pod daemon-set-jqf5z is not available
Dec 28 08:20:40.553: INFO: Wrong image for pod: daemon-set-jqf5z. Expected: docker.io/library/redis:5.0.5-alpine, got: docker.io/library/httpd:2.4.38-alpine.
Dec 28 08:20:40.553: INFO: Pod daemon-set-jqf5z is not available
Dec 28 08:20:41.552: INFO: Pod daemon-set-jxbnx is not available
STEP: Check that daemon pods are still running on every node of the cluster.
Dec 28 08:20:41.561: INFO: Number of nodes with available pods: 2
Dec 28 08:20:41.561: INFO: Node hxx-m-2 is running more than one daemon pod
Dec 28 08:20:42.567: INFO: Number of nodes with available pods: 2
Dec 28 08:20:42.567: INFO: Node hxx-m-2 is running more than one daemon pod
Dec 28 08:20:43.566: INFO: Number of nodes with available pods: 3
Dec 28 08:20:43.566: INFO: Number of running nodes: 3, number of available pods: 3
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-377, will wait for the garbage collector to delete the pods
Dec 28 08:20:43.630: INFO: Deleting DaemonSet.extensions daemon-set took: 3.852445ms
Dec 28 08:20:44.530: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.221299ms
Dec 28 08:20:57.032: INFO: Number of nodes with available pods: 0
Dec 28 08:20:57.032: INFO: Number of running nodes: 0, number of available pods: 0
Dec 28 08:20:57.033: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-377/daemonsets","resourceVersion":"280091"},"items":null}

Dec 28 08:20:57.035: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-377/pods","resourceVersion":"280091"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:20:57.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-377" for this suite.
Dec 28 08:21:03.052: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:21:03.118: INFO: namespace daemonsets-377 deletion completed in 6.073607691s

• [SLOW TEST:51.624 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-apps] Job 
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:21:03.118: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename job
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a job
STEP: Ensuring active pods == parallelism
STEP: Orphaning one of the Job's Pods
Dec 28 08:21:05.646: INFO: Successfully updated pod "adopt-release-4fpfc"
STEP: Checking that the Job readopts the Pod
Dec 28 08:21:05.646: INFO: Waiting up to 15m0s for pod "adopt-release-4fpfc" in namespace "job-2611" to be "adopted"
Dec 28 08:21:05.648: INFO: Pod "adopt-release-4fpfc": Phase="Running", Reason="", readiness=true. Elapsed: 1.290871ms
Dec 28 08:21:07.650: INFO: Pod "adopt-release-4fpfc": Phase="Running", Reason="", readiness=true. Elapsed: 2.003343297s
Dec 28 08:21:07.650: INFO: Pod "adopt-release-4fpfc" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod
Dec 28 08:21:08.156: INFO: Successfully updated pod "adopt-release-4fpfc"
STEP: Checking that the Job releases the Pod
Dec 28 08:21:08.156: INFO: Waiting up to 15m0s for pod "adopt-release-4fpfc" in namespace "job-2611" to be "released"
Dec 28 08:21:08.159: INFO: Pod "adopt-release-4fpfc": Phase="Running", Reason="", readiness=true. Elapsed: 2.777535ms
Dec 28 08:21:10.161: INFO: Pod "adopt-release-4fpfc": Phase="Running", Reason="", readiness=true. Elapsed: 2.005350575s
Dec 28 08:21:10.161: INFO: Pod "adopt-release-4fpfc" satisfied condition "released"
[AfterEach] [sig-apps] Job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:21:10.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2611" for this suite.
Dec 28 08:21:54.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:21:54.239: INFO: namespace job-2611 deletion completed in 44.074736892s

• [SLOW TEST:51.121 seconds]
[sig-apps] Job
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:21:54.240: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod test-webserver-79ddabc4-580f-4e44-91e6-2ebafc1dc851 in namespace container-probe-3667
Dec 28 08:21:56.263: INFO: Started pod test-webserver-79ddabc4-580f-4e44-91e6-2ebafc1dc851 in namespace container-probe-3667
STEP: checking the pod's current state and verifying that restartCount is present
Dec 28 08:21:56.265: INFO: Initial restart count of pod test-webserver-79ddabc4-580f-4e44-91e6-2ebafc1dc851 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:25:56.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3667" for this suite.
Dec 28 08:26:02.564: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:26:02.634: INFO: namespace container-probe-3667 deletion completed in 6.082885617s

• [SLOW TEST:248.394 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:26:02.634: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-8355
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 28 08:26:02.650: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 28 08:26:24.695: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.199.1.163 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8355 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 28 08:26:24.695: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 08:26:25.838: INFO: Found all expected endpoints: [netserver-0]
Dec 28 08:26:25.841: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.199.2.16 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8355 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 28 08:26:25.841: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 08:26:26.999: INFO: Found all expected endpoints: [netserver-1]
Dec 28 08:26:27.002: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.199.0.53 8081 | grep -v '^\s*$'] Namespace:pod-network-test-8355 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 28 08:26:27.002: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 08:26:28.153: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:26:28.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8355" for this suite.
Dec 28 08:26:40.166: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:26:40.238: INFO: namespace pod-network-test-8355 deletion completed in 12.08089866s

• [SLOW TEST:37.604 seconds]
[sig-network] Networking
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts 
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:26:40.238: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
STEP: reading a file in the container
Dec 28 08:26:42.768: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4590 pod-service-account-cd0419a6-04f3-4ead-901e-0d136b32e741 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container
Dec 28 08:26:43.074: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4590 pod-service-account-cd0419a6-04f3-4ead-901e-0d136b32e741 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container
Dec 28 08:26:43.293: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-4590 pod-service-account-cd0419a6-04f3-4ead-901e-0d136b32e741 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:26:43.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-4590" for this suite.
Dec 28 08:26:49.516: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:26:49.580: INFO: namespace svcaccounts-4590 deletion completed in 6.072657267s

• [SLOW TEST:9.343 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:26:49.581: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 28 08:26:49.601: INFO: Waiting up to 5m0s for pod "downward-api-661591f3-997f-487d-bd9e-d40aadbd8577" in namespace "downward-api-7451" to be "success or failure"
Dec 28 08:26:49.603: INFO: Pod "downward-api-661591f3-997f-487d-bd9e-d40aadbd8577": Phase="Pending", Reason="", readiness=false. Elapsed: 1.650293ms
Dec 28 08:26:51.605: INFO: Pod "downward-api-661591f3-997f-487d-bd9e-d40aadbd8577": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003809066s
STEP: Saw pod success
Dec 28 08:26:51.605: INFO: Pod "downward-api-661591f3-997f-487d-bd9e-d40aadbd8577" satisfied condition "success or failure"
Dec 28 08:26:51.606: INFO: Trying to get logs from node hxx-m-2 pod downward-api-661591f3-997f-487d-bd9e-d40aadbd8577 container dapi-container: <nil>
STEP: delete the pod
Dec 28 08:26:51.625: INFO: Waiting for pod downward-api-661591f3-997f-487d-bd9e-d40aadbd8577 to disappear
Dec 28 08:26:51.629: INFO: Pod downward-api-661591f3-997f-487d-bd9e-d40aadbd8577 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:26:51.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7451" for this suite.
Dec 28 08:26:57.641: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:26:57.704: INFO: namespace downward-api-7451 deletion completed in 6.071041613s

• [SLOW TEST:8.122 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide host IP as an env var [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:26:57.704: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 08:26:57.732: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes.
Dec 28 08:26:57.737: INFO: Number of nodes with available pods: 0
Dec 28 08:26:57.737: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Change node label to blue, check that daemon pod is launched.
Dec 28 08:26:57.747: INFO: Number of nodes with available pods: 0
Dec 28 08:26:57.747: INFO: Node hxx-m-1 is running more than one daemon pod
Dec 28 08:26:58.749: INFO: Number of nodes with available pods: 1
Dec 28 08:26:58.749: INFO: Number of running nodes: 1, number of available pods: 1
STEP: Update the node label to green, and wait for daemons to be unscheduled
Dec 28 08:26:58.758: INFO: Number of nodes with available pods: 1
Dec 28 08:26:58.758: INFO: Number of running nodes: 0, number of available pods: 1
Dec 28 08:26:59.761: INFO: Number of nodes with available pods: 0
Dec 28 08:26:59.761: INFO: Number of running nodes: 0, number of available pods: 0
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate
Dec 28 08:26:59.766: INFO: Number of nodes with available pods: 0
Dec 28 08:26:59.766: INFO: Node hxx-m-1 is running more than one daemon pod
Dec 28 08:27:00.769: INFO: Number of nodes with available pods: 0
Dec 28 08:27:00.769: INFO: Node hxx-m-1 is running more than one daemon pod
Dec 28 08:27:01.768: INFO: Number of nodes with available pods: 0
Dec 28 08:27:01.768: INFO: Node hxx-m-1 is running more than one daemon pod
Dec 28 08:27:02.769: INFO: Number of nodes with available pods: 0
Dec 28 08:27:02.769: INFO: Node hxx-m-1 is running more than one daemon pod
Dec 28 08:27:03.769: INFO: Number of nodes with available pods: 0
Dec 28 08:27:03.769: INFO: Node hxx-m-1 is running more than one daemon pod
Dec 28 08:27:04.769: INFO: Number of nodes with available pods: 0
Dec 28 08:27:04.769: INFO: Node hxx-m-1 is running more than one daemon pod
Dec 28 08:27:05.769: INFO: Number of nodes with available pods: 0
Dec 28 08:27:05.769: INFO: Node hxx-m-1 is running more than one daemon pod
Dec 28 08:27:06.768: INFO: Number of nodes with available pods: 0
Dec 28 08:27:06.768: INFO: Node hxx-m-1 is running more than one daemon pod
Dec 28 08:27:07.769: INFO: Number of nodes with available pods: 0
Dec 28 08:27:07.769: INFO: Node hxx-m-1 is running more than one daemon pod
Dec 28 08:27:08.769: INFO: Number of nodes with available pods: 0
Dec 28 08:27:08.769: INFO: Node hxx-m-1 is running more than one daemon pod
Dec 28 08:27:09.768: INFO: Number of nodes with available pods: 0
Dec 28 08:27:09.769: INFO: Node hxx-m-1 is running more than one daemon pod
Dec 28 08:27:10.769: INFO: Number of nodes with available pods: 0
Dec 28 08:27:10.769: INFO: Node hxx-m-1 is running more than one daemon pod
Dec 28 08:27:11.769: INFO: Number of nodes with available pods: 0
Dec 28 08:27:11.769: INFO: Node hxx-m-1 is running more than one daemon pod
Dec 28 08:27:12.769: INFO: Number of nodes with available pods: 0
Dec 28 08:27:12.769: INFO: Node hxx-m-1 is running more than one daemon pod
Dec 28 08:27:13.768: INFO: Number of nodes with available pods: 1
Dec 28 08:27:13.768: INFO: Number of running nodes: 1, number of available pods: 1
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-223, will wait for the garbage collector to delete the pods
Dec 28 08:27:13.826: INFO: Deleting DaemonSet.extensions daemon-set took: 3.749968ms
Dec 28 08:27:14.726: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.186471ms
Dec 28 08:27:21.228: INFO: Number of nodes with available pods: 0
Dec 28 08:27:21.228: INFO: Number of running nodes: 0, number of available pods: 0
Dec 28 08:27:21.230: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-223/daemonsets","resourceVersion":"281942"},"items":null}

Dec 28 08:27:21.231: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-223/pods","resourceVersion":"281942"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:27:21.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-223" for this suite.
Dec 28 08:27:27.254: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:27:27.321: INFO: namespace daemonsets-223 deletion completed in 6.074023529s

• [SLOW TEST:29.617 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:27:27.321: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 28 08:27:27.552: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 28 08:27:29.559: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118447, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118447, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118447, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118447, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 28 08:27:32.568: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod
STEP: 'kubectl attach' the pod, should be denied by the webhook
Dec 28 08:27:34.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 attach --namespace=webhook-7873 to-be-attached-pod -i -c=container1'
Dec 28 08:27:34.672: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:27:34.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7873" for this suite.
Dec 28 08:27:46.685: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:27:46.751: INFO: namespace webhook-7873 deletion completed in 12.07257682s
STEP: Destroying namespace "webhook-7873-markers" for this suite.
Dec 28 08:27:52.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:27:52.823: INFO: namespace webhook-7873-markers deletion completed in 6.07247761s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:25.509 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:27:52.830: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 28 08:27:52.850: INFO: Waiting up to 5m0s for pod "downward-api-d7fe9ea5-bcb0-479f-9e93-f50c898724f7" in namespace "downward-api-2702" to be "success or failure"
Dec 28 08:27:52.852: INFO: Pod "downward-api-d7fe9ea5-bcb0-479f-9e93-f50c898724f7": Phase="Pending", Reason="", readiness=false. Elapsed: 1.512898ms
Dec 28 08:27:54.854: INFO: Pod "downward-api-d7fe9ea5-bcb0-479f-9e93-f50c898724f7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004136144s
STEP: Saw pod success
Dec 28 08:27:54.854: INFO: Pod "downward-api-d7fe9ea5-bcb0-479f-9e93-f50c898724f7" satisfied condition "success or failure"
Dec 28 08:27:54.856: INFO: Trying to get logs from node hxx-m-2 pod downward-api-d7fe9ea5-bcb0-479f-9e93-f50c898724f7 container dapi-container: <nil>
STEP: delete the pod
Dec 28 08:27:54.869: INFO: Waiting for pod downward-api-d7fe9ea5-bcb0-479f-9e93-f50c898724f7 to disappear
Dec 28 08:27:54.871: INFO: Pod downward-api-d7fe9ea5-bcb0-479f-9e93-f50c898724f7 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:27:54.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2702" for this suite.
Dec 28 08:28:00.881: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:28:00.955: INFO: namespace downward-api-2702 deletion completed in 6.08149101s

• [SLOW TEST:8.125 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:28:00.955: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should be possible to delete [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:28:00.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-283" for this suite.
Dec 28 08:28:06.995: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:28:07.061: INFO: namespace kubelet-test-283 deletion completed in 6.07430966s

• [SLOW TEST:6.106 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should be possible to delete [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not conflict [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:28:07.062: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not conflict [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Cleaning up the secret
STEP: Cleaning up the configmap
STEP: Cleaning up the pod
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:28:09.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9299" for this suite.
Dec 28 08:28:15.114: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:28:15.182: INFO: namespace emptydir-wrapper-9299 deletion completed in 6.075964763s

• [SLOW TEST:8.120 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:28:15.182: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 28 08:28:15.665: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 28 08:28:18.678: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 08:28:18.680: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Creating a v1 custom resource
STEP: v2 custom resource should be converted
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:28:19.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5558" for this suite.
Dec 28 08:28:25.819: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:28:25.888: INFO: namespace crd-webhook-5558 deletion completed in 6.108062118s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:10.714 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose 
  should create services for rc  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:28:25.897: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create services for rc  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Dec 28 08:28:25.914: INFO: namespace kubectl-5365
Dec 28 08:28:25.914: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 create -f - --namespace=kubectl-5365'
Dec 28 08:28:26.110: INFO: stderr: ""
Dec 28 08:28:26.110: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 28 08:28:27.113: INFO: Selector matched 1 pods for map[app:redis]
Dec 28 08:28:27.113: INFO: Found 0 / 1
Dec 28 08:28:28.113: INFO: Selector matched 1 pods for map[app:redis]
Dec 28 08:28:28.113: INFO: Found 1 / 1
Dec 28 08:28:28.113: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 28 08:28:28.114: INFO: Selector matched 1 pods for map[app:redis]
Dec 28 08:28:28.114: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 28 08:28:28.114: INFO: wait on redis-master startup in kubectl-5365 
Dec 28 08:28:28.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 logs redis-master-58wz2 redis-master --namespace=kubectl-5365'
Dec 28 08:28:28.203: INFO: stderr: ""
Dec 28 08:28:28.203: INFO: stdout: "1:C 28 Dec 2019 08:28:26.952 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo\n1:C 28 Dec 2019 08:28:26.952 # Redis version=5.0.5, bits=64, commit=00000000, modified=0, pid=1, just started\n1:C 28 Dec 2019 08:28:26.952 # Warning: no config file specified, using the default config. In order to specify a config file use redis-server /path/to/redis.conf\n1:M 28 Dec 2019 08:28:26.954 * Running mode=standalone, port=6379.\n1:M 28 Dec 2019 08:28:26.954 # WARNING: The TCP backlog setting of 511 cannot be enforced because /proc/sys/net/core/somaxconn is set to the lower value of 128.\n1:M 28 Dec 2019 08:28:26.954 # Server initialized\n1:M 28 Dec 2019 08:28:26.954 # WARNING you have Transparent Huge Pages (THP) support enabled in your kernel. This will create latency and memory usage issues with Redis. To fix this issue run the command 'echo never > /sys/kernel/mm/transparent_hugepage/enabled' as root, and add it to your /etc/rc.local in order to retain the setting after a reboot. Redis must be restarted after THP is disabled.\n1:M 28 Dec 2019 08:28:26.954 * Ready to accept connections\n"
STEP: exposing RC
Dec 28 08:28:28.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 expose rc redis-master --name=rm2 --port=1234 --target-port=6379 --namespace=kubectl-5365'
Dec 28 08:28:28.301: INFO: stderr: ""
Dec 28 08:28:28.301: INFO: stdout: "service/rm2 exposed\n"
Dec 28 08:28:28.303: INFO: Service rm2 in namespace kubectl-5365 found.
STEP: exposing service
Dec 28 08:28:30.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 expose service rm2 --name=rm3 --port=2345 --target-port=6379 --namespace=kubectl-5365'
Dec 28 08:28:30.398: INFO: stderr: ""
Dec 28 08:28:30.398: INFO: stdout: "service/rm3 exposed\n"
Dec 28 08:28:30.400: INFO: Service rm3 in namespace kubectl-5365 found.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:28:32.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5365" for this suite.
Dec 28 08:28:44.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:28:44.481: INFO: namespace kubectl-5365 deletion completed in 12.075084428s

• [SLOW TEST:18.584 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl expose
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1105
    should create services for rc  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:28:44.482: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-173f4a90-4ecd-4a61-9883-b1dc86465542
STEP: Creating a pod to test consume configMaps
Dec 28 08:28:44.503: INFO: Waiting up to 5m0s for pod "pod-configmaps-f82157d5-0663-4cde-8922-c7c8d72a99ff" in namespace "configmap-5996" to be "success or failure"
Dec 28 08:28:44.505: INFO: Pod "pod-configmaps-f82157d5-0663-4cde-8922-c7c8d72a99ff": Phase="Pending", Reason="", readiness=false. Elapsed: 1.554594ms
Dec 28 08:28:46.507: INFO: Pod "pod-configmaps-f82157d5-0663-4cde-8922-c7c8d72a99ff": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003830818s
STEP: Saw pod success
Dec 28 08:28:46.507: INFO: Pod "pod-configmaps-f82157d5-0663-4cde-8922-c7c8d72a99ff" satisfied condition "success or failure"
Dec 28 08:28:46.508: INFO: Trying to get logs from node hxx-m-2 pod pod-configmaps-f82157d5-0663-4cde-8922-c7c8d72a99ff container configmap-volume-test: <nil>
STEP: delete the pod
Dec 28 08:28:46.519: INFO: Waiting for pod pod-configmaps-f82157d5-0663-4cde-8922-c7c8d72a99ff to disappear
Dec 28 08:28:46.520: INFO: Pod pod-configmaps-f82157d5-0663-4cde-8922-c7c8d72a99ff no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:28:46.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5996" for this suite.
Dec 28 08:28:52.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:28:52.593: INFO: namespace configmap-5996 deletion completed in 6.070377937s

• [SLOW TEST:8.112 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:28:52.594: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 28 08:28:52.612: INFO: Waiting up to 5m0s for pod "pod-f018e3db-f1fc-450d-9f91-21858a1f5528" in namespace "emptydir-1096" to be "success or failure"
Dec 28 08:28:52.614: INFO: Pod "pod-f018e3db-f1fc-450d-9f91-21858a1f5528": Phase="Pending", Reason="", readiness=false. Elapsed: 1.504485ms
Dec 28 08:28:54.616: INFO: Pod "pod-f018e3db-f1fc-450d-9f91-21858a1f5528": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003846137s
STEP: Saw pod success
Dec 28 08:28:54.616: INFO: Pod "pod-f018e3db-f1fc-450d-9f91-21858a1f5528" satisfied condition "success or failure"
Dec 28 08:28:54.617: INFO: Trying to get logs from node hxx-m-2 pod pod-f018e3db-f1fc-450d-9f91-21858a1f5528 container test-container: <nil>
STEP: delete the pod
Dec 28 08:28:54.628: INFO: Waiting for pod pod-f018e3db-f1fc-450d-9f91-21858a1f5528 to disappear
Dec 28 08:28:54.631: INFO: Pod pod-f018e3db-f1fc-450d-9f91-21858a1f5528 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:28:54.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1096" for this suite.
Dec 28 08:29:00.642: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:29:00.714: INFO: namespace emptydir-1096 deletion completed in 6.078761174s

• [SLOW TEST:8.121 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:29:00.714: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: closing the watch once it receives two notifications
Dec 28 08:29:00.737: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2091 /api/v1/namespaces/watch-2091/configmaps/e2e-watch-test-watch-closed 233bf493-386e-4a78-adff-8d2c9d678b37 282673 0 2019-12-28 08:29:00 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 28 08:29:00.737: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2091 /api/v1/namespaces/watch-2091/configmaps/e2e-watch-test-watch-closed 233bf493-386e-4a78-adff-8d2c9d678b37 282674 0 2019-12-28 08:29:00 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying the configmap a second time, while the watch is closed
STEP: creating a new watch on configmaps from the last resource version observed by the first watch
STEP: deleting the configmap
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed
Dec 28 08:29:00.744: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2091 /api/v1/namespaces/watch-2091/configmaps/e2e-watch-test-watch-closed 233bf493-386e-4a78-adff-8d2c9d678b37 282675 0 2019-12-28 08:29:00 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 28 08:29:00.744: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2091 /api/v1/namespaces/watch-2091/configmaps/e2e-watch-test-watch-closed 233bf493-386e-4a78-adff-8d2c9d678b37 282676 0 2019-12-28 08:29:00 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:29:00.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2091" for this suite.
Dec 28 08:29:06.752: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:29:06.820: INFO: namespace watch-2091 deletion completed in 6.073700629s

• [SLOW TEST:6.105 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:29:06.820: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support rollover [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 08:29:06.841: INFO: Pod name rollover-pod: Found 0 pods out of 1
Dec 28 08:29:11.843: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 28 08:29:11.843: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Dec 28 08:29:13.846: INFO: Creating deployment "test-rollover-deployment"
Dec 28 08:29:13.850: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Dec 28 08:29:15.854: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Dec 28 08:29:15.857: INFO: Ensure that both replica sets have 1 created replica
Dec 28 08:29:15.860: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Dec 28 08:29:15.864: INFO: Updating deployment test-rollover-deployment
Dec 28 08:29:15.864: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Dec 28 08:29:17.868: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Dec 28 08:29:17.871: INFO: Make sure deployment "test-rollover-deployment" is complete
Dec 28 08:29:17.875: INFO: all replica sets need to contain the pod-template-hash label
Dec 28 08:29:17.875: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118553, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118553, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118556, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118553, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 28 08:29:19.879: INFO: all replica sets need to contain the pod-template-hash label
Dec 28 08:29:19.879: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118553, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118553, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118556, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118553, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 28 08:29:21.879: INFO: all replica sets need to contain the pod-template-hash label
Dec 28 08:29:21.879: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118553, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118553, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118556, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118553, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 28 08:29:23.879: INFO: all replica sets need to contain the pod-template-hash label
Dec 28 08:29:23.879: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118553, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118553, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118556, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118553, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 28 08:29:25.879: INFO: all replica sets need to contain the pod-template-hash label
Dec 28 08:29:25.879: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118553, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118553, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118556, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118553, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-7d7dc6548c\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 28 08:29:27.879: INFO: 
Dec 28 08:29:27.879: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 28 08:29:27.884: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-833 /apis/apps/v1/namespaces/deployment-833/deployments/test-rollover-deployment 09c956c1-a769-4d3e-9553-113d8afdbd81 282864 2 2019-12-28 08:29:13 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0042a0028 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-12-28 08:29:13 +0000 UTC,LastTransitionTime:2019-12-28 08:29:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-7d7dc6548c" has successfully progressed.,LastUpdateTime:2019-12-28 08:29:26 +0000 UTC,LastTransitionTime:2019-12-28 08:29:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 28 08:29:27.886: INFO: New ReplicaSet "test-rollover-deployment-7d7dc6548c" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-7d7dc6548c  deployment-833 /apis/apps/v1/namespaces/deployment-833/replicasets/test-rollover-deployment-7d7dc6548c 8b3e63a8-d4ff-40c2-83ba-176fe2d9cdfb 282853 2 2019-12-28 08:29:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 09c956c1-a769-4d3e-9553-113d8afdbd81 0xc0042a04e7 0xc0042a04e8}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 7d7dc6548c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0042a0548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 28 08:29:27.886: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Dec 28 08:29:27.886: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-833 /apis/apps/v1/namespaces/deployment-833/replicasets/test-rollover-controller a8a9a015-bc29-4dda-b4b3-db5904ccf48f 282863 2 2019-12-28 08:29:06 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 09c956c1-a769-4d3e-9553-113d8afdbd81 0xc0042a0417 0xc0042a0418}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0042a0478 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 28 08:29:27.886: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-f6c94f66c  deployment-833 /apis/apps/v1/namespaces/deployment-833/replicasets/test-rollover-deployment-f6c94f66c f63c8f0f-aa5a-46e6-a41c-8116f539c6ba 282794 2 2019-12-28 08:29:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 09c956c1-a769-4d3e-9553-113d8afdbd81 0xc0042a05b0 0xc0042a05b1}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: f6c94f66c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:f6c94f66c] map[] [] []  []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0042a0628 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 28 08:29:27.888: INFO: Pod "test-rollover-deployment-7d7dc6548c-c52zr" is available:
&Pod{ObjectMeta:{test-rollover-deployment-7d7dc6548c-c52zr test-rollover-deployment-7d7dc6548c- deployment-833 /api/v1/namespaces/deployment-833/pods/test-rollover-deployment-7d7dc6548c-c52zr ac7bdadf-78bc-488c-9fc2-e6a97b0fe009 282808 0 2019-12-28 08:29:15 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:7d7dc6548c] map[] [{apps/v1 ReplicaSet test-rollover-deployment-7d7dc6548c 8b3e63a8-d4ff-40c2-83ba-176fe2d9cdfb 0xc0042a0b87 0xc0042a0b88}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-mt5f6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-mt5f6,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-mt5f6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:hxx-m-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 08:29:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 08:29:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 08:29:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 08:29:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.16,PodIP:10.199.0.68,StartTime:2019-12-28 08:29:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-28 08:29:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://aaa7836f46966300def1f1ff0f3e094f0656d3b4dd1b452aa03f8c9f53b9ec02,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.199.0.68,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:29:27.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-833" for this suite.
Dec 28 08:29:33.897: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:29:33.971: INFO: namespace deployment-833 deletion completed in 6.081021388s

• [SLOW TEST:27.151 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:29:33.971: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 28 08:29:33.992: INFO: Waiting up to 5m0s for pod "downwardapi-volume-aaadd9b6-51b5-4eb6-ab5e-c86550daabf0" in namespace "downward-api-8204" to be "success or failure"
Dec 28 08:29:33.994: INFO: Pod "downwardapi-volume-aaadd9b6-51b5-4eb6-ab5e-c86550daabf0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.568744ms
Dec 28 08:29:35.996: INFO: Pod "downwardapi-volume-aaadd9b6-51b5-4eb6-ab5e-c86550daabf0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003848986s
STEP: Saw pod success
Dec 28 08:29:35.996: INFO: Pod "downwardapi-volume-aaadd9b6-51b5-4eb6-ab5e-c86550daabf0" satisfied condition "success or failure"
Dec 28 08:29:35.998: INFO: Trying to get logs from node hxx-m-2 pod downwardapi-volume-aaadd9b6-51b5-4eb6-ab5e-c86550daabf0 container client-container: <nil>
STEP: delete the pod
Dec 28 08:29:36.009: INFO: Waiting for pod downwardapi-volume-aaadd9b6-51b5-4eb6-ab5e-c86550daabf0 to disappear
Dec 28 08:29:36.010: INFO: Pod downwardapi-volume-aaadd9b6-51b5-4eb6-ab5e-c86550daabf0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:29:36.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8204" for this suite.
Dec 28 08:29:42.021: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:29:42.086: INFO: namespace downward-api-8204 deletion completed in 6.072279979s

• [SLOW TEST:8.114 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:29:42.086: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc1
STEP: create the rc2
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well
STEP: delete the rc simpletest-rc-to-be-deleted
STEP: wait for the rc to be deleted
STEP: Gathering metrics
Dec 28 08:29:52.143: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1228 08:29:52.143537      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:29:52.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4057" for this suite.
Dec 28 08:29:58.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:29:58.220: INFO: namespace gc-4057 deletion completed in 6.074101051s

• [SLOW TEST:16.134 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Proxy server 
  should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:29:58.220: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should support --unix-socket=/path  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Starting the proxy
Dec 28 08:29:58.237: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-076158834 proxy --unix-socket=/tmp/kubectl-proxy-unix479658053/test'
STEP: retrieving proxy /api/ output
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:29:58.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9364" for this suite.
Dec 28 08:30:04.305: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:30:04.370: INFO: namespace kubectl-9364 deletion completed in 6.072588144s

• [SLOW TEST:6.150 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Proxy server
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1782
    should support --unix-socket=/path  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:30:04.371: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicationController
STEP: Ensuring resource quota status captures replication controller creation
STEP: Deleting a ReplicationController
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:30:15.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5161" for this suite.
Dec 28 08:30:21.416: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:30:21.479: INFO: namespace resourcequota-5161 deletion completed in 6.070456175s

• [SLOW TEST:17.109 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API 
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:30:21.480: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 28 08:30:21.500: INFO: Waiting up to 5m0s for pod "downward-api-6d93007d-3a21-4fb1-9afa-002b69c8a04a" in namespace "downward-api-6791" to be "success or failure"
Dec 28 08:30:21.502: INFO: Pod "downward-api-6d93007d-3a21-4fb1-9afa-002b69c8a04a": Phase="Pending", Reason="", readiness=false. Elapsed: 1.945686ms
Dec 28 08:30:23.505: INFO: Pod "downward-api-6d93007d-3a21-4fb1-9afa-002b69c8a04a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004341858s
STEP: Saw pod success
Dec 28 08:30:23.505: INFO: Pod "downward-api-6d93007d-3a21-4fb1-9afa-002b69c8a04a" satisfied condition "success or failure"
Dec 28 08:30:23.506: INFO: Trying to get logs from node hxx-m-2 pod downward-api-6d93007d-3a21-4fb1-9afa-002b69c8a04a container dapi-container: <nil>
STEP: delete the pod
Dec 28 08:30:23.518: INFO: Waiting for pod downward-api-6d93007d-3a21-4fb1-9afa-002b69c8a04a to disappear
Dec 28 08:30:23.519: INFO: Pod downward-api-6d93007d-3a21-4fb1-9afa-002b69c8a04a no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:30:23.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6791" for this suite.
Dec 28 08:30:29.530: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:30:29.602: INFO: namespace downward-api-6791 deletion completed in 6.079150291s

• [SLOW TEST:8.122 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:30:29.602: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should scale a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Dec 28 08:30:29.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 create -f - --namespace=kubectl-8038'
Dec 28 08:30:29.764: INFO: stderr: ""
Dec 28 08:30:29.764: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 28 08:30:29.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8038'
Dec 28 08:30:29.841: INFO: stderr: ""
Dec 28 08:30:29.841: INFO: stdout: "update-demo-nautilus-cct5h update-demo-nautilus-hthgr "
Dec 28 08:30:29.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-cct5h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8038'
Dec 28 08:30:29.910: INFO: stderr: ""
Dec 28 08:30:29.910: INFO: stdout: ""
Dec 28 08:30:29.910: INFO: update-demo-nautilus-cct5h is created but not running
Dec 28 08:30:34.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8038'
Dec 28 08:30:34.980: INFO: stderr: ""
Dec 28 08:30:34.980: INFO: stdout: "update-demo-nautilus-cct5h update-demo-nautilus-hthgr "
Dec 28 08:30:34.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-cct5h -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8038'
Dec 28 08:30:35.057: INFO: stderr: ""
Dec 28 08:30:35.057: INFO: stdout: "true"
Dec 28 08:30:35.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-cct5h -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8038'
Dec 28 08:30:35.131: INFO: stderr: ""
Dec 28 08:30:35.131: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 28 08:30:35.131: INFO: validating pod update-demo-nautilus-cct5h
Dec 28 08:30:35.134: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 28 08:30:35.134: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 28 08:30:35.134: INFO: update-demo-nautilus-cct5h is verified up and running
Dec 28 08:30:35.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-hthgr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8038'
Dec 28 08:30:35.206: INFO: stderr: ""
Dec 28 08:30:35.206: INFO: stdout: "true"
Dec 28 08:30:35.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-hthgr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8038'
Dec 28 08:30:35.284: INFO: stderr: ""
Dec 28 08:30:35.284: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 28 08:30:35.284: INFO: validating pod update-demo-nautilus-hthgr
Dec 28 08:30:35.287: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 28 08:30:35.287: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 28 08:30:35.287: INFO: update-demo-nautilus-hthgr is verified up and running
STEP: scaling down the replication controller
Dec 28 08:30:35.288: INFO: scanned /root for discovery docs: <nil>
Dec 28 08:30:35.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 scale rc update-demo-nautilus --replicas=1 --timeout=5m --namespace=kubectl-8038'
Dec 28 08:30:36.379: INFO: stderr: ""
Dec 28 08:30:36.379: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 28 08:30:36.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8038'
Dec 28 08:30:36.454: INFO: stderr: ""
Dec 28 08:30:36.454: INFO: stdout: "update-demo-nautilus-cct5h update-demo-nautilus-hthgr "
STEP: Replicas for name=update-demo: expected=1 actual=2
Dec 28 08:30:41.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8038'
Dec 28 08:30:41.536: INFO: stderr: ""
Dec 28 08:30:41.536: INFO: stdout: "update-demo-nautilus-hthgr "
Dec 28 08:30:41.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-hthgr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8038'
Dec 28 08:30:41.612: INFO: stderr: ""
Dec 28 08:30:41.612: INFO: stdout: "true"
Dec 28 08:30:41.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-hthgr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8038'
Dec 28 08:30:41.686: INFO: stderr: ""
Dec 28 08:30:41.686: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 28 08:30:41.686: INFO: validating pod update-demo-nautilus-hthgr
Dec 28 08:30:41.688: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 28 08:30:41.688: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 28 08:30:41.688: INFO: update-demo-nautilus-hthgr is verified up and running
STEP: scaling up the replication controller
Dec 28 08:30:41.690: INFO: scanned /root for discovery docs: <nil>
Dec 28 08:30:41.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 scale rc update-demo-nautilus --replicas=2 --timeout=5m --namespace=kubectl-8038'
Dec 28 08:30:41.778: INFO: stderr: ""
Dec 28 08:30:41.778: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 28 08:30:41.778: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8038'
Dec 28 08:30:41.858: INFO: stderr: ""
Dec 28 08:30:41.858: INFO: stdout: "update-demo-nautilus-hthgr update-demo-nautilus-kltgf "
Dec 28 08:30:41.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-hthgr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8038'
Dec 28 08:30:41.934: INFO: stderr: ""
Dec 28 08:30:41.934: INFO: stdout: "true"
Dec 28 08:30:41.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-hthgr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8038'
Dec 28 08:30:42.009: INFO: stderr: ""
Dec 28 08:30:42.009: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 28 08:30:42.009: INFO: validating pod update-demo-nautilus-hthgr
Dec 28 08:30:42.011: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 28 08:30:42.011: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 28 08:30:42.011: INFO: update-demo-nautilus-hthgr is verified up and running
Dec 28 08:30:42.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-kltgf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8038'
Dec 28 08:30:42.087: INFO: stderr: ""
Dec 28 08:30:42.087: INFO: stdout: ""
Dec 28 08:30:42.087: INFO: update-demo-nautilus-kltgf is created but not running
Dec 28 08:30:47.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-8038'
Dec 28 08:30:47.164: INFO: stderr: ""
Dec 28 08:30:47.164: INFO: stdout: "update-demo-nautilus-hthgr update-demo-nautilus-kltgf "
Dec 28 08:30:47.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-hthgr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8038'
Dec 28 08:30:47.235: INFO: stderr: ""
Dec 28 08:30:47.235: INFO: stdout: "true"
Dec 28 08:30:47.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-hthgr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8038'
Dec 28 08:30:47.308: INFO: stderr: ""
Dec 28 08:30:47.308: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 28 08:30:47.308: INFO: validating pod update-demo-nautilus-hthgr
Dec 28 08:30:47.310: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 28 08:30:47.310: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 28 08:30:47.310: INFO: update-demo-nautilus-hthgr is verified up and running
Dec 28 08:30:47.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-kltgf -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-8038'
Dec 28 08:30:47.385: INFO: stderr: ""
Dec 28 08:30:47.385: INFO: stdout: "true"
Dec 28 08:30:47.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-kltgf -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-8038'
Dec 28 08:30:47.456: INFO: stderr: ""
Dec 28 08:30:47.456: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 28 08:30:47.456: INFO: validating pod update-demo-nautilus-kltgf
Dec 28 08:30:47.460: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 28 08:30:47.460: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 28 08:30:47.460: INFO: update-demo-nautilus-kltgf is verified up and running
STEP: using delete to clean up resources
Dec 28 08:30:47.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 delete --grace-period=0 --force -f - --namespace=kubectl-8038'
Dec 28 08:30:47.531: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 28 08:30:47.531: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 28 08:30:47.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8038'
Dec 28 08:30:47.608: INFO: stderr: "No resources found in kubectl-8038 namespace.\n"
Dec 28 08:30:47.608: INFO: stdout: ""
Dec 28 08:30:47.609: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods -l name=update-demo --namespace=kubectl-8038 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 28 08:30:47.680: INFO: stderr: ""
Dec 28 08:30:47.680: INFO: stdout: "update-demo-nautilus-hthgr\nupdate-demo-nautilus-kltgf\n"
Dec 28 08:30:48.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8038'
Dec 28 08:30:48.260: INFO: stderr: "No resources found in kubectl-8038 namespace.\n"
Dec 28 08:30:48.260: INFO: stdout: ""
Dec 28 08:30:48.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods -l name=update-demo --namespace=kubectl-8038 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 28 08:30:48.338: INFO: stderr: ""
Dec 28 08:30:48.338: INFO: stdout: "update-demo-nautilus-hthgr\nupdate-demo-nautilus-kltgf\n"
Dec 28 08:30:48.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-8038'
Dec 28 08:30:48.769: INFO: stderr: "No resources found in kubectl-8038 namespace.\n"
Dec 28 08:30:48.769: INFO: stdout: ""
Dec 28 08:30:48.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods -l name=update-demo --namespace=kubectl-8038 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 28 08:30:48.848: INFO: stderr: ""
Dec 28 08:30:48.848: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:30:48.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8038" for this suite.
Dec 28 08:31:16.859: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:31:16.923: INFO: namespace kubectl-8038 deletion completed in 28.072181558s

• [SLOW TEST:47.321 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should scale a replication controller  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:31:16.923: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating secret secrets-3232/secret-test-65303238-7a41-41c0-8909-831e8bc30d44
STEP: Creating a pod to test consume secrets
Dec 28 08:31:16.947: INFO: Waiting up to 5m0s for pod "pod-configmaps-60e58bb9-d0b0-490b-81a7-5e3379e6303c" in namespace "secrets-3232" to be "success or failure"
Dec 28 08:31:16.948: INFO: Pod "pod-configmaps-60e58bb9-d0b0-490b-81a7-5e3379e6303c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.324829ms
Dec 28 08:31:18.950: INFO: Pod "pod-configmaps-60e58bb9-d0b0-490b-81a7-5e3379e6303c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003777557s
STEP: Saw pod success
Dec 28 08:31:18.950: INFO: Pod "pod-configmaps-60e58bb9-d0b0-490b-81a7-5e3379e6303c" satisfied condition "success or failure"
Dec 28 08:31:18.952: INFO: Trying to get logs from node hxx-m-2 pod pod-configmaps-60e58bb9-d0b0-490b-81a7-5e3379e6303c container env-test: <nil>
STEP: delete the pod
Dec 28 08:31:18.966: INFO: Waiting for pod pod-configmaps-60e58bb9-d0b0-490b-81a7-5e3379e6303c to disappear
Dec 28 08:31:18.968: INFO: Pod pod-configmaps-60e58bb9-d0b0-490b-81a7-5e3379e6303c no longer exists
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:31:18.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3232" for this suite.
Dec 28 08:31:24.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:31:25.043: INFO: namespace secrets-3232 deletion completed in 6.071987102s

• [SLOW TEST:8.119 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Docker Containers 
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:31:25.043: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename containers
STEP: Waiting for a default service account to be provisioned in namespace
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Docker Containers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:31:27.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-851" for this suite.
Dec 28 08:31:45.082: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:31:45.149: INFO: namespace containers-851 deletion completed in 18.075849545s

• [SLOW TEST:20.106 seconds]
[k8s.io] Docker Containers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:31:45.149: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-configmap-vj99
STEP: Creating a pod to test atomic-volume-subpath
Dec 28 08:31:45.174: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-vj99" in namespace "subpath-5933" to be "success or failure"
Dec 28 08:31:45.176: INFO: Pod "pod-subpath-test-configmap-vj99": Phase="Pending", Reason="", readiness=false. Elapsed: 1.452996ms
Dec 28 08:31:47.178: INFO: Pod "pod-subpath-test-configmap-vj99": Phase="Running", Reason="", readiness=true. Elapsed: 2.003483311s
Dec 28 08:31:49.179: INFO: Pod "pod-subpath-test-configmap-vj99": Phase="Running", Reason="", readiness=true. Elapsed: 4.005260107s
Dec 28 08:31:51.181: INFO: Pod "pod-subpath-test-configmap-vj99": Phase="Running", Reason="", readiness=true. Elapsed: 6.007119628s
Dec 28 08:31:53.183: INFO: Pod "pod-subpath-test-configmap-vj99": Phase="Running", Reason="", readiness=true. Elapsed: 8.009030645s
Dec 28 08:31:55.185: INFO: Pod "pod-subpath-test-configmap-vj99": Phase="Running", Reason="", readiness=true. Elapsed: 10.010969002s
Dec 28 08:31:57.188: INFO: Pod "pod-subpath-test-configmap-vj99": Phase="Running", Reason="", readiness=true. Elapsed: 12.013467268s
Dec 28 08:31:59.190: INFO: Pod "pod-subpath-test-configmap-vj99": Phase="Running", Reason="", readiness=true. Elapsed: 14.01540124s
Dec 28 08:32:01.191: INFO: Pod "pod-subpath-test-configmap-vj99": Phase="Running", Reason="", readiness=true. Elapsed: 16.01717729s
Dec 28 08:32:03.193: INFO: Pod "pod-subpath-test-configmap-vj99": Phase="Running", Reason="", readiness=true. Elapsed: 18.019139565s
Dec 28 08:32:05.196: INFO: Pod "pod-subpath-test-configmap-vj99": Phase="Running", Reason="", readiness=true. Elapsed: 20.02159214s
Dec 28 08:32:07.198: INFO: Pod "pod-subpath-test-configmap-vj99": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.023666188s
STEP: Saw pod success
Dec 28 08:32:07.198: INFO: Pod "pod-subpath-test-configmap-vj99" satisfied condition "success or failure"
Dec 28 08:32:07.199: INFO: Trying to get logs from node hxx-m-2 pod pod-subpath-test-configmap-vj99 container test-container-subpath-configmap-vj99: <nil>
STEP: delete the pod
Dec 28 08:32:07.211: INFO: Waiting for pod pod-subpath-test-configmap-vj99 to disappear
Dec 28 08:32:07.212: INFO: Pod pod-subpath-test-configmap-vj99 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-vj99
Dec 28 08:32:07.212: INFO: Deleting pod "pod-subpath-test-configmap-vj99" in namespace "subpath-5933"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:32:07.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5933" for this suite.
Dec 28 08:32:13.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:32:13.288: INFO: namespace subpath-5933 deletion completed in 6.071924596s

• [SLOW TEST:28.139 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with configmap pod with mountPath of existing file [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:32:13.288: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 28 08:32:13.307: INFO: Waiting up to 5m0s for pod "downwardapi-volume-7eca2226-4f0e-47c1-a162-f1d24f7ac517" in namespace "projected-4720" to be "success or failure"
Dec 28 08:32:13.309: INFO: Pod "downwardapi-volume-7eca2226-4f0e-47c1-a162-f1d24f7ac517": Phase="Pending", Reason="", readiness=false. Elapsed: 1.908919ms
Dec 28 08:32:15.312: INFO: Pod "downwardapi-volume-7eca2226-4f0e-47c1-a162-f1d24f7ac517": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004500301s
STEP: Saw pod success
Dec 28 08:32:15.312: INFO: Pod "downwardapi-volume-7eca2226-4f0e-47c1-a162-f1d24f7ac517" satisfied condition "success or failure"
Dec 28 08:32:15.313: INFO: Trying to get logs from node hxx-m-2 pod downwardapi-volume-7eca2226-4f0e-47c1-a162-f1d24f7ac517 container client-container: <nil>
STEP: delete the pod
Dec 28 08:32:15.323: INFO: Waiting for pod downwardapi-volume-7eca2226-4f0e-47c1-a162-f1d24f7ac517 to disappear
Dec 28 08:32:15.325: INFO: Pod downwardapi-volume-7eca2226-4f0e-47c1-a162-f1d24f7ac517 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:32:15.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4720" for this suite.
Dec 28 08:32:21.335: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:32:21.404: INFO: namespace projected-4720 deletion completed in 6.076559258s

• [SLOW TEST:8.116 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:32:21.404: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: starting a background goroutine to produce watch events
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:32:25.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1539" for this suite.
Dec 28 08:32:32.111: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:32:32.181: INFO: namespace watch-1539 deletion completed in 6.171943491s

• [SLOW TEST:10.777 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:32:32.182: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-2d8ecb67-bb4a-4034-bf54-d25a3f88bc4e
STEP: Creating a pod to test consume configMaps
Dec 28 08:32:32.204: INFO: Waiting up to 5m0s for pod "pod-configmaps-f7f08aef-2efb-4af0-ae22-68d50ebbb7f0" in namespace "configmap-3164" to be "success or failure"
Dec 28 08:32:32.205: INFO: Pod "pod-configmaps-f7f08aef-2efb-4af0-ae22-68d50ebbb7f0": Phase="Pending", Reason="", readiness=false. Elapsed: 1.372249ms
Dec 28 08:32:34.208: INFO: Pod "pod-configmaps-f7f08aef-2efb-4af0-ae22-68d50ebbb7f0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003792016s
STEP: Saw pod success
Dec 28 08:32:34.208: INFO: Pod "pod-configmaps-f7f08aef-2efb-4af0-ae22-68d50ebbb7f0" satisfied condition "success or failure"
Dec 28 08:32:34.209: INFO: Trying to get logs from node hxx-m-2 pod pod-configmaps-f7f08aef-2efb-4af0-ae22-68d50ebbb7f0 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 28 08:32:34.221: INFO: Waiting for pod pod-configmaps-f7f08aef-2efb-4af0-ae22-68d50ebbb7f0 to disappear
Dec 28 08:32:34.223: INFO: Pod pod-configmaps-f7f08aef-2efb-4af0-ae22-68d50ebbb7f0 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:32:34.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3164" for this suite.
Dec 28 08:32:40.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:32:40.303: INFO: namespace configmap-3164 deletion completed in 6.07776805s

• [SLOW TEST:8.122 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:32:40.304: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 28 08:32:40.323: INFO: Waiting up to 5m0s for pod "downwardapi-volume-30f2baa2-e818-4928-8227-b928767e9df6" in namespace "projected-2329" to be "success or failure"
Dec 28 08:32:40.325: INFO: Pod "downwardapi-volume-30f2baa2-e818-4928-8227-b928767e9df6": Phase="Pending", Reason="", readiness=false. Elapsed: 1.634966ms
Dec 28 08:32:42.327: INFO: Pod "downwardapi-volume-30f2baa2-e818-4928-8227-b928767e9df6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003792433s
STEP: Saw pod success
Dec 28 08:32:42.327: INFO: Pod "downwardapi-volume-30f2baa2-e818-4928-8227-b928767e9df6" satisfied condition "success or failure"
Dec 28 08:32:42.328: INFO: Trying to get logs from node hxx-m-2 pod downwardapi-volume-30f2baa2-e818-4928-8227-b928767e9df6 container client-container: <nil>
STEP: delete the pod
Dec 28 08:32:42.340: INFO: Waiting for pod downwardapi-volume-30f2baa2-e818-4928-8227-b928767e9df6 to disappear
Dec 28 08:32:42.341: INFO: Pod downwardapi-volume-30f2baa2-e818-4928-8227-b928767e9df6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:32:42.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-2329" for this suite.
Dec 28 08:32:48.350: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:32:48.420: INFO: namespace projected-2329 deletion completed in 6.075982034s

• [SLOW TEST:8.116 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Security Context When creating a pod with readOnlyRootFilesystem 
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:32:48.420: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 08:32:48.441: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-a853fc69-e8a0-4127-9d13-56afa8072be2" in namespace "security-context-test-7150" to be "success or failure"
Dec 28 08:32:48.444: INFO: Pod "busybox-readonly-false-a853fc69-e8a0-4127-9d13-56afa8072be2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.419196ms
Dec 28 08:32:50.446: INFO: Pod "busybox-readonly-false-a853fc69-e8a0-4127-9d13-56afa8072be2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004937665s
Dec 28 08:32:50.446: INFO: Pod "busybox-readonly-false-a853fc69-e8a0-4127-9d13-56afa8072be2" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:32:50.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7150" for this suite.
Dec 28 08:32:56.457: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:32:56.526: INFO: namespace security-context-test-7150 deletion completed in 6.076405246s

• [SLOW TEST:8.106 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with readOnlyRootFilesystem
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:165
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test when starting a container that exits 
  should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:32:56.526: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should run with the expected status [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpa': should get the expected 'State'
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpof': should get the expected 'State'
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance]
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase'
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition
STEP: Container 'terminate-cmd-rpn': should get the expected 'State'
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance]
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:33:17.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-241" for this suite.
Dec 28 08:33:23.655: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:33:23.724: INFO: namespace container-runtime-241 deletion completed in 6.077667244s

• [SLOW TEST:27.198 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    when starting a container that exits
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:40
      should run with the expected status [NodeConformance] [Conformance]
      /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:33:23.724: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 28 08:33:24.043: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 28 08:33:26.048: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118804, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118804, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118804, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713118804, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 28 08:33:29.056: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API
STEP: create a namespace for the webhook
STEP: create a configmap should be unconditionally rejected by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:33:29.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-6964" for this suite.
Dec 28 08:33:35.092: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:33:35.161: INFO: namespace webhook-6964 deletion completed in 6.075711459s
STEP: Destroying namespace "webhook-6964-markers" for this suite.
Dec 28 08:33:41.168: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:33:41.233: INFO: namespace webhook-6964-markers deletion completed in 6.071966811s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.516 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:33:41.240: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 28 08:33:42.030: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 28 08:33:45.042: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the webhook via the AdmissionRegistration API
STEP: create a pod that should be denied by the webhook
STEP: create a pod that causes the webhook to hang
STEP: create a configmap that should be denied by the webhook
STEP: create a configmap that should be admitted by the webhook
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook
STEP: create a namespace that bypass the webhook
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:33:55.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3462" for this suite.
Dec 28 08:34:01.115: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:34:01.186: INFO: namespace webhook-3462 deletion completed in 6.078369837s
STEP: Destroying namespace "webhook-3462-markers" for this suite.
Dec 28 08:34:07.193: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:34:07.266: INFO: namespace webhook-3462-markers deletion completed in 6.079551015s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:26.034 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:34:07.274: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: set up a multi version CRD
Dec 28 08:34:07.291: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: mark a version not serverd
STEP: check the unserved version gets removed
STEP: check the other version is not changed
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:34:26.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4686" for this suite.
Dec 28 08:34:32.494: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:34:32.567: INFO: namespace crd-publish-openapi-4686 deletion completed in 6.079936943s

• [SLOW TEST:25.292 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:34:32.567: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 08:34:32.585: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota
STEP: Checking rc "condition-test" has the desired failure condition set
STEP: Scaling down rc "condition-test" to satisfy pod quota
Dec 28 08:34:34.600: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:34:35.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5801" for this suite.
Dec 28 08:34:41.614: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:34:41.680: INFO: namespace replication-controller-5801 deletion completed in 6.073486033s

• [SLOW TEST:9.113 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:34:41.680: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating Pod
STEP: Waiting for the pod running
STEP: Geting the pod
STEP: Reading file content from the nginx-container
Dec 28 08:34:45.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec pod-sharedvolume-2d92fc01-eddc-43ea-a4e8-38e7a0ff102e -c busybox-main-container --namespace=emptydir-806 -- cat /usr/share/volumeshare/shareddata.txt'
Dec 28 08:34:45.931: INFO: stderr: ""
Dec 28 08:34:45.931: INFO: stdout: "Hello from the busy-box sub-container\n"
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:34:45.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-806" for this suite.
Dec 28 08:34:51.942: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:34:52.011: INFO: namespace emptydir-806 deletion completed in 6.075821179s

• [SLOW TEST:10.330 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  pod should support shared volumes between containers [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:34:52.011: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation
Dec 28 08:34:52.028: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 08:34:55.755: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:35:10.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7032" for this suite.
Dec 28 08:35:16.444: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:35:16.509: INFO: namespace crd-publish-openapi-7032 deletion completed in 6.072612395s

• [SLOW TEST:24.499 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:35:16.510: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-743
[It] Should recreate evicted statefulset [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Looking for a node to schedule stateful set and pod
STEP: Creating pod with conflicting port in namespace statefulset-743
STEP: Creating statefulset with conflicting port in namespace statefulset-743
STEP: Waiting until pod test-pod will start running in namespace statefulset-743
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-743
Dec 28 08:35:20.544: INFO: Observed stateful pod in namespace: statefulset-743, name: ss-0, uid: cfd8ab32-654a-4811-9aa9-75b897aaebe7, status phase: Pending. Waiting for statefulset controller to delete.
Dec 28 08:35:20.740: INFO: Observed stateful pod in namespace: statefulset-743, name: ss-0, uid: cfd8ab32-654a-4811-9aa9-75b897aaebe7, status phase: Failed. Waiting for statefulset controller to delete.
Dec 28 08:35:20.743: INFO: Observed stateful pod in namespace: statefulset-743, name: ss-0, uid: cfd8ab32-654a-4811-9aa9-75b897aaebe7, status phase: Failed. Waiting for statefulset controller to delete.
Dec 28 08:35:20.745: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-743
STEP: Removing pod with conflicting port in namespace statefulset-743
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-743 and will be in running state
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 28 08:35:24.757: INFO: Deleting all statefulset in ns statefulset-743
Dec 28 08:35:24.759: INFO: Scaling statefulset ss to 0
Dec 28 08:35:34.769: INFO: Waiting for statefulset status.replicas updated to 0
Dec 28 08:35:34.770: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:35:34.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-743" for this suite.
Dec 28 08:35:40.788: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:35:40.857: INFO: namespace statefulset-743 deletion completed in 6.077808268s

• [SLOW TEST:24.347 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Should recreate evicted statefulset [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] DNS 
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:35:40.857: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3992.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3992.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3992.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@PodARecord;sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3992.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3992.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;podARec=$$(hostname -i| awk -F. '{print $$1"-"$$2"-"$$3"-"$$4".dns-3992.pod.cluster.local"}');check="$$(dig +notcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_udp@PodARecord;check="$$(dig +tcp +noall +answer +search $${podARec} A)" && test -n "$$check" && echo OK > /results/jessie_tcp@PodARecord;sleep 1; done

STEP: creating a pod to probe /etc/hosts
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 28 08:35:42.898: INFO: DNS probes using dns-3992/dns-test-006d00d1-cce1-4c65-88bb-d5d4da2c58c1 succeeded

STEP: deleting the pod
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:35:42.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3992" for this suite.
Dec 28 08:35:48.922: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:35:48.989: INFO: namespace dns-3992 deletion completed in 6.084190349s

• [SLOW TEST:8.132 seconds]
[sig-network] DNS
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide /etc/hosts entries for the cluster [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:35:48.990: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] creating/deleting custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 08:35:49.006: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:35:50.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9068" for this suite.
Dec 28 08:35:56.029: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:35:56.093: INFO: namespace custom-resource-definition-9068 deletion completed in 6.070501841s

• [SLOW TEST:7.103 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    creating/deleting custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:35:56.093: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-fc2dba15-9b3a-4d86-862a-dd224c5c95e6
STEP: Creating a pod to test consume secrets
Dec 28 08:35:56.129: INFO: Waiting up to 5m0s for pod "pod-secrets-5b005efe-47fe-4165-8864-ce4c59fe0a01" in namespace "secrets-3974" to be "success or failure"
Dec 28 08:35:56.130: INFO: Pod "pod-secrets-5b005efe-47fe-4165-8864-ce4c59fe0a01": Phase="Pending", Reason="", readiness=false. Elapsed: 1.380532ms
Dec 28 08:35:58.133: INFO: Pod "pod-secrets-5b005efe-47fe-4165-8864-ce4c59fe0a01": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003652767s
STEP: Saw pod success
Dec 28 08:35:58.133: INFO: Pod "pod-secrets-5b005efe-47fe-4165-8864-ce4c59fe0a01" satisfied condition "success or failure"
Dec 28 08:35:58.134: INFO: Trying to get logs from node hxx-m-2 pod pod-secrets-5b005efe-47fe-4165-8864-ce4c59fe0a01 container secret-volume-test: <nil>
STEP: delete the pod
Dec 28 08:35:58.150: INFO: Waiting for pod pod-secrets-5b005efe-47fe-4165-8864-ce4c59fe0a01 to disappear
Dec 28 08:35:58.151: INFO: Pod pod-secrets-5b005efe-47fe-4165-8864-ce4c59fe0a01 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:35:58.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3974" for this suite.
Dec 28 08:36:04.161: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:36:04.226: INFO: namespace secrets-3974 deletion completed in 6.07250571s
STEP: Destroying namespace "secret-namespace-1196" for this suite.
Dec 28 08:36:10.233: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:36:10.299: INFO: namespace secret-namespace-1196 deletion completed in 6.072828951s

• [SLOW TEST:14.206 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation 
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:36:10.299: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename tables
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/table_conversion.go:47
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:36:10.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-398" for this suite.
Dec 28 08:36:16.328: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:36:16.394: INFO: namespace tables-398 deletion completed in 6.073415179s

• [SLOW TEST:6.095 seconds]
[sig-api-machinery] Servers with support for Table transformation
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:36:16.395: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with projected pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-projected-x6wg
STEP: Creating a pod to test atomic-volume-subpath
Dec 28 08:36:16.418: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-x6wg" in namespace "subpath-7794" to be "success or failure"
Dec 28 08:36:16.420: INFO: Pod "pod-subpath-test-projected-x6wg": Phase="Pending", Reason="", readiness=false. Elapsed: 1.356564ms
Dec 28 08:36:18.422: INFO: Pod "pod-subpath-test-projected-x6wg": Phase="Running", Reason="", readiness=true. Elapsed: 2.003699311s
Dec 28 08:36:20.425: INFO: Pod "pod-subpath-test-projected-x6wg": Phase="Running", Reason="", readiness=true. Elapsed: 4.006375372s
Dec 28 08:36:22.429: INFO: Pod "pod-subpath-test-projected-x6wg": Phase="Running", Reason="", readiness=true. Elapsed: 6.010802848s
Dec 28 08:36:24.432: INFO: Pod "pod-subpath-test-projected-x6wg": Phase="Running", Reason="", readiness=true. Elapsed: 8.013436881s
Dec 28 08:36:26.434: INFO: Pod "pod-subpath-test-projected-x6wg": Phase="Running", Reason="", readiness=true. Elapsed: 10.015879033s
Dec 28 08:36:28.437: INFO: Pod "pod-subpath-test-projected-x6wg": Phase="Running", Reason="", readiness=true. Elapsed: 12.018362941s
Dec 28 08:36:30.439: INFO: Pod "pod-subpath-test-projected-x6wg": Phase="Running", Reason="", readiness=true. Elapsed: 14.020842567s
Dec 28 08:36:32.442: INFO: Pod "pod-subpath-test-projected-x6wg": Phase="Running", Reason="", readiness=true. Elapsed: 16.023675073s
Dec 28 08:36:34.444: INFO: Pod "pod-subpath-test-projected-x6wg": Phase="Running", Reason="", readiness=true. Elapsed: 18.025947131s
Dec 28 08:36:36.446: INFO: Pod "pod-subpath-test-projected-x6wg": Phase="Running", Reason="", readiness=true. Elapsed: 20.028250711s
Dec 28 08:36:38.449: INFO: Pod "pod-subpath-test-projected-x6wg": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.030623037s
STEP: Saw pod success
Dec 28 08:36:38.449: INFO: Pod "pod-subpath-test-projected-x6wg" satisfied condition "success or failure"
Dec 28 08:36:38.450: INFO: Trying to get logs from node hxx-m-2 pod pod-subpath-test-projected-x6wg container test-container-subpath-projected-x6wg: <nil>
STEP: delete the pod
Dec 28 08:36:38.460: INFO: Waiting for pod pod-subpath-test-projected-x6wg to disappear
Dec 28 08:36:38.461: INFO: Pod pod-subpath-test-projected-x6wg no longer exists
STEP: Deleting pod pod-subpath-test-projected-x6wg
Dec 28 08:36:38.461: INFO: Deleting pod "pod-subpath-test-projected-x6wg" in namespace "subpath-7794"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:36:38.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7794" for this suite.
Dec 28 08:36:44.472: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:36:44.540: INFO: namespace subpath-7794 deletion completed in 6.074870111s

• [SLOW TEST:28.145 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with projected pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] KubeletManagedEtcHosts 
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:36:44.540: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Setting up the test
STEP: Creating hostNetwork=false pod
STEP: Creating hostNetwork=true pod
STEP: Running the test
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false
Dec 28 08:36:48.574: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9444 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 28 08:36:48.574: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 08:36:48.717: INFO: Exec stderr: ""
Dec 28 08:36:48.717: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9444 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 28 08:36:48.717: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 08:36:48.894: INFO: Exec stderr: ""
Dec 28 08:36:48.894: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9444 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 28 08:36:48.894: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 08:36:49.042: INFO: Exec stderr: ""
Dec 28 08:36:49.042: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9444 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 28 08:36:49.042: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 08:36:49.196: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount
Dec 28 08:36:49.196: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9444 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 28 08:36:49.196: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 08:36:49.350: INFO: Exec stderr: ""
Dec 28 08:36:49.350: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9444 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 28 08:36:49.350: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 08:36:49.517: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true
Dec 28 08:36:49.517: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9444 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 28 08:36:49.517: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 08:36:49.659: INFO: Exec stderr: ""
Dec 28 08:36:49.659: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9444 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 28 08:36:49.660: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 08:36:49.818: INFO: Exec stderr: ""
Dec 28 08:36:49.818: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9444 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 28 08:36:49.818: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 08:36:49.967: INFO: Exec stderr: ""
Dec 28 08:36:49.967: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9444 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 28 08:36:49.967: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 08:36:50.139: INFO: Exec stderr: ""
[AfterEach] [k8s.io] KubeletManagedEtcHosts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:36:50.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-9444" for this suite.
Dec 28 08:37:34.150: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:37:34.216: INFO: namespace e2e-kubelet-etc-hosts-9444 deletion completed in 44.074114984s

• [SLOW TEST:49.676 seconds]
[k8s.io] KubeletManagedEtcHosts
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial] 
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:37:34.217: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename taint-multiple-pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:345
Dec 28 08:37:34.233: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 28 08:38:34.252: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 08:38:34.253: INFO: Starting informer...
STEP: Starting pods...
Dec 28 08:38:34.464: INFO: Pod1 is running on hxx-m-2. Tainting Node
Dec 28 08:38:36.475: INFO: Pod2 is running on hxx-m-2. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting for Pod1 and Pod2 to be deleted
Dec 28 08:38:43.045: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Dec 28 08:39:03.183: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
[AfterEach] [sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:39:03.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-5583" for this suite.
Dec 28 08:39:09.204: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:39:09.277: INFO: namespace taint-multiple-pods-5583 deletion completed in 6.083919378s

• [SLOW TEST:95.061 seconds]
[sig-scheduling] NoExecuteTaintManager Multiple Pods [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  evicts pods with minTolerationSeconds [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run rc 
  should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:39:09.278: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run rc
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1439
[It] should create an rc from an image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 28 08:39:09.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-290'
Dec 28 08:39:09.463: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 28 08:39:09.463: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: verifying the pod controlled by rc e2e-test-httpd-rc was created
STEP: confirm that you can get logs from an rc
Dec 28 08:39:09.468: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [e2e-test-httpd-rc-l7lf4]
Dec 28 08:39:09.468: INFO: Waiting up to 5m0s for pod "e2e-test-httpd-rc-l7lf4" in namespace "kubectl-290" to be "running and ready"
Dec 28 08:39:09.469: INFO: Pod "e2e-test-httpd-rc-l7lf4": Phase="Pending", Reason="", readiness=false. Elapsed: 1.401683ms
Dec 28 08:39:11.472: INFO: Pod "e2e-test-httpd-rc-l7lf4": Phase="Running", Reason="", readiness=true. Elapsed: 2.004090029s
Dec 28 08:39:11.472: INFO: Pod "e2e-test-httpd-rc-l7lf4" satisfied condition "running and ready"
Dec 28 08:39:11.472: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [e2e-test-httpd-rc-l7lf4]
Dec 28 08:39:11.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 logs rc/e2e-test-httpd-rc --namespace=kubectl-290'
Dec 28 08:39:11.564: INFO: stderr: ""
Dec 28 08:39:11.564: INFO: stdout: "AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.199.0.96. Set the 'ServerName' directive globally to suppress this message\nAH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.199.0.96. Set the 'ServerName' directive globally to suppress this message\n[Sat Dec 28 08:39:10.295209 2019] [mpm_event:notice] [pid 1:tid 140494341442408] AH00489: Apache/2.4.38 (Unix) configured -- resuming normal operations\n[Sat Dec 28 08:39:10.295289 2019] [core:notice] [pid 1:tid 140494341442408] AH00094: Command line: 'httpd -D FOREGROUND'\n"
[AfterEach] Kubectl run rc
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1444
Dec 28 08:39:11.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 delete rc e2e-test-httpd-rc --namespace=kubectl-290'
Dec 28 08:39:11.640: INFO: stderr: ""
Dec 28 08:39:11.640: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:39:11.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-290" for this suite.
Dec 28 08:39:17.650: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:39:17.717: INFO: namespace kubectl-290 deletion completed in 6.074128538s

• [SLOW TEST:8.439 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run rc
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1435
    should create an rc from an image  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] 
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:39:17.717: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: fetching the /apis discovery document
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document
STEP: fetching the /apis/apiextensions.k8s.io discovery document
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:39:17.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1860" for this suite.
Dec 28 08:39:23.743: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:39:23.808: INFO: namespace custom-resource-definition-1860 deletion completed in 6.071361162s

• [SLOW TEST:6.091 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:39:23.808: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec 28 08:39:26.344: INFO: Successfully updated pod "annotationupdatef0698b80-a64d-467d-bd72-0d9a56c39c1c"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:39:30.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9949" for this suite.
Dec 28 08:39:58.374: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:39:58.438: INFO: namespace projected-9949 deletion completed in 28.074473007s

• [SLOW TEST:34.630 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:39:58.438: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for all rs to be garbage collected
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: expected 0 rs, got 1 rs
STEP: expected 0 pods, got 2 pods
STEP: Gathering metrics
Dec 28 08:39:59.977: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W1228 08:39:59.977311      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 28 08:39:59.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6630" for this suite.
Dec 28 08:40:05.987: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:40:06.055: INFO: namespace gc-6630 deletion completed in 6.07597515s

• [SLOW TEST:7.617 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:40:06.056: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 28 08:40:06.075: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39b10769-4f74-4a44-b5b0-c2fc9ca9d6e2" in namespace "projected-5912" to be "success or failure"
Dec 28 08:40:06.077: INFO: Pod "downwardapi-volume-39b10769-4f74-4a44-b5b0-c2fc9ca9d6e2": Phase="Pending", Reason="", readiness=false. Elapsed: 1.727538ms
Dec 28 08:40:08.079: INFO: Pod "downwardapi-volume-39b10769-4f74-4a44-b5b0-c2fc9ca9d6e2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003993989s
STEP: Saw pod success
Dec 28 08:40:08.079: INFO: Pod "downwardapi-volume-39b10769-4f74-4a44-b5b0-c2fc9ca9d6e2" satisfied condition "success or failure"
Dec 28 08:40:08.081: INFO: Trying to get logs from node hxx-m-2 pod downwardapi-volume-39b10769-4f74-4a44-b5b0-c2fc9ca9d6e2 container client-container: <nil>
STEP: delete the pod
Dec 28 08:40:08.090: INFO: Waiting for pod downwardapi-volume-39b10769-4f74-4a44-b5b0-c2fc9ca9d6e2 to disappear
Dec 28 08:40:08.092: INFO: Pod downwardapi-volume-39b10769-4f74-4a44-b5b0-c2fc9ca9d6e2 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:40:08.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5912" for this suite.
Dec 28 08:40:14.103: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:40:14.171: INFO: namespace projected-5912 deletion completed in 6.076412802s

• [SLOW TEST:8.116 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:40:14.172: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 28 08:40:14.189: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 28 08:40:14.196: INFO: Waiting for terminating namespaces to be deleted...
Dec 28 08:40:14.198: INFO: 
Logging pods the kubelet thinks is on node hxx-m-1 before test
Dec 28 08:40:14.212: INFO: kube-proxy-bnhj6 from kube-system started at 2019-12-27 13:58:01 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.212: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 28 08:40:14.212: INFO: agon-75b987dff5-7bht5 from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.212: INFO: 	Container agon ready: true, restart count 2
Dec 28 08:40:14.212: INFO: cluster-registry-controller-manager-76774c98d-7c7mz from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (2 container statuses recorded)
Dec 28 08:40:14.212: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 28 08:40:14.212: INFO: 	Container manager ready: true, restart count 0
Dec 28 08:40:14.212: INFO: sonobuoy-e2e-job-f0b9f709fd414134 from sonobuoy started at 2019-12-28 07:53:22 +0000 UTC (2 container statuses recorded)
Dec 28 08:40:14.212: INFO: 	Container e2e ready: true, restart count 0
Dec 28 08:40:14.212: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 28 08:40:14.212: INFO: kube-flannel-8hgpf from kube-system started at 2019-12-27 13:58:14 +0000 UTC (2 container statuses recorded)
Dec 28 08:40:14.212: INFO: 	Container install-cni ready: true, restart count 0
Dec 28 08:40:14.212: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 28 08:40:14.212: INFO: coredns-66447b44c9-zphmm from kube-system started at 2019-12-27 13:58:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.212: INFO: 	Container coredns ready: true, restart count 0
Dec 28 08:40:14.212: INFO: cert-manager-77f5bf4f5-h86rt from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.212: INFO: 	Container cert-manager ready: true, restart count 0
Dec 28 08:40:14.212: INFO: cert-manager-webhook-578c59dddd-k697c from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.212: INFO: 	Container webhook ready: true, restart count 0
Dec 28 08:40:14.212: INFO: apollo-cfdd64bb4-4lssk from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.212: INFO: 	Container apollo ready: true, restart count 0
Dec 28 08:40:14.212: INFO: sonobuoy-systemd-logs-daemon-set-690f17abb3794222-tlb58 from sonobuoy started at 2019-12-28 07:53:22 +0000 UTC (2 container statuses recorded)
Dec 28 08:40:14.212: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 28 08:40:14.212: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 28 08:40:14.212: INFO: dex-8448b48ff8-2bd5h from cpaas-system started at 2019-12-27 14:09:25 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.212: INFO: 	Container dex ready: true, restart count 0
Dec 28 08:40:14.212: INFO: erebus-5597f9565d-zwjnz from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.212: INFO: 	Container erebus ready: true, restart count 0
Dec 28 08:40:14.212: INFO: auth-controller2-79ff55cd75-cbjbn from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.212: INFO: 	Container auth-controller2-manager ready: true, restart count 0
Dec 28 08:40:14.212: INFO: furion-679c948779-jz6lf from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.212: INFO: 	Container furion ready: true, restart count 0
Dec 28 08:40:14.212: INFO: underlord-5c45b96c5d-nmjjq from cpaas-system started at 2019-12-27 14:46:59 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.212: INFO: 	Container alauda-console ready: true, restart count 0
Dec 28 08:40:14.212: INFO: cert-manager-cainjector-67d4dd59ff-8jhs9 from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.212: INFO: 	Container cainjector ready: true, restart count 0
Dec 28 08:40:14.212: INFO: archon-5fdc59d78c-rhtzm from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.212: INFO: 	Container archon-api ready: true, restart count 0
Dec 28 08:40:14.212: INFO: blink-7d4fb788b5-s9q49 from cpaas-system started at 2019-12-28 07:52:16 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.212: INFO: 	Container alauda-console ready: true, restart count 0
Dec 28 08:40:14.212: INFO: 
Logging pods the kubelet thinks is on node hxx-m-2 before test
Dec 28 08:40:14.217: INFO: kube-proxy-44f6q from kube-system started at 2019-12-27 13:57:35 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.217: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 28 08:40:14.217: INFO: kube-flannel-kjxdf from kube-system started at 2019-12-28 08:39:10 +0000 UTC (2 container statuses recorded)
Dec 28 08:40:14.217: INFO: 	Container install-cni ready: true, restart count 0
Dec 28 08:40:14.217: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 28 08:40:14.217: INFO: kube-scheduler-hxx-m-2 from kube-system started at 2019-12-27 13:57:11 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.217: INFO: 	Container kube-scheduler ready: true, restart count 0
Dec 28 08:40:14.217: INFO: nginx-ingress-controller-f9b5d49fd-t92d6 from cpaas-system started at 2019-12-28 08:39:05 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.217: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec 28 08:40:14.217: INFO: sonobuoy-systemd-logs-daemon-set-690f17abb3794222-jftvp from sonobuoy started at 2019-12-28 07:53:22 +0000 UTC (2 container statuses recorded)
Dec 28 08:40:14.217: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 28 08:40:14.217: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 28 08:40:14.217: INFO: kube-apiserver-hxx-m-2 from kube-system started at 2019-12-27 13:57:11 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.217: INFO: 	Container kube-apiserver ready: true, restart count 0
Dec 28 08:40:14.217: INFO: kube-controller-manager-hxx-m-2 from kube-system started at 2019-12-27 13:57:11 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.217: INFO: 	Container kube-controller-manager ready: true, restart count 0
Dec 28 08:40:14.217: INFO: etcd-hxx-m-2 from kube-system started at 2019-12-27 13:57:11 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.217: INFO: 	Container etcd ready: true, restart count 0
Dec 28 08:40:14.217: INFO: sonobuoy from sonobuoy started at 2019-12-28 07:53:21 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.217: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 28 08:40:14.217: INFO: 
Logging pods the kubelet thinks is on node hxx-m-3 before test
Dec 28 08:40:14.231: INFO: tiller-deploy-7c757c6f9c-67l6b from kube-system started at 2019-12-27 13:59:57 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.231: INFO: 	Container tiller ready: true, restart count 0
Dec 28 08:40:14.231: INFO: captain-7cf8c65b49-xjwlq from cpaas-system started at 2019-12-28 08:38:36 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.231: INFO: 	Container captain ready: true, restart count 0
Dec 28 08:40:14.231: INFO: auth-controller2-79ff55cd75-4clk9 from cpaas-system started at 2019-12-27 14:46:59 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.231: INFO: 	Container auth-controller2-manager ready: true, restart count 0
Dec 28 08:40:14.231: INFO: kube-proxy-5kzvk from kube-system started at 2019-12-27 13:57:57 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.231: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 28 08:40:14.231: INFO: kube-flannel-f87zg from kube-system started at 2019-12-27 13:58:14 +0000 UTC (2 container statuses recorded)
Dec 28 08:40:14.231: INFO: 	Container install-cni ready: true, restart count 0
Dec 28 08:40:14.231: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 28 08:40:14.231: INFO: cert-manager-webhook-578c59dddd-flp52 from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.231: INFO: 	Container webhook ready: true, restart count 0
Dec 28 08:40:14.231: INFO: cert-manager-cainjector-67d4dd59ff-tngcj from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.231: INFO: 	Container cainjector ready: true, restart count 0
Dec 28 08:40:14.231: INFO: erebus-5597f9565d-ptdps from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.231: INFO: 	Container erebus ready: true, restart count 0
Dec 28 08:40:14.231: INFO: sonobuoy-systemd-logs-daemon-set-690f17abb3794222-vskml from sonobuoy started at 2019-12-28 07:53:22 +0000 UTC (2 container statuses recorded)
Dec 28 08:40:14.231: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 28 08:40:14.231: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 28 08:40:14.231: INFO: dex-8448b48ff8-9qn2j from cpaas-system started at 2019-12-27 14:09:25 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.231: INFO: 	Container dex ready: true, restart count 0
Dec 28 08:40:14.231: INFO: cert-manager-77f5bf4f5-2wz6n from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.231: INFO: 	Container cert-manager ready: true, restart count 0
Dec 28 08:40:14.231: INFO: underlord-5c45b96c5d-6cvvc from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.231: INFO: 	Container alauda-console ready: true, restart count 0
Dec 28 08:40:14.231: INFO: blink-7d4fb788b5-rtf6r from cpaas-system started at 2019-12-28 08:38:36 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.231: INFO: 	Container alauda-console ready: true, restart count 0
Dec 28 08:40:14.231: INFO: agon-75b987dff5-bl7sb from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.231: INFO: 	Container agon ready: true, restart count 2
Dec 28 08:40:14.231: INFO: apollo-cfdd64bb4-hjth5 from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.231: INFO: 	Container apollo ready: true, restart count 0
Dec 28 08:40:14.231: INFO: coredns-66447b44c9-5qbrm from kube-system started at 2019-12-27 13:58:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.231: INFO: 	Container coredns ready: true, restart count 0
Dec 28 08:40:14.231: INFO: cluster-registry-controller-manager-76774c98d-rmnh2 from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (2 container statuses recorded)
Dec 28 08:40:14.231: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 28 08:40:14.231: INFO: 	Container manager ready: true, restart count 0
Dec 28 08:40:14.231: INFO: furion-679c948779-xjs2n from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.231: INFO: 	Container furion ready: true, restart count 0
Dec 28 08:40:14.231: INFO: archon-5fdc59d78c-k5jbm from cpaas-system started at 2019-12-27 14:46:59 +0000 UTC (1 container statuses recorded)
Dec 28 08:40:14.231: INFO: 	Container archon-api ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-40be9679-5750-4cff-8432-1a3de20ae3db 95
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 127.0.0.1 on the node which pod4 resides and expect not scheduled
STEP: removing the label kubernetes.io/e2e-40be9679-5750-4cff-8432-1a3de20ae3db off the node hxx-m-2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-40be9679-5750-4cff-8432-1a3de20ae3db
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:45:18.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8951" for this suite.
Dec 28 08:45:26.290: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:45:26.355: INFO: namespace sched-pred-8951 deletion completed in 8.080961278s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:312.183 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:45:26.355: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-4175e01c-db61-4491-bf3c-91c15b884507
STEP: Creating a pod to test consume configMaps
Dec 28 08:45:26.377: INFO: Waiting up to 5m0s for pod "pod-configmaps-729b1248-f62d-4aef-bdc7-8804d453ad10" in namespace "configmap-9016" to be "success or failure"
Dec 28 08:45:26.379: INFO: Pod "pod-configmaps-729b1248-f62d-4aef-bdc7-8804d453ad10": Phase="Pending", Reason="", readiness=false. Elapsed: 1.871239ms
Dec 28 08:45:28.382: INFO: Pod "pod-configmaps-729b1248-f62d-4aef-bdc7-8804d453ad10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004370466s
STEP: Saw pod success
Dec 28 08:45:28.382: INFO: Pod "pod-configmaps-729b1248-f62d-4aef-bdc7-8804d453ad10" satisfied condition "success or failure"
Dec 28 08:45:28.384: INFO: Trying to get logs from node hxx-m-2 pod pod-configmaps-729b1248-f62d-4aef-bdc7-8804d453ad10 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 28 08:45:28.401: INFO: Waiting for pod pod-configmaps-729b1248-f62d-4aef-bdc7-8804d453ad10 to disappear
Dec 28 08:45:28.403: INFO: Pod pod-configmaps-729b1248-f62d-4aef-bdc7-8804d453ad10 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:45:28.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9016" for this suite.
Dec 28 08:45:34.413: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:45:34.476: INFO: namespace configmap-9016 deletion completed in 6.071001341s

• [SLOW TEST:8.121 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:45:34.477: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 28 08:45:34.753: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 28 08:45:36.759: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713119534, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713119534, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713119534, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713119534, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 28 08:45:39.768: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that does not comply to the validation webhook rules
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:45:39.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4526" for this suite.
Dec 28 08:45:45.871: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:45:45.941: INFO: namespace webhook-4526 deletion completed in 6.07719089s
STEP: Destroying namespace "webhook-4526-markers" for this suite.
Dec 28 08:45:51.949: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:45:52.018: INFO: namespace webhook-4526-markers deletion completed in 6.076661674s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.548 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:45:52.025: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 28 08:45:52.044: INFO: Waiting up to 5m0s for pod "pod-7652d599-d0c5-4d94-86a6-cc36e373cece" in namespace "emptydir-6548" to be "success or failure"
Dec 28 08:45:52.046: INFO: Pod "pod-7652d599-d0c5-4d94-86a6-cc36e373cece": Phase="Pending", Reason="", readiness=false. Elapsed: 1.778097ms
Dec 28 08:45:54.048: INFO: Pod "pod-7652d599-d0c5-4d94-86a6-cc36e373cece": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003690046s
STEP: Saw pod success
Dec 28 08:45:54.048: INFO: Pod "pod-7652d599-d0c5-4d94-86a6-cc36e373cece" satisfied condition "success or failure"
Dec 28 08:45:54.049: INFO: Trying to get logs from node hxx-m-2 pod pod-7652d599-d0c5-4d94-86a6-cc36e373cece container test-container: <nil>
STEP: delete the pod
Dec 28 08:45:54.058: INFO: Waiting for pod pod-7652d599-d0c5-4d94-86a6-cc36e373cece to disappear
Dec 28 08:45:54.060: INFO: Pod pod-7652d599-d0c5-4d94-86a6-cc36e373cece no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:45:54.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6548" for this suite.
Dec 28 08:46:00.069: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:46:00.141: INFO: namespace emptydir-6548 deletion completed in 6.078163057s

• [SLOW TEST:8.115 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command in a pod 
  should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:46:00.141: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should print the output to logs [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:46:02.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1978" for this suite.
Dec 28 08:46:46.188: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:46:46.253: INFO: namespace kubelet-test-1978 deletion completed in 44.079079048s

• [SLOW TEST:46.112 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command in a pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:40
    should print the output to logs [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:46:46.253: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a new configmap
STEP: modifying the configmap once
STEP: modifying the configmap a second time
STEP: deleting the configmap
STEP: creating a watch on configmaps from the resource version returned by the first update
STEP: Expecting to observe notifications for all changes to the configmap after the first update
Dec 28 08:46:46.281: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1898 /api/v1/namespaces/watch-1898/configmaps/e2e-watch-test-resource-version 6a717b41-c4c4-4c29-a97c-1e0b19243b60 288934 0 2019-12-28 08:46:46 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 28 08:46:46.281: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-1898 /api/v1/namespaces/watch-1898/configmaps/e2e-watch-test-resource-version 6a717b41-c4c4-4c29-a97c-1e0b19243b60 288935 0 2019-12-28 08:46:46 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:46:46.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1898" for this suite.
Dec 28 08:46:52.291: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:46:52.357: INFO: namespace watch-1898 deletion completed in 6.073427575s

• [SLOW TEST:6.104 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:46:52.358: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 28 08:46:52.374: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 28 08:46:52.381: INFO: Waiting for terminating namespaces to be deleted...
Dec 28 08:46:52.382: INFO: 
Logging pods the kubelet thinks is on node hxx-m-1 before test
Dec 28 08:46:52.397: INFO: blink-7d4fb788b5-s9q49 from cpaas-system started at 2019-12-28 07:52:16 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.397: INFO: 	Container alauda-console ready: true, restart count 0
Dec 28 08:46:52.397: INFO: cert-manager-cainjector-67d4dd59ff-8jhs9 from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.397: INFO: 	Container cainjector ready: true, restart count 0
Dec 28 08:46:52.397: INFO: archon-5fdc59d78c-rhtzm from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.397: INFO: 	Container archon-api ready: true, restart count 0
Dec 28 08:46:52.397: INFO: cluster-registry-controller-manager-76774c98d-7c7mz from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (2 container statuses recorded)
Dec 28 08:46:52.397: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 28 08:46:52.397: INFO: 	Container manager ready: true, restart count 0
Dec 28 08:46:52.397: INFO: sonobuoy-e2e-job-f0b9f709fd414134 from sonobuoy started at 2019-12-28 07:53:22 +0000 UTC (2 container statuses recorded)
Dec 28 08:46:52.397: INFO: 	Container e2e ready: true, restart count 0
Dec 28 08:46:52.397: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 28 08:46:52.397: INFO: kube-proxy-bnhj6 from kube-system started at 2019-12-27 13:58:01 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.397: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 28 08:46:52.397: INFO: agon-75b987dff5-7bht5 from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.397: INFO: 	Container agon ready: true, restart count 2
Dec 28 08:46:52.397: INFO: kube-flannel-8hgpf from kube-system started at 2019-12-27 13:58:14 +0000 UTC (2 container statuses recorded)
Dec 28 08:46:52.397: INFO: 	Container install-cni ready: true, restart count 0
Dec 28 08:46:52.397: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 28 08:46:52.397: INFO: cert-manager-webhook-578c59dddd-k697c from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.397: INFO: 	Container webhook ready: true, restart count 0
Dec 28 08:46:52.397: INFO: apollo-cfdd64bb4-4lssk from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.397: INFO: 	Container apollo ready: true, restart count 0
Dec 28 08:46:52.397: INFO: sonobuoy-systemd-logs-daemon-set-690f17abb3794222-tlb58 from sonobuoy started at 2019-12-28 07:53:22 +0000 UTC (2 container statuses recorded)
Dec 28 08:46:52.397: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 28 08:46:52.397: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 28 08:46:52.397: INFO: coredns-66447b44c9-zphmm from kube-system started at 2019-12-27 13:58:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.397: INFO: 	Container coredns ready: true, restart count 0
Dec 28 08:46:52.397: INFO: cert-manager-77f5bf4f5-h86rt from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.397: INFO: 	Container cert-manager ready: true, restart count 0
Dec 28 08:46:52.397: INFO: auth-controller2-79ff55cd75-cbjbn from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.397: INFO: 	Container auth-controller2-manager ready: true, restart count 0
Dec 28 08:46:52.397: INFO: furion-679c948779-jz6lf from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.397: INFO: 	Container furion ready: true, restart count 0
Dec 28 08:46:52.397: INFO: underlord-5c45b96c5d-nmjjq from cpaas-system started at 2019-12-27 14:46:59 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.397: INFO: 	Container alauda-console ready: true, restart count 0
Dec 28 08:46:52.397: INFO: dex-8448b48ff8-2bd5h from cpaas-system started at 2019-12-27 14:09:25 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.397: INFO: 	Container dex ready: true, restart count 0
Dec 28 08:46:52.397: INFO: erebus-5597f9565d-zwjnz from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.397: INFO: 	Container erebus ready: true, restart count 0
Dec 28 08:46:52.397: INFO: 
Logging pods the kubelet thinks is on node hxx-m-2 before test
Dec 28 08:46:52.402: INFO: etcd-hxx-m-2 from kube-system started at 2019-12-27 13:57:11 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.402: INFO: 	Container etcd ready: true, restart count 0
Dec 28 08:46:52.402: INFO: sonobuoy from sonobuoy started at 2019-12-28 07:53:21 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.402: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 28 08:46:52.402: INFO: kube-proxy-44f6q from kube-system started at 2019-12-27 13:57:35 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.402: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 28 08:46:52.402: INFO: kube-flannel-kjxdf from kube-system started at 2019-12-28 08:39:10 +0000 UTC (2 container statuses recorded)
Dec 28 08:46:52.402: INFO: 	Container install-cni ready: true, restart count 0
Dec 28 08:46:52.402: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 28 08:46:52.402: INFO: kube-scheduler-hxx-m-2 from kube-system started at 2019-12-27 13:57:11 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.402: INFO: 	Container kube-scheduler ready: true, restart count 0
Dec 28 08:46:52.402: INFO: nginx-ingress-controller-f9b5d49fd-t92d6 from cpaas-system started at 2019-12-28 08:39:05 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.402: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec 28 08:46:52.402: INFO: sonobuoy-systemd-logs-daemon-set-690f17abb3794222-jftvp from sonobuoy started at 2019-12-28 07:53:22 +0000 UTC (2 container statuses recorded)
Dec 28 08:46:52.402: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 28 08:46:52.402: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 28 08:46:52.402: INFO: kube-apiserver-hxx-m-2 from kube-system started at 2019-12-27 13:57:11 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.402: INFO: 	Container kube-apiserver ready: true, restart count 0
Dec 28 08:46:52.402: INFO: kube-controller-manager-hxx-m-2 from kube-system started at 2019-12-27 13:57:11 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.402: INFO: 	Container kube-controller-manager ready: true, restart count 0
Dec 28 08:46:52.402: INFO: 
Logging pods the kubelet thinks is on node hxx-m-3 before test
Dec 28 08:46:52.416: INFO: coredns-66447b44c9-5qbrm from kube-system started at 2019-12-27 13:58:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.416: INFO: 	Container coredns ready: true, restart count 0
Dec 28 08:46:52.416: INFO: cluster-registry-controller-manager-76774c98d-rmnh2 from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (2 container statuses recorded)
Dec 28 08:46:52.416: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 28 08:46:52.416: INFO: 	Container manager ready: true, restart count 0
Dec 28 08:46:52.416: INFO: furion-679c948779-xjs2n from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.416: INFO: 	Container furion ready: true, restart count 0
Dec 28 08:46:52.416: INFO: archon-5fdc59d78c-k5jbm from cpaas-system started at 2019-12-27 14:46:59 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.416: INFO: 	Container archon-api ready: true, restart count 0
Dec 28 08:46:52.416: INFO: tiller-deploy-7c757c6f9c-67l6b from kube-system started at 2019-12-27 13:59:57 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.416: INFO: 	Container tiller ready: true, restart count 0
Dec 28 08:46:52.416: INFO: captain-7cf8c65b49-xjwlq from cpaas-system started at 2019-12-28 08:38:36 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.416: INFO: 	Container captain ready: true, restart count 0
Dec 28 08:46:52.416: INFO: auth-controller2-79ff55cd75-4clk9 from cpaas-system started at 2019-12-27 14:46:59 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.416: INFO: 	Container auth-controller2-manager ready: true, restart count 0
Dec 28 08:46:52.416: INFO: erebus-5597f9565d-ptdps from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.416: INFO: 	Container erebus ready: true, restart count 0
Dec 28 08:46:52.416: INFO: sonobuoy-systemd-logs-daemon-set-690f17abb3794222-vskml from sonobuoy started at 2019-12-28 07:53:22 +0000 UTC (2 container statuses recorded)
Dec 28 08:46:52.416: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 28 08:46:52.416: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 28 08:46:52.416: INFO: kube-proxy-5kzvk from kube-system started at 2019-12-27 13:57:57 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.416: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 28 08:46:52.416: INFO: kube-flannel-f87zg from kube-system started at 2019-12-27 13:58:14 +0000 UTC (2 container statuses recorded)
Dec 28 08:46:52.416: INFO: 	Container install-cni ready: true, restart count 0
Dec 28 08:46:52.416: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 28 08:46:52.416: INFO: cert-manager-webhook-578c59dddd-flp52 from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.416: INFO: 	Container webhook ready: true, restart count 0
Dec 28 08:46:52.416: INFO: cert-manager-cainjector-67d4dd59ff-tngcj from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.416: INFO: 	Container cainjector ready: true, restart count 0
Dec 28 08:46:52.416: INFO: dex-8448b48ff8-9qn2j from cpaas-system started at 2019-12-27 14:09:25 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.416: INFO: 	Container dex ready: true, restart count 0
Dec 28 08:46:52.416: INFO: cert-manager-77f5bf4f5-2wz6n from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.416: INFO: 	Container cert-manager ready: true, restart count 0
Dec 28 08:46:52.416: INFO: underlord-5c45b96c5d-6cvvc from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.416: INFO: 	Container alauda-console ready: true, restart count 0
Dec 28 08:46:52.416: INFO: blink-7d4fb788b5-rtf6r from cpaas-system started at 2019-12-28 08:38:36 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.416: INFO: 	Container alauda-console ready: true, restart count 0
Dec 28 08:46:52.416: INFO: agon-75b987dff5-bl7sb from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.416: INFO: 	Container agon ready: true, restart count 2
Dec 28 08:46:52.416: INFO: apollo-cfdd64bb4-hjth5 from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 08:46:52.416: INFO: 	Container apollo ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to schedule Pod with nonempty NodeSelector.
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.15e47ca3cbde05b7], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match node selector.]
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:46:53.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-1929" for this suite.
Dec 28 08:46:59.441: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:46:59.508: INFO: namespace sched-pred-1929 deletion completed in 6.074718739s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:7.151 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that NodeSelector is respected if not matching  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:46:59.509: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec 28 08:46:59.526: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:47:02.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5811" for this suite.
Dec 28 08:47:08.349: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:47:08.427: INFO: namespace init-container-5811 deletion completed in 6.085342078s

• [SLOW TEST:8.918 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-network] Services 
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:47:08.427: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1140
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service
STEP: creating service externalsvc in namespace services-1140
STEP: creating replication controller externalsvc in namespace services-1140
I1228 08:47:08.511536      22 runners.go:184] Created replication controller with name: externalsvc, namespace: services-1140, replica count: 2
I1228 08:47:11.561921      22 runners.go:184] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName
Dec 28 08:47:11.570: INFO: Creating new exec pod
Dec 28 08:47:13.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=services-1140 execpodxp5hx -- /bin/sh -x -c nslookup clusterip-service'
Dec 28 08:47:13.800: INFO: stderr: "+ nslookup clusterip-service\n"
Dec 28 08:47:13.800: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-1140.svc.cluster.local\tcanonical name = externalsvc.services-1140.svc.cluster.local.\nName:\texternalsvc.services-1140.svc.cluster.local\nAddress: 10.100.167.56\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-1140, will wait for the garbage collector to delete the pods
Dec 28 08:47:13.856: INFO: Deleting ReplicationController externalsvc took: 3.533928ms
Dec 28 08:47:14.756: INFO: Terminating ReplicationController externalsvc pods took: 900.199271ms
Dec 28 08:47:26.965: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:47:26.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1140" for this suite.
Dec 28 08:47:32.981: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:47:33.050: INFO: namespace services-1140 deletion completed in 6.076085453s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:24.623 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:47:33.050: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 08:47:33.070: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Dec 28 08:47:38.072: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 28 08:47:38.072: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 28 08:47:40.087: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9528 /apis/apps/v1/namespaces/deployment-9528/deployments/test-cleanup-deployment 0a55e9b9-6e25-4188-855d-0128fa06bec1 289333 1 2019-12-28 08:47:38 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001bf94f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-12-28 08:47:38 +0000 UTC,LastTransitionTime:2019-12-28 08:47:38 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-65db99849b" has successfully progressed.,LastUpdateTime:2019-12-28 08:47:39 +0000 UTC,LastTransitionTime:2019-12-28 08:47:38 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 28 08:47:40.089: INFO: New ReplicaSet "test-cleanup-deployment-65db99849b" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-65db99849b  deployment-9528 /apis/apps/v1/namespaces/deployment-9528/replicasets/test-cleanup-deployment-65db99849b 5d78e80e-52d0-4b00-8639-7f1a1cc3499e 289322 1 2019-12-28 08:47:38 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 0a55e9b9-6e25-4188-855d-0128fa06bec1 0xc001bf9f57 0xc001bf9f58}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 65db99849b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc001bf9fb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 28 08:47:40.090: INFO: Pod "test-cleanup-deployment-65db99849b-fn8nj" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-65db99849b-fn8nj test-cleanup-deployment-65db99849b- deployment-9528 /api/v1/namespaces/deployment-9528/pods/test-cleanup-deployment-65db99849b-fn8nj 851a53b8-ef4d-4cb0-a7a0-1e6019beef56 289321 0 2019-12-28 08:47:38 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:65db99849b] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-65db99849b 5d78e80e-52d0-4b00-8639-7f1a1cc3499e 0xc00226c6c7 0xc00226c6c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-jks2v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-jks2v,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-jks2v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:hxx-m-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 08:47:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 08:47:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 08:47:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 08:47:38 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.16,PodIP:10.199.0.110,StartTime:2019-12-28 08:47:38 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-28 08:47:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://6ab99f77f0bc6108baf5cdf943f637deb82adf8f18444678b5fb028059905453,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.199.0.110,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:47:40.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-9528" for this suite.
Dec 28 08:47:46.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:47:46.172: INFO: namespace deployment-9528 deletion completed in 6.078603451s

• [SLOW TEST:13.122 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:47:46.172: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-87bd4f88-7936-438b-8fd2-c8c4ef3072cf
STEP: Creating a pod to test consume secrets
Dec 28 08:47:46.193: INFO: Waiting up to 5m0s for pod "pod-secrets-aad11414-e1ba-4de3-91d9-6caec0c947a3" in namespace "secrets-8094" to be "success or failure"
Dec 28 08:47:46.195: INFO: Pod "pod-secrets-aad11414-e1ba-4de3-91d9-6caec0c947a3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.50153ms
Dec 28 08:47:48.197: INFO: Pod "pod-secrets-aad11414-e1ba-4de3-91d9-6caec0c947a3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003655512s
STEP: Saw pod success
Dec 28 08:47:48.197: INFO: Pod "pod-secrets-aad11414-e1ba-4de3-91d9-6caec0c947a3" satisfied condition "success or failure"
Dec 28 08:47:48.198: INFO: Trying to get logs from node hxx-m-2 pod pod-secrets-aad11414-e1ba-4de3-91d9-6caec0c947a3 container secret-volume-test: <nil>
STEP: delete the pod
Dec 28 08:47:48.207: INFO: Waiting for pod pod-secrets-aad11414-e1ba-4de3-91d9-6caec0c947a3 to disappear
Dec 28 08:47:48.209: INFO: Pod pod-secrets-aad11414-e1ba-4de3-91d9-6caec0c947a3 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:47:48.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8094" for this suite.
Dec 28 08:47:54.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:47:54.289: INFO: namespace secrets-8094 deletion completed in 6.078011617s

• [SLOW TEST:8.117 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:47:54.290: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 28 08:47:54.310: INFO: Waiting up to 5m0s for pod "downwardapi-volume-821fd322-7c3d-4091-a7ae-6316efb2b11e" in namespace "downward-api-6775" to be "success or failure"
Dec 28 08:47:54.312: INFO: Pod "downwardapi-volume-821fd322-7c3d-4091-a7ae-6316efb2b11e": Phase="Pending", Reason="", readiness=false. Elapsed: 1.361293ms
Dec 28 08:47:56.314: INFO: Pod "downwardapi-volume-821fd322-7c3d-4091-a7ae-6316efb2b11e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003643124s
STEP: Saw pod success
Dec 28 08:47:56.314: INFO: Pod "downwardapi-volume-821fd322-7c3d-4091-a7ae-6316efb2b11e" satisfied condition "success or failure"
Dec 28 08:47:56.315: INFO: Trying to get logs from node hxx-m-2 pod downwardapi-volume-821fd322-7c3d-4091-a7ae-6316efb2b11e container client-container: <nil>
STEP: delete the pod
Dec 28 08:47:56.325: INFO: Waiting for pod downwardapi-volume-821fd322-7c3d-4091-a7ae-6316efb2b11e to disappear
Dec 28 08:47:56.327: INFO: Pod downwardapi-volume-821fd322-7c3d-4091-a7ae-6316efb2b11e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:47:56.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6775" for this suite.
Dec 28 08:48:02.336: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:48:02.404: INFO: namespace downward-api-6775 deletion completed in 6.074855586s

• [SLOW TEST:8.114 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:48:02.404: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-0a3b0e97-c6ce-4262-a72d-e19ad9967841 in namespace container-probe-5264
Dec 28 08:48:04.430: INFO: Started pod busybox-0a3b0e97-c6ce-4262-a72d-e19ad9967841 in namespace container-probe-5264
STEP: checking the pod's current state and verifying that restartCount is present
Dec 28 08:48:04.431: INFO: Initial restart count of pod busybox-0a3b0e97-c6ce-4262-a72d-e19ad9967841 is 0
Dec 28 08:48:56.491: INFO: Restart count of pod container-probe-5264/busybox-0a3b0e97-c6ce-4262-a72d-e19ad9967841 is now 1 (52.05987464s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:48:56.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5264" for this suite.
Dec 28 08:49:02.506: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:49:02.573: INFO: namespace container-probe-5264 deletion completed in 6.074835036s

• [SLOW TEST:60.169 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:49:02.573: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation
Dec 28 08:49:02.591: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation
Dec 28 08:49:17.001: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 08:49:20.718: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:49:35.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5407" for this suite.
Dec 28 08:49:41.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:49:41.490: INFO: namespace crd-publish-openapi-5407 deletion completed in 6.077436522s

• [SLOW TEST:38.918 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial] 
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:49:41.491: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename taint-single-pod
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/taints.go:164
Dec 28 08:49:41.506: INFO: Waiting up to 1m0s for all nodes to be ready
Dec 28 08:50:41.524: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 08:50:41.526: INFO: Starting informer...
STEP: Starting pod...
Dec 28 08:50:41.733: INFO: Pod is running on hxx-m-2. Tainting Node
STEP: Trying to apply a taint on the Node
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting short time to make sure Pod is queued for deletion
Dec 28 08:50:41.741: INFO: Pod wasn't evicted. Proceeding
Dec 28 08:50:41.741: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute
STEP: Waiting some time to make sure that toleration time passed.
Dec 28 08:51:56.751: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:51:56.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-29" for this suite.
Dec 28 08:52:08.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:52:08.831: INFO: namespace taint-single-pod-29 deletion completed in 12.077214578s

• [SLOW TEST:147.341 seconds]
[sig-scheduling] NoExecuteTaintManager Single Pod [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  removing taint cancels eviction [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-node] Downward API 
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:52:08.831: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward api env vars
Dec 28 08:52:08.853: INFO: Waiting up to 5m0s for pod "downward-api-5ddcf90d-eb59-402f-bd9f-5bf6e93bf8f5" in namespace "downward-api-9588" to be "success or failure"
Dec 28 08:52:08.855: INFO: Pod "downward-api-5ddcf90d-eb59-402f-bd9f-5bf6e93bf8f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.525399ms
Dec 28 08:52:10.858: INFO: Pod "downward-api-5ddcf90d-eb59-402f-bd9f-5bf6e93bf8f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005063121s
STEP: Saw pod success
Dec 28 08:52:10.858: INFO: Pod "downward-api-5ddcf90d-eb59-402f-bd9f-5bf6e93bf8f5" satisfied condition "success or failure"
Dec 28 08:52:10.860: INFO: Trying to get logs from node hxx-m-2 pod downward-api-5ddcf90d-eb59-402f-bd9f-5bf6e93bf8f5 container dapi-container: <nil>
STEP: delete the pod
Dec 28 08:52:10.877: INFO: Waiting for pod downward-api-5ddcf90d-eb59-402f-bd9f-5bf6e93bf8f5 to disappear
Dec 28 08:52:10.879: INFO: Pod downward-api-5ddcf90d-eb59-402f-bd9f-5bf6e93bf8f5 no longer exists
[AfterEach] [sig-node] Downward API
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:52:10.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9588" for this suite.
Dec 28 08:52:16.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:52:16.955: INFO: namespace downward-api-9588 deletion completed in 6.073802348s

• [SLOW TEST:8.124 seconds]
[sig-node] Downward API
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downward_api.go:32
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:52:16.956: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a ResourceQuota
STEP: Getting a ResourceQuota
STEP: Updating a ResourceQuota
STEP: Verifying a ResourceQuota was modified
STEP: Deleting a ResourceQuota
STEP: Verifying the deleted ResourceQuota
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:52:16.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-2679" for this suite.
Dec 28 08:52:22.993: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:52:23.063: INFO: namespace resourcequota-2679 deletion completed in 6.076412419s

• [SLOW TEST:6.107 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:52:23.063: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's args
Dec 28 08:52:23.082: INFO: Waiting up to 5m0s for pod "var-expansion-252f6414-547a-48a9-90a4-659108b70246" in namespace "var-expansion-3627" to be "success or failure"
Dec 28 08:52:23.084: INFO: Pod "var-expansion-252f6414-547a-48a9-90a4-659108b70246": Phase="Pending", Reason="", readiness=false. Elapsed: 1.433315ms
Dec 28 08:52:25.086: INFO: Pod "var-expansion-252f6414-547a-48a9-90a4-659108b70246": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003652902s
STEP: Saw pod success
Dec 28 08:52:25.086: INFO: Pod "var-expansion-252f6414-547a-48a9-90a4-659108b70246" satisfied condition "success or failure"
Dec 28 08:52:25.088: INFO: Trying to get logs from node hxx-m-2 pod var-expansion-252f6414-547a-48a9-90a4-659108b70246 container dapi-container: <nil>
STEP: delete the pod
Dec 28 08:52:25.098: INFO: Waiting for pod var-expansion-252f6414-547a-48a9-90a4-659108b70246 to disappear
Dec 28 08:52:25.099: INFO: Pod var-expansion-252f6414-547a-48a9-90a4-659108b70246 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:52:25.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3627" for this suite.
Dec 28 08:52:31.109: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:52:31.175: INFO: namespace var-expansion-3627 deletion completed in 6.074094816s

• [SLOW TEST:8.113 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:52:31.176: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test substitution in container's command
Dec 28 08:52:31.194: INFO: Waiting up to 5m0s for pod "var-expansion-e740c801-af01-4966-a2f0-41c6602ce644" in namespace "var-expansion-816" to be "success or failure"
Dec 28 08:52:31.196: INFO: Pod "var-expansion-e740c801-af01-4966-a2f0-41c6602ce644": Phase="Pending", Reason="", readiness=false. Elapsed: 1.547111ms
Dec 28 08:52:33.198: INFO: Pod "var-expansion-e740c801-af01-4966-a2f0-41c6602ce644": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004015654s
STEP: Saw pod success
Dec 28 08:52:33.198: INFO: Pod "var-expansion-e740c801-af01-4966-a2f0-41c6602ce644" satisfied condition "success or failure"
Dec 28 08:52:33.200: INFO: Trying to get logs from node hxx-m-2 pod var-expansion-e740c801-af01-4966-a2f0-41c6602ce644 container dapi-container: <nil>
STEP: delete the pod
Dec 28 08:52:33.210: INFO: Waiting for pod var-expansion-e740c801-af01-4966-a2f0-41c6602ce644 to disappear
Dec 28 08:52:33.212: INFO: Pod var-expansion-e740c801-af01-4966-a2f0-41c6602ce644 no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:52:33.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-816" for this suite.
Dec 28 08:52:39.222: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:52:39.290: INFO: namespace var-expansion-816 deletion completed in 6.075588264s

• [SLOW TEST:8.114 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:52:39.290: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:52:46.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6759" for this suite.
Dec 28 08:52:52.324: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:52:52.390: INFO: namespace resourcequota-6759 deletion completed in 6.074445189s

• [SLOW TEST:13.100 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Watchers 
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:52:52.390: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename watch
STEP: Waiting for a default service account to be provisioned in namespace
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a watch on configmaps with label A
STEP: creating a watch on configmaps with label B
STEP: creating a watch on configmaps with label A or B
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification
Dec 28 08:52:52.412: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6019 /api/v1/namespaces/watch-6019/configmaps/e2e-watch-test-configmap-a c382fc00-6521-4153-9293-b2924569975a 290911 0 2019-12-28 08:52:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 28 08:52:52.412: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6019 /api/v1/namespaces/watch-6019/configmaps/e2e-watch-test-configmap-a c382fc00-6521-4153-9293-b2924569975a 290911 0 2019-12-28 08:52:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: modifying configmap A and ensuring the correct watchers observe the notification
Dec 28 08:53:02.416: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6019 /api/v1/namespaces/watch-6019/configmaps/e2e-watch-test-configmap-a c382fc00-6521-4153-9293-b2924569975a 290951 0 2019-12-28 08:52:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
Dec 28 08:53:02.416: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6019 /api/v1/namespaces/watch-6019/configmaps/e2e-watch-test-configmap-a c382fc00-6521-4153-9293-b2924569975a 290951 0 2019-12-28 08:52:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification
Dec 28 08:53:12.421: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6019 /api/v1/namespaces/watch-6019/configmaps/e2e-watch-test-configmap-a c382fc00-6521-4153-9293-b2924569975a 290992 0 2019-12-28 08:52:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 28 08:53:12.421: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6019 /api/v1/namespaces/watch-6019/configmaps/e2e-watch-test-configmap-a c382fc00-6521-4153-9293-b2924569975a 290992 0 2019-12-28 08:52:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: deleting configmap A and ensuring the correct watchers observe the notification
Dec 28 08:53:22.425: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6019 /api/v1/namespaces/watch-6019/configmaps/e2e-watch-test-configmap-a c382fc00-6521-4153-9293-b2924569975a 291033 0 2019-12-28 08:52:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
Dec 28 08:53:22.425: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-6019 /api/v1/namespaces/watch-6019/configmaps/e2e-watch-test-configmap-a c382fc00-6521-4153-9293-b2924569975a 291033 0 2019-12-28 08:52:52 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] []  []},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification
Dec 28 08:53:32.429: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6019 /api/v1/namespaces/watch-6019/configmaps/e2e-watch-test-configmap-b bd5e5a43-7041-4706-97ab-5be28e167f42 291094 0 2019-12-28 08:53:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 28 08:53:32.430: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6019 /api/v1/namespaces/watch-6019/configmaps/e2e-watch-test-configmap-b bd5e5a43-7041-4706-97ab-5be28e167f42 291094 0 2019-12-28 08:53:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
STEP: deleting configmap B and ensuring the correct watchers observe the notification
Dec 28 08:53:42.433: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6019 /api/v1/namespaces/watch-6019/configmaps/e2e-watch-test-configmap-b bd5e5a43-7041-4706-97ab-5be28e167f42 291135 0 2019-12-28 08:53:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
Dec 28 08:53:42.433: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-6019 /api/v1/namespaces/watch-6019/configmaps/e2e-watch-test-configmap-b bd5e5a43-7041-4706-97ab-5be28e167f42 291135 0 2019-12-28 08:53:32 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] []  []},Data:map[string]string{},BinaryData:map[string][]byte{},}
[AfterEach] [sig-api-machinery] Watchers
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:53:52.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6019" for this suite.
Dec 28 08:53:58.443: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:53:58.512: INFO: namespace watch-6019 deletion completed in 6.0751892s

• [SLOW TEST:66.121 seconds]
[sig-api-machinery] Watchers
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:53:58.512: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name projected-secret-test-3c113b34-90c5-4014-b9ef-8b3b0cf2099b
STEP: Creating a pod to test consume secrets
Dec 28 08:53:58.533: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2978a836-7625-4373-86d3-1a063ba92002" in namespace "projected-1447" to be "success or failure"
Dec 28 08:53:58.534: INFO: Pod "pod-projected-secrets-2978a836-7625-4373-86d3-1a063ba92002": Phase="Pending", Reason="", readiness=false. Elapsed: 1.443934ms
Dec 28 08:54:00.536: INFO: Pod "pod-projected-secrets-2978a836-7625-4373-86d3-1a063ba92002": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003565876s
STEP: Saw pod success
Dec 28 08:54:00.537: INFO: Pod "pod-projected-secrets-2978a836-7625-4373-86d3-1a063ba92002" satisfied condition "success or failure"
Dec 28 08:54:00.538: INFO: Trying to get logs from node hxx-m-2 pod pod-projected-secrets-2978a836-7625-4373-86d3-1a063ba92002 container secret-volume-test: <nil>
STEP: delete the pod
Dec 28 08:54:00.547: INFO: Waiting for pod pod-projected-secrets-2978a836-7625-4373-86d3-1a063ba92002 to disappear
Dec 28 08:54:00.549: INFO: Pod pod-projected-secrets-2978a836-7625-4373-86d3-1a063ba92002 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:54:00.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1447" for this suite.
Dec 28 08:54:06.558: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:54:06.626: INFO: namespace projected-1447 deletion completed in 6.074800761s

• [SLOW TEST:8.114 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe 
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:54:06.626: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 08:54:06.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 create -f - --namespace=kubectl-9232'
Dec 28 08:54:06.945: INFO: stderr: ""
Dec 28 08:54:06.945: INFO: stdout: "replicationcontroller/redis-master created\n"
Dec 28 08:54:06.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 create -f - --namespace=kubectl-9232'
Dec 28 08:54:07.101: INFO: stderr: ""
Dec 28 08:54:07.101: INFO: stdout: "service/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 28 08:54:08.103: INFO: Selector matched 1 pods for map[app:redis]
Dec 28 08:54:08.104: INFO: Found 0 / 1
Dec 28 08:54:09.103: INFO: Selector matched 1 pods for map[app:redis]
Dec 28 08:54:09.103: INFO: Found 1 / 1
Dec 28 08:54:09.103: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Dec 28 08:54:09.105: INFO: Selector matched 1 pods for map[app:redis]
Dec 28 08:54:09.105: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 28 08:54:09.105: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 describe pod redis-master-4skwn --namespace=kubectl-9232'
Dec 28 08:54:09.188: INFO: stderr: ""
Dec 28 08:54:09.188: INFO: stdout: "Name:         redis-master-4skwn\nNamespace:    kubectl-9232\nPriority:     0\nNode:         hxx-m-2/10.0.128.16\nStart Time:   Sat, 28 Dec 2019 08:54:06 +0000\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nStatus:       Running\nIP:           10.199.0.120\nIPs:\n  IP:           10.199.0.120\nControlled By:  ReplicationController/redis-master\nContainers:\n  redis-master:\n    Container ID:   docker://731b51565373b9a85b5ee81993cf06e030e80cabd1f28f27ca575abf2319be1d\n    Image:          docker.io/library/redis:5.0.5-alpine\n    Image ID:       docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 28 Dec 2019 08:54:07 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from default-token-p6465 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  default-token-p6465:\n    Type:        Secret (a volume populated by a Secret)\n    SecretName:  default-token-p6465\n    Optional:    false\nQoS Class:       BestEffort\nNode-Selectors:  <none>\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  3s    default-scheduler  Successfully assigned kubectl-9232/redis-master-4skwn to hxx-m-2\n  Normal  Pulled     2s    kubelet, hxx-m-2   Container image \"docker.io/library/redis:5.0.5-alpine\" already present on machine\n  Normal  Created    2s    kubelet, hxx-m-2   Created container redis-master\n  Normal  Started    2s    kubelet, hxx-m-2   Started container redis-master\n"
Dec 28 08:54:09.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 describe rc redis-master --namespace=kubectl-9232'
Dec 28 08:54:09.277: INFO: stderr: ""
Dec 28 08:54:09.277: INFO: stdout: "Name:         redis-master\nNamespace:    kubectl-9232\nSelector:     app=redis,role=master\nLabels:       app=redis\n              role=master\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=redis\n           role=master\n  Containers:\n   redis-master:\n    Image:        docker.io/library/redis:5.0.5-alpine\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: redis-master-4skwn\n"
Dec 28 08:54:09.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 describe service redis-master --namespace=kubectl-9232'
Dec 28 08:54:09.356: INFO: stderr: ""
Dec 28 08:54:09.356: INFO: stdout: "Name:              redis-master\nNamespace:         kubectl-9232\nLabels:            app=redis\n                   role=master\nAnnotations:       <none>\nSelector:          app=redis,role=master\nType:              ClusterIP\nIP:                10.105.193.205\nPort:              <unset>  6379/TCP\nTargetPort:        redis-server/TCP\nEndpoints:         10.199.0.120:6379\nSession Affinity:  None\nEvents:            <none>\n"
Dec 28 08:54:09.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 describe node hxx-m-1'
Dec 28 08:54:09.456: INFO: stderr: ""
Dec 28 08:54:09.456: INFO: stdout: "Name:               hxx-m-1\nRoles:              node\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    global=true\n                    ingress=false\n                    ip=10.0.128.42\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=hxx-m-1\n                    kubernetes.io/os=linux\n                    log=true\n                    node-role.kubernetes.io/node=\nAnnotations:        flannel.alpha.coreos.com/backend-data: {\"VtepMAC\":\"de:eb:10:d3:14:86\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 10.0.128.42\n                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 27 Dec 2019 13:58:01 +0000\nTaints:             <none>\nUnschedulable:      false\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Sat, 28 Dec 2019 08:53:39 +0000   Fri, 27 Dec 2019 13:58:01 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Sat, 28 Dec 2019 08:53:39 +0000   Fri, 27 Dec 2019 13:58:01 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Sat, 28 Dec 2019 08:53:39 +0000   Fri, 27 Dec 2019 13:58:01 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Sat, 28 Dec 2019 08:53:39 +0000   Fri, 27 Dec 2019 13:58:21 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.0.128.42\n  Hostname:    hxx-m-1\nCapacity:\n cpu:                8\n ephemeral-storage:  51473868Ki\n hugepages-2Mi:      0\n memory:             16265872Ki\n pods:               110\nAllocatable:\n cpu:                8\n ephemeral-storage:  47438316671\n hugepages-2Mi:      0\n memory:             16163472Ki\n pods:               110\nSystem Info:\n Machine ID:                 0ea734564f9a4e2881b866b82d679dfc\n System UUID:                76729676-2E42-4F05-9CDD-C6D412C7234C\n Boot ID:                    c2d146bd-96ba-47db-a038-25cbe99666d4\n Kernel Version:             3.10.0-957.21.3.el7.x86_64\n OS Image:                   CentOS Linux 7 (Core)\n Operating System:           linux\n Architecture:               amd64\n Container Runtime Version:  docker://18.9.9\n Kubelet Version:            v1.16.3\n Kube-Proxy Version:         v1.16.3\nPodCIDR:                     10.199.2.0/24\nPodCIDRs:                    10.199.2.0/24\nNon-terminated Pods:         (18 in total)\n  Namespace                  Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  AGE\n  ---------                  ----                                                       ------------  ----------  ---------------  -------------  ---\n  cert-manager               cert-manager-77f5bf4f5-h86rt                               10m (0%)      100m (1%)   32Mi (0%)        128Mi (0%)     18h\n  cert-manager               cert-manager-cainjector-67d4dd59ff-8jhs9                   10m (0%)      100m (1%)   32Mi (0%)        128Mi (0%)     18h\n  cert-manager               cert-manager-webhook-578c59dddd-k697c                      10m (0%)      100m (1%)   32Mi (0%)        128Mi (0%)     18h\n  cpaas-system               agon-75b987dff5-7bht5                                      256m (3%)     2 (25%)     512Mi (3%)       4Gi (25%)      18h\n  cpaas-system               apollo-cfdd64bb4-4lssk                                     256m (3%)     2 (25%)     512Mi (3%)       4Gi (25%)      18h\n  cpaas-system               archon-5fdc59d78c-rhtzm                                    256m (3%)     2 (25%)     512Mi (3%)       4Gi (25%)      18h\n  cpaas-system               auth-controller2-79ff55cd75-cbjbn                          256m (3%)     2 (25%)     512Mi (3%)       4Gi (25%)      18h\n  cpaas-system               blink-7d4fb788b5-s9q49                                     0 (0%)        0 (0%)      0 (0%)           0 (0%)         61m\n  cpaas-system               cluster-registry-controller-manager-76774c98d-7c7mz        512m (6%)     4 (50%)     1Gi (6%)         8Gi (51%)      18h\n  cpaas-system               dex-8448b48ff8-2bd5h                                       256m (3%)     2 (25%)     512Mi (3%)       4Gi (25%)      18h\n  cpaas-system               erebus-5597f9565d-zwjnz                                    256m (3%)     2 (25%)     512Mi (3%)       4Gi (25%)      18h\n  cpaas-system               furion-679c948779-jz6lf                                    256m (3%)     2 (25%)     512Mi (3%)       4Gi (25%)      18h\n  cpaas-system               underlord-5c45b96c5d-nmjjq                                 256m (3%)     2 (25%)     512Mi (3%)       4Gi (25%)      18h\n  kube-system                coredns-66447b44c9-zphmm                                   100m (1%)     0 (0%)      70Mi (0%)        170Mi (1%)     18h\n  kube-system                kube-flannel-8hgpf                                         50m (0%)      300m (3%)   64M (0%)         500M (3%)      18h\n  kube-system                kube-proxy-bnhj6                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         18h\n  sonobuoy                   sonobuoy-e2e-job-f0b9f709fd414134                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         60m\n  sonobuoy                   sonobuoy-systemd-logs-daemon-set-690f17abb3794222-tlb58    0 (0%)        0 (0%)      0 (0%)           0 (0%)         60m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                2740m (34%)      20600m (257%)\n  memory             5475364Ki (33%)  44030584064 (266%)\n  ephemeral-storage  0 (0%)           0 (0%)\nEvents:              <none>\n"
Dec 28 08:54:09.456: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 describe namespace kubectl-9232'
Dec 28 08:54:09.534: INFO: stderr: ""
Dec 28 08:54:09.534: INFO: stdout: "Name:         kubectl-9232\nLabels:       e2e-framework=kubectl\n              e2e-run=f813ffda-7eda-45bb-a767-6bc472fd87d3\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo resource limits.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:54:09.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9232" for this suite.
Dec 28 08:54:37.544: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:54:37.610: INFO: namespace kubectl-9232 deletion completed in 28.073471919s

• [SLOW TEST:30.984 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl describe
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1000
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:54:37.610: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec 28 08:54:37.626: INFO: PodSpec: initContainers in spec.initContainers
Dec 28 08:55:21.721: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-991c1f30-96ff-4294-9096-a6983396a58f", GenerateName:"", Namespace:"init-container-8143", SelfLink:"/api/v1/namespaces/init-container-8143/pods/pod-init-991c1f30-96ff-4294-9096-a6983396a58f", UID:"177000dc-9f27-493f-b821-abf5b9fd258e", ResourceVersion:"291632", Generation:0, CreationTimestamp:v1.Time{Time:time.Time{wall:0x0, ext:63713120077, loc:(*time.Location)(0x84bfb00)}}, DeletionTimestamp:(*v1.Time)(nil), DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"626668592"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ClusterName:"", ManagedFields:[]v1.ManagedFieldsEntry(nil)}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"default-token-xvl44", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(0xc005be9280), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(nil), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xvl44", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"docker.io/library/busybox:1.29", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xvl44", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"k8s.gcr.io/pause:3.1", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:52428800, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"52428800", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"default-token-xvl44", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004fee288), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"hxx-m-2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0037675c0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004fee310)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004fee330)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004fee338), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004fee33c), PreemptionPolicy:(*v1.PreemptionPolicy)(nil), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713120077, loc:(*time.Location)(0x84bfb00)}}, Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713120077, loc:(*time.Location)(0x84bfb00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713120077, loc:(*time.Location)(0x84bfb00)}}, Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:v1.Time{Time:time.Time{wall:0x0, ext:0, loc:(*time.Location)(nil)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713120077, loc:(*time.Location)(0x84bfb00)}}, Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.0.128.16", PodIP:"10.199.0.121", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.199.0.121"}}, StartTime:(*v1.Time)(0xc006aff700), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0008e91f0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0008e9260)}, Ready:false, RestartCount:3, Image:"busybox:1.29", ImageID:"docker-pullable://busybox@sha256:8ccbac733d19c0dd4d70b4f0c1e12245b5fa3ad24758a11035ee505c629c0796", ContainerID:"docker://f0c53874ef69d3f7ad82f422659846d7252e31046553cec8e6208fa3e4633c92", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc006aff740), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"docker.io/library/busybox:1.29", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc006aff720), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"k8s.gcr.io/pause:3.1", ImageID:"", ContainerID:"", Started:(*bool)(0xc004fee3bf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:55:21.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-8143" for this suite.
Dec 28 08:55:33.738: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:55:33.803: INFO: namespace init-container-8143 deletion completed in 12.074807834s

• [SLOW TEST:56.193 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[k8s.io] Pods 
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:55:33.804: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 08:55:33.821: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: creating the pod
STEP: submitting the pod to kubernetes
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:55:35.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6900" for this suite.
Dec 28 08:56:19.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:56:20.041: INFO: namespace pods-6900 deletion completed in 44.072721223s

• [SLOW TEST:46.237 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should support remote command execution over websockets [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:56:20.041: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 28 08:56:20.469: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 28 08:56:23.479: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Listing all of the created validation webhooks
STEP: Creating a configMap that should be mutated
STEP: Deleting the collection of validation webhooks
STEP: Creating a configMap that should not be mutated
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:56:23.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-960" for this suite.
Dec 28 08:56:29.563: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:56:29.632: INFO: namespace webhook-960 deletion completed in 6.075683545s
STEP: Destroying namespace "webhook-960-markers" for this suite.
Dec 28 08:56:35.640: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:56:35.706: INFO: namespace webhook-960-markers deletion completed in 6.073353585s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:15.671 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:56:35.712: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a simple DaemonSet "daemon-set"
STEP: Check that daemon pods launch on every node of the cluster.
Dec 28 08:56:35.745: INFO: Number of nodes with available pods: 0
Dec 28 08:56:35.745: INFO: Node hxx-m-1 is running more than one daemon pod
Dec 28 08:56:36.750: INFO: Number of nodes with available pods: 0
Dec 28 08:56:36.750: INFO: Node hxx-m-1 is running more than one daemon pod
Dec 28 08:56:37.750: INFO: Number of nodes with available pods: 3
Dec 28 08:56:37.750: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived.
Dec 28 08:56:37.760: INFO: Number of nodes with available pods: 3
Dec 28 08:56:37.760: INFO: Number of running nodes: 3, number of available pods: 3
STEP: Wait for the failed daemon pod to be completely deleted.
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-89, will wait for the garbage collector to delete the pods
Dec 28 08:56:37.819: INFO: Deleting DaemonSet.extensions daemon-set took: 3.612706ms
Dec 28 08:56:37.919: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.200557ms
Dec 28 08:56:51.220: INFO: Number of nodes with available pods: 0
Dec 28 08:56:51.221: INFO: Number of running nodes: 0, number of available pods: 0
Dec 28 08:56:51.222: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-89/daemonsets","resourceVersion":"292191"},"items":null}

Dec 28 08:56:51.223: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-89/pods","resourceVersion":"292191"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:56:51.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-89" for this suite.
Dec 28 08:56:57.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:56:57.304: INFO: namespace daemonsets-89 deletion completed in 6.072177047s

• [SLOW TEST:21.592 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod 
  should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:56:57.305: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1668
[It] should create a pod from an image when restart is Never  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 28 08:56:57.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 run e2e-test-httpd-pod --restart=Never --generator=run-pod/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-7182'
Dec 28 08:56:57.399: INFO: stderr: ""
Dec 28 08:56:57.399: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created
[AfterEach] Kubectl run pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1673
Dec 28 08:56:57.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 delete pods e2e-test-httpd-pod --namespace=kubectl-7182'
Dec 28 08:57:10.928: INFO: stderr: ""
Dec 28 08:57:10.928: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:57:10.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7182" for this suite.
Dec 28 08:57:16.940: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:57:17.009: INFO: namespace kubectl-7182 deletion completed in 6.077841728s

• [SLOW TEST:19.705 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1664
    should create a pod from an image when restart is Never  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:57:17.010: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0777 on tmpfs
Dec 28 08:57:17.034: INFO: Waiting up to 5m0s for pod "pod-f5a93fdf-2963-4e75-8557-6507bfe08352" in namespace "emptydir-7644" to be "success or failure"
Dec 28 08:57:17.036: INFO: Pod "pod-f5a93fdf-2963-4e75-8557-6507bfe08352": Phase="Pending", Reason="", readiness=false. Elapsed: 1.957769ms
Dec 28 08:57:19.038: INFO: Pod "pod-f5a93fdf-2963-4e75-8557-6507bfe08352": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004344699s
STEP: Saw pod success
Dec 28 08:57:19.038: INFO: Pod "pod-f5a93fdf-2963-4e75-8557-6507bfe08352" satisfied condition "success or failure"
Dec 28 08:57:19.039: INFO: Trying to get logs from node hxx-m-2 pod pod-f5a93fdf-2963-4e75-8557-6507bfe08352 container test-container: <nil>
STEP: delete the pod
Dec 28 08:57:19.055: INFO: Waiting for pod pod-f5a93fdf-2963-4e75-8557-6507bfe08352 to disappear
Dec 28 08:57:19.057: INFO: Pod pod-f5a93fdf-2963-4e75-8557-6507bfe08352 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:57:19.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7644" for this suite.
Dec 28 08:57:25.067: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:57:25.133: INFO: namespace emptydir-7644 deletion completed in 6.07355156s

• [SLOW TEST:8.123 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:57:25.134: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 28 08:57:25.154: INFO: Waiting up to 5m0s for pod "pod-53e4769b-0a10-43e5-ba42-2d80f86bdc7c" in namespace "emptydir-3659" to be "success or failure"
Dec 28 08:57:25.155: INFO: Pod "pod-53e4769b-0a10-43e5-ba42-2d80f86bdc7c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.492751ms
Dec 28 08:57:27.158: INFO: Pod "pod-53e4769b-0a10-43e5-ba42-2d80f86bdc7c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004010747s
STEP: Saw pod success
Dec 28 08:57:27.158: INFO: Pod "pod-53e4769b-0a10-43e5-ba42-2d80f86bdc7c" satisfied condition "success or failure"
Dec 28 08:57:27.159: INFO: Trying to get logs from node hxx-m-2 pod pod-53e4769b-0a10-43e5-ba42-2d80f86bdc7c container test-container: <nil>
STEP: delete the pod
Dec 28 08:57:27.170: INFO: Waiting for pod pod-53e4769b-0a10-43e5-ba42-2d80f86bdc7c to disappear
Dec 28 08:57:27.172: INFO: Pod pod-53e4769b-0a10-43e5-ba42-2d80f86bdc7c no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:57:27.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3659" for this suite.
Dec 28 08:57:33.182: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:57:33.252: INFO: namespace emptydir-3659 deletion completed in 6.07741713s

• [SLOW TEST:8.119 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info 
  should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:57:33.253: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should check if Kubernetes master services is included in cluster-info  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: validating cluster-info
Dec 28 08:57:33.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 cluster-info'
Dec 28 08:57:33.352: INFO: stderr: ""
Dec 28 08:57:33.352: INFO: stdout: "\x1b[0;32mKubernetes master\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\x1b[0;32mKubeDNS\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:57:33.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-721" for this suite.
Dec 28 08:57:39.361: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:57:39.433: INFO: namespace kubectl-721 deletion completed in 6.078594001s

• [SLOW TEST:6.180 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:974
    should check if Kubernetes master services is included in cluster-info  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Services 
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:57:39.433: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should provide secure master service  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:57:39.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1824" for this suite.
Dec 28 08:57:45.461: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:57:45.522: INFO: namespace services-1824 deletion completed in 6.068001793s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:6.089 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide secure master service  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-auth] ServiceAccounts 
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:57:45.522: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename svcaccounts
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: getting the auto-created API token
Dec 28 08:57:46.048: INFO: created pod pod-service-account-defaultsa
Dec 28 08:57:46.048: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Dec 28 08:57:46.051: INFO: created pod pod-service-account-mountsa
Dec 28 08:57:46.051: INFO: pod pod-service-account-mountsa service account token volume mount: true
Dec 28 08:57:46.054: INFO: created pod pod-service-account-nomountsa
Dec 28 08:57:46.054: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Dec 28 08:57:46.057: INFO: created pod pod-service-account-defaultsa-mountspec
Dec 28 08:57:46.057: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Dec 28 08:57:46.062: INFO: created pod pod-service-account-mountsa-mountspec
Dec 28 08:57:46.062: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Dec 28 08:57:46.066: INFO: created pod pod-service-account-nomountsa-mountspec
Dec 28 08:57:46.066: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Dec 28 08:57:46.072: INFO: created pod pod-service-account-defaultsa-nomountspec
Dec 28 08:57:46.072: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Dec 28 08:57:46.074: INFO: created pod pod-service-account-mountsa-nomountspec
Dec 28 08:57:46.074: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Dec 28 08:57:46.077: INFO: created pod pod-service-account-nomountsa-nomountspec
Dec 28 08:57:46.077: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:57:46.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-8455" for this suite.
Dec 28 08:59:20.087: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:59:20.155: INFO: namespace svcaccounts-8455 deletion completed in 1m34.074959979s

• [SLOW TEST:94.632 seconds]
[sig-auth] ServiceAccounts
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:59:20.155: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 28 08:59:24.189: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 28 08:59:24.190: INFO: Pod pod-with-prestop-http-hook still exists
Dec 28 08:59:26.191: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 28 08:59:26.193: INFO: Pod pod-with-prestop-http-hook still exists
Dec 28 08:59:28.191: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Dec 28 08:59:28.193: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:59:28.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4464" for this suite.
Dec 28 08:59:40.218: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 08:59:40.283: INFO: namespace container-lifecycle-hook-4464 deletion completed in 12.07756788s

• [SLOW TEST:20.130 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 08:59:40.285: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation
Dec 28 08:59:40.302: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 08:59:44.035: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 08:59:58.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8417" for this suite.
Dec 28 09:00:04.606: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:00:04.675: INFO: namespace crd-publish-openapi-8417 deletion completed in 6.076834821s

• [SLOW TEST:24.391 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:00:04.676: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 28 09:00:05.342: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 28 09:00:07.349: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713120405, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713120405, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713120405, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713120405, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 28 09:00:10.360: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API
STEP: Creating a dummy validating-webhook-configuration object
STEP: Deleting the validating-webhook-configuration, which should be possible to remove
STEP: Creating a dummy mutating-webhook-configuration object
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:00:10.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7945" for this suite.
Dec 28 09:00:16.403: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:00:16.469: INFO: namespace webhook-7945 deletion completed in 6.072186547s
STEP: Destroying namespace "webhook-7945-markers" for this suite.
Dec 28 09:00:22.476: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:00:22.540: INFO: namespace webhook-7945-markers deletion completed in 6.070985924s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:17.870 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-apps] Deployment 
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:00:22.546: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 09:00:22.566: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Dec 28 09:00:22.570: INFO: Pod name sample-pod: Found 0 pods out of 1
Dec 28 09:00:27.573: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running
Dec 28 09:00:27.573: INFO: Creating deployment "test-rolling-update-deployment"
Dec 28 09:00:27.575: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Dec 28 09:00:27.578: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Dec 28 09:00:29.582: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Dec 28 09:00:29.584: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 28 09:00:29.588: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-7033 /apis/apps/v1/namespaces/deployment-7033/deployments/test-rolling-update-deployment 3b9c9026-2375-44a6-9563-9ce0ff21e8dc 293444 1 2019-12-28 09:00:27 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc004af3a18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2019-12-28 09:00:27 +0000 UTC,LastTransitionTime:2019-12-28 09:00:27 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-55d946486" has successfully progressed.,LastUpdateTime:2019-12-28 09:00:29 +0000 UTC,LastTransitionTime:2019-12-28 09:00:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Dec 28 09:00:29.590: INFO: New ReplicaSet "test-rolling-update-deployment-55d946486" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-55d946486  deployment-7033 /apis/apps/v1/namespaces/deployment-7033/replicasets/test-rolling-update-deployment-55d946486 cea9e43b-6f7a-4c84-8e25-326b3b8eda02 293433 1 2019-12-28 09:00:27 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 3b9c9026-2375-44a6-9563-9ce0ff21e8dc 0xc008dc6920 0xc008dc6921}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 55d946486,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc008dc69c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Dec 28 09:00:29.590: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Dec 28 09:00:29.590: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-7033 /apis/apps/v1/namespaces/deployment-7033/replicasets/test-rolling-update-controller 71ff0e72-704c-4148-bdf9-45e054e3c249 293443 2 2019-12-28 09:00:22 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 3b9c9026-2375-44a6-9563-9ce0ff21e8dc 0xc008dc6857 0xc008dc6858}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc008dc68b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 28 09:00:29.592: INFO: Pod "test-rolling-update-deployment-55d946486-bsnnz" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-55d946486-bsnnz test-rolling-update-deployment-55d946486- deployment-7033 /api/v1/namespaces/deployment-7033/pods/test-rolling-update-deployment-55d946486-bsnnz 53045e95-ecd2-4fe9-b398-a53afef43457 293432 0 2019-12-28 09:00:27 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:55d946486] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-55d946486 cea9e43b-6f7a-4c84-8e25-326b3b8eda02 0xc007f42050 0xc007f42051}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-2jx5d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-2jx5d,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:redis,Image:docker.io/library/redis:5.0.5-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-2jx5d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:hxx-m-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:00:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:00:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:00:29 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:00:27 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.16,PodIP:10.199.0.141,StartTime:2019-12-28 09:00:27 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:redis,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-28 09:00:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:redis:5.0.5-alpine,ImageID:docker-pullable://redis@sha256:50899ea1ceed33fa03232f3ac57578a424faa1742c1ac9c7a7bdb95cdf19b858,ContainerID:docker://d6e8427fedd339da57fde73efe6a6e5950d8ff287108a6072b45e8f8e8d214d4,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.199.0.141,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:00:29.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-7033" for this suite.
Dec 28 09:00:35.601: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:00:35.668: INFO: namespace deployment-7033 deletion completed in 6.073748559s

• [SLOW TEST:13.122 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:00:35.668: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on node default medium
Dec 28 09:00:35.687: INFO: Waiting up to 5m0s for pod "pod-82197b49-2b06-47b3-8980-e2689156f365" in namespace "emptydir-1913" to be "success or failure"
Dec 28 09:00:35.689: INFO: Pod "pod-82197b49-2b06-47b3-8980-e2689156f365": Phase="Pending", Reason="", readiness=false. Elapsed: 1.716599ms
Dec 28 09:00:37.691: INFO: Pod "pod-82197b49-2b06-47b3-8980-e2689156f365": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003806421s
STEP: Saw pod success
Dec 28 09:00:37.691: INFO: Pod "pod-82197b49-2b06-47b3-8980-e2689156f365" satisfied condition "success or failure"
Dec 28 09:00:37.693: INFO: Trying to get logs from node hxx-m-2 pod pod-82197b49-2b06-47b3-8980-e2689156f365 container test-container: <nil>
STEP: delete the pod
Dec 28 09:00:37.702: INFO: Waiting for pod pod-82197b49-2b06-47b3-8980-e2689156f365 to disappear
Dec 28 09:00:37.704: INFO: Pod pod-82197b49-2b06-47b3-8980-e2689156f365 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:00:37.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1913" for this suite.
Dec 28 09:00:43.714: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:00:43.784: INFO: namespace emptydir-1913 deletion completed in 6.077291161s

• [SLOW TEST:8.116 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:00:43.784: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-8975
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a new StatefulSet
Dec 28 09:00:43.806: INFO: Found 0 stateful pods, waiting for 3
Dec 28 09:00:53.808: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 28 09:00:53.809: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 28 09:00:53.809: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from docker.io/library/httpd:2.4.38-alpine to docker.io/library/httpd:2.4.39-alpine
Dec 28 09:00:53.828: INFO: Updating stateful set ss2
STEP: Creating a new revision
STEP: Not applying an update when the partition is greater than the number of replicas
STEP: Performing a canary update
Dec 28 09:01:03.850: INFO: Updating stateful set ss2
Dec 28 09:01:03.855: INFO: Waiting for Pod statefulset-8975/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 28 09:01:13.860: INFO: Waiting for Pod statefulset-8975/ss2-2 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
STEP: Restoring Pods to the correct revision when they are deleted
Dec 28 09:01:23.889: INFO: Found 2 stateful pods, waiting for 3
Dec 28 09:01:33.891: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 28 09:01:33.891: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 28 09:01:33.891: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update
Dec 28 09:01:33.909: INFO: Updating stateful set ss2
Dec 28 09:01:33.913: INFO: Waiting for Pod statefulset-8975/ss2-1 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
Dec 28 09:01:43.931: INFO: Updating stateful set ss2
Dec 28 09:01:43.935: INFO: Waiting for StatefulSet statefulset-8975/ss2 to complete update
Dec 28 09:01:43.935: INFO: Waiting for Pod statefulset-8975/ss2-0 to have revision ss2-84f9d6bf57 update revision ss2-65c7964b94
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 28 09:01:53.939: INFO: Deleting all statefulset in ns statefulset-8975
Dec 28 09:01:53.940: INFO: Scaling statefulset ss2 to 0
Dec 28 09:02:13.949: INFO: Waiting for statefulset status.replicas updated to 0
Dec 28 09:02:13.955: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:02:13.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8975" for this suite.
Dec 28 09:02:19.976: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:02:20.043: INFO: namespace statefulset-8975 deletion completed in 6.077798022s

• [SLOW TEST:96.260 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:02:20.044: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 28 09:02:20.064: INFO: Waiting up to 5m0s for pod "downwardapi-volume-96a012a2-9706-4b20-a03c-e65a66f1c42a" in namespace "downward-api-8615" to be "success or failure"
Dec 28 09:02:20.069: INFO: Pod "downwardapi-volume-96a012a2-9706-4b20-a03c-e65a66f1c42a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.517592ms
Dec 28 09:02:22.071: INFO: Pod "downwardapi-volume-96a012a2-9706-4b20-a03c-e65a66f1c42a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.006808439s
STEP: Saw pod success
Dec 28 09:02:22.071: INFO: Pod "downwardapi-volume-96a012a2-9706-4b20-a03c-e65a66f1c42a" satisfied condition "success or failure"
Dec 28 09:02:22.072: INFO: Trying to get logs from node hxx-m-2 pod downwardapi-volume-96a012a2-9706-4b20-a03c-e65a66f1c42a container client-container: <nil>
STEP: delete the pod
Dec 28 09:02:22.088: INFO: Waiting for pod downwardapi-volume-96a012a2-9706-4b20-a03c-e65a66f1c42a to disappear
Dec 28 09:02:22.090: INFO: Pod downwardapi-volume-96a012a2-9706-4b20-a03c-e65a66f1c42a no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:02:22.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8615" for this suite.
Dec 28 09:02:28.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:02:28.167: INFO: namespace downward-api-8615 deletion completed in 6.073804932s

• [SLOW TEST:8.123 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run job 
  should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:02:28.167: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl run job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1595
[It] should create a job from an image when restart is OnFailure  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 28 09:02:28.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 run e2e-test-httpd-job --restart=OnFailure --generator=job/v1 --image=docker.io/library/httpd:2.4.38-alpine --namespace=kubectl-1517'
Dec 28 09:02:28.263: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 28 09:02:28.263: INFO: stdout: "job.batch/e2e-test-httpd-job created\n"
STEP: verifying the job e2e-test-httpd-job was created
[AfterEach] Kubectl run job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1600
Dec 28 09:02:28.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 delete jobs e2e-test-httpd-job --namespace=kubectl-1517'
Dec 28 09:02:28.348: INFO: stderr: ""
Dec 28 09:02:28.348: INFO: stdout: "job.batch \"e2e-test-httpd-job\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:02:28.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1517" for this suite.
Dec 28 09:02:56.359: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:02:56.425: INFO: namespace kubectl-1517 deletion completed in 28.073606055s

• [SLOW TEST:28.258 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1591
    should create a job from an image when restart is OnFailure  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:02:56.425: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Service
STEP: Ensuring resource quota status captures service creation
STEP: Deleting a Service
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:03:07.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-5759" for this suite.
Dec 28 09:03:13.479: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:03:13.549: INFO: namespace resourcequota-5759 deletion completed in 6.076900616s

• [SLOW TEST:17.124 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Probing container 
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:03:13.549: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod liveness-072843ac-f988-4c57-8aef-880db597b20a in namespace container-probe-7928
Dec 28 09:03:15.574: INFO: Started pod liveness-072843ac-f988-4c57-8aef-880db597b20a in namespace container-probe-7928
STEP: checking the pod's current state and verifying that restartCount is present
Dec 28 09:03:15.576: INFO: Initial restart count of pod liveness-072843ac-f988-4c57-8aef-880db597b20a is 0
Dec 28 09:03:35.601: INFO: Restart count of pod container-probe-7928/liveness-072843ac-f988-4c57-8aef-880db597b20a is now 1 (20.025071031s elapsed)
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:03:35.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7928" for this suite.
Dec 28 09:03:41.622: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:03:41.687: INFO: namespace container-probe-7928 deletion completed in 6.073261172s

• [SLOW TEST:28.139 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:03:41.688: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-51a8a17f-fd2d-440f-a754-a0b11c00001d
STEP: Creating a pod to test consume secrets
Dec 28 09:03:41.709: INFO: Waiting up to 5m0s for pod "pod-secrets-1e9f0f5b-73e2-4786-a9d9-d7816523b083" in namespace "secrets-5798" to be "success or failure"
Dec 28 09:03:41.710: INFO: Pod "pod-secrets-1e9f0f5b-73e2-4786-a9d9-d7816523b083": Phase="Pending", Reason="", readiness=false. Elapsed: 1.666107ms
Dec 28 09:03:43.713: INFO: Pod "pod-secrets-1e9f0f5b-73e2-4786-a9d9-d7816523b083": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004057014s
STEP: Saw pod success
Dec 28 09:03:43.713: INFO: Pod "pod-secrets-1e9f0f5b-73e2-4786-a9d9-d7816523b083" satisfied condition "success or failure"
Dec 28 09:03:43.714: INFO: Trying to get logs from node hxx-m-2 pod pod-secrets-1e9f0f5b-73e2-4786-a9d9-d7816523b083 container secret-volume-test: <nil>
STEP: delete the pod
Dec 28 09:03:43.724: INFO: Waiting for pod pod-secrets-1e9f0f5b-73e2-4786-a9d9-d7816523b083 to disappear
Dec 28 09:03:43.725: INFO: Pod pod-secrets-1e9f0f5b-73e2-4786-a9d9-d7816523b083 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:03:43.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5798" for this suite.
Dec 28 09:03:49.735: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:03:49.802: INFO: namespace secrets-5798 deletion completed in 6.074592637s

• [SLOW TEST:8.114 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition 
  listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:03:49.802: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename custom-resource-definition
STEP: Waiting for a default service account to be provisioned in namespace
[It] listing custom resource definition objects works  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 09:03:49.819: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:03:55.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6551" for this suite.
Dec 28 09:04:01.477: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:04:01.552: INFO: namespace custom-resource-definition-6551 deletion completed in 6.089166917s

• [SLOW TEST:11.749 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/custom_resource_definition.go:42
    listing custom resource definition objects works  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:04:01.552: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 28 09:04:05.596: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 28 09:04:05.598: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 28 09:04:07.598: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 28 09:04:07.600: INFO: Pod pod-with-poststart-exec-hook still exists
Dec 28 09:04:09.598: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Dec 28 09:04:09.600: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:04:09.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3733" for this suite.
Dec 28 09:04:21.609: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:04:21.679: INFO: namespace container-lifecycle-hook-3733 deletion completed in 12.076520611s

• [SLOW TEST:20.127 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] PreStop 
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:04:21.679: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename prestop
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pre_stop.go:173
[It] should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating server pod server in namespace prestop-2072
STEP: Waiting for pods to come up.
STEP: Creating tester pod tester in namespace prestop-2072
STEP: Deleting pre-stop pod
Dec 28 09:04:30.724: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod
[AfterEach] [k8s.io] [sig-node] PreStop
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:04:30.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-2072" for this suite.
Dec 28 09:05:14.739: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:05:14.807: INFO: namespace prestop-2072 deletion completed in 44.076592491s

• [SLOW TEST:53.128 seconds]
[k8s.io] [sig-node] PreStop
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should call prestop when killing a pod  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:05:14.807: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name s-test-opt-del-f0a47727-59d4-4512-b901-3fd680717987
STEP: Creating secret with name s-test-opt-upd-2e93b636-2ee9-400e-8f31-dc99cc151f93
STEP: Creating the pod
STEP: Deleting secret s-test-opt-del-f0a47727-59d4-4512-b901-3fd680717987
STEP: Updating secret s-test-opt-upd-2e93b636-2ee9-400e-8f31-dc99cc151f93
STEP: Creating secret with name s-test-opt-create-70a09ec6-b6e3-4a4b-9517-5f4dae3c200a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:06:45.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5162" for this suite.
Dec 28 09:07:13.159: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:07:13.230: INFO: namespace secrets-5162 deletion completed in 28.081749485s

• [SLOW TEST:118.423 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:07:13.230: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 28 09:07:13.250: INFO: Waiting up to 5m0s for pod "downwardapi-volume-be9ee1e1-931c-4f02-880a-feea3fec9144" in namespace "downward-api-4020" to be "success or failure"
Dec 28 09:07:13.252: INFO: Pod "downwardapi-volume-be9ee1e1-931c-4f02-880a-feea3fec9144": Phase="Pending", Reason="", readiness=false. Elapsed: 1.378303ms
Dec 28 09:07:15.254: INFO: Pod "downwardapi-volume-be9ee1e1-931c-4f02-880a-feea3fec9144": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003769836s
STEP: Saw pod success
Dec 28 09:07:15.254: INFO: Pod "downwardapi-volume-be9ee1e1-931c-4f02-880a-feea3fec9144" satisfied condition "success or failure"
Dec 28 09:07:15.256: INFO: Trying to get logs from node hxx-m-2 pod downwardapi-volume-be9ee1e1-931c-4f02-880a-feea3fec9144 container client-container: <nil>
STEP: delete the pod
Dec 28 09:07:15.266: INFO: Waiting for pod downwardapi-volume-be9ee1e1-931c-4f02-880a-feea3fec9144 to disappear
Dec 28 09:07:15.268: INFO: Pod downwardapi-volume-be9ee1e1-931c-4f02-880a-feea3fec9144 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:07:15.268: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4020" for this suite.
Dec 28 09:07:21.277: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:07:21.342: INFO: namespace downward-api-4020 deletion completed in 6.07222542s

• [SLOW TEST:8.112 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory request [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:07:21.343: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 09:07:21.359: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 28 09:07:25.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 --namespace=crd-publish-openapi-4308 create -f -'
Dec 28 09:07:25.360: INFO: stderr: ""
Dec 28 09:07:25.360: INFO: stdout: "e2e-test-crd-publish-openapi-3793-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 28 09:07:25.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 --namespace=crd-publish-openapi-4308 delete e2e-test-crd-publish-openapi-3793-crds test-cr'
Dec 28 09:07:25.438: INFO: stderr: ""
Dec 28 09:07:25.438: INFO: stdout: "e2e-test-crd-publish-openapi-3793-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Dec 28 09:07:25.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 --namespace=crd-publish-openapi-4308 apply -f -'
Dec 28 09:07:25.586: INFO: stderr: ""
Dec 28 09:07:25.586: INFO: stdout: "e2e-test-crd-publish-openapi-3793-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Dec 28 09:07:25.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 --namespace=crd-publish-openapi-4308 delete e2e-test-crd-publish-openapi-3793-crds test-cr'
Dec 28 09:07:25.662: INFO: stderr: ""
Dec 28 09:07:25.662: INFO: stdout: "e2e-test-crd-publish-openapi-3793-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 28 09:07:25.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 explain e2e-test-crd-publish-openapi-3793-crds'
Dec 28 09:07:25.805: INFO: stderr: ""
Dec 28 09:07:25.805: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-3793-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<map[string]>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:07:29.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4308" for this suite.
Dec 28 09:07:35.553: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:07:35.620: INFO: namespace crd-publish-openapi-4308 deletion completed in 6.074711535s

• [SLOW TEST:14.277 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:07:35.620: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-map-6c4710e6-813b-42b5-9f0e-17e5948c27cd
STEP: Creating a pod to test consume secrets
Dec 28 09:07:35.642: INFO: Waiting up to 5m0s for pod "pod-secrets-34b1e023-9451-4418-8073-719e14feb010" in namespace "secrets-3001" to be "success or failure"
Dec 28 09:07:35.643: INFO: Pod "pod-secrets-34b1e023-9451-4418-8073-719e14feb010": Phase="Pending", Reason="", readiness=false. Elapsed: 1.44011ms
Dec 28 09:07:37.646: INFO: Pod "pod-secrets-34b1e023-9451-4418-8073-719e14feb010": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003808348s
STEP: Saw pod success
Dec 28 09:07:37.646: INFO: Pod "pod-secrets-34b1e023-9451-4418-8073-719e14feb010" satisfied condition "success or failure"
Dec 28 09:07:37.647: INFO: Trying to get logs from node hxx-m-2 pod pod-secrets-34b1e023-9451-4418-8073-719e14feb010 container secret-volume-test: <nil>
STEP: delete the pod
Dec 28 09:07:37.657: INFO: Waiting for pod pod-secrets-34b1e023-9451-4418-8073-719e14feb010 to disappear
Dec 28 09:07:37.659: INFO: Pod pod-secrets-34b1e023-9451-4418-8073-719e14feb010 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:07:37.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3001" for this suite.
Dec 28 09:07:43.668: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:07:43.733: INFO: namespace secrets-3001 deletion completed in 6.072080256s

• [SLOW TEST:8.113 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:07:43.734: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-0b76544c-63de-4838-bacb-15e98a181aec
STEP: Creating a pod to test consume secrets
Dec 28 09:07:43.755: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-45e071fb-e406-4330-a80e-26b8e5eb594f" in namespace "projected-7935" to be "success or failure"
Dec 28 09:07:43.758: INFO: Pod "pod-projected-secrets-45e071fb-e406-4330-a80e-26b8e5eb594f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.531607ms
Dec 28 09:07:45.760: INFO: Pod "pod-projected-secrets-45e071fb-e406-4330-a80e-26b8e5eb594f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004770848s
STEP: Saw pod success
Dec 28 09:07:45.760: INFO: Pod "pod-projected-secrets-45e071fb-e406-4330-a80e-26b8e5eb594f" satisfied condition "success or failure"
Dec 28 09:07:45.762: INFO: Trying to get logs from node hxx-m-2 pod pod-projected-secrets-45e071fb-e406-4330-a80e-26b8e5eb594f container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 28 09:07:45.772: INFO: Waiting for pod pod-projected-secrets-45e071fb-e406-4330-a80e-26b8e5eb594f to disappear
Dec 28 09:07:45.773: INFO: Pod pod-projected-secrets-45e071fb-e406-4330-a80e-26b8e5eb594f no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:07:45.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7935" for this suite.
Dec 28 09:07:51.783: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:07:51.856: INFO: namespace projected-7935 deletion completed in 6.080417172s

• [SLOW TEST:8.122 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes 
  should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:07:51.857: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename subpath
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:37
STEP: Setting up data
[It] should support subpaths with secret pod [LinuxOnly] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod pod-subpath-test-secret-rv8q
STEP: Creating a pod to test atomic-volume-subpath
Dec 28 09:07:51.880: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-rv8q" in namespace "subpath-5163" to be "success or failure"
Dec 28 09:07:51.882: INFO: Pod "pod-subpath-test-secret-rv8q": Phase="Pending", Reason="", readiness=false. Elapsed: 1.486403ms
Dec 28 09:07:53.884: INFO: Pod "pod-subpath-test-secret-rv8q": Phase="Running", Reason="", readiness=true. Elapsed: 2.003539507s
Dec 28 09:07:55.886: INFO: Pod "pod-subpath-test-secret-rv8q": Phase="Running", Reason="", readiness=true. Elapsed: 4.005721616s
Dec 28 09:07:57.888: INFO: Pod "pod-subpath-test-secret-rv8q": Phase="Running", Reason="", readiness=true. Elapsed: 6.007854152s
Dec 28 09:07:59.890: INFO: Pod "pod-subpath-test-secret-rv8q": Phase="Running", Reason="", readiness=true. Elapsed: 8.009949093s
Dec 28 09:08:01.893: INFO: Pod "pod-subpath-test-secret-rv8q": Phase="Running", Reason="", readiness=true. Elapsed: 10.012533396s
Dec 28 09:08:03.895: INFO: Pod "pod-subpath-test-secret-rv8q": Phase="Running", Reason="", readiness=true. Elapsed: 12.014720506s
Dec 28 09:08:05.898: INFO: Pod "pod-subpath-test-secret-rv8q": Phase="Running", Reason="", readiness=true. Elapsed: 14.017213843s
Dec 28 09:08:07.900: INFO: Pod "pod-subpath-test-secret-rv8q": Phase="Running", Reason="", readiness=true. Elapsed: 16.019716417s
Dec 28 09:08:09.902: INFO: Pod "pod-subpath-test-secret-rv8q": Phase="Running", Reason="", readiness=true. Elapsed: 18.021992086s
Dec 28 09:08:11.905: INFO: Pod "pod-subpath-test-secret-rv8q": Phase="Running", Reason="", readiness=true. Elapsed: 20.024479536s
Dec 28 09:08:13.907: INFO: Pod "pod-subpath-test-secret-rv8q": Phase="Succeeded", Reason="", readiness=false. Elapsed: 22.026741508s
STEP: Saw pod success
Dec 28 09:08:13.907: INFO: Pod "pod-subpath-test-secret-rv8q" satisfied condition "success or failure"
Dec 28 09:08:13.909: INFO: Trying to get logs from node hxx-m-2 pod pod-subpath-test-secret-rv8q container test-container-subpath-secret-rv8q: <nil>
STEP: delete the pod
Dec 28 09:08:13.919: INFO: Waiting for pod pod-subpath-test-secret-rv8q to disappear
Dec 28 09:08:13.921: INFO: Pod pod-subpath-test-secret-rv8q no longer exists
STEP: Deleting pod pod-subpath-test-secret-rv8q
Dec 28 09:08:13.921: INFO: Deleting pod "pod-subpath-test-secret-rv8q" in namespace "subpath-5163"
[AfterEach] [sig-storage] Subpath
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:08:13.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5163" for this suite.
Dec 28 09:08:19.933: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:08:19.999: INFO: namespace subpath-5163 deletion completed in 6.073672108s

• [SLOW TEST:28.142 seconds]
[sig-storage] Subpath
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/subpath.go:33
    should support subpaths with secret pod [LinuxOnly] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:08:19.999: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ReplicaSet
STEP: Ensuring resource quota status captures replicaset creation
STEP: Deleting a ReplicaSet
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:08:31.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6330" for this suite.
Dec 28 09:08:37.047: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:08:37.112: INFO: namespace resourcequota-6330 deletion completed in 6.07233095s

• [SLOW TEST:17.113 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:08:37.112: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-14b8a95d-084a-495a-a28b-1e9979620511
STEP: Creating a pod to test consume secrets
Dec 28 09:08:37.135: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-99c651c7-ba0d-45e9-9ed7-f2b057bb19c3" in namespace "projected-8947" to be "success or failure"
Dec 28 09:08:37.136: INFO: Pod "pod-projected-secrets-99c651c7-ba0d-45e9-9ed7-f2b057bb19c3": Phase="Pending", Reason="", readiness=false. Elapsed: 1.346858ms
Dec 28 09:08:39.138: INFO: Pod "pod-projected-secrets-99c651c7-ba0d-45e9-9ed7-f2b057bb19c3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003658377s
STEP: Saw pod success
Dec 28 09:08:39.139: INFO: Pod "pod-projected-secrets-99c651c7-ba0d-45e9-9ed7-f2b057bb19c3" satisfied condition "success or failure"
Dec 28 09:08:39.140: INFO: Trying to get logs from node hxx-m-2 pod pod-projected-secrets-99c651c7-ba0d-45e9-9ed7-f2b057bb19c3 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 28 09:08:39.151: INFO: Waiting for pod pod-projected-secrets-99c651c7-ba0d-45e9-9ed7-f2b057bb19c3 to disappear
Dec 28 09:08:39.152: INFO: Pod pod-projected-secrets-99c651c7-ba0d-45e9-9ed7-f2b057bb19c3 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:08:39.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8947" for this suite.
Dec 28 09:08:45.169: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:08:45.236: INFO: namespace projected-8947 deletion completed in 6.081161783s

• [SLOW TEST:8.124 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[k8s.io] Pods 
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:08:45.236: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up watch
STEP: submitting the pod to kubernetes
Dec 28 09:08:45.256: INFO: observed the pod list
STEP: verifying the pod is in kubernetes
STEP: verifying pod creation was observed
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec 28 09:08:52.272: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
STEP: verifying pod deletion was observed
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:08:52.274: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1447" for this suite.
Dec 28 09:08:58.283: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:08:58.348: INFO: namespace pods-1447 deletion completed in 6.071986363s

• [SLOW TEST:13.112 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be submitted and removed [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:08:58.348: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating replication controller my-hostname-basic-20fc158e-3636-42d6-8843-83fa6d659179
Dec 28 09:08:58.369: INFO: Pod name my-hostname-basic-20fc158e-3636-42d6-8843-83fa6d659179: Found 0 pods out of 1
Dec 28 09:09:03.372: INFO: Pod name my-hostname-basic-20fc158e-3636-42d6-8843-83fa6d659179: Found 1 pods out of 1
Dec 28 09:09:03.372: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-20fc158e-3636-42d6-8843-83fa6d659179" are running
Dec 28 09:09:03.374: INFO: Pod "my-hostname-basic-20fc158e-3636-42d6-8843-83fa6d659179-887dj" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-28 09:08:58 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-28 09:09:00 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-28 09:09:00 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-28 09:08:58 +0000 UTC Reason: Message:}])
Dec 28 09:09:03.374: INFO: Trying to dial the pod
Dec 28 09:09:08.380: INFO: Controller my-hostname-basic-20fc158e-3636-42d6-8843-83fa6d659179: Got expected result from replica 1 [my-hostname-basic-20fc158e-3636-42d6-8843-83fa6d659179-887dj]: "my-hostname-basic-20fc158e-3636-42d6-8843-83fa6d659179-887dj", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:09:08.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2699" for this suite.
Dec 28 09:09:14.389: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:09:14.453: INFO: namespace replication-controller-2699 deletion completed in 6.07125066s

• [SLOW TEST:16.105 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-cli] Kubectl client Update Demo 
  should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:09:14.454: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should do a rolling update of a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the initial replication controller
Dec 28 09:09:14.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 create -f - --namespace=kubectl-628'
Dec 28 09:09:14.675: INFO: stderr: ""
Dec 28 09:09:14.675: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 28 09:09:14.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-628'
Dec 28 09:09:14.750: INFO: stderr: ""
Dec 28 09:09:14.750: INFO: stdout: "update-demo-nautilus-679rr update-demo-nautilus-72dkj "
Dec 28 09:09:14.750: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-679rr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-628'
Dec 28 09:09:14.824: INFO: stderr: ""
Dec 28 09:09:14.824: INFO: stdout: ""
Dec 28 09:09:14.824: INFO: update-demo-nautilus-679rr is created but not running
Dec 28 09:09:19.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-628'
Dec 28 09:09:19.904: INFO: stderr: ""
Dec 28 09:09:19.904: INFO: stdout: "update-demo-nautilus-679rr update-demo-nautilus-72dkj "
Dec 28 09:09:19.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-679rr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-628'
Dec 28 09:09:19.980: INFO: stderr: ""
Dec 28 09:09:19.980: INFO: stdout: "true"
Dec 28 09:09:19.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-679rr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-628'
Dec 28 09:09:20.049: INFO: stderr: ""
Dec 28 09:09:20.049: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 28 09:09:20.049: INFO: validating pod update-demo-nautilus-679rr
Dec 28 09:09:20.052: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 28 09:09:20.052: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 28 09:09:20.052: INFO: update-demo-nautilus-679rr is verified up and running
Dec 28 09:09:20.052: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-72dkj -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-628'
Dec 28 09:09:20.129: INFO: stderr: ""
Dec 28 09:09:20.129: INFO: stdout: "true"
Dec 28 09:09:20.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-72dkj -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-628'
Dec 28 09:09:20.206: INFO: stderr: ""
Dec 28 09:09:20.206: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 28 09:09:20.206: INFO: validating pod update-demo-nautilus-72dkj
Dec 28 09:09:20.209: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 28 09:09:20.209: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 28 09:09:20.209: INFO: update-demo-nautilus-72dkj is verified up and running
STEP: rolling-update to new replication controller
Dec 28 09:09:20.211: INFO: scanned /root for discovery docs: <nil>
Dec 28 09:09:20.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 rolling-update update-demo-nautilus --update-period=1s -f - --namespace=kubectl-628'
Dec 28 09:09:42.489: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 28 09:09:42.489: INFO: stdout: "Created update-demo-kitten\nScaling up update-demo-kitten from 0 to 2, scaling down update-demo-nautilus from 2 to 0 (keep 2 pods available, don't exceed 3 pods)\nScaling update-demo-kitten up to 1\nScaling update-demo-nautilus down to 1\nScaling update-demo-kitten up to 2\nScaling update-demo-nautilus down to 0\nUpdate succeeded. Deleting old controller: update-demo-nautilus\nRenaming update-demo-kitten to update-demo-nautilus\nreplicationcontroller/update-demo-nautilus rolling updated\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 28 09:09:42.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-628'
Dec 28 09:09:42.565: INFO: stderr: ""
Dec 28 09:09:42.565: INFO: stdout: "update-demo-kitten-tdnt7 update-demo-kitten-v5gxw "
Dec 28 09:09:42.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-kitten-tdnt7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-628'
Dec 28 09:09:42.640: INFO: stderr: ""
Dec 28 09:09:42.640: INFO: stdout: "true"
Dec 28 09:09:42.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-kitten-tdnt7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-628'
Dec 28 09:09:42.713: INFO: stderr: ""
Dec 28 09:09:42.713: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 28 09:09:42.713: INFO: validating pod update-demo-kitten-tdnt7
Dec 28 09:09:42.716: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 28 09:09:42.716: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 28 09:09:42.716: INFO: update-demo-kitten-tdnt7 is verified up and running
Dec 28 09:09:42.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-kitten-v5gxw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-628'
Dec 28 09:09:42.791: INFO: stderr: ""
Dec 28 09:09:42.791: INFO: stdout: "true"
Dec 28 09:09:42.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-kitten-v5gxw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-628'
Dec 28 09:09:42.869: INFO: stderr: ""
Dec 28 09:09:42.869: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/kitten:1.0"
Dec 28 09:09:42.869: INFO: validating pod update-demo-kitten-v5gxw
Dec 28 09:09:42.873: INFO: got data: {
  "image": "kitten.jpg"
}

Dec 28 09:09:42.873: INFO: Unmarshalled json jpg/img => {kitten.jpg} , expecting kitten.jpg .
Dec 28 09:09:42.873: INFO: update-demo-kitten-v5gxw is verified up and running
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:09:42.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-628" for this suite.
Dec 28 09:09:54.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:09:54.958: INFO: namespace kubectl-628 deletion completed in 12.082822631s

• [SLOW TEST:40.505 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should do a rolling update of a replication controller  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox command that always fails in a pod 
  should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:09:54.958: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[BeforeEach] when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:81
[It] should have an terminated reason [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:09:58.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-1456" for this suite.
Dec 28 09:10:04.994: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:10:05.058: INFO: namespace kubelet-test-1456 deletion completed in 6.071019599s

• [SLOW TEST:10.100 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox command that always fails in a pod
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:78
    should have an terminated reason [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Security Context when creating containers with AllowPrivilegeEscalation 
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:10:05.058: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 09:10:05.081: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-cf3a841f-da46-4a99-b4da-0a13f5a1d0fb" in namespace "security-context-test-8266" to be "success or failure"
Dec 28 09:10:05.084: INFO: Pod "alpine-nnp-false-cf3a841f-da46-4a99-b4da-0a13f5a1d0fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.710772ms
Dec 28 09:10:07.086: INFO: Pod "alpine-nnp-false-cf3a841f-da46-4a99-b4da-0a13f5a1d0fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005041701s
Dec 28 09:10:07.086: INFO: Pod "alpine-nnp-false-cf3a841f-da46-4a99-b4da-0a13f5a1d0fb" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:10:07.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8266" for this suite.
Dec 28 09:10:13.101: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:10:13.164: INFO: namespace security-context-test-8266 deletion completed in 6.070446699s

• [SLOW TEST:8.106 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when creating containers with AllowPrivilegeEscalation
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:277
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:10:13.165: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 09:10:13.180: INFO: Creating deployment "webserver-deployment"
Dec 28 09:10:13.183: INFO: Waiting for observed generation 1
Dec 28 09:10:15.187: INFO: Waiting for all required pods to come up
Dec 28 09:10:15.189: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running
Dec 28 09:10:17.193: INFO: Waiting for deployment "webserver-deployment" to complete
Dec 28 09:10:17.197: INFO: Updating deployment "webserver-deployment" with a non-existent image
Dec 28 09:10:17.201: INFO: Updating deployment webserver-deployment
Dec 28 09:10:17.201: INFO: Waiting for observed generation 2
Dec 28 09:10:19.205: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Dec 28 09:10:19.206: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Dec 28 09:10:19.207: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 28 09:10:19.212: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Dec 28 09:10:19.212: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Dec 28 09:10:19.213: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Dec 28 09:10:19.216: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Dec 28 09:10:19.216: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Dec 28 09:10:19.220: INFO: Updating deployment webserver-deployment
Dec 28 09:10:19.220: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Dec 28 09:10:19.222: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Dec 28 09:10:19.225: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 28 09:10:19.231: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-8859 /apis/apps/v1/namespaces/deployment-8859/deployments/webserver-deployment 6af098ac-d838-4741-a3b0-506088168e6a 297000 3 2019-12-28 09:10:13 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0062a74c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-c7997dcc8" is progressing.,LastUpdateTime:2019-12-28 09:10:17 +0000 UTC,LastTransitionTime:2019-12-28 09:10:13 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-12-28 09:10:19 +0000 UTC,LastTransitionTime:2019-12-28 09:10:19 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Dec 28 09:10:19.235: INFO: New ReplicaSet "webserver-deployment-c7997dcc8" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-c7997dcc8  deployment-8859 /apis/apps/v1/namespaces/deployment-8859/replicasets/webserver-deployment-c7997dcc8 2898143a-1d07-4842-bc82-3006543ec470 296994 3 2019-12-28 09:10:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 6af098ac-d838-4741-a3b0-506088168e6a 0xc0062a79b7 0xc0062a79b8}] []  []},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: c7997dcc8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [] []  []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0062a7a28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 28 09:10:19.235: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Dec 28 09:10:19.235: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-595b5b9587  deployment-8859 /apis/apps/v1/namespaces/deployment-8859/replicasets/webserver-deployment-595b5b9587 2aa0fc23-daa0-4779-a00a-0c690e84fcaa 296991 3 2019-12-28 09:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 6af098ac-d838-4741-a3b0-506088168e6a 0xc0062a78f7 0xc0062a78f8}] []  []},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 595b5b9587,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0062a7958 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Dec 28 09:10:19.240: INFO: Pod "webserver-deployment-595b5b9587-2lkk7" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2lkk7 webserver-deployment-595b5b9587- deployment-8859 /api/v1/namespaces/deployment-8859/pods/webserver-deployment-595b5b9587-2lkk7 e4eed5a3-185b-4b28-b0d4-f747a07cf730 296897 0 2019-12-28 09:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2aa0fc23-daa0-4779-a00a-0c690e84fcaa 0xc0062a7ed7 0xc0062a7ed8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5vh4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5vh4h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5vh4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:hxx-m-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.16,PodIP:10.199.0.168,StartTime:2019-12-28 09:10:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-28 09:10:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://53121bf67b7bfad3ab6788e11dfca034b819902e827973d7adf2782f5684f25d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.199.0.168,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 28 09:10:19.240: INFO: Pod "webserver-deployment-595b5b9587-2tkvs" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-2tkvs webserver-deployment-595b5b9587- deployment-8859 /api/v1/namespaces/deployment-8859/pods/webserver-deployment-595b5b9587-2tkvs 1732a5f7-5446-4fb4-b221-74d285977cce 297008 0 2019-12-28 09:10:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2aa0fc23-daa0-4779-a00a-0c690e84fcaa 0xc006662057 0xc006662058}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5vh4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5vh4h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5vh4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 28 09:10:19.240: INFO: Pod "webserver-deployment-595b5b9587-4pktl" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-4pktl webserver-deployment-595b5b9587- deployment-8859 /api/v1/namespaces/deployment-8859/pods/webserver-deployment-595b5b9587-4pktl a2d9eb3a-c4e3-4a5f-9a2a-85ac90c28075 296874 0 2019-12-28 09:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2aa0fc23-daa0-4779-a00a-0c690e84fcaa 0xc006662160 0xc006662161}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5vh4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5vh4h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5vh4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:hxx-m-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.42,PodIP:10.199.2.33,StartTime:2019-12-28 09:10:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-28 09:10:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://bb5a7dfca4dd8bc06d5f057cfd1c6a1075dd1fed22399a06b78d70b98f97172f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.199.2.33,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 28 09:10:19.240: INFO: Pod "webserver-deployment-595b5b9587-52mh5" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-52mh5 webserver-deployment-595b5b9587- deployment-8859 /api/v1/namespaces/deployment-8859/pods/webserver-deployment-595b5b9587-52mh5 289ea2d1-767c-469e-b005-77a3420e3c89 297005 0 2019-12-28 09:10:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2aa0fc23-daa0-4779-a00a-0c690e84fcaa 0xc0066622d7 0xc0066622d8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5vh4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5vh4h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5vh4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 28 09:10:19.240: INFO: Pod "webserver-deployment-595b5b9587-59rzw" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-59rzw webserver-deployment-595b5b9587- deployment-8859 /api/v1/namespaces/deployment-8859/pods/webserver-deployment-595b5b9587-59rzw 30617208-d53d-441c-a77d-90d0c0460a06 296893 0 2019-12-28 09:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2aa0fc23-daa0-4779-a00a-0c690e84fcaa 0xc0066623d0 0xc0066623d1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5vh4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5vh4h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5vh4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:hxx-m-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.35,PodIP:10.199.1.176,StartTime:2019-12-28 09:10:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-28 09:10:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://f19d91b2c74936553815d3bb9e97940d63395eb755481b57f5984dd913f77a1d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.199.1.176,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 28 09:10:19.241: INFO: Pod "webserver-deployment-595b5b9587-5qb8c" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-5qb8c webserver-deployment-595b5b9587- deployment-8859 /api/v1/namespaces/deployment-8859/pods/webserver-deployment-595b5b9587-5qb8c 7ab18c80-ad6a-41b6-a5b1-380aaa9e62bf 296888 0 2019-12-28 09:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2aa0fc23-daa0-4779-a00a-0c690e84fcaa 0xc006662547 0xc006662548}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5vh4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5vh4h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5vh4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:hxx-m-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.35,PodIP:10.199.1.178,StartTime:2019-12-28 09:10:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-28 09:10:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://133e54d983d67e93b3459c334c4bf053ccf02f0b3d6551591837765013c61202,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.199.1.178,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 28 09:10:19.241: INFO: Pod "webserver-deployment-595b5b9587-64gmv" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-64gmv webserver-deployment-595b5b9587- deployment-8859 /api/v1/namespaces/deployment-8859/pods/webserver-deployment-595b5b9587-64gmv e8ed90da-802c-4644-986e-744c821c5987 297003 0 2019-12-28 09:10:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2aa0fc23-daa0-4779-a00a-0c690e84fcaa 0xc0066626c7 0xc0066626c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5vh4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5vh4h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5vh4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:hxx-m-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 28 09:10:19.241: INFO: Pod "webserver-deployment-595b5b9587-69j68" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-69j68 webserver-deployment-595b5b9587- deployment-8859 /api/v1/namespaces/deployment-8859/pods/webserver-deployment-595b5b9587-69j68 ade84964-ee86-4631-b4fe-d8a7480aa39b 296900 0 2019-12-28 09:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2aa0fc23-daa0-4779-a00a-0c690e84fcaa 0xc0066627e0 0xc0066627e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5vh4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5vh4h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5vh4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:hxx-m-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.16,PodIP:10.199.0.166,StartTime:2019-12-28 09:10:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-28 09:10:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://bc3b8488098a8f0cf731601dec3e0d8bd10872a95920301fff30ef7938e4e4ab,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.199.0.166,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 28 09:10:19.241: INFO: Pod "webserver-deployment-595b5b9587-6f9z4" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-6f9z4 webserver-deployment-595b5b9587- deployment-8859 /api/v1/namespaces/deployment-8859/pods/webserver-deployment-595b5b9587-6f9z4 5668ebbe-1e54-466b-83a9-ca1e383eac76 297009 0 2019-12-28 09:10:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2aa0fc23-daa0-4779-a00a-0c690e84fcaa 0xc006662957 0xc006662958}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5vh4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5vh4h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5vh4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 28 09:10:19.242: INFO: Pod "webserver-deployment-595b5b9587-7l4rw" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-7l4rw webserver-deployment-595b5b9587- deployment-8859 /api/v1/namespaces/deployment-8859/pods/webserver-deployment-595b5b9587-7l4rw 89117c16-f39f-429d-b674-ec761528da73 296891 0 2019-12-28 09:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2aa0fc23-daa0-4779-a00a-0c690e84fcaa 0xc006662a50 0xc006662a51}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5vh4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5vh4h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5vh4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:hxx-m-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.35,PodIP:10.199.1.177,StartTime:2019-12-28 09:10:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-28 09:10:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://1d6245d8a18ef8e8d7e204b2d937600b4897860f78a85d0b1b3fdc6d3af2c6d8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.199.1.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 28 09:10:19.242: INFO: Pod "webserver-deployment-595b5b9587-bsptk" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-bsptk webserver-deployment-595b5b9587- deployment-8859 /api/v1/namespaces/deployment-8859/pods/webserver-deployment-595b5b9587-bsptk 35c079aa-ede5-47b8-be6d-e681bb57718d 297006 0 2019-12-28 09:10:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2aa0fc23-daa0-4779-a00a-0c690e84fcaa 0xc006662bc7 0xc006662bc8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5vh4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5vh4h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5vh4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:hxx-m-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 28 09:10:19.242: INFO: Pod "webserver-deployment-595b5b9587-pkswx" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-pkswx webserver-deployment-595b5b9587- deployment-8859 /api/v1/namespaces/deployment-8859/pods/webserver-deployment-595b5b9587-pkswx ba8c57eb-fa4d-4f14-b7ed-4b4e560973a2 297012 0 2019-12-28 09:10:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2aa0fc23-daa0-4779-a00a-0c690e84fcaa 0xc006662ce0 0xc006662ce1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5vh4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5vh4h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5vh4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 28 09:10:19.242: INFO: Pod "webserver-deployment-595b5b9587-rc4mh" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-rc4mh webserver-deployment-595b5b9587- deployment-8859 /api/v1/namespaces/deployment-8859/pods/webserver-deployment-595b5b9587-rc4mh fbf0bc0b-d5e1-4d88-805c-5747560d26b5 296906 0 2019-12-28 09:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2aa0fc23-daa0-4779-a00a-0c690e84fcaa 0xc006662dd0 0xc006662dd1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5vh4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5vh4h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5vh4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:hxx-m-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.16,PodIP:10.199.0.167,StartTime:2019-12-28 09:10:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-28 09:10:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://5e1d1d20a4c5a8cbea1adae4bc8dbf18ad0a0d3b13aa897e4bf3c6e9dfb76577,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.199.0.167,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 28 09:10:19.242: INFO: Pod "webserver-deployment-595b5b9587-wjdzx" is available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-wjdzx webserver-deployment-595b5b9587- deployment-8859 /api/v1/namespaces/deployment-8859/pods/webserver-deployment-595b5b9587-wjdzx d54eb4fd-9a19-4eee-ab46-c7c9fd7812c7 296903 0 2019-12-28 09:10:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2aa0fc23-daa0-4779-a00a-0c690e84fcaa 0xc006662f47 0xc006662f48}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5vh4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5vh4h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5vh4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:hxx-m-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.16,PodIP:10.199.0.169,StartTime:2019-12-28 09:10:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-28 09:10:14 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:httpd:2.4.38-alpine,ImageID:docker-pullable://httpd@sha256:eb8ccf084cf3e80eece1add239effefd171eb39adbc154d33c14260d905d4060,ContainerID:docker://e3d62e7a0f917b9a85ae35df06ed445af0092b5511395b23e6336390025f3f5b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.199.0.169,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 28 09:10:19.242: INFO: Pod "webserver-deployment-595b5b9587-x4hvm" is not available:
&Pod{ObjectMeta:{webserver-deployment-595b5b9587-x4hvm webserver-deployment-595b5b9587- deployment-8859 /api/v1/namespaces/deployment-8859/pods/webserver-deployment-595b5b9587-x4hvm 2fd2af75-2ba7-49b4-8dbf-d8cc9dec5f0e 296996 0 2019-12-28 09:10:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:595b5b9587] map[] [{apps/v1 ReplicaSet webserver-deployment-595b5b9587 2aa0fc23-daa0-4779-a00a-0c690e84fcaa 0xc0066630c7 0xc0066630c8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5vh4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5vh4h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5vh4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:hxx-m-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 28 09:10:19.242: INFO: Pod "webserver-deployment-c7997dcc8-2d2p2" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-2d2p2 webserver-deployment-c7997dcc8- deployment-8859 /api/v1/namespaces/deployment-8859/pods/webserver-deployment-c7997dcc8-2d2p2 5768a40a-586d-4749-ac9c-0884a9f20b63 297004 0 2019-12-28 09:10:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 2898143a-1d07-4842-bc82-3006543ec470 0xc0066631e0 0xc0066631e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5vh4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5vh4h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5vh4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 28 09:10:19.242: INFO: Pod "webserver-deployment-c7997dcc8-88ckz" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-88ckz webserver-deployment-c7997dcc8- deployment-8859 /api/v1/namespaces/deployment-8859/pods/webserver-deployment-c7997dcc8-88ckz 06f86f15-4042-49d9-b236-4f4701f88967 296969 0 2019-12-28 09:10:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 2898143a-1d07-4842-bc82-3006543ec470 0xc0066632e0 0xc0066632e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5vh4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5vh4h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5vh4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:hxx-m-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.42,PodIP:,StartTime:2019-12-28 09:10:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 28 09:10:19.243: INFO: Pod "webserver-deployment-c7997dcc8-8s872" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-8s872 webserver-deployment-c7997dcc8- deployment-8859 /api/v1/namespaces/deployment-8859/pods/webserver-deployment-c7997dcc8-8s872 009e449d-689a-4bf6-b84d-07da1f236ef0 297007 0 2019-12-28 09:10:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 2898143a-1d07-4842-bc82-3006543ec470 0xc006663450 0xc006663451}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5vh4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5vh4h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5vh4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:hxx-m-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 28 09:10:19.243: INFO: Pod "webserver-deployment-c7997dcc8-ghd9d" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-ghd9d webserver-deployment-c7997dcc8- deployment-8859 /api/v1/namespaces/deployment-8859/pods/webserver-deployment-c7997dcc8-ghd9d b55584c9-e7cf-402d-81de-9126ae1ba736 296943 0 2019-12-28 09:10:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 2898143a-1d07-4842-bc82-3006543ec470 0xc006663570 0xc006663571}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5vh4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5vh4h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5vh4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:hxx-m-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.16,PodIP:,StartTime:2019-12-28 09:10:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 28 09:10:19.243: INFO: Pod "webserver-deployment-c7997dcc8-jp9m8" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-jp9m8 webserver-deployment-c7997dcc8- deployment-8859 /api/v1/namespaces/deployment-8859/pods/webserver-deployment-c7997dcc8-jp9m8 8ce94558-fd35-4d18-94e8-6117e8c67da0 296947 0 2019-12-28 09:10:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 2898143a-1d07-4842-bc82-3006543ec470 0xc0066636e0 0xc0066636e1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5vh4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5vh4h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5vh4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:hxx-m-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.35,PodIP:,StartTime:2019-12-28 09:10:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 28 09:10:19.243: INFO: Pod "webserver-deployment-c7997dcc8-twcc4" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-twcc4 webserver-deployment-c7997dcc8- deployment-8859 /api/v1/namespaces/deployment-8859/pods/webserver-deployment-c7997dcc8-twcc4 ec4f3d51-1cf5-48cb-b9ff-5746fbca095f 297013 0 2019-12-28 09:10:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 2898143a-1d07-4842-bc82-3006543ec470 0xc006663850 0xc006663851}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5vh4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5vh4h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5vh4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 28 09:10:19.243: INFO: Pod "webserver-deployment-c7997dcc8-v5r8k" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-v5r8k webserver-deployment-c7997dcc8- deployment-8859 /api/v1/namespaces/deployment-8859/pods/webserver-deployment-c7997dcc8-v5r8k 53faff44-8540-4c14-97a2-c13977eb05b4 296973 0 2019-12-28 09:10:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 2898143a-1d07-4842-bc82-3006543ec470 0xc006663950 0xc006663951}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5vh4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5vh4h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5vh4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:hxx-m-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.42,PodIP:,StartTime:2019-12-28 09:10:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Dec 28 09:10:19.244: INFO: Pod "webserver-deployment-c7997dcc8-xkwlh" is not available:
&Pod{ObjectMeta:{webserver-deployment-c7997dcc8-xkwlh webserver-deployment-c7997dcc8- deployment-8859 /api/v1/namespaces/deployment-8859/pods/webserver-deployment-c7997dcc8-xkwlh 8b55ce62-2488-4e20-b8a5-617ba967dd7b 296971 0 2019-12-28 09:10:17 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:c7997dcc8] map[] [{apps/v1 ReplicaSet webserver-deployment-c7997dcc8 2898143a-1d07-4842-bc82-3006543ec470 0xc006663ac0 0xc006663ac1}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-5vh4h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-5vh4h,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-5vh4h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:hxx-m-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:17 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:10:17 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.16,PodIP:,StartTime:2019-12-28 09:10:17 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:10:19.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8859" for this suite.
Dec 28 09:10:25.284: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:10:25.372: INFO: namespace deployment-8859 deletion completed in 6.111391777s

• [SLOW TEST:12.207 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:10:25.372: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating pod
Dec 28 09:10:27.405: INFO: Pod pod-hostip-da955acc-032e-4eca-8bd0-7e515ef2dd13 has hostIP: 10.0.128.16
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:10:27.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5034" for this suite.
Dec 28 09:10:55.414: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:10:55.479: INFO: namespace pods-5034 deletion completed in 28.071718424s

• [SLOW TEST:30.107 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should get a host IP [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:10:55.479: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0644 on tmpfs
Dec 28 09:10:55.499: INFO: Waiting up to 5m0s for pod "pod-bb44e92b-fadb-45f0-9d86-564550c39612" in namespace "emptydir-32" to be "success or failure"
Dec 28 09:10:55.500: INFO: Pod "pod-bb44e92b-fadb-45f0-9d86-564550c39612": Phase="Pending", Reason="", readiness=false. Elapsed: 1.576828ms
Dec 28 09:10:57.503: INFO: Pod "pod-bb44e92b-fadb-45f0-9d86-564550c39612": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003793092s
STEP: Saw pod success
Dec 28 09:10:57.503: INFO: Pod "pod-bb44e92b-fadb-45f0-9d86-564550c39612" satisfied condition "success or failure"
Dec 28 09:10:57.504: INFO: Trying to get logs from node hxx-m-2 pod pod-bb44e92b-fadb-45f0-9d86-564550c39612 container test-container: <nil>
STEP: delete the pod
Dec 28 09:10:57.515: INFO: Waiting for pod pod-bb44e92b-fadb-45f0-9d86-564550c39612 to disappear
Dec 28 09:10:57.516: INFO: Pod pod-bb44e92b-fadb-45f0-9d86-564550c39612 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:10:57.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-32" for this suite.
Dec 28 09:11:03.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:11:03.593: INFO: namespace emptydir-32 deletion completed in 6.074564455s

• [SLOW TEST:8.114 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:11:03.594: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 28 09:11:03.620: INFO: Waiting up to 5m0s for pod "downwardapi-volume-eb821feb-9eba-4ed4-b76c-5662821df32c" in namespace "projected-4751" to be "success or failure"
Dec 28 09:11:03.621: INFO: Pod "downwardapi-volume-eb821feb-9eba-4ed4-b76c-5662821df32c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.706516ms
Dec 28 09:11:05.623: INFO: Pod "downwardapi-volume-eb821feb-9eba-4ed4-b76c-5662821df32c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003509776s
STEP: Saw pod success
Dec 28 09:11:05.623: INFO: Pod "downwardapi-volume-eb821feb-9eba-4ed4-b76c-5662821df32c" satisfied condition "success or failure"
Dec 28 09:11:05.625: INFO: Trying to get logs from node hxx-m-2 pod downwardapi-volume-eb821feb-9eba-4ed4-b76c-5662821df32c container client-container: <nil>
STEP: delete the pod
Dec 28 09:11:05.635: INFO: Waiting for pod downwardapi-volume-eb821feb-9eba-4ed4-b76c-5662821df32c to disappear
Dec 28 09:11:05.636: INFO: Pod downwardapi-volume-eb821feb-9eba-4ed4-b76c-5662821df32c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:11:05.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4751" for this suite.
Dec 28 09:11:11.645: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:11:11.716: INFO: namespace projected-4751 deletion completed in 6.077180904s

• [SLOW TEST:8.122 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide podname only [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:11:11.716: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 28 09:11:12.197: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 28 09:11:15.208: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API
STEP: create a configmap that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:11:15.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4738" for this suite.
Dec 28 09:11:21.239: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:11:21.306: INFO: namespace webhook-4738 deletion completed in 6.074843795s
STEP: Destroying namespace "webhook-4738-markers" for this suite.
Dec 28 09:11:27.313: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:11:27.379: INFO: namespace webhook-4738-markers deletion completed in 6.07332325s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:15.671 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:11:27.386: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a ConfigMap
STEP: Ensuring resource quota status captures configMap creation
STEP: Deleting a ConfigMap
STEP: Ensuring resource quota status released usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:11:43.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-6471" for this suite.
Dec 28 09:11:49.436: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:11:49.505: INFO: namespace resourcequota-6471 deletion completed in 6.076295924s

• [SLOW TEST:22.119 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:11:49.506: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 28 09:11:49.716: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 28 09:11:52.727: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the crd webhook via the AdmissionRegistration API
STEP: Creating a custom resource definition that should be denied by the webhook
Dec 28 09:11:52.739: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:11:52.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4167" for this suite.
Dec 28 09:11:58.757: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:11:58.823: INFO: namespace webhook-4167 deletion completed in 6.072973865s
STEP: Destroying namespace "webhook-4167-markers" for this suite.
Dec 28 09:12:04.830: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:12:04.898: INFO: namespace webhook-4167-markers deletion completed in 6.075001653s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:15.399 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:12:04.905: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a ReplicationController is created
STEP: When the matched label of one of its pods change
Dec 28 09:12:04.928: INFO: Pod name pod-release: Found 0 pods out of 1
Dec 28 09:12:09.930: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:12:10.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2062" for this suite.
Dec 28 09:12:16.948: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:12:17.014: INFO: namespace replication-controller-2062 deletion completed in 6.073608391s

• [SLOW TEST:12.109 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:12:17.015: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec 28 09:12:19.549: INFO: Successfully updated pod "labelsupdate204dfd9e-c3f4-4f35-aaf7-1870969d97f0"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:12:23.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5497" for this suite.
Dec 28 09:12:45.575: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:12:45.651: INFO: namespace downward-api-5497 deletion completed in 22.083098074s

• [SLOW TEST:28.636 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:12:45.651: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-45
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 28 09:12:45.668: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 28 09:13:03.715: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.199.0.188:8080/dial?request=hostName&protocol=http&host=10.199.0.187&port=8080&tries=1'] Namespace:pod-network-test-45 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 28 09:13:03.715: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 09:13:03.857: INFO: Waiting for endpoints: map[]
Dec 28 09:13:03.859: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.199.0.188:8080/dial?request=hostName&protocol=http&host=10.199.2.46&port=8080&tries=1'] Namespace:pod-network-test-45 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 28 09:13:03.859: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 09:13:04.023: INFO: Waiting for endpoints: map[]
Dec 28 09:13:04.026: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.199.0.188:8080/dial?request=hostName&protocol=http&host=10.199.1.186&port=8080&tries=1'] Namespace:pod-network-test-45 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 28 09:13:04.026: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 09:13:04.182: INFO: Waiting for endpoints: map[]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:13:04.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-45" for this suite.
Dec 28 09:13:16.192: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:13:16.259: INFO: namespace pod-network-test-45 deletion completed in 12.074108913s

• [SLOW TEST:30.607 seconds]
[sig-network] Networking
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for intra-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI 
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:13:16.259: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec 28 09:13:18.796: INFO: Successfully updated pod "labelsupdate95ae935b-2d94-469e-a494-7eac4bc2e9a7"
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:13:20.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3240" for this suite.
Dec 28 09:13:32.816: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:13:32.884: INFO: namespace projected-3240 deletion completed in 12.07535112s

• [SLOW TEST:16.625 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should update labels on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:13:32.884: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 28 09:13:33.197: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 28 09:13:35.202: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713121213, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713121213, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713121213, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713121213, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 28 09:13:38.210: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the mutating pod webhook via the AdmissionRegistration API
STEP: create a pod that should be updated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:13:38.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8191" for this suite.
Dec 28 09:13:50.249: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:13:50.318: INFO: namespace webhook-8191 deletion completed in 12.077219737s
STEP: Destroying namespace "webhook-8191-markers" for this suite.
Dec 28 09:13:56.325: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:13:56.390: INFO: namespace webhook-8191-markers deletion completed in 6.072283303s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:23.513 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Variable Expansion 
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:13:56.398: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename var-expansion
STEP: Waiting for a default service account to be provisioned in namespace
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test env composition
Dec 28 09:13:56.417: INFO: Waiting up to 5m0s for pod "var-expansion-7fee050e-6ed8-4712-9905-86cedc44e25b" in namespace "var-expansion-3088" to be "success or failure"
Dec 28 09:13:56.419: INFO: Pod "var-expansion-7fee050e-6ed8-4712-9905-86cedc44e25b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008057ms
Dec 28 09:13:58.421: INFO: Pod "var-expansion-7fee050e-6ed8-4712-9905-86cedc44e25b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004481463s
STEP: Saw pod success
Dec 28 09:13:58.421: INFO: Pod "var-expansion-7fee050e-6ed8-4712-9905-86cedc44e25b" satisfied condition "success or failure"
Dec 28 09:13:58.423: INFO: Trying to get logs from node hxx-m-2 pod var-expansion-7fee050e-6ed8-4712-9905-86cedc44e25b container dapi-container: <nil>
STEP: delete the pod
Dec 28 09:13:58.434: INFO: Waiting for pod var-expansion-7fee050e-6ed8-4712-9905-86cedc44e25b to disappear
Dec 28 09:13:58.435: INFO: Pod var-expansion-7fee050e-6ed8-4712-9905-86cedc44e25b no longer exists
[AfterEach] [k8s.io] Variable Expansion
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:13:58.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-3088" for this suite.
Dec 28 09:14:04.445: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:14:04.510: INFO: namespace var-expansion-3088 deletion completed in 6.072103933s

• [SLOW TEST:8.112 seconds]
[k8s.io] Variable Expansion
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-network] Networking Granular Checks: Pods 
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:14:04.510: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename pod-network-test
STEP: Waiting for a default service account to be provisioned in namespace
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Performing setup for networking test in namespace pod-network-test-7859
STEP: creating a selector
STEP: Creating the service pods in kubernetes
Dec 28 09:14:04.526: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
STEP: Creating test pods
Dec 28 09:14:28.569: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.199.1.187:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7859 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 28 09:14:28.569: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 09:14:28.710: INFO: Found all expected endpoints: [netserver-0]
Dec 28 09:14:28.712: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.199.0.193:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7859 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 28 09:14:28.712: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 09:14:28.861: INFO: Found all expected endpoints: [netserver-1]
Dec 28 09:14:28.864: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.199.2.47:8080/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7859 PodName:host-test-container-pod ContainerName:agnhost Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false}
Dec 28 09:14:28.864: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
Dec 28 09:14:29.026: INFO: Found all expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:14:29.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-7859" for this suite.
Dec 28 09:14:41.036: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:14:41.100: INFO: namespace pod-network-test-7859 deletion completed in 12.071341275s

• [SLOW TEST:36.590 seconds]
[sig-network] Networking
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:25
  Granular Checks: Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/networking.go:28
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:14:41.100: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a service in the namespace
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there is no service in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:14:47.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6440" for this suite.
Dec 28 09:14:53.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:14:53.247: INFO: namespace namespaces-6440 deletion completed in 6.08205959s
STEP: Destroying namespace "nsdeletetest-12" for this suite.
Dec 28 09:14:53.249: INFO: Namespace nsdeletetest-12 was already deleted
STEP: Destroying namespace "nsdeletetest-5730" for this suite.
Dec 28 09:14:59.255: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:14:59.321: INFO: namespace nsdeletetest-5730 deletion completed in 6.072442428s

• [SLOW TEST:18.221 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Secrets 
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:14:59.322: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name secret-emptykey-test-747af873-6f2a-44f7-b66b-a6f078704926
[AfterEach] [sig-api-machinery] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:14:59.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8800" for this suite.
Dec 28 09:15:05.348: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:15:05.415: INFO: namespace secrets-8800 deletion completed in 6.073815022s

• [SLOW TEST:6.093 seconds]
[sig-api-machinery] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets.go:32
  should fail to create secret due to empty secret key [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[k8s.io] Probing container 
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:15:05.415: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:16:05.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9610" for this suite.
Dec 28 09:16:33.447: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:16:33.514: INFO: namespace container-probe-9610 deletion completed in 28.073721931s

• [SLOW TEST:88.099 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:16:33.514: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 09:16:33.531: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: client-side validation (kubectl create and apply) allows request with any unknown properties
Dec 28 09:16:37.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 --namespace=crd-publish-openapi-2464 create -f -'
Dec 28 09:16:37.570: INFO: stderr: ""
Dec 28 09:16:37.570: INFO: stdout: "e2e-test-crd-publish-openapi-5687-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 28 09:16:37.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 --namespace=crd-publish-openapi-2464 delete e2e-test-crd-publish-openapi-5687-crds test-cr'
Dec 28 09:16:37.649: INFO: stderr: ""
Dec 28 09:16:37.649: INFO: stdout: "e2e-test-crd-publish-openapi-5687-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Dec 28 09:16:37.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 --namespace=crd-publish-openapi-2464 apply -f -'
Dec 28 09:16:37.787: INFO: stderr: ""
Dec 28 09:16:37.787: INFO: stdout: "e2e-test-crd-publish-openapi-5687-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Dec 28 09:16:37.787: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 --namespace=crd-publish-openapi-2464 delete e2e-test-crd-publish-openapi-5687-crds test-cr'
Dec 28 09:16:37.862: INFO: stderr: ""
Dec 28 09:16:37.862: INFO: stdout: "e2e-test-crd-publish-openapi-5687-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR
Dec 28 09:16:37.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 explain e2e-test-crd-publish-openapi-5687-crds'
Dec 28 09:16:37.999: INFO: stderr: ""
Dec 28 09:16:37.999: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-5687-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:16:41.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2464" for this suite.
Dec 28 09:16:47.711: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:16:47.776: INFO: namespace crd-publish-openapi-2464 deletion completed in 6.071312468s

• [SLOW TEST:14.262 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run --rm job 
  should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:16:47.776: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create a job from an image, then delete the job  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: executing a command with run --rm and attach with stdin
Dec 28 09:16:47.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 --namespace=kubectl-3639 run e2e-test-rm-busybox-job --image=docker.io/library/busybox:1.29 --rm=true --generator=job/v1 --restart=OnFailure --attach=true --stdin -- sh -c cat && echo 'stdin closed''
Dec 28 09:16:49.903: INFO: stderr: "kubectl run --generator=job/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\nIf you don't see a command prompt, try pressing enter.\n"
Dec 28 09:16:49.903: INFO: stdout: "abcd1234stdin closed\njob.batch \"e2e-test-rm-busybox-job\" deleted\n"
STEP: verifying the job e2e-test-rm-busybox-job was deleted
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:16:51.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3639" for this suite.
Dec 28 09:17:03.916: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:17:03.984: INFO: namespace kubectl-3639 deletion completed in 12.074548835s

• [SLOW TEST:16.208 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl run --rm job
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1751
    should create a job from an image, then delete the job  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:17:03.984: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 09:17:04.012: INFO: (0) /api/v1/nodes/hxx-m-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 9.869145ms)
Dec 28 09:17:04.015: INFO: (1) /api/v1/nodes/hxx-m-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.219129ms)
Dec 28 09:17:04.017: INFO: (2) /api/v1/nodes/hxx-m-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.196006ms)
Dec 28 09:17:04.019: INFO: (3) /api/v1/nodes/hxx-m-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.23434ms)
Dec 28 09:17:04.021: INFO: (4) /api/v1/nodes/hxx-m-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.305009ms)
Dec 28 09:17:04.023: INFO: (5) /api/v1/nodes/hxx-m-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.105773ms)
Dec 28 09:17:04.026: INFO: (6) /api/v1/nodes/hxx-m-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.16451ms)
Dec 28 09:17:04.028: INFO: (7) /api/v1/nodes/hxx-m-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.164278ms)
Dec 28 09:17:04.030: INFO: (8) /api/v1/nodes/hxx-m-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.151346ms)
Dec 28 09:17:04.033: INFO: (9) /api/v1/nodes/hxx-m-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.018931ms)
Dec 28 09:17:04.035: INFO: (10) /api/v1/nodes/hxx-m-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.119932ms)
Dec 28 09:17:04.038: INFO: (11) /api/v1/nodes/hxx-m-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.667596ms)
Dec 28 09:17:04.040: INFO: (12) /api/v1/nodes/hxx-m-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.054881ms)
Dec 28 09:17:04.042: INFO: (13) /api/v1/nodes/hxx-m-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.388375ms)
Dec 28 09:17:04.045: INFO: (14) /api/v1/nodes/hxx-m-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.604426ms)
Dec 28 09:17:04.048: INFO: (15) /api/v1/nodes/hxx-m-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.423254ms)
Dec 28 09:17:04.050: INFO: (16) /api/v1/nodes/hxx-m-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.25831ms)
Dec 28 09:17:04.052: INFO: (17) /api/v1/nodes/hxx-m-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.0348ms)
Dec 28 09:17:04.054: INFO: (18) /api/v1/nodes/hxx-m-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.978799ms)
Dec 28 09:17:04.056: INFO: (19) /api/v1/nodes/hxx-m-1:10250/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.247599ms)
[AfterEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:17:04.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-750" for this suite.
Dec 28 09:17:10.065: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:17:10.133: INFO: namespace proxy-750 deletion completed in 6.074931391s

• [SLOW TEST:6.150 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node with explicit kubelet port using proxy subresource  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet 
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:17:10.133: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename replicaset
STEP: Waiting for a default service account to be provisioned in namespace
[It] should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 09:17:10.150: INFO: Creating ReplicaSet my-hostname-basic-0dbb38ba-8841-423e-bcbf-636f93d98797
Dec 28 09:17:10.154: INFO: Pod name my-hostname-basic-0dbb38ba-8841-423e-bcbf-636f93d98797: Found 0 pods out of 1
Dec 28 09:17:15.157: INFO: Pod name my-hostname-basic-0dbb38ba-8841-423e-bcbf-636f93d98797: Found 1 pods out of 1
Dec 28 09:17:15.157: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-0dbb38ba-8841-423e-bcbf-636f93d98797" is running
Dec 28 09:17:15.158: INFO: Pod "my-hostname-basic-0dbb38ba-8841-423e-bcbf-636f93d98797-df6nj" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-28 09:17:10 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-28 09:17:12 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-28 09:17:12 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2019-12-28 09:17:10 +0000 UTC Reason: Message:}])
Dec 28 09:17:15.158: INFO: Trying to dial the pod
Dec 28 09:17:20.165: INFO: Controller my-hostname-basic-0dbb38ba-8841-423e-bcbf-636f93d98797: Got expected result from replica 1 [my-hostname-basic-0dbb38ba-8841-423e-bcbf-636f93d98797-df6nj]: "my-hostname-basic-0dbb38ba-8841-423e-bcbf-636f93d98797-df6nj", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:17:20.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5632" for this suite.
Dec 28 09:17:26.176: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:17:26.251: INFO: namespace replicaset-5632 deletion completed in 6.083497804s

• [SLOW TEST:16.118 seconds]
[sig-apps] ReplicaSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:17:26.251: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 28 09:17:26.267: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 28 09:17:26.274: INFO: Waiting for terminating namespaces to be deleted...
Dec 28 09:17:26.275: INFO: 
Logging pods the kubelet thinks is on node hxx-m-1 before test
Dec 28 09:17:26.284: INFO: kube-flannel-8hgpf from kube-system started at 2019-12-27 13:58:14 +0000 UTC (2 container statuses recorded)
Dec 28 09:17:26.284: INFO: 	Container install-cni ready: true, restart count 0
Dec 28 09:17:26.284: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 28 09:17:26.284: INFO: coredns-66447b44c9-zphmm from kube-system started at 2019-12-27 13:58:22 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.284: INFO: 	Container coredns ready: true, restart count 0
Dec 28 09:17:26.284: INFO: cert-manager-77f5bf4f5-h86rt from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.284: INFO: 	Container cert-manager ready: true, restart count 0
Dec 28 09:17:26.284: INFO: cert-manager-webhook-578c59dddd-k697c from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.284: INFO: 	Container webhook ready: true, restart count 0
Dec 28 09:17:26.284: INFO: apollo-cfdd64bb4-4lssk from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.284: INFO: 	Container apollo ready: true, restart count 0
Dec 28 09:17:26.284: INFO: sonobuoy-systemd-logs-daemon-set-690f17abb3794222-tlb58 from sonobuoy started at 2019-12-28 07:53:22 +0000 UTC (2 container statuses recorded)
Dec 28 09:17:26.284: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 28 09:17:26.284: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 28 09:17:26.284: INFO: dex-8448b48ff8-2bd5h from cpaas-system started at 2019-12-27 14:09:25 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.284: INFO: 	Container dex ready: true, restart count 0
Dec 28 09:17:26.284: INFO: erebus-5597f9565d-zwjnz from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.284: INFO: 	Container erebus ready: true, restart count 0
Dec 28 09:17:26.284: INFO: auth-controller2-79ff55cd75-cbjbn from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.284: INFO: 	Container auth-controller2-manager ready: true, restart count 0
Dec 28 09:17:26.284: INFO: furion-679c948779-jz6lf from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.284: INFO: 	Container furion ready: true, restart count 0
Dec 28 09:17:26.284: INFO: underlord-5c45b96c5d-nmjjq from cpaas-system started at 2019-12-27 14:46:59 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.284: INFO: 	Container alauda-console ready: true, restart count 0
Dec 28 09:17:26.284: INFO: cert-manager-cainjector-67d4dd59ff-8jhs9 from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.284: INFO: 	Container cainjector ready: true, restart count 0
Dec 28 09:17:26.284: INFO: archon-5fdc59d78c-rhtzm from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.284: INFO: 	Container archon-api ready: true, restart count 0
Dec 28 09:17:26.284: INFO: blink-7d4fb788b5-s9q49 from cpaas-system started at 2019-12-28 07:52:16 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.284: INFO: 	Container alauda-console ready: true, restart count 0
Dec 28 09:17:26.284: INFO: kube-proxy-bnhj6 from kube-system started at 2019-12-27 13:58:01 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.284: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 28 09:17:26.284: INFO: agon-75b987dff5-7bht5 from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.284: INFO: 	Container agon ready: true, restart count 2
Dec 28 09:17:26.284: INFO: cluster-registry-controller-manager-76774c98d-7c7mz from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (2 container statuses recorded)
Dec 28 09:17:26.284: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 28 09:17:26.284: INFO: 	Container manager ready: true, restart count 0
Dec 28 09:17:26.284: INFO: sonobuoy-e2e-job-f0b9f709fd414134 from sonobuoy started at 2019-12-28 07:53:22 +0000 UTC (2 container statuses recorded)
Dec 28 09:17:26.284: INFO: 	Container e2e ready: true, restart count 0
Dec 28 09:17:26.284: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 28 09:17:26.284: INFO: 
Logging pods the kubelet thinks is on node hxx-m-2 before test
Dec 28 09:17:26.294: INFO: kube-apiserver-hxx-m-2 from kube-system started at 2019-12-27 13:57:11 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.294: INFO: 	Container kube-apiserver ready: true, restart count 0
Dec 28 09:17:26.294: INFO: kube-controller-manager-hxx-m-2 from kube-system started at 2019-12-27 13:57:11 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.294: INFO: 	Container kube-controller-manager ready: true, restart count 0
Dec 28 09:17:26.294: INFO: etcd-hxx-m-2 from kube-system started at 2019-12-27 13:57:11 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.294: INFO: 	Container etcd ready: true, restart count 0
Dec 28 09:17:26.294: INFO: sonobuoy from sonobuoy started at 2019-12-28 07:53:21 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.294: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 28 09:17:26.294: INFO: nginx-ingress-controller-f9b5d49fd-rx6tc from cpaas-system started at 2019-12-28 08:50:55 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.294: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec 28 09:17:26.294: INFO: kube-flannel-7zk95 from kube-system started at 2019-12-28 08:51:13 +0000 UTC (2 container statuses recorded)
Dec 28 09:17:26.294: INFO: 	Container install-cni ready: true, restart count 0
Dec 28 09:17:26.294: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 28 09:17:26.294: INFO: kube-proxy-44f6q from kube-system started at 2019-12-27 13:57:35 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.294: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 28 09:17:26.294: INFO: kube-scheduler-hxx-m-2 from kube-system started at 2019-12-27 13:57:11 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.294: INFO: 	Container kube-scheduler ready: true, restart count 0
Dec 28 09:17:26.294: INFO: sonobuoy-systemd-logs-daemon-set-690f17abb3794222-jftvp from sonobuoy started at 2019-12-28 07:53:22 +0000 UTC (2 container statuses recorded)
Dec 28 09:17:26.294: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 28 09:17:26.294: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 28 09:17:26.294: INFO: 
Logging pods the kubelet thinks is on node hxx-m-3 before test
Dec 28 09:17:26.310: INFO: apollo-cfdd64bb4-hjth5 from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.310: INFO: 	Container apollo ready: true, restart count 0
Dec 28 09:17:26.310: INFO: agon-75b987dff5-bl7sb from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.310: INFO: 	Container agon ready: true, restart count 2
Dec 28 09:17:26.310: INFO: coredns-66447b44c9-5qbrm from kube-system started at 2019-12-27 13:58:22 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.310: INFO: 	Container coredns ready: true, restart count 0
Dec 28 09:17:26.310: INFO: furion-679c948779-xjs2n from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.310: INFO: 	Container furion ready: true, restart count 0
Dec 28 09:17:26.310: INFO: archon-5fdc59d78c-k5jbm from cpaas-system started at 2019-12-27 14:46:59 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.310: INFO: 	Container archon-api ready: true, restart count 0
Dec 28 09:17:26.310: INFO: cluster-registry-controller-manager-76774c98d-rmnh2 from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (2 container statuses recorded)
Dec 28 09:17:26.310: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 28 09:17:26.310: INFO: 	Container manager ready: true, restart count 0
Dec 28 09:17:26.310: INFO: captain-7cf8c65b49-xjwlq from cpaas-system started at 2019-12-28 08:38:36 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.310: INFO: 	Container captain ready: true, restart count 0
Dec 28 09:17:26.310: INFO: auth-controller2-79ff55cd75-4clk9 from cpaas-system started at 2019-12-27 14:46:59 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.310: INFO: 	Container auth-controller2-manager ready: true, restart count 0
Dec 28 09:17:26.310: INFO: tiller-deploy-7c757c6f9c-67l6b from kube-system started at 2019-12-27 13:59:57 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.310: INFO: 	Container tiller ready: true, restart count 0
Dec 28 09:17:26.310: INFO: kube-flannel-f87zg from kube-system started at 2019-12-27 13:58:14 +0000 UTC (2 container statuses recorded)
Dec 28 09:17:26.310: INFO: 	Container install-cni ready: true, restart count 0
Dec 28 09:17:26.310: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 28 09:17:26.310: INFO: cert-manager-webhook-578c59dddd-flp52 from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.310: INFO: 	Container webhook ready: true, restart count 0
Dec 28 09:17:26.310: INFO: cert-manager-cainjector-67d4dd59ff-tngcj from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.310: INFO: 	Container cainjector ready: true, restart count 0
Dec 28 09:17:26.310: INFO: erebus-5597f9565d-ptdps from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.310: INFO: 	Container erebus ready: true, restart count 0
Dec 28 09:17:26.310: INFO: sonobuoy-systemd-logs-daemon-set-690f17abb3794222-vskml from sonobuoy started at 2019-12-28 07:53:22 +0000 UTC (2 container statuses recorded)
Dec 28 09:17:26.310: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 28 09:17:26.310: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 28 09:17:26.310: INFO: kube-proxy-5kzvk from kube-system started at 2019-12-27 13:57:57 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.310: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 28 09:17:26.310: INFO: cert-manager-77f5bf4f5-2wz6n from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.310: INFO: 	Container cert-manager ready: true, restart count 0
Dec 28 09:17:26.310: INFO: underlord-5c45b96c5d-6cvvc from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.310: INFO: 	Container alauda-console ready: true, restart count 0
Dec 28 09:17:26.310: INFO: blink-7d4fb788b5-rtf6r from cpaas-system started at 2019-12-28 08:38:36 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.310: INFO: 	Container alauda-console ready: true, restart count 0
Dec 28 09:17:26.310: INFO: dex-8448b48ff8-9qn2j from cpaas-system started at 2019-12-27 14:09:25 +0000 UTC (1 container statuses recorded)
Dec 28 09:17:26.310: INFO: 	Container dex ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: verifying the node has the label node hxx-m-1
STEP: verifying the node has the label node hxx-m-2
STEP: verifying the node has the label node hxx-m-3
Dec 28 09:17:26.391: INFO: Pod cert-manager-77f5bf4f5-2wz6n requesting resource cpu=10m on Node hxx-m-3
Dec 28 09:17:26.391: INFO: Pod cert-manager-77f5bf4f5-h86rt requesting resource cpu=10m on Node hxx-m-1
Dec 28 09:17:26.391: INFO: Pod cert-manager-cainjector-67d4dd59ff-8jhs9 requesting resource cpu=10m on Node hxx-m-1
Dec 28 09:17:26.391: INFO: Pod cert-manager-cainjector-67d4dd59ff-tngcj requesting resource cpu=10m on Node hxx-m-3
Dec 28 09:17:26.391: INFO: Pod cert-manager-webhook-578c59dddd-flp52 requesting resource cpu=10m on Node hxx-m-3
Dec 28 09:17:26.391: INFO: Pod cert-manager-webhook-578c59dddd-k697c requesting resource cpu=10m on Node hxx-m-1
Dec 28 09:17:26.391: INFO: Pod agon-75b987dff5-7bht5 requesting resource cpu=256m on Node hxx-m-1
Dec 28 09:17:26.391: INFO: Pod agon-75b987dff5-bl7sb requesting resource cpu=256m on Node hxx-m-3
Dec 28 09:17:26.391: INFO: Pod apollo-cfdd64bb4-4lssk requesting resource cpu=256m on Node hxx-m-1
Dec 28 09:17:26.391: INFO: Pod apollo-cfdd64bb4-hjth5 requesting resource cpu=256m on Node hxx-m-3
Dec 28 09:17:26.391: INFO: Pod archon-5fdc59d78c-k5jbm requesting resource cpu=256m on Node hxx-m-3
Dec 28 09:17:26.391: INFO: Pod archon-5fdc59d78c-rhtzm requesting resource cpu=256m on Node hxx-m-1
Dec 28 09:17:26.391: INFO: Pod auth-controller2-79ff55cd75-4clk9 requesting resource cpu=256m on Node hxx-m-3
Dec 28 09:17:26.391: INFO: Pod auth-controller2-79ff55cd75-cbjbn requesting resource cpu=256m on Node hxx-m-1
Dec 28 09:17:26.391: INFO: Pod blink-7d4fb788b5-rtf6r requesting resource cpu=0m on Node hxx-m-3
Dec 28 09:17:26.391: INFO: Pod blink-7d4fb788b5-s9q49 requesting resource cpu=0m on Node hxx-m-1
Dec 28 09:17:26.391: INFO: Pod captain-7cf8c65b49-xjwlq requesting resource cpu=512m on Node hxx-m-3
Dec 28 09:17:26.391: INFO: Pod cluster-registry-controller-manager-76774c98d-7c7mz requesting resource cpu=512m on Node hxx-m-1
Dec 28 09:17:26.391: INFO: Pod cluster-registry-controller-manager-76774c98d-rmnh2 requesting resource cpu=512m on Node hxx-m-3
Dec 28 09:17:26.391: INFO: Pod dex-8448b48ff8-2bd5h requesting resource cpu=256m on Node hxx-m-1
Dec 28 09:17:26.391: INFO: Pod dex-8448b48ff8-9qn2j requesting resource cpu=256m on Node hxx-m-3
Dec 28 09:17:26.391: INFO: Pod erebus-5597f9565d-ptdps requesting resource cpu=256m on Node hxx-m-3
Dec 28 09:17:26.391: INFO: Pod erebus-5597f9565d-zwjnz requesting resource cpu=256m on Node hxx-m-1
Dec 28 09:17:26.391: INFO: Pod furion-679c948779-jz6lf requesting resource cpu=256m on Node hxx-m-1
Dec 28 09:17:26.391: INFO: Pod furion-679c948779-xjs2n requesting resource cpu=256m on Node hxx-m-3
Dec 28 09:17:26.391: INFO: Pod nginx-ingress-controller-f9b5d49fd-rx6tc requesting resource cpu=256m on Node hxx-m-2
Dec 28 09:17:26.391: INFO: Pod underlord-5c45b96c5d-6cvvc requesting resource cpu=256m on Node hxx-m-3
Dec 28 09:17:26.391: INFO: Pod underlord-5c45b96c5d-nmjjq requesting resource cpu=256m on Node hxx-m-1
Dec 28 09:17:26.391: INFO: Pod coredns-66447b44c9-5qbrm requesting resource cpu=100m on Node hxx-m-3
Dec 28 09:17:26.391: INFO: Pod coredns-66447b44c9-zphmm requesting resource cpu=100m on Node hxx-m-1
Dec 28 09:17:26.391: INFO: Pod etcd-hxx-m-2 requesting resource cpu=0m on Node hxx-m-2
Dec 28 09:17:26.391: INFO: Pod kube-apiserver-hxx-m-2 requesting resource cpu=250m on Node hxx-m-2
Dec 28 09:17:26.391: INFO: Pod kube-controller-manager-hxx-m-2 requesting resource cpu=200m on Node hxx-m-2
Dec 28 09:17:26.391: INFO: Pod kube-flannel-7zk95 requesting resource cpu=50m on Node hxx-m-2
Dec 28 09:17:26.391: INFO: Pod kube-flannel-8hgpf requesting resource cpu=50m on Node hxx-m-1
Dec 28 09:17:26.391: INFO: Pod kube-flannel-f87zg requesting resource cpu=50m on Node hxx-m-3
Dec 28 09:17:26.391: INFO: Pod kube-proxy-44f6q requesting resource cpu=0m on Node hxx-m-2
Dec 28 09:17:26.391: INFO: Pod kube-proxy-5kzvk requesting resource cpu=0m on Node hxx-m-3
Dec 28 09:17:26.391: INFO: Pod kube-proxy-bnhj6 requesting resource cpu=0m on Node hxx-m-1
Dec 28 09:17:26.391: INFO: Pod kube-scheduler-hxx-m-2 requesting resource cpu=100m on Node hxx-m-2
Dec 28 09:17:26.391: INFO: Pod tiller-deploy-7c757c6f9c-67l6b requesting resource cpu=0m on Node hxx-m-3
Dec 28 09:17:26.391: INFO: Pod sonobuoy requesting resource cpu=0m on Node hxx-m-2
Dec 28 09:17:26.391: INFO: Pod sonobuoy-e2e-job-f0b9f709fd414134 requesting resource cpu=0m on Node hxx-m-1
Dec 28 09:17:26.391: INFO: Pod sonobuoy-systemd-logs-daemon-set-690f17abb3794222-jftvp requesting resource cpu=0m on Node hxx-m-2
Dec 28 09:17:26.391: INFO: Pod sonobuoy-systemd-logs-daemon-set-690f17abb3794222-tlb58 requesting resource cpu=0m on Node hxx-m-1
Dec 28 09:17:26.391: INFO: Pod sonobuoy-systemd-logs-daemon-set-690f17abb3794222-vskml requesting resource cpu=0m on Node hxx-m-3
STEP: Starting Pods to consume most of the cluster CPU.
Dec 28 09:17:26.391: INFO: Creating a pod which consumes cpu=3323m on Node hxx-m-3
Dec 28 09:17:26.403: INFO: Creating a pod which consumes cpu=3682m on Node hxx-m-1
Dec 28 09:17:26.406: INFO: Creating a pod which consumes cpu=5000m on Node hxx-m-2
STEP: Creating another pod that requires unavailable amount of CPU.
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-295c2c38-602e-4963-bf87-cf0fe3b307d9.15e47e4ecd9fd557], Reason = [Scheduled], Message = [Successfully assigned sched-pred-146/filler-pod-295c2c38-602e-4963-bf87-cf0fe3b307d9 to hxx-m-3]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-295c2c38-602e-4963-bf87-cf0fe3b307d9.15e47e4ef2c878d4], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-295c2c38-602e-4963-bf87-cf0fe3b307d9.15e47e4ef4d0a188], Reason = [Created], Message = [Created container filler-pod-295c2c38-602e-4963-bf87-cf0fe3b307d9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-295c2c38-602e-4963-bf87-cf0fe3b307d9.15e47e4efe3c3c29], Reason = [Started], Message = [Started container filler-pod-295c2c38-602e-4963-bf87-cf0fe3b307d9]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e2c727bc-8c79-409f-a037-4de5004d90f1.15e47e4ecdddf2f0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-146/filler-pod-e2c727bc-8c79-409f-a037-4de5004d90f1 to hxx-m-1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e2c727bc-8c79-409f-a037-4de5004d90f1.15e47e4ef5c61e48], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e2c727bc-8c79-409f-a037-4de5004d90f1.15e47e4ef737061b], Reason = [Created], Message = [Created container filler-pod-e2c727bc-8c79-409f-a037-4de5004d90f1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-e2c727bc-8c79-409f-a037-4de5004d90f1.15e47e4f019bc0f9], Reason = [Started], Message = [Started container filler-pod-e2c727bc-8c79-409f-a037-4de5004d90f1]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-eb2ce7b9-4d21-46d2-95e0-bd1113bc095b.15e47e4ecdf19ad4], Reason = [Scheduled], Message = [Successfully assigned sched-pred-146/filler-pod-eb2ce7b9-4d21-46d2-95e0-bd1113bc095b to hxx-m-2]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-eb2ce7b9-4d21-46d2-95e0-bd1113bc095b.15e47e4ef4620f5e], Reason = [Pulled], Message = [Container image "k8s.gcr.io/pause:3.1" already present on machine]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-eb2ce7b9-4d21-46d2-95e0-bd1113bc095b.15e47e4ef5aa9ddd], Reason = [Created], Message = [Created container filler-pod-eb2ce7b9-4d21-46d2-95e0-bd1113bc095b]
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-eb2ce7b9-4d21-46d2-95e0-bd1113bc095b.15e47e4eff528cd9], Reason = [Started], Message = [Started container filler-pod-eb2ce7b9-4d21-46d2-95e0-bd1113bc095b]
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.15e47e4f45c81e8f], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu.]
STEP: removing the label node off the node hxx-m-1
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node hxx-m-2
STEP: verifying the node doesn't have the label node
STEP: removing the label node off the node hxx-m-3
STEP: verifying the node doesn't have the label node
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:17:29.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-146" for this suite.
Dec 28 09:17:35.462: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:17:35.528: INFO: namespace sched-pred-146 deletion completed in 6.073935336s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:9.277 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates resource limits of pods that are allowed to run  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch 
  should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:17:35.528: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should add annotations for pods in rc  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating Redis RC
Dec 28 09:17:35.545: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 create -f - --namespace=kubectl-5423'
Dec 28 09:17:35.751: INFO: stderr: ""
Dec 28 09:17:35.751: INFO: stdout: "replicationcontroller/redis-master created\n"
STEP: Waiting for Redis master to start.
Dec 28 09:17:36.753: INFO: Selector matched 1 pods for map[app:redis]
Dec 28 09:17:36.753: INFO: Found 0 / 1
Dec 28 09:17:37.753: INFO: Selector matched 1 pods for map[app:redis]
Dec 28 09:17:37.753: INFO: Found 1 / 1
Dec 28 09:17:37.753: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods
Dec 28 09:17:37.755: INFO: Selector matched 1 pods for map[app:redis]
Dec 28 09:17:37.755: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Dec 28 09:17:37.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 patch pod redis-master-w4shz --namespace=kubectl-5423 -p {"metadata":{"annotations":{"x":"y"}}}'
Dec 28 09:17:37.829: INFO: stderr: ""
Dec 28 09:17:37.829: INFO: stdout: "pod/redis-master-w4shz patched\n"
STEP: checking annotations
Dec 28 09:17:37.831: INFO: Selector matched 1 pods for map[app:redis]
Dec 28 09:17:37.831: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:17:37.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5423" for this suite.
Dec 28 09:18:05.840: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:18:05.910: INFO: namespace kubectl-5423 deletion completed in 28.076418582s

• [SLOW TEST:30.381 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl patch
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1346
    should add annotations for pods in rc  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:18:05.910: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-8729
[It] should have a working scale subresource [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating statefulset ss in namespace statefulset-8729
Dec 28 09:18:05.933: INFO: Found 0 stateful pods, waiting for 1
Dec 28 09:18:15.936: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource
STEP: updating a scale subresource
STEP: verifying the statefulset Spec.Replicas was modified
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 28 09:18:15.945: INFO: Deleting all statefulset in ns statefulset-8729
Dec 28 09:18:15.949: INFO: Scaling statefulset ss to 0
Dec 28 09:18:35.959: INFO: Waiting for statefulset status.replicas updated to 0
Dec 28 09:18:35.960: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:18:35.966: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8729" for this suite.
Dec 28 09:18:41.977: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:18:42.048: INFO: namespace statefulset-8729 deletion completed in 6.079057975s

• [SLOW TEST:36.138 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should have a working scale subresource [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Pods Set QOS Class 
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:18:42.049: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:179
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying QOS class is set on the pod
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:18:42.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-245" for this suite.
Dec 28 09:18:54.083: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:18:54.148: INFO: namespace pods-245 deletion completed in 12.075405373s

• [SLOW TEST:12.099 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Pods Set QOS Class
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] 
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:18:54.148: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename crd-webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:126
STEP: Setting up server cert
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication
STEP: Deploying the custom resource conversion webhook pod
STEP: Wait for the deployment to be ready
Dec 28 09:18:54.427: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Dec 28 09:18:56.432: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713121534, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713121534, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713121534, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713121534, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-64d485d9bb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 28 09:18:59.440: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 09:18:59.443: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Creating a v1 custom resource
STEP: Create a v2 custom resource
STEP: List CRs in v1
STEP: List CRs in v2
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:19:00.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-9272" for this suite.
Dec 28 09:19:06.590: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:19:06.656: INFO: namespace crd-webhook-9272 deletion completed in 6.073593636s
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/crd_conversion_webhook.go:137

• [SLOW TEST:12.515 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Pods 
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:19:06.664: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: updating the pod
Dec 28 09:19:09.194: INFO: Successfully updated pod "pod-update-activedeadlineseconds-9c2a3cb0-1de2-49ed-96f7-eab3f14d6fc1"
Dec 28 09:19:09.194: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-9c2a3cb0-1de2-49ed-96f7-eab3f14d6fc1" in namespace "pods-2745" to be "terminated due to deadline exceeded"
Dec 28 09:19:09.195: INFO: Pod "pod-update-activedeadlineseconds-9c2a3cb0-1de2-49ed-96f7-eab3f14d6fc1": Phase="Running", Reason="", readiness=true. Elapsed: 1.328344ms
Dec 28 09:19:11.198: INFO: Pod "pod-update-activedeadlineseconds-9c2a3cb0-1de2-49ed-96f7-eab3f14d6fc1": Phase="Running", Reason="", readiness=true. Elapsed: 2.003719504s
Dec 28 09:19:13.200: INFO: Pod "pod-update-activedeadlineseconds-9c2a3cb0-1de2-49ed-96f7-eab3f14d6fc1": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 4.005947363s
Dec 28 09:19:13.200: INFO: Pod "pod-update-activedeadlineseconds-9c2a3cb0-1de2-49ed-96f7-eab3f14d6fc1" satisfied condition "terminated due to deadline exceeded"
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:19:13.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2745" for this suite.
Dec 28 09:19:19.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:19:19.276: INFO: namespace pods-2745 deletion completed in 6.073515157s

• [SLOW TEST:12.613 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:19:19.277: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap that has name configmap-test-emptyKey-3bd93dc9-a726-49b2-875a-f82304bcd727
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:19:19.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2302" for this suite.
Dec 28 09:19:25.302: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:19:25.368: INFO: namespace configmap-2302 deletion completed in 6.072714801s

• [SLOW TEST:6.091 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should fail to create ConfigMap with empty key [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:19:25.368: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: delete the pod with lifecycle hook
Dec 28 09:19:29.404: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 28 09:19:29.406: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 28 09:19:31.406: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 28 09:19:31.408: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 28 09:19:33.406: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 28 09:19:33.408: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 28 09:19:35.406: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 28 09:19:35.408: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 28 09:19:37.406: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 28 09:19:37.408: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 28 09:19:39.406: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 28 09:19:39.408: INFO: Pod pod-with-prestop-exec-hook still exists
Dec 28 09:19:41.406: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Dec 28 09:19:41.408: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:19:41.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6590" for this suite.
Dec 28 09:19:53.429: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:19:53.500: INFO: namespace container-lifecycle-hook-6590 deletion completed in 12.077808875s

• [SLOW TEST:28.132 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:19:53.500: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-279b62c9-2683-4f14-a8f1-7304cc1b6160
STEP: Creating a pod to test consume configMaps
Dec 28 09:19:53.524: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7e6f0622-b3b8-4bbe-86d6-de918e610251" in namespace "projected-4041" to be "success or failure"
Dec 28 09:19:53.525: INFO: Pod "pod-projected-configmaps-7e6f0622-b3b8-4bbe-86d6-de918e610251": Phase="Pending", Reason="", readiness=false. Elapsed: 1.580631ms
Dec 28 09:19:55.528: INFO: Pod "pod-projected-configmaps-7e6f0622-b3b8-4bbe-86d6-de918e610251": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003801041s
STEP: Saw pod success
Dec 28 09:19:55.528: INFO: Pod "pod-projected-configmaps-7e6f0622-b3b8-4bbe-86d6-de918e610251" satisfied condition "success or failure"
Dec 28 09:19:55.529: INFO: Trying to get logs from node hxx-m-2 pod pod-projected-configmaps-7e6f0622-b3b8-4bbe-86d6-de918e610251 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 28 09:19:55.540: INFO: Waiting for pod pod-projected-configmaps-7e6f0622-b3b8-4bbe-86d6-de918e610251 to disappear
Dec 28 09:19:55.542: INFO: Pod pod-projected-configmaps-7e6f0622-b3b8-4bbe-86d6-de918e610251 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:19:55.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4041" for this suite.
Dec 28 09:20:01.552: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:20:01.623: INFO: namespace projected-4041 deletion completed in 6.079118635s

• [SLOW TEST:8.123 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-node] ConfigMap 
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:20:01.624: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap configmap-594/configmap-test-296ec3b0-1ffe-4171-bfa0-4b9330328c9e
STEP: Creating a pod to test consume configMaps
Dec 28 09:20:01.645: INFO: Waiting up to 5m0s for pod "pod-configmaps-59fae223-875c-417a-b0f4-2a659e4df210" in namespace "configmap-594" to be "success or failure"
Dec 28 09:20:01.647: INFO: Pod "pod-configmaps-59fae223-875c-417a-b0f4-2a659e4df210": Phase="Pending", Reason="", readiness=false. Elapsed: 1.597058ms
Dec 28 09:20:03.649: INFO: Pod "pod-configmaps-59fae223-875c-417a-b0f4-2a659e4df210": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003707525s
STEP: Saw pod success
Dec 28 09:20:03.649: INFO: Pod "pod-configmaps-59fae223-875c-417a-b0f4-2a659e4df210" satisfied condition "success or failure"
Dec 28 09:20:03.650: INFO: Trying to get logs from node hxx-m-2 pod pod-configmaps-59fae223-875c-417a-b0f4-2a659e4df210 container env-test: <nil>
STEP: delete the pod
Dec 28 09:20:03.660: INFO: Waiting for pod pod-configmaps-59fae223-875c-417a-b0f4-2a659e4df210 to disappear
Dec 28 09:20:03.662: INFO: Pod pod-configmaps-59fae223-875c-417a-b0f4-2a659e4df210 no longer exists
[AfterEach] [sig-node] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:20:03.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-594" for this suite.
Dec 28 09:20:09.672: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:20:09.739: INFO: namespace configmap-594 deletion completed in 6.074270853s

• [SLOW TEST:8.115 seconds]
[sig-node] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap.go:32
  should be consumable via the environment [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a container with runAsUser 
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:20:09.739: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 09:20:09.760: INFO: Waiting up to 5m0s for pod "busybox-user-65534-c26b03a2-9c2d-4707-a268-c5ebe0ed8e9b" in namespace "security-context-test-1642" to be "success or failure"
Dec 28 09:20:09.762: INFO: Pod "busybox-user-65534-c26b03a2-9c2d-4707-a268-c5ebe0ed8e9b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.347176ms
Dec 28 09:20:11.764: INFO: Pod "busybox-user-65534-c26b03a2-9c2d-4707-a268-c5ebe0ed8e9b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004664026s
Dec 28 09:20:11.764: INFO: Pod "busybox-user-65534-c26b03a2-9c2d-4707-a268-c5ebe0ed8e9b" satisfied condition "success or failure"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:20:11.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-1642" for this suite.
Dec 28 09:20:17.775: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:20:17.844: INFO: namespace security-context-test-1642 deletion completed in 6.077058389s

• [SLOW TEST:8.105 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a container with runAsUser
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:44
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI 
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:20:17.844: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:40
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 28 09:20:17.864: INFO: Waiting up to 5m0s for pod "downwardapi-volume-2cbac7cb-1084-4dd2-ab83-a8ec1530ed8c" in namespace "projected-5876" to be "success or failure"
Dec 28 09:20:17.865: INFO: Pod "downwardapi-volume-2cbac7cb-1084-4dd2-ab83-a8ec1530ed8c": Phase="Pending", Reason="", readiness=false. Elapsed: 1.472815ms
Dec 28 09:20:19.868: INFO: Pod "downwardapi-volume-2cbac7cb-1084-4dd2-ab83-a8ec1530ed8c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003930821s
STEP: Saw pod success
Dec 28 09:20:19.868: INFO: Pod "downwardapi-volume-2cbac7cb-1084-4dd2-ab83-a8ec1530ed8c" satisfied condition "success or failure"
Dec 28 09:20:19.869: INFO: Trying to get logs from node hxx-m-2 pod downwardapi-volume-2cbac7cb-1084-4dd2-ab83-a8ec1530ed8c container client-container: <nil>
STEP: delete the pod
Dec 28 09:20:19.879: INFO: Waiting for pod downwardapi-volume-2cbac7cb-1084-4dd2-ab83-a8ec1530ed8c to disappear
Dec 28 09:20:19.880: INFO: Pod downwardapi-volume-2cbac7cb-1084-4dd2-ab83-a8ec1530ed8c no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:20:19.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5876" for this suite.
Dec 28 09:20:25.891: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:20:25.957: INFO: namespace projected-5876 deletion completed in 6.074183861s

• [SLOW TEST:8.112 seconds]
[sig-storage] Projected downwardAPI
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_downwardapi.go:34
  should provide container's cpu limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-network] DNS 
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:20:25.957: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename dns
STEP: Waiting for a default service account to be provisioned in namespace
[It] should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test externalName service
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8594.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8594.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8594.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8594.svc.cluster.local; sleep 1; done

STEP: creating a pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 28 09:20:29.988: INFO: DNS probes using dns-test-ab016a2a-2609-41d4-b9d2-06437b3070ff succeeded

STEP: deleting the pod
STEP: changing the externalName to bar.example.com
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8594.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-8594.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8594.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-8594.svc.cluster.local; sleep 1; done

STEP: creating a second pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 28 09:20:34.018: INFO: File wheezy_udp@dns-test-service-3.dns-8594.svc.cluster.local from pod  dns-8594/dns-test-a9862b14-0dd5-41d4-a4db-0cf9ec28853b contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 28 09:20:34.020: INFO: File jessie_udp@dns-test-service-3.dns-8594.svc.cluster.local from pod  dns-8594/dns-test-a9862b14-0dd5-41d4-a4db-0cf9ec28853b contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 28 09:20:34.020: INFO: Lookups using dns-8594/dns-test-a9862b14-0dd5-41d4-a4db-0cf9ec28853b failed for: [wheezy_udp@dns-test-service-3.dns-8594.svc.cluster.local jessie_udp@dns-test-service-3.dns-8594.svc.cluster.local]

Dec 28 09:20:39.023: INFO: File wheezy_udp@dns-test-service-3.dns-8594.svc.cluster.local from pod  dns-8594/dns-test-a9862b14-0dd5-41d4-a4db-0cf9ec28853b contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 28 09:20:39.025: INFO: File jessie_udp@dns-test-service-3.dns-8594.svc.cluster.local from pod  dns-8594/dns-test-a9862b14-0dd5-41d4-a4db-0cf9ec28853b contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 28 09:20:39.025: INFO: Lookups using dns-8594/dns-test-a9862b14-0dd5-41d4-a4db-0cf9ec28853b failed for: [wheezy_udp@dns-test-service-3.dns-8594.svc.cluster.local jessie_udp@dns-test-service-3.dns-8594.svc.cluster.local]

Dec 28 09:20:44.029: INFO: File wheezy_udp@dns-test-service-3.dns-8594.svc.cluster.local from pod  dns-8594/dns-test-a9862b14-0dd5-41d4-a4db-0cf9ec28853b contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 28 09:20:44.032: INFO: File jessie_udp@dns-test-service-3.dns-8594.svc.cluster.local from pod  dns-8594/dns-test-a9862b14-0dd5-41d4-a4db-0cf9ec28853b contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 28 09:20:44.032: INFO: Lookups using dns-8594/dns-test-a9862b14-0dd5-41d4-a4db-0cf9ec28853b failed for: [wheezy_udp@dns-test-service-3.dns-8594.svc.cluster.local jessie_udp@dns-test-service-3.dns-8594.svc.cluster.local]

Dec 28 09:20:49.023: INFO: File wheezy_udp@dns-test-service-3.dns-8594.svc.cluster.local from pod  dns-8594/dns-test-a9862b14-0dd5-41d4-a4db-0cf9ec28853b contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 28 09:20:49.025: INFO: File jessie_udp@dns-test-service-3.dns-8594.svc.cluster.local from pod  dns-8594/dns-test-a9862b14-0dd5-41d4-a4db-0cf9ec28853b contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 28 09:20:49.025: INFO: Lookups using dns-8594/dns-test-a9862b14-0dd5-41d4-a4db-0cf9ec28853b failed for: [wheezy_udp@dns-test-service-3.dns-8594.svc.cluster.local jessie_udp@dns-test-service-3.dns-8594.svc.cluster.local]

Dec 28 09:20:54.023: INFO: File wheezy_udp@dns-test-service-3.dns-8594.svc.cluster.local from pod  dns-8594/dns-test-a9862b14-0dd5-41d4-a4db-0cf9ec28853b contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 28 09:20:54.025: INFO: File jessie_udp@dns-test-service-3.dns-8594.svc.cluster.local from pod  dns-8594/dns-test-a9862b14-0dd5-41d4-a4db-0cf9ec28853b contains 'foo.example.com.
' instead of 'bar.example.com.'
Dec 28 09:20:54.025: INFO: Lookups using dns-8594/dns-test-a9862b14-0dd5-41d4-a4db-0cf9ec28853b failed for: [wheezy_udp@dns-test-service-3.dns-8594.svc.cluster.local jessie_udp@dns-test-service-3.dns-8594.svc.cluster.local]

Dec 28 09:20:59.024: INFO: DNS probes using dns-test-a9862b14-0dd5-41d4-a4db-0cf9ec28853b succeeded

STEP: deleting the pod
STEP: changing the service to type=ClusterIP
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8594.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-8594.svc.cluster.local; sleep 1; done

STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-8594.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-8594.svc.cluster.local; sleep 1; done

STEP: creating a third pod to probe DNS
STEP: submitting the pod to kubernetes
STEP: retrieving the pod
STEP: looking for the results for each expected name from probers
Dec 28 09:21:01.065: INFO: DNS probes using dns-test-79431f94-06b3-480d-80b2-364b5baf9d36 succeeded

STEP: deleting the pod
STEP: deleting the test externalName service
[AfterEach] [sig-network] DNS
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:21:01.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8594" for this suite.
Dec 28 09:21:07.126: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:21:07.193: INFO: namespace dns-8594 deletion completed in 6.093940047s

• [SLOW TEST:41.236 seconds]
[sig-network] DNS
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret 
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:21:07.193: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with secret that has name projected-secret-test-map-3099b3be-06ef-4143-bb52-5ac48e13d107
STEP: Creating a pod to test consume secrets
Dec 28 09:21:07.216: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-468a1a74-ef1f-42d7-bff8-68886e801349" in namespace "projected-6129" to be "success or failure"
Dec 28 09:21:07.218: INFO: Pod "pod-projected-secrets-468a1a74-ef1f-42d7-bff8-68886e801349": Phase="Pending", Reason="", readiness=false. Elapsed: 2.118005ms
Dec 28 09:21:09.221: INFO: Pod "pod-projected-secrets-468a1a74-ef1f-42d7-bff8-68886e801349": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004838358s
STEP: Saw pod success
Dec 28 09:21:09.221: INFO: Pod "pod-projected-secrets-468a1a74-ef1f-42d7-bff8-68886e801349" satisfied condition "success or failure"
Dec 28 09:21:09.222: INFO: Trying to get logs from node hxx-m-2 pod pod-projected-secrets-468a1a74-ef1f-42d7-bff8-68886e801349 container projected-secret-volume-test: <nil>
STEP: delete the pod
Dec 28 09:21:09.232: INFO: Waiting for pod pod-projected-secrets-468a1a74-ef1f-42d7-bff8-68886e801349 to disappear
Dec 28 09:21:09.233: INFO: Pod pod-projected-secrets-468a1a74-ef1f-42d7-bff8-68886e801349 no longer exists
[AfterEach] [sig-storage] Projected secret
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:21:09.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6129" for this suite.
Dec 28 09:21:15.242: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:21:15.308: INFO: namespace projected-6129 deletion completed in 6.07194406s

• [SLOW TEST:8.115 seconds]
[sig-storage] Projected secret
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_secret.go:35
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Pods Extended [k8s.io] Delete Grace Period 
  should be submitted and removed [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:21:15.308: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/node/pods.go:47
[It] should be submitted and removed [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: setting up selector
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
Dec 28 09:21:17.338: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-076158834 proxy -p 0'
STEP: deleting the pod gracefully
STEP: verifying the kubelet observed the termination notice
Dec 28 09:21:32.408: INFO: no pod exists with the name we were looking for, assuming the termination request was observed and completed
[AfterEach] [k8s.io] [sig-node] Pods Extended
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:21:32.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-711" for this suite.
Dec 28 09:21:38.420: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:21:38.485: INFO: namespace pods-711 deletion completed in 6.072854472s

• [SLOW TEST:23.177 seconds]
[k8s.io] [sig-node] Pods Extended
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  [k8s.io] Delete Grace Period
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    should be submitted and removed [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-network] Services 
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:21:38.485: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a service externalname-service with the type=ExternalName in namespace services-2509
STEP: changing the ExternalName service to type=ClusterIP
STEP: creating replication controller externalname-service in namespace services-2509
I1228 09:21:38.515180      22 runners.go:184] Created replication controller with name: externalname-service, namespace: services-2509, replica count: 2
Dec 28 09:21:41.565: INFO: Creating new exec pod
I1228 09:21:41.565607      22 runners.go:184] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 28 09:21:44.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=services-2509 execpod64ht4 -- /bin/sh -x -c nc -zv -t -w 2 externalname-service 80'
Dec 28 09:21:44.786: INFO: stderr: "+ nc -zv -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Dec 28 09:21:44.786: INFO: stdout: ""
Dec 28 09:21:44.786: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=services-2509 execpod64ht4 -- /bin/sh -x -c nc -zv -t -w 2 10.106.246.21 80'
Dec 28 09:21:44.997: INFO: stderr: "+ nc -zv -t -w 2 10.106.246.21 80\nConnection to 10.106.246.21 80 port [tcp/http] succeeded!\n"
Dec 28 09:21:44.997: INFO: stdout: ""
Dec 28 09:21:44.997: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:21:45.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2509" for this suite.
Dec 28 09:21:51.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:21:51.088: INFO: namespace services-2509 deletion completed in 6.078668636s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:12.603 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:21:51.089: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-volume-map-94c88b7f-1cd6-4144-964f-94e95005a9b1
STEP: Creating a pod to test consume configMaps
Dec 28 09:21:51.110: INFO: Waiting up to 5m0s for pod "pod-configmaps-ac8da654-a269-47e2-a541-925848166330" in namespace "configmap-5630" to be "success or failure"
Dec 28 09:21:51.114: INFO: Pod "pod-configmaps-ac8da654-a269-47e2-a541-925848166330": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069086ms
Dec 28 09:21:53.116: INFO: Pod "pod-configmaps-ac8da654-a269-47e2-a541-925848166330": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00641272s
STEP: Saw pod success
Dec 28 09:21:53.116: INFO: Pod "pod-configmaps-ac8da654-a269-47e2-a541-925848166330" satisfied condition "success or failure"
Dec 28 09:21:53.118: INFO: Trying to get logs from node hxx-m-2 pod pod-configmaps-ac8da654-a269-47e2-a541-925848166330 container configmap-volume-test: <nil>
STEP: delete the pod
Dec 28 09:21:53.128: INFO: Waiting for pod pod-configmaps-ac8da654-a269-47e2-a541-925848166330 to disappear
Dec 28 09:21:53.130: INFO: Pod pod-configmaps-ac8da654-a269-47e2-a541-925848166330 no longer exists
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:21:53.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5630" for this suite.
Dec 28 09:21:59.139: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:21:59.203: INFO: namespace configmap-5630 deletion completed in 6.070619286s

• [SLOW TEST:8.114 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 
  should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:21:59.203: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename proxy
STEP: Waiting for a default service account to be provisioned in namespace
[It] should proxy logs on node using proxy subresource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 09:21:59.230: INFO: (0) /api/v1/nodes/hxx-m-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 8.281772ms)
Dec 28 09:21:59.232: INFO: (1) /api/v1/nodes/hxx-m-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.469197ms)
Dec 28 09:21:59.235: INFO: (2) /api/v1/nodes/hxx-m-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.466524ms)
Dec 28 09:21:59.237: INFO: (3) /api/v1/nodes/hxx-m-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.316782ms)
Dec 28 09:21:59.239: INFO: (4) /api/v1/nodes/hxx-m-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.108939ms)
Dec 28 09:21:59.241: INFO: (5) /api/v1/nodes/hxx-m-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.196658ms)
Dec 28 09:21:59.244: INFO: (6) /api/v1/nodes/hxx-m-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.447755ms)
Dec 28 09:21:59.246: INFO: (7) /api/v1/nodes/hxx-m-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.127464ms)
Dec 28 09:21:59.248: INFO: (8) /api/v1/nodes/hxx-m-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.23734ms)
Dec 28 09:21:59.252: INFO: (9) /api/v1/nodes/hxx-m-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 3.763886ms)
Dec 28 09:21:59.254: INFO: (10) /api/v1/nodes/hxx-m-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.294176ms)
Dec 28 09:21:59.256: INFO: (11) /api/v1/nodes/hxx-m-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.086019ms)
Dec 28 09:21:59.259: INFO: (12) /api/v1/nodes/hxx-m-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.196944ms)
Dec 28 09:21:59.261: INFO: (13) /api/v1/nodes/hxx-m-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.047853ms)
Dec 28 09:21:59.263: INFO: (14) /api/v1/nodes/hxx-m-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.016737ms)
Dec 28 09:21:59.265: INFO: (15) /api/v1/nodes/hxx-m-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.928464ms)
Dec 28 09:21:59.267: INFO: (16) /api/v1/nodes/hxx-m-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.020218ms)
Dec 28 09:21:59.269: INFO: (17) /api/v1/nodes/hxx-m-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 1.943988ms)
Dec 28 09:21:59.271: INFO: (18) /api/v1/nodes/hxx-m-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.067082ms)
Dec 28 09:21:59.273: INFO: (19) /api/v1/nodes/hxx-m-1/proxy/logs/: <pre>
<a href="anaconda/">anaconda/</a>
<a href="audit/">audit/</a>
<a href="boot.log">boot.log</... (200; 2.035673ms)
[AfterEach] version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:21:59.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9098" for this suite.
Dec 28 09:22:05.282: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:22:05.348: INFO: namespace proxy-9098 deletion completed in 6.072511134s

• [SLOW TEST:6.145 seconds]
[sig-network] Proxy
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  version v1
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/proxy.go:57
    should proxy logs on node using proxy subresource  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-storage] EmptyDir wrapper volumes 
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:22:05.348: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename emptydir-wrapper
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating 50 configmaps
STEP: Creating RC which spawns configmap-volume pods
Dec 28 09:22:05.611: INFO: Pod name wrapped-volume-race-0636a70f-f291-4ffe-b729-7fda1ca6b456: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-0636a70f-f291-4ffe-b729-7fda1ca6b456 in namespace emptydir-wrapper-1867, will wait for the garbage collector to delete the pods
Dec 28 09:22:19.720: INFO: Deleting ReplicationController wrapped-volume-race-0636a70f-f291-4ffe-b729-7fda1ca6b456 took: 4.283675ms
Dec 28 09:22:20.620: INFO: Terminating ReplicationController wrapped-volume-race-0636a70f-f291-4ffe-b729-7fda1ca6b456 pods took: 900.200215ms
STEP: Creating RC which spawns configmap-volume pods
Dec 28 09:23:02.230: INFO: Pod name wrapped-volume-race-390824bd-b523-47ed-bdaf-09f7f05acff2: Found 0 pods out of 5
Dec 28 09:23:07.236: INFO: Pod name wrapped-volume-race-390824bd-b523-47ed-bdaf-09f7f05acff2: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-390824bd-b523-47ed-bdaf-09f7f05acff2 in namespace emptydir-wrapper-1867, will wait for the garbage collector to delete the pods
Dec 28 09:23:17.302: INFO: Deleting ReplicationController wrapped-volume-race-390824bd-b523-47ed-bdaf-09f7f05acff2 took: 3.533008ms
Dec 28 09:23:18.202: INFO: Terminating ReplicationController wrapped-volume-race-390824bd-b523-47ed-bdaf-09f7f05acff2 pods took: 900.211984ms
STEP: Creating RC which spawns configmap-volume pods
Dec 28 09:23:52.113: INFO: Pod name wrapped-volume-race-0d16cf36-40ab-4c21-a125-2c0c7405136a: Found 0 pods out of 5
Dec 28 09:23:57.118: INFO: Pod name wrapped-volume-race-0d16cf36-40ab-4c21-a125-2c0c7405136a: Found 5 pods out of 5
STEP: Ensuring each pod is running
STEP: deleting ReplicationController wrapped-volume-race-0d16cf36-40ab-4c21-a125-2c0c7405136a in namespace emptydir-wrapper-1867, will wait for the garbage collector to delete the pods
Dec 28 09:24:09.186: INFO: Deleting ReplicationController wrapped-volume-race-0d16cf36-40ab-4c21-a125-2c0c7405136a took: 4.253987ms
Dec 28 09:24:10.086: INFO: Terminating ReplicationController wrapped-volume-race-0d16cf36-40ab-4c21-a125-2c0c7405136a pods took: 900.193109ms
STEP: Cleaning up the configMaps
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:24:51.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-1867" for this suite.
Dec 28 09:24:57.440: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:24:57.509: INFO: namespace emptydir-wrapper-1867 deletion completed in 6.084394264s

• [SLOW TEST:172.161 seconds]
[sig-storage] EmptyDir wrapper volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] InitContainer [NodeConformance] 
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:24:57.509: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename init-container
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/init_container.go:44
[It] should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
Dec 28 09:24:57.525: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [k8s.io] InitContainer [NodeConformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:25:01.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9450" for this suite.
Dec 28 09:25:13.362: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:25:13.427: INFO: namespace init-container-9450 deletion completed in 12.072101634s

• [SLOW TEST:15.918 seconds]
[k8s.io] InitContainer [NodeConformance]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should invoke init containers on a RestartAlways pod [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets 
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:25:13.428: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename secrets
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating secret with name secret-test-40945107-3ebb-4528-b69a-a0e39922484e
STEP: Creating a pod to test consume secrets
Dec 28 09:25:13.450: INFO: Waiting up to 5m0s for pod "pod-secrets-7d9a2d39-848d-4915-afe7-34b39d729171" in namespace "secrets-8573" to be "success or failure"
Dec 28 09:25:13.452: INFO: Pod "pod-secrets-7d9a2d39-848d-4915-afe7-34b39d729171": Phase="Pending", Reason="", readiness=false. Elapsed: 2.366593ms
Dec 28 09:25:15.456: INFO: Pod "pod-secrets-7d9a2d39-848d-4915-afe7-34b39d729171": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.005726257s
STEP: Saw pod success
Dec 28 09:25:15.456: INFO: Pod "pod-secrets-7d9a2d39-848d-4915-afe7-34b39d729171" satisfied condition "success or failure"
Dec 28 09:25:15.458: INFO: Trying to get logs from node hxx-m-2 pod pod-secrets-7d9a2d39-848d-4915-afe7-34b39d729171 container secret-volume-test: <nil>
STEP: delete the pod
Dec 28 09:25:15.478: INFO: Waiting for pod pod-secrets-7d9a2d39-848d-4915-afe7-34b39d729171 to disappear
Dec 28 09:25:15.480: INFO: Pod pod-secrets-7d9a2d39-848d-4915-afe7-34b39d729171 no longer exists
[AfterEach] [sig-storage] Secrets
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:25:15.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8573" for this suite.
Dec 28 09:25:21.490: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:25:21.555: INFO: namespace secrets-8573 deletion completed in 6.073074046s

• [SLOW TEST:8.128 seconds]
[sig-storage] Secrets
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/secrets_volume.go:35
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo 
  should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:25:21.556: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Update Demo
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:277
[It] should create and stop a replication controller  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating a replication controller
Dec 28 09:25:21.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 create -f - --namespace=kubectl-4675'
Dec 28 09:25:21.781: INFO: stderr: ""
Dec 28 09:25:21.781: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up.
Dec 28 09:25:21.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4675'
Dec 28 09:25:21.854: INFO: stderr: ""
Dec 28 09:25:21.854: INFO: stdout: "update-demo-nautilus-gwz6r update-demo-nautilus-xcrxw "
Dec 28 09:25:21.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-gwz6r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4675'
Dec 28 09:25:21.926: INFO: stderr: ""
Dec 28 09:25:21.926: INFO: stdout: ""
Dec 28 09:25:21.926: INFO: update-demo-nautilus-gwz6r is created but not running
Dec 28 09:25:26.926: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo --namespace=kubectl-4675'
Dec 28 09:25:27.004: INFO: stderr: ""
Dec 28 09:25:27.004: INFO: stdout: "update-demo-nautilus-gwz6r update-demo-nautilus-xcrxw "
Dec 28 09:25:27.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-gwz6r -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4675'
Dec 28 09:25:27.079: INFO: stderr: ""
Dec 28 09:25:27.079: INFO: stdout: "true"
Dec 28 09:25:27.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-gwz6r -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4675'
Dec 28 09:25:27.160: INFO: stderr: ""
Dec 28 09:25:27.160: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 28 09:25:27.161: INFO: validating pod update-demo-nautilus-gwz6r
Dec 28 09:25:27.164: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 28 09:25:27.164: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 28 09:25:27.164: INFO: update-demo-nautilus-gwz6r is verified up and running
Dec 28 09:25:27.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-xcrxw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-4675'
Dec 28 09:25:27.243: INFO: stderr: ""
Dec 28 09:25:27.243: INFO: stdout: "true"
Dec 28 09:25:27.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods update-demo-nautilus-xcrxw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-4675'
Dec 28 09:25:27.317: INFO: stderr: ""
Dec 28 09:25:27.317: INFO: stdout: "gcr.io/kubernetes-e2e-test-images/nautilus:1.0"
Dec 28 09:25:27.317: INFO: validating pod update-demo-nautilus-xcrxw
Dec 28 09:25:27.320: INFO: got data: {
  "image": "nautilus.jpg"
}

Dec 28 09:25:27.320: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Dec 28 09:25:27.320: INFO: update-demo-nautilus-xcrxw is verified up and running
STEP: using delete to clean up resources
Dec 28 09:25:27.320: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 delete --grace-period=0 --force -f - --namespace=kubectl-4675'
Dec 28 09:25:27.402: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 28 09:25:27.402: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Dec 28 09:25:27.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4675'
Dec 28 09:25:27.480: INFO: stderr: "No resources found in kubectl-4675 namespace.\n"
Dec 28 09:25:27.480: INFO: stdout: ""
Dec 28 09:25:27.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods -l name=update-demo --namespace=kubectl-4675 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 28 09:25:27.556: INFO: stderr: ""
Dec 28 09:25:27.556: INFO: stdout: "update-demo-nautilus-gwz6r\nupdate-demo-nautilus-xcrxw\n"
Dec 28 09:25:28.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4675'
Dec 28 09:25:28.137: INFO: stderr: "No resources found in kubectl-4675 namespace.\n"
Dec 28 09:25:28.137: INFO: stdout: ""
Dec 28 09:25:28.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods -l name=update-demo --namespace=kubectl-4675 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 28 09:25:28.217: INFO: stderr: ""
Dec 28 09:25:28.217: INFO: stdout: "update-demo-nautilus-gwz6r\nupdate-demo-nautilus-xcrxw\n"
Dec 28 09:25:28.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get rc,svc -l name=update-demo --no-headers --namespace=kubectl-4675'
Dec 28 09:25:28.639: INFO: stderr: "No resources found in kubectl-4675 namespace.\n"
Dec 28 09:25:28.639: INFO: stdout: ""
Dec 28 09:25:28.639: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods -l name=update-demo --namespace=kubectl-4675 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 28 09:25:28.719: INFO: stderr: ""
Dec 28 09:25:28.719: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:25:28.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4675" for this suite.
Dec 28 09:25:40.729: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:25:40.802: INFO: namespace kubectl-4675 deletion completed in 12.079954717s

• [SLOW TEST:19.246 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Update Demo
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:275
    should create and stop a replication controller  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:25:40.802: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-10283339-ff8e-469b-9cfe-98c067d888ea
STEP: Creating a pod to test consume configMaps
Dec 28 09:25:40.828: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-50e525c6-4ff0-4b8b-ab2c-34fe1697e314" in namespace "projected-3390" to be "success or failure"
Dec 28 09:25:40.830: INFO: Pod "pod-projected-configmaps-50e525c6-4ff0-4b8b-ab2c-34fe1697e314": Phase="Pending", Reason="", readiness=false. Elapsed: 2.151506ms
Dec 28 09:25:42.832: INFO: Pod "pod-projected-configmaps-50e525c6-4ff0-4b8b-ab2c-34fe1697e314": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004397932s
STEP: Saw pod success
Dec 28 09:25:42.832: INFO: Pod "pod-projected-configmaps-50e525c6-4ff0-4b8b-ab2c-34fe1697e314" satisfied condition "success or failure"
Dec 28 09:25:42.833: INFO: Trying to get logs from node hxx-m-2 pod pod-projected-configmaps-50e525c6-4ff0-4b8b-ab2c-34fe1697e314 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 28 09:25:42.843: INFO: Waiting for pod pod-projected-configmaps-50e525c6-4ff0-4b8b-ab2c-34fe1697e314 to disappear
Dec 28 09:25:42.848: INFO: Pod pod-projected-configmaps-50e525c6-4ff0-4b8b-ab2c-34fe1697e314 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:25:42.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3390" for this suite.
Dec 28 09:25:48.861: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:25:48.929: INFO: namespace projected-3390 deletion completed in 6.075870835s

• [SLOW TEST:8.127 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment 
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:25:48.930: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename deployment
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:68
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 09:25:48.946: INFO: Creating deployment "test-recreate-deployment"
Dec 28 09:25:48.949: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Dec 28 09:25:48.954: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Dec 28 09:25:50.959: INFO: Waiting deployment "test-recreate-deployment" to complete
Dec 28 09:25:50.960: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Dec 28 09:25:50.963: INFO: Updating deployment test-recreate-deployment
Dec 28 09:25:50.963: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/deployment.go:62
Dec 28 09:25:50.999: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5572 /apis/apps/v1/namespaces/deployment-5572/deployments/test-recreate-deployment 2a164622-08f4-4069-9296-eb38579f3c80 303481 2 2019-12-28 09:25:48 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] []  []},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc006a7a608 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2019-12-28 09:25:50 +0000 UTC,LastTransitionTime:2019-12-28 09:25:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-5f94c574ff" is progressing.,LastUpdateTime:2019-12-28 09:25:50 +0000 UTC,LastTransitionTime:2019-12-28 09:25:48 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Dec 28 09:25:51.001: INFO: New ReplicaSet "test-recreate-deployment-5f94c574ff" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-5f94c574ff  deployment-5572 /apis/apps/v1/namespaces/deployment-5572/replicasets/test-recreate-deployment-5f94c574ff fecdff09-4abd-498c-b0fa-1a9861e989d6 303478 1 2019-12-28 09:25:50 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 2a164622-08f4-4069-9296-eb38579f3c80 0xc005041557 0xc005041558}] []  []},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 5f94c574ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [] []  []} {[] [] [{httpd docker.io/library/httpd:2.4.38-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc0050415b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 28 09:25:51.001: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Dec 28 09:25:51.001: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-68fc85c7bb  deployment-5572 /apis/apps/v1/namespaces/deployment-5572/replicasets/test-recreate-deployment-68fc85c7bb b101d849-9c4a-48e4-b989-f75ad7624ea6 303468 2 2019-12-28 09:25:48 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 2a164622-08f4-4069-9296-eb38579f3c80 0xc005041627 0xc005041628}] []  []},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 68fc85c7bb,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:68fc85c7bb] map[] [] []  []} {[] [] [{redis docker.io/library/redis:5.0.5-alpine [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,} false false false}] [] Always 0xc005041688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Dec 28 09:25:51.003: INFO: Pod "test-recreate-deployment-5f94c574ff-cjptz" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-5f94c574ff-cjptz test-recreate-deployment-5f94c574ff- deployment-5572 /api/v1/namespaces/deployment-5572/pods/test-recreate-deployment-5f94c574ff-cjptz 6923cebe-b2b3-48c9-9aca-ef9fc826e7cd 303480 0 2019-12-28 09:25:50 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:5f94c574ff] map[] [{apps/v1 ReplicaSet test-recreate-deployment-5f94c574ff fecdff09-4abd-498c-b0fa-1a9861e989d6 0xc005041af7 0xc005041af8}] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-hphkt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-hphkt,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:httpd,Image:docker.io/library/httpd:2.4.38-alpine,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-hphkt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:hxx-m-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:25:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:25:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:25:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:25:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.16,PodIP:,StartTime:2019-12-28 09:25:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:docker.io/library/httpd:2.4.38-alpine,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:25:51.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5572" for this suite.
Dec 28 09:25:57.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:25:57.083: INFO: namespace deployment-5572 deletion completed in 6.077618125s

• [SLOW TEST:8.153 seconds]
[sig-apps] Deployment
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume 
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:25:57.083: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 28 09:25:57.103: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0b893d4a-3352-4783-9c76-0fcf62c5459e" in namespace "downward-api-5456" to be "success or failure"
Dec 28 09:25:57.105: INFO: Pod "downwardapi-volume-0b893d4a-3352-4783-9c76-0fcf62c5459e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.340247ms
Dec 28 09:25:59.108: INFO: Pod "downwardapi-volume-0b893d4a-3352-4783-9c76-0fcf62c5459e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004668201s
STEP: Saw pod success
Dec 28 09:25:59.108: INFO: Pod "downwardapi-volume-0b893d4a-3352-4783-9c76-0fcf62c5459e" satisfied condition "success or failure"
Dec 28 09:25:59.109: INFO: Trying to get logs from node hxx-m-2 pod downwardapi-volume-0b893d4a-3352-4783-9c76-0fcf62c5459e container client-container: <nil>
STEP: delete the pod
Dec 28 09:25:59.119: INFO: Waiting for pod downwardapi-volume-0b893d4a-3352-4783-9c76-0fcf62c5459e to disappear
Dec 28 09:25:59.121: INFO: Pod downwardapi-volume-0b893d4a-3352-4783-9c76-0fcf62c5459e no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:25:59.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5456" for this suite.
Dec 28 09:26:05.131: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:26:05.197: INFO: namespace downward-api-5456 deletion completed in 6.07323742s

• [SLOW TEST:8.114 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController 
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:26:05.198: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename replication-controller
STEP: Waiting for a default service account to be provisioned in namespace
[It] should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Given a Pod with a 'name' label pod-adoption is created
STEP: When a replication controller with a matching selector is created
STEP: Then the orphan pod is adopted
[AfterEach] [sig-apps] ReplicationController
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:26:08.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6288" for this suite.
Dec 28 09:26:20.237: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:26:20.309: INFO: namespace replication-controller-6288 deletion completed in 12.07970505s

• [SLOW TEST:15.112 seconds]
[sig-apps] ReplicationController
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Probing container 
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:26:20.309: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename container-probe
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/container_probe.go:52
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating pod busybox-6609029e-42c8-43c5-8e2f-9cb53688a045 in namespace container-probe-4651
Dec 28 09:26:22.333: INFO: Started pod busybox-6609029e-42c8-43c5-8e2f-9cb53688a045 in namespace container-probe-4651
STEP: checking the pod's current state and verifying that restartCount is present
Dec 28 09:26:22.335: INFO: Initial restart count of pod busybox-6609029e-42c8-43c5-8e2f-9cb53688a045 is 0
STEP: deleting the pod
[AfterEach] [k8s.io] Probing container
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:30:22.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4651" for this suite.
Dec 28 09:30:28.649: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:30:28.715: INFO: namespace container-probe-4651 deletion completed in 6.081760277s

• [SLOW TEST:248.406 seconds]
[k8s.io] Probing container
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:30:28.715: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-4157
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating stateful set ss in namespace statefulset-4157
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4157
Dec 28 09:30:28.739: INFO: Found 0 stateful pods, waiting for 1
Dec 28 09:30:38.742: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod
Dec 28 09:30:38.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 28 09:30:39.052: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 28 09:30:39.052: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 28 09:30:39.052: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 28 09:30:39.054: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 28 09:30:49.057: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 28 09:30:49.057: INFO: Waiting for statefulset status.replicas updated to 0
Dec 28 09:30:49.065: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Dec 28 09:30:49.065: INFO: ss-0  hxx-m-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:39 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:28 +0000 UTC  }]
Dec 28 09:30:49.065: INFO: 
Dec 28 09:30:49.065: INFO: StatefulSet ss has not reached scale 3, at 1
Dec 28 09:30:50.068: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997923339s
Dec 28 09:30:51.071: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995177631s
Dec 28 09:30:52.073: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.992050001s
Dec 28 09:30:53.076: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.989566526s
Dec 28 09:30:54.078: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.987057147s
Dec 28 09:30:55.081: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.9847154s
Dec 28 09:30:56.083: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.982035248s
Dec 28 09:30:57.086: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.979452728s
Dec 28 09:30:58.089: INFO: Verifying statefulset ss doesn't scale past 3 for another 976.82567ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4157
Dec 28 09:30:59.091: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:30:59.311: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 28 09:30:59.311: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 28 09:30:59.311: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 28 09:30:59.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:30:59.539: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 28 09:30:59.539: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 28 09:30:59.539: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 28 09:30:59.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:30:59.761: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Dec 28 09:30:59.761: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 28 09:30:59.761: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 28 09:30:59.763: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 28 09:30:59.763: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 28 09:30:59.763: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod
Dec 28 09:30:59.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 28 09:30:59.987: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 28 09:30:59.987: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 28 09:30:59.987: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 28 09:30:59.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 28 09:31:00.233: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 28 09:31:00.233: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 28 09:31:00.233: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 28 09:31:00.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 28 09:31:00.462: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 28 09:31:00.462: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 28 09:31:00.462: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 28 09:31:00.462: INFO: Waiting for statefulset status.replicas updated to 0
Dec 28 09:31:00.464: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Dec 28 09:31:10.468: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 28 09:31:10.468: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 28 09:31:10.468: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 28 09:31:10.474: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Dec 28 09:31:10.474: INFO: ss-0  hxx-m-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:28 +0000 UTC  }]
Dec 28 09:31:10.474: INFO: ss-1  hxx-m-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  }]
Dec 28 09:31:10.474: INFO: ss-2  hxx-m-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  }]
Dec 28 09:31:10.474: INFO: 
Dec 28 09:31:10.474: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 28 09:31:11.477: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Dec 28 09:31:11.477: INFO: ss-0  hxx-m-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:28 +0000 UTC  }]
Dec 28 09:31:11.477: INFO: ss-1  hxx-m-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  }]
Dec 28 09:31:11.477: INFO: ss-2  hxx-m-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  }]
Dec 28 09:31:11.477: INFO: 
Dec 28 09:31:11.477: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 28 09:31:12.480: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Dec 28 09:31:12.480: INFO: ss-0  hxx-m-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:28 +0000 UTC  }]
Dec 28 09:31:12.480: INFO: ss-1  hxx-m-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  }]
Dec 28 09:31:12.480: INFO: ss-2  hxx-m-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  }]
Dec 28 09:31:12.480: INFO: 
Dec 28 09:31:12.480: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 28 09:31:13.483: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Dec 28 09:31:13.483: INFO: ss-0  hxx-m-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:28 +0000 UTC  }]
Dec 28 09:31:13.483: INFO: ss-1  hxx-m-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  }]
Dec 28 09:31:13.483: INFO: ss-2  hxx-m-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  }]
Dec 28 09:31:13.483: INFO: 
Dec 28 09:31:13.483: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 28 09:31:14.486: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Dec 28 09:31:14.486: INFO: ss-0  hxx-m-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:28 +0000 UTC  }]
Dec 28 09:31:14.486: INFO: ss-1  hxx-m-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  }]
Dec 28 09:31:14.486: INFO: ss-2  hxx-m-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  }]
Dec 28 09:31:14.486: INFO: 
Dec 28 09:31:14.486: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 28 09:31:15.488: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Dec 28 09:31:15.488: INFO: ss-0  hxx-m-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:28 +0000 UTC  }]
Dec 28 09:31:15.488: INFO: ss-1  hxx-m-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  }]
Dec 28 09:31:15.488: INFO: ss-2  hxx-m-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  }]
Dec 28 09:31:15.488: INFO: 
Dec 28 09:31:15.488: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 28 09:31:16.491: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Dec 28 09:31:16.491: INFO: ss-0  hxx-m-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:28 +0000 UTC  }]
Dec 28 09:31:16.491: INFO: ss-1  hxx-m-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  }]
Dec 28 09:31:16.491: INFO: ss-2  hxx-m-3  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  }]
Dec 28 09:31:16.491: INFO: 
Dec 28 09:31:16.491: INFO: StatefulSet ss has not reached scale 0, at 3
Dec 28 09:31:17.494: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Dec 28 09:31:17.494: INFO: ss-0  hxx-m-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:28 +0000 UTC  }]
Dec 28 09:31:17.494: INFO: ss-1  hxx-m-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  }]
Dec 28 09:31:17.494: INFO: 
Dec 28 09:31:17.494: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 28 09:31:18.496: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Dec 28 09:31:18.496: INFO: ss-0  hxx-m-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:28 +0000 UTC  }]
Dec 28 09:31:18.497: INFO: ss-1  hxx-m-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  }]
Dec 28 09:31:18.497: INFO: 
Dec 28 09:31:18.497: INFO: StatefulSet ss has not reached scale 0, at 2
Dec 28 09:31:19.499: INFO: POD   NODE     PHASE    GRACE  CONDITIONS
Dec 28 09:31:19.499: INFO: ss-0  hxx-m-2  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:28 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:28 +0000 UTC  }]
Dec 28 09:31:19.499: INFO: ss-1  hxx-m-1  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2019-12-28 09:30:49 +0000 UTC  }]
Dec 28 09:31:19.499: INFO: 
Dec 28 09:31:19.499: INFO: StatefulSet ss has not reached scale 0, at 2
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4157
Dec 28 09:31:20.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:31:20.597: INFO: rc: 1
Dec 28 09:31:20.597: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  error: unable to upgrade connection: container not found ("webserver")
 [] <nil> 0xc000d72120 exit status 1 <nil> <nil> true [0xc0032fc048 0xc0032fc060 0xc0032fc078] [0xc0032fc048 0xc0032fc060 0xc0032fc078] [0xc0032fc058 0xc0032fc070] [0x10efe30 0x10efe30] 0xc001c91320 <nil>}:
Command stdout:

stderr:
error: unable to upgrade connection: container not found ("webserver")

error:
exit status 1
Dec 28 09:31:30.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:31:30.669: INFO: rc: 1
Dec 28 09:31:30.670: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc007f1fa10 exit status 1 <nil> <nil> true [0xc000011498 0xc0000118b0 0xc000011b68] [0xc000011498 0xc0000118b0 0xc000011b68] [0xc0000116f8 0xc000011ae0] [0x10efe30 0x10efe30] 0xc002f7ca80 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:31:40.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:31:40.744: INFO: rc: 1
Dec 28 09:31:40.744: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d724e0 exit status 1 <nil> <nil> true [0xc0032fc080 0xc0032fc098 0xc0032fc0b0] [0xc0032fc080 0xc0032fc098 0xc0032fc0b0] [0xc0032fc090 0xc0032fc0a8] [0x10efe30 0x10efe30] 0xc001c91920 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:31:50.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:31:50.817: INFO: rc: 1
Dec 28 09:31:50.817: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d729c0 exit status 1 <nil> <nil> true [0xc0032fc0b8 0xc0032fc0d0 0xc0032fc0e8] [0xc0032fc0b8 0xc0032fc0d0 0xc0032fc0e8] [0xc0032fc0c8 0xc0032fc0e0] [0x10efe30 0x10efe30] 0xc006f40180 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:32:00.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:32:00.890: INFO: rc: 1
Dec 28 09:32:00.890: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001d21d10 exit status 1 <nil> <nil> true [0xc002d68498 0xc002d684d8 0xc002d68510] [0xc002d68498 0xc002d684d8 0xc002d68510] [0xc002d684c8 0xc002d684f8] [0x10efe30 0x10efe30] 0xc003be1320 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:32:10.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:32:10.963: INFO: rc: 1
Dec 28 09:32:10.963: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc000d72f00 exit status 1 <nil> <nil> true [0xc0032fc120 0xc0032fc138 0xc0032fc150] [0xc0032fc120 0xc0032fc138 0xc0032fc150] [0xc0032fc130 0xc0032fc148] [0x10efe30 0x10efe30] 0xc006f406c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:32:20.964: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:32:21.036: INFO: rc: 1
Dec 28 09:32:21.037: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc007f1fec0 exit status 1 <nil> <nil> true [0xc000011c60 0xc000011de0 0xc000011ec8] [0xc000011c60 0xc000011de0 0xc000011ec8] [0xc000011d60 0xc000011e80] [0x10efe30 0x10efe30] 0xc002f7cf00 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:32:31.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:32:31.109: INFO: rc: 1
Dec 28 09:32:31.109: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc00413a150 exit status 1 <nil> <nil> true [0xc002d68520 0xc002d68560 0xc002d68598] [0xc002d68520 0xc002d68560 0xc002d68598] [0xc002d68540 0xc002d68588] [0x10efe30 0x10efe30] 0xc003be16e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:32:41.109: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:32:41.179: INFO: rc: 1
Dec 28 09:32:41.179: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0072da270 exit status 1 <nil> <nil> true [0xc000011f00 0xc000011fa8 0xc000b9a350] [0xc000011f00 0xc000011fa8 0xc000b9a350] [0xc000011f58 0xc000b9a258] [0x10efe30 0x10efe30] 0xc002f7d2c0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:32:51.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:32:51.257: INFO: rc: 1
Dec 28 09:32:51.257: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc007f1e540 exit status 1 <nil> <nil> true [0xc000010e00 0xc000010fd0 0xc0000112d8] [0xc000010e00 0xc000010fd0 0xc0000112d8] [0xc000010f08 0xc000011130] [0x10efe30 0x10efe30] 0xc0029cdd40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:33:01.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:33:01.335: INFO: rc: 1
Dec 28 09:33:01.335: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001d20330 exit status 1 <nil> <nil> true [0xc0001a80a0 0xc0001a9288 0xc0001a9658] [0xc0001a80a0 0xc0001a9288 0xc0001a9658] [0xc0001a8cf8 0xc0001a9638] [0x10efe30 0x10efe30] 0xc001c90480 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:33:11.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:33:11.412: INFO: rc: 1
Dec 28 09:33:11.413: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0021137d0 exit status 1 <nil> <nil> true [0xc0000e06a0 0xc00018e660 0xc00018e730] [0xc0000e06a0 0xc00018e660 0xc00018e730] [0xc00018e638 0xc00018e710] [0x10efe30 0x10efe30] 0xc00375c300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:33:21.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:33:21.493: INFO: rc: 1
Dec 28 09:33:21.493: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc007f1ec60 exit status 1 <nil> <nil> true [0xc000011498 0xc0000118b0 0xc000011b68] [0xc000011498 0xc0000118b0 0xc000011b68] [0xc0000116f8 0xc000011ae0] [0x10efe30 0x10efe30] 0xc006a82300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:33:31.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:33:31.569: INFO: rc: 1
Dec 28 09:33:31.569: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc007f1f410 exit status 1 <nil> <nil> true [0xc000011c60 0xc000011de0 0xc000011ec8] [0xc000011c60 0xc000011de0 0xc000011ec8] [0xc000011d60 0xc000011e80] [0x10efe30 0x10efe30] 0xc006a82840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:33:41.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:33:41.643: INFO: rc: 1
Dec 28 09:33:41.643: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001d20690 exit status 1 <nil> <nil> true [0xc0001a9720 0xc0001a9c70 0xc0001a9d68] [0xc0001a9720 0xc0001a9c70 0xc0001a9d68] [0xc0001a9bb8 0xc0001a9d30] [0x10efe30 0x10efe30] 0xc001c90b40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:33:51.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:33:51.718: INFO: rc: 1
Dec 28 09:33:51.718: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc007f1fbc0 exit status 1 <nil> <nil> true [0xc000011f00 0xc000011fa8 0xc002d68040] [0xc000011f00 0xc000011fa8 0xc002d68040] [0xc000011f58 0xc002d68028] [0x10efe30 0x10efe30] 0xc006a83440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:34:01.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:34:01.794: INFO: rc: 1
Dec 28 09:34:01.794: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc007f1ff80 exit status 1 <nil> <nil> true [0xc002d68050 0xc002d680a0 0xc002d680b8] [0xc002d68050 0xc002d680a0 0xc002d680b8] [0xc002d68088 0xc002d680b0] [0x10efe30 0x10efe30] 0xc003be0120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:34:11.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:34:11.865: INFO: rc: 1
Dec 28 09:34:11.865: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002113bc0 exit status 1 <nil> <nil> true [0xc00018e750 0xc00018e840 0xc00018e968] [0xc00018e750 0xc00018e840 0xc00018e968] [0xc00018e820 0xc00018e930] [0x10efe30 0x10efe30] 0xc00375c720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:34:21.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:34:21.943: INFO: rc: 1
Dec 28 09:34:21.943: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002113f20 exit status 1 <nil> <nil> true [0xc00018e980 0xc00018ea70 0xc00018eaf8] [0xc00018e980 0xc00018ea70 0xc00018eaf8] [0xc00018ea30 0xc00018eae8] [0x10efe30 0x10efe30] 0xc00375d020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:34:31.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:34:32.014: INFO: rc: 1
Dec 28 09:34:32.014: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e34390 exit status 1 <nil> <nil> true [0xc002d680d0 0xc002d680f8 0xc002d68120] [0xc002d680d0 0xc002d680f8 0xc002d68120] [0xc002d680f0 0xc002d68108] [0x10efe30 0x10efe30] 0xc003be04e0 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:34:42.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:34:42.087: INFO: rc: 1
Dec 28 09:34:42.087: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc001e346f0 exit status 1 <nil> <nil> true [0xc002d68138 0xc002d68150 0xc002d68190] [0xc002d68138 0xc002d68150 0xc002d68190] [0xc002d68148 0xc002d68168] [0x10efe30 0x10efe30] 0xc003be0960 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:34:52.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:34:52.160: INFO: rc: 1
Dec 28 09:34:52.160: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc007f1e600 exit status 1 <nil> <nil> true [0xc002d68008 0xc002d68050 0xc002d680a0] [0xc002d68008 0xc002d68050 0xc002d680a0] [0xc002d68040 0xc002d68088] [0x10efe30 0x10efe30] 0xc006a82300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:35:02.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:35:02.230: INFO: rc: 1
Dec 28 09:35:02.230: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc007f1ed50 exit status 1 <nil> <nil> true [0xc002d680a8 0xc002d680d0 0xc002d680f8] [0xc002d680a8 0xc002d680d0 0xc002d680f8] [0xc002d680b8 0xc002d680f0] [0x10efe30 0x10efe30] 0xc006a82840 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:35:12.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:35:12.317: INFO: rc: 1
Dec 28 09:35:12.317: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc007f1f590 exit status 1 <nil> <nil> true [0xc002d68100 0xc002d68138 0xc002d68150] [0xc002d68100 0xc002d68138 0xc002d68150] [0xc002d68120 0xc002d68148] [0x10efe30 0x10efe30] 0xc006a83440 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:35:22.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:35:22.392: INFO: rc: 1
Dec 28 09:35:22.392: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc002113800 exit status 1 <nil> <nil> true [0xc000010c30 0xc000010f08 0xc000011130] [0xc000010c30 0xc000010f08 0xc000011130] [0xc000010eb8 0xc0000110a8] [0x10efe30 0x10efe30] 0xc0029cdd40 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:35:32.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:35:32.467: INFO: rc: 1
Dec 28 09:35:32.467: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0021c4390 exit status 1 <nil> <nil> true [0xc00018e278 0xc00018e690 0xc00018e750] [0xc00018e278 0xc00018e690 0xc00018e750] [0xc00018e660 0xc00018e730] [0x10efe30 0x10efe30] 0xc00375c300 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:35:42.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:35:42.541: INFO: rc: 1
Dec 28 09:35:42.541: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc007f1fcb0 exit status 1 <nil> <nil> true [0xc002d68158 0xc002d681a0 0xc002d681e0] [0xc002d68158 0xc002d681a0 0xc002d681e0] [0xc002d68190 0xc002d681d0] [0x10efe30 0x10efe30] 0xc003be0120 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:35:52.541: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:35:52.615: INFO: rc: 1
Dec 28 09:35:52.615: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0021c4720 exit status 1 <nil> <nil> true [0xc00018e810 0xc00018e8a0 0xc00018e980] [0xc00018e810 0xc00018e8a0 0xc00018e980] [0xc00018e840 0xc00018e968] [0x10efe30 0x10efe30] 0xc00375c720 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:36:02.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:36:02.689: INFO: rc: 1
Dec 28 09:36:02.689: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0021c4ae0 exit status 1 <nil> <nil> true [0xc00018e9f8 0xc00018eab8 0xc00018eb80] [0xc00018e9f8 0xc00018eab8 0xc00018eb80] [0xc00018ea70 0xc00018eaf8] [0x10efe30 0x10efe30] 0xc00375d020 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:36:12.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:36:12.765: INFO: rc: 1
Dec 28 09:36:12.765: INFO: Waiting 10s to retry failed RunHostCmd: error running &{/usr/local/bin/kubectl [kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true] []  <nil>  Error from server (NotFound): pods "ss-0" not found
 [] <nil> 0xc0021c4ea0 exit status 1 <nil> <nil> true [0xc00018ec20 0xc00018ecb0 0xc00018edd8] [0xc00018ec20 0xc00018ecb0 0xc00018edd8] [0xc00018ec90 0xc00018edd0] [0x10efe30 0x10efe30] 0xc00375d680 <nil>}:
Command stdout:

stderr:
Error from server (NotFound): pods "ss-0" not found

error:
exit status 1
Dec 28 09:36:22.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-4157 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:36:22.838: INFO: rc: 1
Dec 28 09:36:22.838: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: 
Dec 28 09:36:22.838: INFO: Scaling statefulset ss to 0
Dec 28 09:36:22.843: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 28 09:36:22.845: INFO: Deleting all statefulset in ns statefulset-4157
Dec 28 09:36:22.846: INFO: Scaling statefulset ss to 0
Dec 28 09:36:22.851: INFO: Waiting for statefulset status.replicas updated to 0
Dec 28 09:36:22.852: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:36:22.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-4157" for this suite.
Dec 28 09:36:28.874: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:36:28.942: INFO: namespace statefulset-4157 deletion completed in 6.079664154s

• [SLOW TEST:360.226 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services 
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:36:28.942: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename services
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:91
[It] should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating service nodeport-test with type=NodePort in namespace services-3089
STEP: creating replication controller nodeport-test in namespace services-3089
I1228 09:36:28.967123      22 runners.go:184] Created replication controller with name: nodeport-test, namespace: services-3089, replica count: 2
Dec 28 09:36:32.017: INFO: Creating new exec pod
I1228 09:36:32.017520      22 runners.go:184] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Dec 28 09:36:35.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=services-3089 execpodtb255 -- /bin/sh -x -c nc -zv -t -w 2 nodeport-test 80'
Dec 28 09:36:35.255: INFO: stderr: "+ nc -zv -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Dec 28 09:36:35.255: INFO: stdout: ""
Dec 28 09:36:35.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=services-3089 execpodtb255 -- /bin/sh -x -c nc -zv -t -w 2 10.98.242.128 80'
Dec 28 09:36:35.473: INFO: stderr: "+ nc -zv -t -w 2 10.98.242.128 80\nConnection to 10.98.242.128 80 port [tcp/http] succeeded!\n"
Dec 28 09:36:35.473: INFO: stdout: ""
Dec 28 09:36:35.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=services-3089 execpodtb255 -- /bin/sh -x -c nc -zv -t -w 2 10.0.128.42 32030'
Dec 28 09:36:35.698: INFO: stderr: "+ nc -zv -t -w 2 10.0.128.42 32030\nConnection to 10.0.128.42 32030 port [tcp/32030] succeeded!\n"
Dec 28 09:36:35.698: INFO: stdout: ""
Dec 28 09:36:35.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=services-3089 execpodtb255 -- /bin/sh -x -c nc -zv -t -w 2 10.0.128.16 32030'
Dec 28 09:36:35.912: INFO: stderr: "+ nc -zv -t -w 2 10.0.128.16 32030\nConnection to 10.0.128.16 32030 port [tcp/32030] succeeded!\n"
Dec 28 09:36:35.912: INFO: stdout: ""
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:36:35.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-3089" for this suite.
Dec 28 09:36:41.924: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:36:41.994: INFO: namespace services-3089 deletion completed in 6.078153448s
[AfterEach] [sig-network] Services
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/service.go:95

• [SLOW TEST:13.052 seconds]
[sig-network] Services
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/network/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:36:41.994: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 28 09:36:43.020: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:36:43.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8829" for this suite.
Dec 28 09:36:49.037: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:36:49.103: INFO: namespace container-runtime-8829 deletion completed in 6.073496257s

• [SLOW TEST:7.109 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl rolling-update 
  should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:36:49.103: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl rolling-update
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1499
[It] should support rolling-update to same image  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: running the image docker.io/library/httpd:2.4.38-alpine
Dec 28 09:36:49.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 run e2e-test-httpd-rc --image=docker.io/library/httpd:2.4.38-alpine --generator=run/v1 --namespace=kubectl-7809'
Dec 28 09:36:49.199: INFO: stderr: "kubectl run --generator=run/v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.\n"
Dec 28 09:36:49.199: INFO: stdout: "replicationcontroller/e2e-test-httpd-rc created\n"
STEP: verifying the rc e2e-test-httpd-rc was created
STEP: rolling-update to same image controller
Dec 28 09:36:49.203: INFO: scanned /root for discovery docs: <nil>
Dec 28 09:36:49.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 rolling-update e2e-test-httpd-rc --update-period=1s --image=docker.io/library/httpd:2.4.38-alpine --image-pull-policy=IfNotPresent --namespace=kubectl-7809'
Dec 28 09:37:02.110: INFO: stderr: "Command \"rolling-update\" is deprecated, use \"rollout\" instead\n"
Dec 28 09:37:02.110: INFO: stdout: "Created e2e-test-httpd-rc-b0d72a40fc9a80ed653c5485b7f998db\nScaling up e2e-test-httpd-rc-b0d72a40fc9a80ed653c5485b7f998db from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-b0d72a40fc9a80ed653c5485b7f998db up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-b0d72a40fc9a80ed653c5485b7f998db to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
Dec 28 09:37:02.110: INFO: stdout: "Created e2e-test-httpd-rc-b0d72a40fc9a80ed653c5485b7f998db\nScaling up e2e-test-httpd-rc-b0d72a40fc9a80ed653c5485b7f998db from 0 to 1, scaling down e2e-test-httpd-rc from 1 to 0 (keep 1 pods available, don't exceed 2 pods)\nScaling e2e-test-httpd-rc-b0d72a40fc9a80ed653c5485b7f998db up to 1\nScaling e2e-test-httpd-rc down to 0\nUpdate succeeded. Deleting old controller: e2e-test-httpd-rc\nRenaming e2e-test-httpd-rc-b0d72a40fc9a80ed653c5485b7f998db to e2e-test-httpd-rc\nreplicationcontroller/e2e-test-httpd-rc rolling updated\n"
STEP: waiting for all containers in run=e2e-test-httpd-rc pods to come up.
Dec 28 09:37:02.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l run=e2e-test-httpd-rc --namespace=kubectl-7809'
Dec 28 09:37:02.187: INFO: stderr: ""
Dec 28 09:37:02.187: INFO: stdout: "e2e-test-httpd-rc-b0d72a40fc9a80ed653c5485b7f998db-krvlq "
Dec 28 09:37:02.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods e2e-test-httpd-rc-b0d72a40fc9a80ed653c5485b7f998db-krvlq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "e2e-test-httpd-rc") (exists . "state" "running"))}}true{{end}}{{end}}{{end}} --namespace=kubectl-7809'
Dec 28 09:37:02.258: INFO: stderr: ""
Dec 28 09:37:02.258: INFO: stdout: "true"
Dec 28 09:37:02.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods e2e-test-httpd-rc-b0d72a40fc9a80ed653c5485b7f998db-krvlq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "e2e-test-httpd-rc"}}{{.image}}{{end}}{{end}}{{end}} --namespace=kubectl-7809'
Dec 28 09:37:02.333: INFO: stderr: ""
Dec 28 09:37:02.333: INFO: stdout: "docker.io/library/httpd:2.4.38-alpine"
Dec 28 09:37:02.333: INFO: e2e-test-httpd-rc-b0d72a40fc9a80ed653c5485b7f998db-krvlq is verified up and running
[AfterEach] Kubectl rolling-update
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1505
Dec 28 09:37:02.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 delete rc e2e-test-httpd-rc --namespace=kubectl-7809'
Dec 28 09:37:02.410: INFO: stderr: ""
Dec 28 09:37:02.410: INFO: stdout: "replicationcontroller \"e2e-test-httpd-rc\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:37:02.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7809" for this suite.
Dec 28 09:37:14.421: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:37:14.490: INFO: namespace kubectl-7809 deletion completed in 12.075759719s

• [SLOW TEST:25.387 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl rolling-update
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1494
    should support rolling-update to same image  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application 
  should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:37:14.490: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[It] should create and stop a working application  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating all guestbook components
Dec 28 09:37:14.506: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-slave
  labels:
    app: redis
    role: slave
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: redis
    role: slave
    tier: backend

Dec 28 09:37:14.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 create -f - --namespace=kubectl-3817'
Dec 28 09:37:14.683: INFO: stderr: ""
Dec 28 09:37:14.683: INFO: stdout: "service/redis-slave created\n"
Dec 28 09:37:14.683: INFO: apiVersion: v1
kind: Service
metadata:
  name: redis-master
  labels:
    app: redis
    role: master
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: redis
    role: master
    tier: backend

Dec 28 09:37:14.683: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 create -f - --namespace=kubectl-3817'
Dec 28 09:37:14.837: INFO: stderr: ""
Dec 28 09:37:14.837: INFO: stdout: "service/redis-master created\n"
Dec 28 09:37:14.837: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Dec 28 09:37:14.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 create -f - --namespace=kubectl-3817'
Dec 28 09:37:14.992: INFO: stderr: ""
Dec 28 09:37:14.992: INFO: stdout: "service/frontend created\n"
Dec 28 09:37:14.992: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: php-redis
        image: gcr.io/google-samples/gb-frontend:v6
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access environment variables to find service host
          # info, comment out the 'value: dns' line above, and uncomment the
          # line below:
          # value: env
        ports:
        - containerPort: 80

Dec 28 09:37:14.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 create -f - --namespace=kubectl-3817'
Dec 28 09:37:15.135: INFO: stderr: ""
Dec 28 09:37:15.135: INFO: stdout: "deployment.apps/frontend created\n"
Dec 28 09:37:15.135: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-master
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
      role: master
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: master
        tier: backend
    spec:
      containers:
      - name: master
        image: docker.io/library/redis:5.0.5-alpine
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Dec 28 09:37:15.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 create -f - --namespace=kubectl-3817'
Dec 28 09:37:15.278: INFO: stderr: ""
Dec 28 09:37:15.278: INFO: stdout: "deployment.apps/redis-master created\n"
Dec 28 09:37:15.278: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis-slave
spec:
  replicas: 2
  selector:
    matchLabels:
      app: redis
      role: slave
      tier: backend
  template:
    metadata:
      labels:
        app: redis
        role: slave
        tier: backend
    spec:
      containers:
      - name: slave
        image: docker.io/library/redis:5.0.5-alpine
        # We are only implementing the dns option of:
        # https://github.com/kubernetes/examples/blob/97c7ed0eb6555a4b667d2877f965d392e00abc45/guestbook/redis-slave/run.sh
        command: [ "redis-server", "--slaveof", "redis-master", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        env:
        - name: GET_HOSTS_FROM
          value: dns
          # If your cluster config does not include a dns service, then to
          # instead access an environment variable to find the master
          # service's host, comment out the 'value: dns' line above, and
          # uncomment the line below:
          # value: env
        ports:
        - containerPort: 6379

Dec 28 09:37:15.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 create -f - --namespace=kubectl-3817'
Dec 28 09:37:15.434: INFO: stderr: ""
Dec 28 09:37:15.434: INFO: stdout: "deployment.apps/redis-slave created\n"
STEP: validating guestbook app
Dec 28 09:37:15.434: INFO: Waiting for all frontend pods to be Running.
Dec 28 09:37:20.485: INFO: Waiting for frontend to serve content.
Dec 28 09:37:20.496: INFO: Trying to add a new entry to the guestbook.
Dec 28 09:37:20.506: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources
Dec 28 09:37:20.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 delete --grace-period=0 --force -f - --namespace=kubectl-3817'
Dec 28 09:37:20.593: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 28 09:37:20.593: INFO: stdout: "service \"redis-slave\" force deleted\n"
STEP: using delete to clean up resources
Dec 28 09:37:20.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 delete --grace-period=0 --force -f - --namespace=kubectl-3817'
Dec 28 09:37:20.681: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 28 09:37:20.681: INFO: stdout: "service \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 28 09:37:20.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 delete --grace-period=0 --force -f - --namespace=kubectl-3817'
Dec 28 09:37:20.763: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 28 09:37:20.763: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 28 09:37:20.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 delete --grace-period=0 --force -f - --namespace=kubectl-3817'
Dec 28 09:37:20.840: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 28 09:37:20.840: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources
Dec 28 09:37:20.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 delete --grace-period=0 --force -f - --namespace=kubectl-3817'
Dec 28 09:37:20.916: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 28 09:37:20.916: INFO: stdout: "deployment.apps \"redis-master\" force deleted\n"
STEP: using delete to clean up resources
Dec 28 09:37:20.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 delete --grace-period=0 --force -f - --namespace=kubectl-3817'
Dec 28 09:37:20.988: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 28 09:37:20.988: INFO: stdout: "deployment.apps \"redis-slave\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:37:20.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3817" for this suite.
Dec 28 09:37:33.000: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:37:33.068: INFO: namespace kubectl-3817 deletion completed in 12.075900688s

• [SLOW TEST:18.578 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Guestbook application
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:333
    should create and stop a working application  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet [k8s.io] Basic StatefulSet functionality [StatefulSetBasic] 
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:37:33.068: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename statefulset
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:62
[BeforeEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:77
STEP: Creating service test in namespace statefulset-8298
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Initializing watcher for selector baz=blah,foo=bar
STEP: Creating stateful set ss in namespace statefulset-8298
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-8298
Dec 28 09:37:33.091: INFO: Found 0 stateful pods, waiting for 1
Dec 28 09:37:43.094: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod
Dec 28 09:37:43.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-8298 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 28 09:37:43.317: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 28 09:37:43.317: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 28 09:37:43.317: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 28 09:37:43.319: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Dec 28 09:37:53.322: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 28 09:37:53.322: INFO: Waiting for statefulset status.replicas updated to 0
Dec 28 09:37:53.328: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999998605s
Dec 28 09:37:54.330: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.998088772s
Dec 28 09:37:55.333: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.995873019s
Dec 28 09:37:56.335: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.993546024s
Dec 28 09:37:57.337: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.991201054s
Dec 28 09:37:58.340: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.989025068s
Dec 28 09:37:59.342: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.986590322s
Dec 28 09:38:00.345: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.984083845s
Dec 28 09:38:01.347: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.981580962s
Dec 28 09:38:02.350: INFO: Verifying statefulset ss doesn't scale past 1 for another 978.960267ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-8298
Dec 28 09:38:03.353: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-8298 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:38:03.579: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 28 09:38:03.579: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 28 09:38:03.579: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 28 09:38:03.581: INFO: Found 1 stateful pods, waiting for 3
Dec 28 09:38:13.583: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Dec 28 09:38:13.583: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Dec 28 09:38:13.583: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order
STEP: Scale down will halt with unhealthy stateful pod
Dec 28 09:38:13.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-8298 ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 28 09:38:13.796: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 28 09:38:13.796: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 28 09:38:13.796: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 28 09:38:13.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-8298 ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 28 09:38:14.022: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 28 09:38:14.022: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 28 09:38:14.022: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 28 09:38:14.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-8298 ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Dec 28 09:38:14.238: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Dec 28 09:38:14.238: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Dec 28 09:38:14.238: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Dec 28 09:38:14.238: INFO: Waiting for statefulset status.replicas updated to 0
Dec 28 09:38:14.240: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Dec 28 09:38:24.244: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Dec 28 09:38:24.244: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Dec 28 09:38:24.244: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Dec 28 09:38:24.250: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999998575s
Dec 28 09:38:25.252: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997965748s
Dec 28 09:38:26.255: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.995325182s
Dec 28 09:38:27.258: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.992874507s
Dec 28 09:38:28.260: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.990010722s
Dec 28 09:38:29.263: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.987647422s
Dec 28 09:38:30.265: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.984704679s
Dec 28 09:38:31.268: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.982239196s
Dec 28 09:38:32.270: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.979699632s
Dec 28 09:38:33.273: INFO: Verifying statefulset ss doesn't scale past 3 for another 977.332448ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-8298
Dec 28 09:38:34.275: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-8298 ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:38:34.490: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 28 09:38:34.490: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 28 09:38:34.490: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 28 09:38:34.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-8298 ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:38:34.726: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 28 09:38:34.726: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 28 09:38:34.726: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 28 09:38:34.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 exec --namespace=statefulset-8298 ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Dec 28 09:38:34.953: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Dec 28 09:38:34.953: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Dec 28 09:38:34.953: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Dec 28 09:38:34.953: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order
[AfterEach] [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/statefulset.go:88
Dec 28 09:38:44.964: INFO: Deleting all statefulset in ns statefulset-8298
Dec 28 09:38:44.966: INFO: Scaling statefulset ss to 0
Dec 28 09:38:44.971: INFO: Waiting for statefulset status.replicas updated to 0
Dec 28 09:38:44.973: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:38:44.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8298" for this suite.
Dec 28 09:38:50.989: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:38:51.065: INFO: namespace statefulset-8298 deletion completed in 6.083431481s

• [SLOW TEST:77.997 seconds]
[sig-apps] StatefulSet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  [k8s.io] Basic StatefulSet functionality [StatefulSetBasic]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:38:51.065: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-map-3f218cb3-31b8-4346-b7fd-b1e3890913d5
STEP: Creating a pod to test consume configMaps
Dec 28 09:38:51.086: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f021ed77-7737-450b-a3c0-09d1d5273f21" in namespace "projected-1515" to be "success or failure"
Dec 28 09:38:51.088: INFO: Pod "pod-projected-configmaps-f021ed77-7737-450b-a3c0-09d1d5273f21": Phase="Pending", Reason="", readiness=false. Elapsed: 1.686446ms
Dec 28 09:38:53.090: INFO: Pod "pod-projected-configmaps-f021ed77-7737-450b-a3c0-09d1d5273f21": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003946446s
STEP: Saw pod success
Dec 28 09:38:53.090: INFO: Pod "pod-projected-configmaps-f021ed77-7737-450b-a3c0-09d1d5273f21" satisfied condition "success or failure"
Dec 28 09:38:53.092: INFO: Trying to get logs from node hxx-m-2 pod pod-projected-configmaps-f021ed77-7737-450b-a3c0-09d1d5273f21 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 28 09:38:53.108: INFO: Waiting for pod pod-projected-configmaps-f021ed77-7737-450b-a3c0-09d1d5273f21 to disappear
Dec 28 09:38:53.109: INFO: Pod pod-projected-configmaps-f021ed77-7737-450b-a3c0-09d1d5273f21 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:38:53.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1515" for this suite.
Dec 28 09:38:59.120: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:38:59.184: INFO: namespace projected-1515 deletion completed in 6.071973911s

• [SLOW TEST:8.119 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume with mappings as non-root [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:38:59.184: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating projection with configMap that has name projected-configmap-test-upd-fb8c5fee-921f-4ad0-9bad-9f5a6e5f7cf5
STEP: Creating the pod
STEP: Updating configmap projected-configmap-test-upd-fb8c5fee-921f-4ad0-9bad-9f5a6e5f7cf5
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:39:03.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8307" for this suite.
Dec 28 09:39:25.243: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:39:25.310: INFO: namespace projected-8307 deletion completed in 22.074715972s

• [SLOW TEST:26.126 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] ResourceQuota 
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:39:25.310: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename resourcequota
STEP: Waiting for a default service account to be provisioned in namespace
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Counting existing ResourceQuota
STEP: Creating a ResourceQuota
STEP: Ensuring resource quota status is calculated
STEP: Creating a Pod that fits quota
STEP: Ensuring ResourceQuota status captures the pod usage
STEP: Not allowing a pod to be created that exceeds remaining quota
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources)
STEP: Ensuring a pod cannot update its resource requirements
STEP: Ensuring attempts to update pod resource requirements did not change quota usage
STEP: Deleting the pod
STEP: Ensuring resource quota status released the pod usage
[AfterEach] [sig-api-machinery] ResourceQuota
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:39:38.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4436" for this suite.
Dec 28 09:39:44.365: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:39:44.434: INFO: namespace resourcequota-4436 deletion completed in 6.075426131s

• [SLOW TEST:19.124 seconds]
[sig-api-machinery] ResourceQuota
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:39:44.434: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 28 09:39:45.020: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 28 09:39:48.032: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 09:39:48.034: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5842-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource while v1 is storage version
STEP: Patching Custom Resource Definition to set v2 as storage
STEP: Patching the custom resource while v2 is storage version
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:39:49.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2171" for this suite.
Dec 28 09:39:55.172: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:39:55.239: INFO: namespace webhook-2171 deletion completed in 6.074530513s
STEP: Destroying namespace "webhook-2171-markers" for this suite.
Dec 28 09:40:01.246: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:40:01.318: INFO: namespace webhook-2171-markers deletion completed in 6.07958789s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:16.891 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:40:01.326: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 28 09:40:01.863: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 28 09:40:04.875: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 09:40:04.877: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-270-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:40:05.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1416" for this suite.
Dec 28 09:40:11.945: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:40:12.011: INFO: namespace webhook-1416 deletion completed in 6.072953686s
STEP: Destroying namespace "webhook-1416-markers" for this suite.
Dec 28 09:40:18.018: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:40:18.089: INFO: namespace webhook-1416-markers deletion completed in 6.078001581s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:16.770 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:40:18.096: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 09:40:18.189: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"11c81495-e2bd-4549-b2da-45ab91d78cf7", Controller:(*bool)(0xc003c34d92), BlockOwnerDeletion:(*bool)(0xc003c34d93)}}
Dec 28 09:40:18.193: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"af2be7d1-c597-4335-9f85-3ff00aeb4e3c", Controller:(*bool)(0xc002ea06a6), BlockOwnerDeletion:(*bool)(0xc002ea06a7)}}
Dec 28 09:40:18.196: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"0179034a-df81-4fe6-bbb8-0853c4f0ab62", Controller:(*bool)(0xc003c34f62), BlockOwnerDeletion:(*bool)(0xc003c34f63)}}
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:40:23.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-359" for this suite.
Dec 28 09:40:29.211: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:40:29.274: INFO: namespace gc-359 deletion completed in 6.070950733s

• [SLOW TEST:11.178 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Downward API volume 
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:40:29.275: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 28 09:40:29.295: INFO: Waiting up to 5m0s for pod "downwardapi-volume-47de03dd-0a17-45d7-a4c7-ab68e536bf7f" in namespace "downward-api-7628" to be "success or failure"
Dec 28 09:40:29.297: INFO: Pod "downwardapi-volume-47de03dd-0a17-45d7-a4c7-ab68e536bf7f": Phase="Pending", Reason="", readiness=false. Elapsed: 1.746053ms
Dec 28 09:40:31.299: INFO: Pod "downwardapi-volume-47de03dd-0a17-45d7-a4c7-ab68e536bf7f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004089527s
STEP: Saw pod success
Dec 28 09:40:31.299: INFO: Pod "downwardapi-volume-47de03dd-0a17-45d7-a4c7-ab68e536bf7f" satisfied condition "success or failure"
Dec 28 09:40:31.301: INFO: Trying to get logs from node hxx-m-2 pod downwardapi-volume-47de03dd-0a17-45d7-a4c7-ab68e536bf7f container client-container: <nil>
STEP: delete the pod
Dec 28 09:40:31.310: INFO: Waiting for pod downwardapi-volume-47de03dd-0a17-45d7-a4c7-ab68e536bf7f to disappear
Dec 28 09:40:31.312: INFO: Pod downwardapi-volume-47de03dd-0a17-45d7-a4c7-ab68e536bf7f no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:40:31.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7628" for this suite.
Dec 28 09:40:37.322: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:40:37.394: INFO: namespace downward-api-7628 deletion completed in 6.079058225s

• [SLOW TEST:8.119 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide container's memory limit [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume 
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:40:37.394: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test downward API volume plugin
Dec 28 09:40:37.413: INFO: Waiting up to 5m0s for pod "downwardapi-volume-1f1dd167-8f6c-47ff-b2e5-184179a52e88" in namespace "downward-api-2057" to be "success or failure"
Dec 28 09:40:37.414: INFO: Pod "downwardapi-volume-1f1dd167-8f6c-47ff-b2e5-184179a52e88": Phase="Pending", Reason="", readiness=false. Elapsed: 1.513617ms
Dec 28 09:40:39.416: INFO: Pod "downwardapi-volume-1f1dd167-8f6c-47ff-b2e5-184179a52e88": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003577234s
STEP: Saw pod success
Dec 28 09:40:39.416: INFO: Pod "downwardapi-volume-1f1dd167-8f6c-47ff-b2e5-184179a52e88" satisfied condition "success or failure"
Dec 28 09:40:39.418: INFO: Trying to get logs from node hxx-m-2 pod downwardapi-volume-1f1dd167-8f6c-47ff-b2e5-184179a52e88 container client-container: <nil>
STEP: delete the pod
Dec 28 09:40:39.429: INFO: Waiting for pod downwardapi-volume-1f1dd167-8f6c-47ff-b2e5-184179a52e88 to disappear
Dec 28 09:40:39.430: INFO: Pod downwardapi-volume-1f1dd167-8f6c-47ff-b2e5-184179a52e88 no longer exists
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:40:39.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2057" for this suite.
Dec 28 09:40:45.439: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:40:45.504: INFO: namespace downward-api-2057 deletion completed in 6.071446479s

• [SLOW TEST:8.110 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] [sig-node] Events 
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:40:45.504: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename events
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: creating the pod
STEP: submitting the pod to kubernetes
STEP: verifying the pod is in kubernetes
STEP: retrieving the pod
Dec 28 09:40:47.533: INFO: &Pod{ObjectMeta:{send-events-9b6b4834-bfb7-490e-b871-55618cf104ab  events-5017 /api/v1/namespaces/events-5017/pods/send-events-9b6b4834-bfb7-490e-b871-55618cf104ab 64e76cd6-643b-4602-96d0-e327326ff0ed 308203 0 2019-12-28 09:40:45 +0000 UTC <nil> <nil> map[name:foo time:522144357] map[] [] []  []},Spec:PodSpec{Volumes:[]Volume{Volume{Name:default-token-zlqvn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:&SecretVolumeSource{SecretName:default-token-zlqvn,Items:[]KeyToPath{},DefaultMode:*420,Optional:nil,},NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:nil,StorageOS:nil,CSI:nil,},},},Containers:[]Container{Container{Name:p,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,Command:[],Args:[serve-hostname],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:80,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:default-token-zlqvn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*30,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:hxx-m-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:nil,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:40:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:40:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:40:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2019-12-28 09:40:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.0.128.16,PodIP:10.199.0.244,StartTime:2019-12-28 09:40:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:p,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2019-12-28 09:40:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:gcr.io/kubernetes-e2e-test-images/agnhost:2.6,ImageID:docker-pullable://gcr.io/kubernetes-e2e-test-images/agnhost@sha256:4057a5580c7b59c4fe10d8ab2732c9dec35eea80fd41f7bafc7bd5acc7edf727,ContainerID:docker://9aa124f3a33c4aaeddc06360727418626fca43f6a3e0644f3de335c4cc3f0a16,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.199.0.244,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

STEP: checking for scheduler event about the pod
Dec 28 09:40:49.535: INFO: Saw scheduler event for our pod.
STEP: checking for kubelet event about the pod
Dec 28 09:40:51.538: INFO: Saw kubelet event for our pod.
STEP: deleting the pod
[AfterEach] [k8s.io] [sig-node] Events
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:40:51.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-5017" for this suite.
Dec 28 09:41:35.550: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:41:35.616: INFO: namespace events-5017 deletion completed in 44.072064173s

• [SLOW TEST:50.112 seconds]
[k8s.io] [sig-node] Events
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should be sent by kubelets and the scheduler about pods scheduling and running  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:41:35.616: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-f23bb27b-3bb6-4912-89b9-81c9dd2dcde8
STEP: Creating the pod
STEP: Updating configmap configmap-test-upd-f23bb27b-3bb6-4912-89b9-81c9dd2dcde8
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:42:55.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-841" for this suite.
Dec 28 09:43:07.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:43:07.989: INFO: namespace configmap-841 deletion completed in 12.078682931s

• [SLOW TEST:92.373 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:43:07.989: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the rc
STEP: delete the rc
STEP: wait for the rc to be deleted
Dec 28 09:43:14.019: INFO: 0 pods remaining
Dec 28 09:43:14.019: INFO: 0 pods has nil DeletionTimestamp
Dec 28 09:43:14.019: INFO: 
STEP: Gathering metrics
Dec 28 09:43:15.021: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
W1228 09:43:15.021680      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
Dec 28 09:43:15.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-1839" for this suite.
Dec 28 09:43:21.031: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:43:21.098: INFO: namespace gc-1839 deletion completed in 6.074879213s

• [SLOW TEST:13.109 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] 
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:43:21.099: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename daemonsets
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:102
[It] should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 09:43:21.127: INFO: Create a RollingUpdate DaemonSet
Dec 28 09:43:21.129: INFO: Check that daemon pods launch on every node of the cluster
Dec 28 09:43:21.132: INFO: Number of nodes with available pods: 0
Dec 28 09:43:21.132: INFO: Node hxx-m-1 is running more than one daemon pod
Dec 28 09:43:22.137: INFO: Number of nodes with available pods: 0
Dec 28 09:43:22.138: INFO: Node hxx-m-1 is running more than one daemon pod
Dec 28 09:43:23.138: INFO: Number of nodes with available pods: 3
Dec 28 09:43:23.138: INFO: Number of running nodes: 3, number of available pods: 3
Dec 28 09:43:23.138: INFO: Update the DaemonSet to trigger a rollout
Dec 28 09:43:23.141: INFO: Updating DaemonSet daemon-set
Dec 28 09:43:32.150: INFO: Roll back the DaemonSet before rollout is complete
Dec 28 09:43:32.154: INFO: Updating DaemonSet daemon-set
Dec 28 09:43:32.154: INFO: Make sure DaemonSet rollback is complete
Dec 28 09:43:32.156: INFO: Wrong image for pod: daemon-set-762vv. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 28 09:43:32.156: INFO: Pod daemon-set-762vv is not available
Dec 28 09:43:33.161: INFO: Wrong image for pod: daemon-set-762vv. Expected: docker.io/library/httpd:2.4.38-alpine, got: foo:non-existent.
Dec 28 09:43:33.161: INFO: Pod daemon-set-762vv is not available
Dec 28 09:43:34.161: INFO: Pod daemon-set-g622p is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/daemon_set.go:68
STEP: Deleting DaemonSet "daemon-set"
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8924, will wait for the garbage collector to delete the pods
Dec 28 09:43:34.222: INFO: Deleting DaemonSet.extensions daemon-set took: 3.87124ms
Dec 28 09:43:35.122: INFO: Terminating DaemonSet.extensions daemon-set pods took: 900.18034ms
Dec 28 09:43:41.324: INFO: Number of nodes with available pods: 0
Dec 28 09:43:41.324: INFO: Number of running nodes: 0, number of available pods: 0
Dec 28 09:43:41.325: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"selfLink":"/apis/apps/v1/namespaces/daemonsets-8924/daemonsets","resourceVersion":"309247"},"items":null}

Dec 28 09:43:41.327: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"selfLink":"/api/v1/namespaces/daemonsets-8924/pods","resourceVersion":"309247"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:43:41.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-8924" for this suite.
Dec 28 09:43:47.344: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:43:47.414: INFO: namespace daemonsets-8924 deletion completed in 6.077435705s

• [SLOW TEST:26.315 seconds]
[sig-apps] Daemon set [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:43:47.414: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on node default medium
Dec 28 09:43:47.435: INFO: Waiting up to 5m0s for pod "pod-720c13b6-6867-4dba-952a-fb3a46732006" in namespace "emptydir-1366" to be "success or failure"
Dec 28 09:43:47.437: INFO: Pod "pod-720c13b6-6867-4dba-952a-fb3a46732006": Phase="Pending", Reason="", readiness=false. Elapsed: 1.693769ms
Dec 28 09:43:49.439: INFO: Pod "pod-720c13b6-6867-4dba-952a-fb3a46732006": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.004050295s
STEP: Saw pod success
Dec 28 09:43:49.439: INFO: Pod "pod-720c13b6-6867-4dba-952a-fb3a46732006" satisfied condition "success or failure"
Dec 28 09:43:49.440: INFO: Trying to get logs from node hxx-m-2 pod pod-720c13b6-6867-4dba-952a-fb3a46732006 container test-container: <nil>
STEP: delete the pod
Dec 28 09:43:49.452: INFO: Waiting for pod pod-720c13b6-6867-4dba-952a-fb3a46732006 to disappear
Dec 28 09:43:49.453: INFO: Pod pod-720c13b6-6867-4dba-952a-fb3a46732006 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:43:49.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1366" for this suite.
Dec 28 09:43:55.463: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:43:55.528: INFO: namespace emptydir-1366 deletion completed in 6.072982969s

• [SLOW TEST:8.115 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes 
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:43:55.529: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir volume type on tmpfs
Dec 28 09:43:55.549: INFO: Waiting up to 5m0s for pod "pod-8e2353af-7266-423e-b505-f42b992e7607" in namespace "emptydir-5908" to be "success or failure"
Dec 28 09:43:55.550: INFO: Pod "pod-8e2353af-7266-423e-b505-f42b992e7607": Phase="Pending", Reason="", readiness=false. Elapsed: 1.685344ms
Dec 28 09:43:57.553: INFO: Pod "pod-8e2353af-7266-423e-b505-f42b992e7607": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003891043s
STEP: Saw pod success
Dec 28 09:43:57.553: INFO: Pod "pod-8e2353af-7266-423e-b505-f42b992e7607" satisfied condition "success or failure"
Dec 28 09:43:57.554: INFO: Trying to get logs from node hxx-m-2 pod pod-8e2353af-7266-423e-b505-f42b992e7607 container test-container: <nil>
STEP: delete the pod
Dec 28 09:43:57.566: INFO: Waiting for pod pod-8e2353af-7266-423e-b505-f42b992e7607 to disappear
Dec 28 09:43:57.567: INFO: Pod pod-8e2353af-7266-423e-b505-f42b992e7607 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:43:57.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5908" for this suite.
Dec 28 09:44:03.577: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:44:03.645: INFO: namespace emptydir-5908 deletion completed in 6.074568625s

• [SLOW TEST:8.116 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] Aggregator 
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:44:03.645: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename aggregator
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:77
Dec 28 09:44:03.661: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
[It] Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Registering the sample API server.
Dec 28 09:44:03.876: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Dec 28 09:44:05.900: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713123043, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713123043, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713123043, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713123043, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-8447597c78\" is progressing."}}, CollisionCount:(*int32)(nil)}
Dec 28 09:44:08.722: INFO: Waited 816.359689ms for the sample-apiserver to be ready to handle requests.
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:44:09.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-7516" for this suite.
Dec 28 09:44:15.864: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:44:15.930: INFO: namespace aggregator-7516 deletion completed in 6.16731535s

• [SLOW TEST:12.285 seconds]
[sig-api-machinery] Aggregator
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.10 Sample API Server using the current Aggregator [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl label 
  should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:44:15.930: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubectl
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:225
[BeforeEach] Kubectl label
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1192
STEP: creating the pod
Dec 28 09:44:15.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 create -f - --namespace=kubectl-1977'
Dec 28 09:44:16.224: INFO: stderr: ""
Dec 28 09:44:16.224: INFO: stdout: "pod/pause created\n"
Dec 28 09:44:16.224: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Dec 28 09:44:16.224: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1977" to be "running and ready"
Dec 28 09:44:16.226: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 1.716055ms
Dec 28 09:44:18.230: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.006101007s
Dec 28 09:44:18.230: INFO: Pod "pause" satisfied condition "running and ready"
Dec 28 09:44:18.230: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: adding the label testing-label with value testing-label-value to a pod
Dec 28 09:44:18.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 label pods pause testing-label=testing-label-value --namespace=kubectl-1977'
Dec 28 09:44:18.312: INFO: stderr: ""
Dec 28 09:44:18.312: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value
Dec 28 09:44:18.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pod pause -L testing-label --namespace=kubectl-1977'
Dec 28 09:44:18.382: INFO: stderr: ""
Dec 28 09:44:18.382: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod
Dec 28 09:44:18.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 label pods pause testing-label- --namespace=kubectl-1977'
Dec 28 09:44:18.459: INFO: stderr: ""
Dec 28 09:44:18.459: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod doesn't have the label testing-label
Dec 28 09:44:18.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pod pause -L testing-label --namespace=kubectl-1977'
Dec 28 09:44:18.532: INFO: stderr: ""
Dec 28 09:44:18.533: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1199
STEP: using delete to clean up resources
Dec 28 09:44:18.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 delete --grace-period=0 --force -f - --namespace=kubectl-1977'
Dec 28 09:44:18.606: INFO: stderr: "warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Dec 28 09:44:18.606: INFO: stdout: "pod \"pause\" force deleted\n"
Dec 28 09:44:18.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get rc,svc -l name=pause --no-headers --namespace=kubectl-1977'
Dec 28 09:44:18.682: INFO: stderr: "No resources found in kubectl-1977 namespace.\n"
Dec 28 09:44:18.682: INFO: stdout: ""
Dec 28 09:44:18.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 get pods -l name=pause --namespace=kubectl-1977 -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Dec 28 09:44:18.754: INFO: stderr: ""
Dec 28 09:44:18.754: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:44:18.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1977" for this suite.
Dec 28 09:44:24.765: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:44:24.831: INFO: namespace kubectl-1977 deletion completed in 6.073572246s

• [SLOW TEST:8.901 seconds]
[sig-cli] Kubectl client
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/framework.go:23
  Kubectl label
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/kubectl/kubectl.go:1189
    should update the label on a resource  [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSS
------------------------------
[k8s.io] Pods 
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:44:24.831: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename pods
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/pods.go:165
[It] should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 09:44:26.863: INFO: Waiting up to 5m0s for pod "client-envvars-d95cd06f-1c0d-4670-ac98-a9b406803262" in namespace "pods-9450" to be "success or failure"
Dec 28 09:44:26.865: INFO: Pod "client-envvars-d95cd06f-1c0d-4670-ac98-a9b406803262": Phase="Pending", Reason="", readiness=false. Elapsed: 1.622414ms
Dec 28 09:44:28.867: INFO: Pod "client-envvars-d95cd06f-1c0d-4670-ac98-a9b406803262": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003881384s
STEP: Saw pod success
Dec 28 09:44:28.867: INFO: Pod "client-envvars-d95cd06f-1c0d-4670-ac98-a9b406803262" satisfied condition "success or failure"
Dec 28 09:44:28.869: INFO: Trying to get logs from node hxx-m-2 pod client-envvars-d95cd06f-1c0d-4670-ac98-a9b406803262 container env3cont: <nil>
STEP: delete the pod
Dec 28 09:44:28.882: INFO: Waiting for pod client-envvars-d95cd06f-1c0d-4670-ac98-a9b406803262 to disappear
Dec 28 09:44:28.886: INFO: Pod client-envvars-d95cd06f-1c0d-4670-ac98-a9b406803262 no longer exists
[AfterEach] [k8s.io] Pods
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:44:28.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9450" for this suite.
Dec 28 09:44:56.895: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:44:56.964: INFO: namespace pods-9450 deletion completed in 28.075084065s

• [SLOW TEST:32.133 seconds]
[k8s.io] Pods
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  should contain environment variables for services [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes 
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:44:56.964: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename emptydir
STEP: Waiting for a default service account to be provisioned in namespace
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a pod to test emptydir 0666 on tmpfs
Dec 28 09:44:56.984: INFO: Waiting up to 5m0s for pod "pod-ef1b98c4-6184-46c4-b87e-520d3fc043df" in namespace "emptydir-9321" to be "success or failure"
Dec 28 09:44:56.986: INFO: Pod "pod-ef1b98c4-6184-46c4-b87e-520d3fc043df": Phase="Pending", Reason="", readiness=false. Elapsed: 1.439264ms
Dec 28 09:44:58.988: INFO: Pod "pod-ef1b98c4-6184-46c4-b87e-520d3fc043df": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003461367s
STEP: Saw pod success
Dec 28 09:44:58.988: INFO: Pod "pod-ef1b98c4-6184-46c4-b87e-520d3fc043df" satisfied condition "success or failure"
Dec 28 09:44:58.989: INFO: Trying to get logs from node hxx-m-2 pod pod-ef1b98c4-6184-46c4-b87e-520d3fc043df container test-container: <nil>
STEP: delete the pod
Dec 28 09:44:59.000: INFO: Waiting for pod pod-ef1b98c4-6184-46c4-b87e-520d3fc043df to disappear
Dec 28 09:44:59.003: INFO: Pod pod-ef1b98c4-6184-46c4-b87e-520d3fc043df no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:44:59.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9321" for this suite.
Dec 28 09:45:05.012: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:45:05.087: INFO: namespace emptydir-9321 deletion completed in 6.081433209s

• [SLOW TEST:8.123 seconds]
[sig-storage] EmptyDir volumes
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/empty_dir.go:41
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] 
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:45:05.087: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename namespaces
STEP: Waiting for a default service account to be provisioned in namespace
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating a test namespace
STEP: Waiting for a default service account to be provisioned in namespace
STEP: Creating a pod in the namespace
STEP: Waiting for the pod to have running status
STEP: Deleting the namespace
STEP: Waiting for the namespace to be removed.
STEP: Recreating the namespace
STEP: Verifying there are no pods in the namespace
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:45:27.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2447" for this suite.
Dec 28 09:45:33.153: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:45:33.223: INFO: namespace namespaces-2447 deletion completed in 6.076824806s
STEP: Destroying namespace "nsdeletetest-5502" for this suite.
Dec 28 09:45:33.224: INFO: Namespace nsdeletetest-5502 was already deleted
STEP: Destroying namespace "nsdeletetest-9736" for this suite.
Dec 28 09:45:39.231: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:45:39.295: INFO: namespace nsdeletetest-9736 deletion completed in 6.070562305s

• [SLOW TEST:34.208 seconds]
[sig-api-machinery] Namespaces [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:45:39.295: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Failed
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 28 09:45:41.322: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:45:41.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7470" for this suite.
Dec 28 09:45:47.339: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:45:47.408: INFO: namespace container-runtime-7470 deletion completed in 6.076376087s

• [SLOW TEST:8.113 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] 
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:45:47.408: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename sched-pred
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:87
Dec 28 09:45:47.424: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Dec 28 09:45:47.431: INFO: Waiting for terminating namespaces to be deleted...
Dec 28 09:45:47.433: INFO: 
Logging pods the kubelet thinks is on node hxx-m-1 before test
Dec 28 09:45:47.447: INFO: sonobuoy-systemd-logs-daemon-set-690f17abb3794222-tlb58 from sonobuoy started at 2019-12-28 07:53:22 +0000 UTC (2 container statuses recorded)
Dec 28 09:45:47.447: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 28 09:45:47.447: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 28 09:45:47.447: INFO: coredns-66447b44c9-zphmm from kube-system started at 2019-12-27 13:58:22 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.447: INFO: 	Container coredns ready: true, restart count 0
Dec 28 09:45:47.447: INFO: cert-manager-77f5bf4f5-h86rt from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.447: INFO: 	Container cert-manager ready: true, restart count 0
Dec 28 09:45:47.447: INFO: cert-manager-webhook-578c59dddd-k697c from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.448: INFO: 	Container webhook ready: true, restart count 0
Dec 28 09:45:47.448: INFO: apollo-cfdd64bb4-4lssk from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.448: INFO: 	Container apollo ready: true, restart count 0
Dec 28 09:45:47.448: INFO: underlord-5c45b96c5d-nmjjq from cpaas-system started at 2019-12-27 14:46:59 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.448: INFO: 	Container alauda-console ready: true, restart count 0
Dec 28 09:45:47.448: INFO: dex-8448b48ff8-2bd5h from cpaas-system started at 2019-12-27 14:09:25 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.448: INFO: 	Container dex ready: true, restart count 0
Dec 28 09:45:47.448: INFO: erebus-5597f9565d-zwjnz from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.448: INFO: 	Container erebus ready: true, restart count 0
Dec 28 09:45:47.448: INFO: auth-controller2-79ff55cd75-cbjbn from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.448: INFO: 	Container auth-controller2-manager ready: true, restart count 0
Dec 28 09:45:47.448: INFO: furion-679c948779-jz6lf from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.448: INFO: 	Container furion ready: true, restart count 0
Dec 28 09:45:47.448: INFO: cert-manager-cainjector-67d4dd59ff-8jhs9 from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.448: INFO: 	Container cainjector ready: true, restart count 0
Dec 28 09:45:47.448: INFO: archon-5fdc59d78c-rhtzm from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.448: INFO: 	Container archon-api ready: true, restart count 0
Dec 28 09:45:47.448: INFO: blink-7d4fb788b5-s9q49 from cpaas-system started at 2019-12-28 07:52:16 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.448: INFO: 	Container alauda-console ready: true, restart count 0
Dec 28 09:45:47.448: INFO: kube-proxy-bnhj6 from kube-system started at 2019-12-27 13:58:01 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.448: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 28 09:45:47.448: INFO: agon-75b987dff5-7bht5 from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.448: INFO: 	Container agon ready: true, restart count 2
Dec 28 09:45:47.448: INFO: cluster-registry-controller-manager-76774c98d-7c7mz from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (2 container statuses recorded)
Dec 28 09:45:47.448: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 28 09:45:47.448: INFO: 	Container manager ready: true, restart count 0
Dec 28 09:45:47.448: INFO: sonobuoy-e2e-job-f0b9f709fd414134 from sonobuoy started at 2019-12-28 07:53:22 +0000 UTC (2 container statuses recorded)
Dec 28 09:45:47.448: INFO: 	Container e2e ready: true, restart count 0
Dec 28 09:45:47.448: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Dec 28 09:45:47.448: INFO: kube-flannel-8hgpf from kube-system started at 2019-12-27 13:58:14 +0000 UTC (2 container statuses recorded)
Dec 28 09:45:47.448: INFO: 	Container install-cni ready: true, restart count 0
Dec 28 09:45:47.448: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 28 09:45:47.448: INFO: 
Logging pods the kubelet thinks is on node hxx-m-2 before test
Dec 28 09:45:47.452: INFO: kube-scheduler-hxx-m-2 from kube-system started at 2019-12-27 13:57:11 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.452: INFO: 	Container kube-scheduler ready: true, restart count 0
Dec 28 09:45:47.452: INFO: sonobuoy-systemd-logs-daemon-set-690f17abb3794222-jftvp from sonobuoy started at 2019-12-28 07:53:22 +0000 UTC (2 container statuses recorded)
Dec 28 09:45:47.452: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 28 09:45:47.452: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 28 09:45:47.452: INFO: kube-apiserver-hxx-m-2 from kube-system started at 2019-12-27 13:57:11 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.452: INFO: 	Container kube-apiserver ready: true, restart count 0
Dec 28 09:45:47.452: INFO: kube-controller-manager-hxx-m-2 from kube-system started at 2019-12-27 13:57:11 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.452: INFO: 	Container kube-controller-manager ready: true, restart count 0
Dec 28 09:45:47.452: INFO: nginx-ingress-controller-f9b5d49fd-rx6tc from cpaas-system started at 2019-12-28 08:50:55 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.452: INFO: 	Container nginx-ingress-controller ready: true, restart count 0
Dec 28 09:45:47.452: INFO: kube-flannel-7zk95 from kube-system started at 2019-12-28 08:51:13 +0000 UTC (2 container statuses recorded)
Dec 28 09:45:47.452: INFO: 	Container install-cni ready: true, restart count 0
Dec 28 09:45:47.452: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 28 09:45:47.452: INFO: etcd-hxx-m-2 from kube-system started at 2019-12-27 13:57:11 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.452: INFO: 	Container etcd ready: true, restart count 0
Dec 28 09:45:47.452: INFO: sonobuoy from sonobuoy started at 2019-12-28 07:53:21 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.452: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Dec 28 09:45:47.452: INFO: kube-proxy-44f6q from kube-system started at 2019-12-27 13:57:35 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.452: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 28 09:45:47.452: INFO: 
Logging pods the kubelet thinks is on node hxx-m-3 before test
Dec 28 09:45:47.466: INFO: coredns-66447b44c9-5qbrm from kube-system started at 2019-12-27 13:58:22 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.466: INFO: 	Container coredns ready: true, restart count 0
Dec 28 09:45:47.466: INFO: cluster-registry-controller-manager-76774c98d-rmnh2 from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (2 container statuses recorded)
Dec 28 09:45:47.466: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Dec 28 09:45:47.466: INFO: 	Container manager ready: true, restart count 0
Dec 28 09:45:47.466: INFO: furion-679c948779-xjs2n from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.467: INFO: 	Container furion ready: true, restart count 0
Dec 28 09:45:47.467: INFO: archon-5fdc59d78c-k5jbm from cpaas-system started at 2019-12-27 14:46:59 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.467: INFO: 	Container archon-api ready: true, restart count 0
Dec 28 09:45:47.467: INFO: tiller-deploy-7c757c6f9c-67l6b from kube-system started at 2019-12-27 13:59:57 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.467: INFO: 	Container tiller ready: true, restart count 0
Dec 28 09:45:47.467: INFO: captain-7cf8c65b49-xjwlq from cpaas-system started at 2019-12-28 08:38:36 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.467: INFO: 	Container captain ready: true, restart count 0
Dec 28 09:45:47.467: INFO: auth-controller2-79ff55cd75-4clk9 from cpaas-system started at 2019-12-27 14:46:59 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.467: INFO: 	Container auth-controller2-manager ready: true, restart count 0
Dec 28 09:45:47.467: INFO: sonobuoy-systemd-logs-daemon-set-690f17abb3794222-vskml from sonobuoy started at 2019-12-28 07:53:22 +0000 UTC (2 container statuses recorded)
Dec 28 09:45:47.467: INFO: 	Container sonobuoy-worker ready: true, restart count 1
Dec 28 09:45:47.467: INFO: 	Container systemd-logs ready: true, restart count 0
Dec 28 09:45:47.467: INFO: kube-proxy-5kzvk from kube-system started at 2019-12-27 13:57:57 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.467: INFO: 	Container kube-proxy ready: true, restart count 0
Dec 28 09:45:47.467: INFO: kube-flannel-f87zg from kube-system started at 2019-12-27 13:58:14 +0000 UTC (2 container statuses recorded)
Dec 28 09:45:47.467: INFO: 	Container install-cni ready: true, restart count 0
Dec 28 09:45:47.467: INFO: 	Container kube-flannel ready: true, restart count 0
Dec 28 09:45:47.467: INFO: cert-manager-webhook-578c59dddd-flp52 from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.467: INFO: 	Container webhook ready: true, restart count 0
Dec 28 09:45:47.467: INFO: cert-manager-cainjector-67d4dd59ff-tngcj from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.467: INFO: 	Container cainjector ready: true, restart count 0
Dec 28 09:45:47.467: INFO: erebus-5597f9565d-ptdps from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.467: INFO: 	Container erebus ready: true, restart count 0
Dec 28 09:45:47.467: INFO: dex-8448b48ff8-9qn2j from cpaas-system started at 2019-12-27 14:09:25 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.467: INFO: 	Container dex ready: true, restart count 0
Dec 28 09:45:47.467: INFO: cert-manager-77f5bf4f5-2wz6n from cert-manager started at 2019-12-27 14:11:22 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.467: INFO: 	Container cert-manager ready: true, restart count 0
Dec 28 09:45:47.467: INFO: underlord-5c45b96c5d-6cvvc from cpaas-system started at 2019-12-27 14:15:15 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.467: INFO: 	Container alauda-console ready: true, restart count 0
Dec 28 09:45:47.467: INFO: blink-7d4fb788b5-rtf6r from cpaas-system started at 2019-12-28 08:38:36 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.467: INFO: 	Container alauda-console ready: true, restart count 0
Dec 28 09:45:47.467: INFO: agon-75b987dff5-bl7sb from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.467: INFO: 	Container agon ready: true, restart count 2
Dec 28 09:45:47.467: INFO: apollo-cfdd64bb4-hjth5 from cpaas-system started at 2019-12-27 14:15:14 +0000 UTC (1 container statuses recorded)
Dec 28 09:45:47.467: INFO: 	Container apollo ready: true, restart count 0
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Trying to launch a pod without a label to get a node which can launch it.
STEP: Explicitly delete pod here to free the resource it takes.
STEP: Trying to apply a random label on the found node.
STEP: verifying the node has the label kubernetes.io/e2e-2da0210b-7087-4080-818d-ce2d18dca67d 90
STEP: Trying to create a pod(pod1) with hostport 54321 and hostIP 127.0.0.1 and expect scheduled
STEP: Trying to create another pod(pod2) with hostport 54321 but hostIP 127.0.0.2 on the node which pod1 resides and expect scheduled
STEP: Trying to create a third pod(pod3) with hostport 54321, hostIP 127.0.0.2 but use UDP protocol on the node which pod2 resides
STEP: removing the label kubernetes.io/e2e-2da0210b-7087-4080-818d-ce2d18dca67d off the node hxx-m-2
STEP: verifying the node doesn't have the label kubernetes.io/e2e-2da0210b-7087-4080-818d-ce2d18dca67d
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:45:55.518: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-9585" for this suite.
Dec 28 09:46:13.526: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:46:13.588: INFO: namespace sched-pred-9585 deletion completed in 18.068487928s
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/predicates.go:78

• [SLOW TEST:26.180 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/scheduling/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[k8s.io] Kubelet when scheduling a busybox Pod with hostAliases 
  should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:46:13.589: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename kubelet-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:37
[It] should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[AfterEach] [k8s.io] Kubelet
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:46:15.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5918" for this suite.
Dec 28 09:47:03.631: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:47:03.694: INFO: namespace kubelet-test-5918 deletion completed in 48.070140531s

• [SLOW TEST:50.105 seconds]
[k8s.io] Kubelet
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when scheduling a busybox Pod with hostAliases
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/kubelet.go:136
    should write entries to /etc/hosts [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:47:03.694: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name configmap-test-upd-51a917ca-39be-4de1-94a4-2366dffd19d0
STEP: Creating the pod
STEP: Waiting for pod with text data
STEP: Waiting for pod with binary data
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:47:05.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3757" for this suite.
Dec 28 09:47:23.742: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:47:23.806: INFO: namespace configmap-3757 deletion completed in 18.07146781s

• [SLOW TEST:20.112 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  binary data should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap 
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:47:23.806: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename configmap
STEP: Waiting for a default service account to be provisioned in namespace
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name cm-test-opt-del-59c13c14-8cbc-44f7-993a-700d029a0cb8
STEP: Creating configMap with name cm-test-opt-upd-1ae8d4f1-717c-4cc8-be11-9e627489f646
STEP: Creating the pod
STEP: Deleting configmap cm-test-opt-del-59c13c14-8cbc-44f7-993a-700d029a0cb8
STEP: Updating configmap cm-test-opt-upd-1ae8d4f1-717c-4cc8-be11-9e627489f646
STEP: Creating configMap with name cm-test-opt-create-65517000-f8bc-406f-b583-c8f89a37e80a
STEP: waiting to observe update in volume
[AfterEach] [sig-storage] ConfigMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:47:29.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2935" for this suite.
Dec 28 09:47:41.890: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:47:41.963: INFO: namespace configmap-2935 deletion completed in 12.079254423s

• [SLOW TEST:18.156 seconds]
[sig-storage] ConfigMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/configmap_volume.go:34
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] 
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:47:41.963: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename webhook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:88
STEP: Setting up server cert
STEP: Create role binding to let webhook read extension-apiserver-authentication
STEP: Deploying the webhook pod
STEP: Wait for the deployment to be ready
Dec 28 09:47:42.285: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Dec 28 09:47:44.290: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713123262, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713123262, loc:(*time.Location)(0x84bfb00)}}, Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:v1.Time{Time:time.Time{wall:0x0, ext:63713123262, loc:(*time.Location)(0x84bfb00)}}, LastTransitionTime:v1.Time{Time:time.Time{wall:0x0, ext:63713123262, loc:(*time.Location)(0x84bfb00)}}, Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-86d95b659d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service
STEP: Verifying the service has paired with the endpoint
Dec 28 09:47:47.298: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 09:47:47.300: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7943-crds.webhook.example.com via the AdmissionRegistration API
STEP: Creating a custom resource that should be mutated by the webhook
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:47:48.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-137" for this suite.
Dec 28 09:47:54.386: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:47:54.454: INFO: namespace webhook-137 deletion completed in 6.075087531s
STEP: Destroying namespace "webhook-137-markers" for this suite.
Dec 28 09:48:00.460: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:48:00.524: INFO: namespace webhook-137-markers deletion completed in 6.069714362s
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/webhook.go:103

• [SLOW TEST:18.568 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SS
------------------------------
[sig-storage] Projected configMap 
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:48:00.531: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename projected
STEP: Waiting for a default service account to be provisioned in namespace
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating configMap with name projected-configmap-test-volume-fdebe876-d52b-4146-bb9c-8ad13d977a33
STEP: Creating a pod to test consume configMaps
Dec 28 09:48:00.552: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-8b02ef6d-9926-4d6e-8b47-e1c64867fc96" in namespace "projected-9685" to be "success or failure"
Dec 28 09:48:00.554: INFO: Pod "pod-projected-configmaps-8b02ef6d-9926-4d6e-8b47-e1c64867fc96": Phase="Pending", Reason="", readiness=false. Elapsed: 1.584766ms
Dec 28 09:48:02.556: INFO: Pod "pod-projected-configmaps-8b02ef6d-9926-4d6e-8b47-e1c64867fc96": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00379105s
STEP: Saw pod success
Dec 28 09:48:02.556: INFO: Pod "pod-projected-configmaps-8b02ef6d-9926-4d6e-8b47-e1c64867fc96" satisfied condition "success or failure"
Dec 28 09:48:02.557: INFO: Trying to get logs from node hxx-m-2 pod pod-projected-configmaps-8b02ef6d-9926-4d6e-8b47-e1c64867fc96 container projected-configmap-volume-test: <nil>
STEP: delete the pod
Dec 28 09:48:02.568: INFO: Waiting for pod pod-projected-configmaps-8b02ef6d-9926-4d6e-8b47-e1c64867fc96 to disappear
Dec 28 09:48:02.569: INFO: Pod pod-projected-configmaps-8b02ef6d-9926-4d6e-8b47-e1c64867fc96 no longer exists
[AfterEach] [sig-storage] Projected configMap
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:48:02.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9685" for this suite.
Dec 28 09:48:08.578: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:48:08.647: INFO: namespace projected-9685 deletion completed in 6.076018092s

• [SLOW TEST:8.117 seconds]
[sig-storage] Projected configMap
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/projected_configmap.go:35
  should be consumable from pods in volume [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[k8s.io] Security Context When creating a pod with privileged 
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:48:08.647: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename security-context-test
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:40
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 09:48:08.667: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-cfcba264-0203-4f12-8f7b-c717a4e12269" in namespace "security-context-test-4257" to be "success or failure"
Dec 28 09:48:08.668: INFO: Pod "busybox-privileged-false-cfcba264-0203-4f12-8f7b-c717a4e12269": Phase="Pending", Reason="", readiness=false. Elapsed: 1.397729ms
Dec 28 09:48:10.671: INFO: Pod "busybox-privileged-false-cfcba264-0203-4f12-8f7b-c717a4e12269": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.003869597s
Dec 28 09:48:10.671: INFO: Pod "busybox-privileged-false-cfcba264-0203-4f12-8f7b-c717a4e12269" satisfied condition "success or failure"
Dec 28 09:48:10.676: INFO: Got logs for pod "busybox-privileged-false-cfcba264-0203-4f12-8f7b-c717a4e12269": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [k8s.io] Security Context
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:48:10.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4257" for this suite.
Dec 28 09:48:16.686: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:48:16.748: INFO: namespace security-context-test-4257 deletion completed in 6.069318206s

• [SLOW TEST:8.100 seconds]
[k8s.io] Security Context
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  When creating a pod with privileged
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/security_context.go:226
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector 
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:48:16.748: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename gc
STEP: Waiting for a default service account to be provisioned in namespace
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the deployment
STEP: Wait for the Deployment to create new ReplicaSet
STEP: delete the deployment
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the rs
STEP: Gathering metrics
Dec 28 09:48:47.283: INFO: For apiserver_request_total:
For apiserver_request_latencies_summary:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

W1228 09:48:47.283923      22 metrics_grabber.go:79] Master node is not registered. Grabbing metrics from Scheduler, ControllerManager and ClusterAutoscaler is disabled.
[AfterEach] [sig-api-machinery] Garbage collector
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:48:47.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9376" for this suite.
Dec 28 09:48:53.293: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:48:53.356: INFO: namespace gc-9376 deletion completed in 6.070912853s

• [SLOW TEST:36.608 seconds]
[sig-api-machinery] Garbage collector
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume 
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:48:53.357: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename downward-api
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:40
[It] should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: Creating the pod
Dec 28 09:48:55.893: INFO: Successfully updated pod "annotationupdate60dcbe55-4091-4c08-b051-eb7f7fe83794"
[AfterEach] [sig-storage] Downward API volume
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:48:59.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6219" for this suite.
Dec 28 09:49:11.921: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:49:11.993: INFO: namespace downward-api-6219 deletion completed in 12.080377848s

• [SLOW TEST:18.637 seconds]
[sig-storage] Downward API volume
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/downwardapi_volume.go:35
  should update annotations on modification [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] 
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:49:11.994: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename crd-publish-openapi
STEP: Waiting for a default service account to be provisioned in namespace
[It] works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
Dec 28 09:49:12.014: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: client-side validation (kubectl create and apply) allows request with known and required properties
Dec 28 09:49:15.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 --namespace=crd-publish-openapi-9243 create -f -'
Dec 28 09:49:16.074: INFO: stderr: ""
Dec 28 09:49:16.074: INFO: stdout: "e2e-test-crd-publish-openapi-6779-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 28 09:49:16.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 --namespace=crd-publish-openapi-9243 delete e2e-test-crd-publish-openapi-6779-crds test-foo'
Dec 28 09:49:16.151: INFO: stderr: ""
Dec 28 09:49:16.151: INFO: stdout: "e2e-test-crd-publish-openapi-6779-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Dec 28 09:49:16.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 --namespace=crd-publish-openapi-9243 apply -f -'
Dec 28 09:49:16.292: INFO: stderr: ""
Dec 28 09:49:16.292: INFO: stdout: "e2e-test-crd-publish-openapi-6779-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Dec 28 09:49:16.292: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 --namespace=crd-publish-openapi-9243 delete e2e-test-crd-publish-openapi-6779-crds test-foo'
Dec 28 09:49:16.366: INFO: stderr: ""
Dec 28 09:49:16.366: INFO: stdout: "e2e-test-crd-publish-openapi-6779-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: client-side validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema
Dec 28 09:49:16.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 --namespace=crd-publish-openapi-9243 create -f -'
Dec 28 09:49:16.486: INFO: rc: 1
Dec 28 09:49:16.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 --namespace=crd-publish-openapi-9243 apply -f -'
Dec 28 09:49:16.618: INFO: rc: 1
STEP: client-side validation (kubectl create and apply) rejects request without required properties
Dec 28 09:49:16.618: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 --namespace=crd-publish-openapi-9243 create -f -'
Dec 28 09:49:16.752: INFO: rc: 1
Dec 28 09:49:16.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 --namespace=crd-publish-openapi-9243 apply -f -'
Dec 28 09:49:16.882: INFO: rc: 1
STEP: kubectl explain works to explain CR properties
Dec 28 09:49:16.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 explain e2e-test-crd-publish-openapi-6779-crds'
Dec 28 09:49:17.015: INFO: stderr: ""
Dec 28 09:49:17.015: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6779-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively
Dec 28 09:49:17.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 explain e2e-test-crd-publish-openapi-6779-crds.metadata'
Dec 28 09:49:17.154: INFO: stderr: ""
Dec 28 09:49:17.154: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6779-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   clusterName\t<string>\n     The name of the cluster which the object belongs to. This is used to\n     distinguish resources with same name and namespace in different clusters.\n     This field is not set anywhere right now and apiserver is going to ignore\n     it if set in create or update request.\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC. Populated by the system.\n     Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested. Populated by the system when a graceful deletion is\n     requested. Read-only. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server. If this field is specified and the generated name exists, the\n     server will NOT return a 409 - instead, it will either return 201 Created\n     or 500 with Reason ServerTimeout indicating a unique name could not be\n     found in the time allotted, and the client should retry (optionally after\n     the time indicated in the Retry-After header). Applied only if Name is not\n     specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty. Must\n     be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources. Populated by the system.\n     Read-only. Value must be treated as opaque by clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     SelfLink is a URL representing this object. Populated by the system.\n     Read-only. DEPRECATED Kubernetes will stop propagating this field in 1.20\n     release and the field is planned to be removed in 1.21 release.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations. Populated by the system. Read-only.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Dec 28 09:49:17.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 explain e2e-test-crd-publish-openapi-6779-crds.spec'
Dec 28 09:49:17.290: INFO: stderr: ""
Dec 28 09:49:17.290: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6779-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Dec 28 09:49:17.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 explain e2e-test-crd-publish-openapi-6779-crds.spec.bars'
Dec 28 09:49:17.419: INFO: stderr: ""
Dec 28 09:49:17.419: INFO: stdout: "KIND:     E2e-test-crd-publish-openapi-6779-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist
Dec 28 09:49:17.420: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-076158834 explain e2e-test-crd-publish-openapi-6779-crds.spec.bars2'
Dec 28 09:49:17.566: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:49:21.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9243" for this suite.
Dec 28 09:49:27.269: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:49:27.333: INFO: namespace crd-publish-openapi-9243 deletion completed in 6.070751856s

• [SLOW TEST:15.339 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSS
------------------------------
[k8s.io] Container Lifecycle Hook when create a pod with lifecycle hook 
  should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:49:27.333: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename container-lifecycle-hook
STEP: Waiting for a default service account to be provisioned in namespace
[BeforeEach] when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:64
STEP: create the container to handle the HTTPGet hook request.
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the pod with lifecycle hook
STEP: check poststart hook
STEP: delete the pod with lifecycle hook
Dec 28 09:49:31.376: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 28 09:49:31.379: INFO: Pod pod-with-poststart-http-hook still exists
Dec 28 09:49:33.379: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 28 09:49:33.381: INFO: Pod pod-with-poststart-http-hook still exists
Dec 28 09:49:35.379: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Dec 28 09:49:35.381: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [k8s.io] Container Lifecycle Hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:49:35.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-3529" for this suite.
Dec 28 09:50:03.392: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:50:03.455: INFO: namespace container-lifecycle-hook-3529 deletion completed in 28.071546303s

• [SLOW TEST:36.122 seconds]
[k8s.io] Container Lifecycle Hook
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  when create a pod with lifecycle hook
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/lifecycle_hook.go:42
    should execute poststart http hook properly [NodeConformance] [Conformance]
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSS
------------------------------
[k8s.io] Container Runtime blackbox test on terminated container 
  should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
[BeforeEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:151
STEP: Creating a kubernetes client
Dec 28 09:50:03.456: INFO: >>> kubeConfig: /tmp/kubeconfig-076158834
STEP: Building a namespace api object, basename container-runtime
STEP: Waiting for a default service account to be provisioned in namespace
[It] should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
STEP: create the container
STEP: wait for the container to reach Succeeded
STEP: get the container status
STEP: the container should be terminated
STEP: the termination message should be set
Dec 28 09:50:05.484: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container
[AfterEach] [k8s.io] Container Runtime
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:152
Dec 28 09:50:05.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9496" for this suite.
Dec 28 09:50:11.501: INFO: Waiting up to 30s for server preferred namespaced resources to be successfully discovered
Dec 28 09:50:11.565: INFO: namespace container-runtime-9496 deletion completed in 6.071231945s

• [SLOW TEST:8.110 seconds]
[k8s.io] Container Runtime
/workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:693
  blackbox test
  /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:39
    on terminated container
    /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/common/runtime.go:132
      should report termination message [LinuxOnly] from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      /workspace/anago-v1.16.3-beta.0.56+b3cbbae08ec52a/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/framework/framework.go:698
------------------------------
SSSSSSSDec 28 09:50:11.565: INFO: Running AfterSuite actions on all nodes
Dec 28 09:50:11.565: INFO: Running AfterSuite actions on node 1
Dec 28 09:50:11.565: INFO: Skipping dumping logs from cluster

Ran 276 of 4732 Specs in 7004.926 seconds
SUCCESS! -- 276 Passed | 0 Failed | 0 Pending | 4456 Skipped
PASS

Ginkgo ran 1 suite in 1h56m46.320690947s
Test Suite Passed
